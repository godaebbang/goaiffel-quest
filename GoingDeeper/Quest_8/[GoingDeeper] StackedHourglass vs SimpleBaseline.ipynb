{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cb2d0d",
   "metadata": {},
   "source": [
    "# **0. 루브릭**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4786eb",
   "metadata": {},
   "source": [
    "| 평가문항 | 상세기준 |\n",
    "|:----|:----|\n",
    "| 1. tfrecord를 활용한 데이터셋 구성과 전처리를 통해 프로젝트 베이스라인 구성을 확인하였다. | MPII 데이터셋을 기반으로 1epoch에 30분 이내에 학습 가능한 베이스라인을 구축하였다. |\n",
    "| 2. simplebaseline 모델을 정상적으로 구현하였다. | simplebaseline 모델을 구현하여 실습코드의 모델을 대체하여 정상적으로 학습이 진행되었다. |\n",
    "| 3. Hourglass 모델과 simplebaseline 모델을 비교분석한 결과를 체계적으로 정리하였다. | 두 모델의 pose estimation 테스트결과 이미지 및 학습진행상황 등을 체계적으로 비교분석하였다. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e65323b",
   "metadata": {},
   "source": [
    "# **1. 실험하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4656e09",
   "metadata": {},
   "source": [
    "## **1. 데이터 전처리하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589d7a4",
   "metadata": {},
   "source": [
    "앞으로 사용할 라이브러리를 불러옵니다.\n",
    "\n",
    "고정해서 사용할 변수도 만들어 둡시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ee1bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "\n",
    "# 주의! ray를 tensorflow보다 먼저 import하면 오류가 발생할 수 있음\n",
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21de7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로 설정하기\n",
    "PROJECT_PATH = os.getenv('HOME') + '/aiffel/mpii'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'mpii_human_pose_v1_u12_2', 'validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411dc02",
   "metadata": {},
   "source": [
    "### **json 파싱하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf3263b",
   "metadata": {},
   "source": [
    "이 파일들은 이미지에 담겨 있는 사람들의 pose keypoint 정보들을 가지고 있어서 Pose Estimation을 위한 label로 삼을 수 있습니다.\n",
    "\n",
    "우선 json이 어떻게 구성되어 있는지 파악해 보기 위해 json 파일을 열어 샘플로 annotation 정보를 1개만 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19073624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# annotation 정보 확인하기\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaeada5",
   "metadata": {},
   "source": [
    "`joints` 가 우리가 label 로 사용할 keypoint 의 label 입니다. 이미지 형상과 사람의 포즈에 따라 모든 label 이 이미지에 나타나지 않기 때문에 `joints_vis` 를 이용해서 실제로 사용할 수 있는 keypoint 인지 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033745b",
   "metadata": {},
   "source": [
    "`joints` 순서는 아래와 같은 순서로 배치되어 저장해 뒀습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fab398",
   "metadata": {},
   "source": [
    "* 0 - 오른쪽 발목\n",
    "* 1 - 오른쪽 무릎\n",
    "* 2 - 오른쪽 엉덩이\n",
    "* 3 - 왼쪽 엉덩이\n",
    "* 4 - 왼쪽 무릎\n",
    "* 5 - 왼쪽 발목\n",
    "* 6 - 골반\n",
    "* 7 - 가슴(흉부)\n",
    "* 8 - 목\n",
    "* 9 - 머리 위\n",
    "* 10 - 오른쪽 손목\n",
    "* 11 - 오른쪽 팔꿈치\n",
    "* 12 - 오른쪽 어깨\n",
    "* 13 - 왼쪽 어깨\n",
    "* 14 - 왼쪽 팔꿈치\n",
    "* 15 - 왼쪽 손목"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f949f9d",
   "metadata": {},
   "source": [
    "`scale`과 `center`는 사람 몸의 크기와 중심점 입니다.`scale`은 200을 곱해야 온전한 크기가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c21cd4",
   "metadata": {},
   "source": [
    "이제 json annotation 을 파싱하는 함수를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e197486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json annotation을 파싱하는 함수 구현하기\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a49760",
   "metadata": {},
   "source": [
    "한 번 parse_one_annotation()함수를 테스트 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3941ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n"
     ]
    }
   ],
   "source": [
    "# parse_one_annotation()함수를 테스트하기\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    test = parse_one_annotation(train_annos[0], IMAGE_PATH)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a45c98",
   "metadata": {},
   "source": [
    "이제 우리가 원하는 정보만 뽑아낼 수 있게 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b305a3",
   "metadata": {},
   "source": [
    "**Q. MPII 데이터셋에서 json 파일에서 'scale' 정보가 의미하는 바는 무엇인가요?**\n",
    "\n",
    "MPII 데이터셋에서 'scale'은 사람의 크기 비율을 나타내는데, 관절의 위치 및 사람의 신체 부분 크기를 상대적으로 표현하는 데 사용됩니다.\n",
    "\n",
    "일반적으로 관절의 좌표는 이미지의 크기에 따라 상대적으로 바뀌는 값이기 때문에 이를 보정하기 위해 scale 값을 제공하는 것이며, 이를 통해서 다른 크기의 이미지에서도 신체 부위의 위치를 일관성 있게 분석할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b97be",
   "metadata": {},
   "source": [
    "## **2. TFRecord 파일 만들기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd538dab",
   "metadata": {},
   "source": [
    "### **TFRecord 파일 만들기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2dd190",
   "metadata": {},
   "source": [
    "하지만 실제 프로젝트에서는 튜토리얼 데이터셋보다 훨씬 큰 크기의 데이터를 다뤄야 합니다.\n",
    "\n",
    "일반적으로 학습 과정에서 gpu 의 연산 속도보다 HDD I/O 가 느리기 때문에 병목 현상이 발생하고 대단위 프로젝트 실험에서 효율성이 떨어지는 것을 관찰할 수 있습니다.\n",
    "\n",
    "따라서 \"학습 데이터를 어떻게 빠르게 읽는가?\" 에 대한 고민을 반드시 수행하셔야 더 많은 실험을 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009109b",
   "metadata": {},
   "source": [
    "Tensorflow에서는 위 변환을 자동화해주는 도구를 제공합니다. 데이터셋을 TFRecord 형태로 표현하는 것입니다.\n",
    "\n",
    "TFRecord 는 binary record sequence 를 저장하기 위한 형식입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaaaaef",
   "metadata": {},
   "source": [
    "내부적으로 `protocol buffer` 라는 것을 이용하는데, `protocol buffer` 는 크로스 플랫폼에서 사용할 수 있는 직렬화 데이터 라이브러리라고 생각하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ba233",
   "metadata": {},
   "source": [
    "이제 구현을 시작하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabfc0e",
   "metadata": {},
   "source": [
    "앞서 추출한 annotation을 TFRecord로 변환하는 함수를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6c4b1",
   "metadata": {},
   "source": [
    "TFRecord 는 `tf.train.Example`들의 합으로 이루어지므로 하나의 annotation을 하나의 `tf.train.Example`로 만들어 주는 함수부터 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7d46cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "\n",
    "    # byte 인코딩을 위한 함수\n",
    "    def _bytes_feature(value):\n",
    "        if isinstance(value, type(tf.constant(0))):\n",
    "            value = value.numpy()\n",
    "        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ef76b",
   "metadata": {},
   "source": [
    "하나의 annotation이 `tf.train.Example`이 되었다면 이제 여러 annotation에 대해 작업할 수 있도록 함수를 만들어야 합니다.\n",
    "\n",
    "그런데 여기서 하나의 TFRecord를 만들지 않고 여러 TFRecord를 만들어 볼 것입니다. \n",
    "\n",
    "우선 얼마나 많은 TFRecord를 만들지 결정할 함수를 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73112060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7307e9",
   "metadata": {},
   "source": [
    "데이터를 여러 그룹으로 나누는 sharding은 데이터 저장과 병렬 처리에 유리합니다. 전체 데이터를 n개의 그룹으로 나누어 n개의 TFRecord 파일을 생성합니다. 기업 단위의 큰 데이터를 여러 장비에 나누어 담는 것이 일반적입니다. \n",
    "\n",
    "적절한 파일 크기와 개수를 설정하는 것이 중요합니다. 너무 작은 파일로 많이 나누면 학습 중 잦은 입출력이 요구되고, 너무 큰 파일로 적게 나누면 입출력 시간이 길어져 GPU 계산 시간보다 입출력 시간이 더 걸릴 수 있습니다. 적절한 크기와 개수는 상황에 따라 다릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5bdba6",
   "metadata": {},
   "source": [
    "설명은 어렵지만 실행해보면 단순합니다. 그럼 `chunkify` 함수를 테스트 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be180de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "test_chunks = chunkify([0] * 1000, 64)\n",
    "print(test_chunks)\n",
    "print(len(test_chunks))\n",
    "print(len(test_chunks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2509c00",
   "metadata": {},
   "source": [
    "0이 1000개 들어 있는 리스트가 64개로 쪼개졌습니다. \n",
    "\n",
    "`chunkify` 함수를 테스트 해봤으니 하나의 chunk를 TFRecord로 만들어 줄 함수를 만듭시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef388df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 chunk를 TFRecord로 만들어 줄 함수 구현하기\n",
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno in chunk:\n",
    "            tf_example = generate_tfexample(anno)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c920fe",
   "metadata": {},
   "source": [
    "`chunk` 안에는 여러 annotation들이 있고, annotation들은 `tf.train.Example`로 변환된 후에 문자열로 직렬화되어 TFRecord에 담깁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c21ecd",
   "metadata": {},
   "source": [
    "또 한 가지 주의해서 봐야할 것은 함수 정의 위에 `@ray.remote`가 있다는 점입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f069e9",
   "metadata": {},
   "source": [
    "Ray는 병렬 처리를 위한 라이브러리인데, 파이썬에서 기본적으로 제공하는 multiprocessing 패키지보다 편하게 다양한 환경에서 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec8c6d",
   "metadata": {},
   "source": [
    "이제 모든 준비가 되었으니 전체 데이터를 적당한 수의 TFRecord 파일로 만들어주는 함수를 만듭시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4043fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터를 적당한 수의 TFRecord 파일로 만들어주는 함수 구현하기\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, '{}/{}_{}_of_{}.tfrecords'.format(\n",
    "                TFRECORD_PATH,\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b301db",
   "metadata": {},
   "source": [
    "함수는 모두 준비되었습니다. 이제 다음으로 넘어가 함수를 실행할 차례입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cb4484",
   "metadata": {},
   "source": [
    "## **3. Ray**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf29876",
   "metadata": {},
   "source": [
    "앞서 작성한 함수를 사용해 데이터를 TFRecord로 만들어 줍니다. \n",
    "\n",
    "train 데이터는 64개로, val 데이터는 8개의 파일로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad56dc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 00:44:08,556\tWARNING services.py:1729 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=3.92gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': '/aiffel/aiffel/mpii/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': '/aiffel/aiffel/mpii/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m start to build tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=124)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=125)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "\n",
    "ray.init()\n",
    "\n",
    "print('Start to parse annotations.')\n",
    "if not os.path.exists(TFRECORD_PATH):\n",
    "    os.makedirs(TFRECORD_PATH)\n",
    "\n",
    "with open(TRAIN_JSON) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    train_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH)\n",
    "        for anno in train_annos\n",
    "    ]\n",
    "    print('First train annotation: ', train_annotations[0])\n",
    "\n",
    "with open(VALID_JSON) as val_json:\n",
    "    val_annos = json.load(val_json)\n",
    "    val_annotations = [\n",
    "        parse_one_annotation(anno, IMAGE_PATH) \n",
    "        for anno in val_annos\n",
    "    ]\n",
    "    print('First val annotation: ', val_annotations[0])\n",
    "    \n",
    "print('Start to build TF Records.')\n",
    "build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "    len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4c421",
   "metadata": {},
   "source": [
    "**Q. multiprocessing 과 ray 의 사용상 차이점은 무엇인가요? 위 링크를 참고해서 대답해 봅시다.**\n",
    "\n",
    "**MP**는 병렬화를 위해 추상적 구조를 새로 설계해야 하지만 **ray**는 쓰던 코드에서 거의 수정 없이 병렬화 할 수 있는 장점이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f01c70",
   "metadata": {},
   "source": [
    "## **4. data label 로 만들기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca04e9c",
   "metadata": {},
   "source": [
    "지금까지 만든 함수들을 개별 함수로도 만들 수 있지만 객체 형태로 조합해 봅시다.\n",
    "\n",
    "객체 형태로 만들면 선언부는 복잡해 보여도 훨씬 장점이 많습니다.\n",
    "\n",
    "함수에서 객체의 메서드로 수정할 때는 `self`를 추가해야 하는 점을 잊지 맙시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3e4c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=123)\u001b[0m finished building tf records for /aiffel/aiffel/mpii/tfrecords_mpii/val_0008_of_0008.tfrecords"
     ]
    }
   ],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y, self.heatmap_shape)\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "        \n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "        \n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "        \n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "        \n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        \n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "        \n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "        \n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "        \n",
    "    \n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "                \n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y, heatmap_shape):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * heatmap_shape[1]), dtype=tf.int32)\n",
    "        \n",
    "        num_heatmap = heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(heatmap_shape[1], heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "        \n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "        \n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8fb8c",
   "metadata": {},
   "source": [
    "이제 데이터 전처리가 완료되었으니 모델을 만들러 가봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144f85bc",
   "metadata": {},
   "source": [
    "## **5. 모델을 학습해보자**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b30aebc",
   "metadata": {},
   "source": [
    "### **Hourglass 모델 만들기**"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "7b0faae4",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3695824",
   "metadata": {},
   "source": [
    "직육면체 박스는 residual block 이었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15db8d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff3901",
   "metadata": {},
   "source": [
    "hourglass 모델을 잘 생각해 보면 마치 양파처럼 가장 바깥의 layer 를 제거하면 똑같은 구조가 나타나는 것을 알 수 있습니다.\n",
    "\n",
    "재귀 함수를 이용해 HourglassModule 을 반복하면 order 가 1이 되면 BottleneckBlock 으로 대체해 주면 아주 간결하게 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19fa868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourglass 모듈 구현하기\n",
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ffd8b",
   "metadata": {},
   "source": [
    "> intermediate output을 위한 linear layer"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9949b11d",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5571974",
   "metadata": {},
   "source": [
    "여러 모듈을 쌓을수록 모델이 깊어지는 만큼 학습이 어려워, **Intermediate supervision**을 적용하였습니다.\n",
    "\n",
    "도식에서 보이는 모듈 사이의 네트워크의 파란 박스는 모델 중간에 계산되는 히트맵 결과를 출력하는 convolution layer입니다. 이 히트맵과 ground truth의 차이를 **intermediate loss (auxilary loss)**로 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e4b5773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450ad86",
   "metadata": {},
   "source": [
    "따라서 stacked 되는 hourglass 층 사이사이에 LinearLayer 를 삽입하고 중간 loss 를 계산해 줍니다.\n",
    "\n",
    "지금까지 만든 hourglass 를 여러 층으로 쌓으면 stacked hourglass 가 됩니다."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5280a24f",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c25262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackedHourglassNetwork 함수 구현하기\n",
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), \n",
    "        num_stack=4, \n",
    "        num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='Stacked_Hourglass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "091e2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackedHourglassNetwork 모델 생성하기\n",
    "stacked_model = StackedHourglassNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31b8636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Stacked_Hourglass\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 64) 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 128, 128, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 64) 256         re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 128, 128, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 128, 128, 64) 4160        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128, 128, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 128, 128, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 128, 128, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 128 8320        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 128 8320        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 128, 128, 128 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 128)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 128)  512         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 64, 64, 128)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 64)   8256        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 64, 64, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   36928       re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 64, 64, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 128)  8320        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 64, 128)  0           max_pooling2d[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 128)  512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 64, 64, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 128)  16512       re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 128)  512         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, 64, 64, 128)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 128)  512         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, 64, 64, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 256)  33024       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 64, 64, 256)  0           conv2d_8[0][0]                   \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 256)  1024        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 32, 32, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 256)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 256)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 256)  1024        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_25 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 256)  0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 256)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 256)    1024        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_34 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 128)    512         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 128)    512         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 4, 4, 256)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 4, 4, 256)    1024        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_43 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 4, 4, 128)    32896       re_lu_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 4, 4, 128)    512         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_44 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 4, 4, 128)    147584      re_lu_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 4, 4, 128)    512         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_45 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 4, 4, 256)    33024       re_lu_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 4, 4, 256)    1024        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_46 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 256)    1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 4, 4, 128)    32896       re_lu_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_37 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 4, 4, 128)    512         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_47 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 128)    512         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 4, 4, 128)    147584      re_lu_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_38 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 4, 4, 128)    512         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_48 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 128)    512         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 4, 4, 256)    33024       re_lu_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 256)    0           add_14[0][0]                     \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 4, 4, 256)    1024        add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 8, 8, 256)    0           add_11[0][0]                     \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_49 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 8, 8, 256)    1024        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 4, 4, 128)    32896       re_lu_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_40 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 4, 4, 128)    512         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 256)  1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_50 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 8, 8, 128)    512         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_28 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 4, 4, 128)    147584      re_lu_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 4, 4, 128)    512         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 16, 16, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_51 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 8, 8, 128)    512         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 4, 4, 256)    33024       re_lu_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_42 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 256)    0           add_15[0][0]                     \n",
      "                                                                 conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 16, 16, 128)  512         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    (None, 8, 8, 256)    0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 8, 8, 256)    0           add_12[0][0]                     \n",
      "                                                                 conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 8, 8, 256)    0           up_sampling2d[0][0]              \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 8, 8, 256)    1024        tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 256)  0           add_8[0][0]                      \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_52 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 16, 16, 256)  1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_31 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 8, 8, 128)    512         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 32, 32, 256)  1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_53 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 16, 16, 128)  512         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_19 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 8, 8, 128)    512         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 32, 32, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_54 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 16, 16, 128)  512         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add[0][0]       \n",
      "                                                                 conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 32, 32, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)  (None, 16, 16, 256)  0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 256)  0           add_9[0][0]                      \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 16, 16, 256)  0           up_sampling2d_1[0][0]            \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 16, 16, 256)  1024        tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 256)  0           add_5[0][0]                      \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_55 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 32, 32, 256)  1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 16, 16, 128)  512         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 64, 64, 256)  1024        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_56 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 32, 32, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 16, 16, 128)  512         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 64, 64, 128)  512         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_57 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 32, 32, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 128)  512         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)  (None, 32, 32, 256)  0           add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 32, 32, 256)  0           add_6[0][0]                      \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 32, 32, 256)  0           up_sampling2d_2[0][0]            \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 256)  1024        tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 64, 64, 256)  0           add_2[0][0]                      \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_58 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 256)  1024        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 32, 128)  512         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_59 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 32, 32, 128)  512         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_60 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_2[0][0]     \n",
      "                                                                 conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 64, 64, 256)  0           add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 64, 256)  0           add_3[0][0]                      \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (None, 64, 64, 256)  0           up_sampling2d_3[0][0]            \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 64, 64, 256)  1024        tf.__operators__.add_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_61 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 64, 64, 128)  512         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_62 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 64, 64, 128)  512         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_63 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 64, 64, 256)  65792       add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 64, 64, 256)  1024        conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_64 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 64, 64, 16)   4112        re_lu_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 64, 64, 256)  65792       re_lu_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 64, 64, 256)  4352        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 64, 64, 256)  0           conv2d_68[0][0]                  \n",
      "                                                                 conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 32, 32, 256)  0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 32, 32, 256)  1024        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_71 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 32, 32, 128)  512         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_72 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 32, 32, 128)  512         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_73 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, 32, 32, 256)  0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 16, 16, 256)  0           add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 16, 16, 256)  1024        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_80 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 16, 16, 128)  512         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_81 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 16, 16, 128)  512         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_82 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 16, 16, 256)  0           max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 8, 8, 256)    0           add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 256)    1024        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_89 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_89[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 128)    512         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_90 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 128)    512         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_91 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_7[0][0]            \n",
      "                                                                 conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 4, 4, 256)    0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 4, 4, 256)    1024        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_98 (ReLU)                 (None, 4, 4, 256)    0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 4, 4, 128)    512         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_99 (ReLU)                 (None, 4, 4, 128)    0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 4, 4, 128)    512         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_100 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_8[0][0]            \n",
      "                                                                 conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 4, 4, 256)    1024        add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_101 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 256)    1024        add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_92 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 4, 4, 128)    512         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 8, 8, 128)    32896       re_lu_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_102 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 128)    512         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_93 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 4, 4, 128)    512         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 8, 8, 128)    147584      re_lu_93[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_103 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 128)    512         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_94 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 4, 4, 256)    0           add_33[0][0]                     \n",
      "                                                                 conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 8, 8, 256)    33024       re_lu_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 4, 4, 256)    1024        add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 8, 8, 256)    0           add_30[0][0]                     \n",
      "                                                                 conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_104 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 8, 8, 256)    1024        add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_95 (ReLU)                 (None, 8, 8, 256)    0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 4, 4, 128)    512         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_95[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 16, 16, 256)  1024        add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_105 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 8, 8, 128)    512         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_83 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_96 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 4, 4, 128)    512         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 16, 16, 128)  512         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_106 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 8, 8, 128)    512         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_84 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_97 (ReLU)                 (None, 8, 8, 128)    0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 4, 4, 256)    0           add_34[0][0]                     \n",
      "                                                                 conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 128)  512         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 8, 8, 256)    0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 8, 8, 256)    0           add_31[0][0]                     \n",
      "                                                                 conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_85 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (None, 8, 8, 256)    0           up_sampling2d_4[0][0]            \n",
      "                                                                 add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 8, 8, 256)    1024        tf.__operators__.add_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 16, 16, 256)  0           add_27[0][0]                     \n",
      "                                                                 conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_107 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 16, 16, 256)  1024        add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_86 (ReLU)                 (None, 16, 16, 256)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 8, 8, 128)    512         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 16, 16, 128)  32896       re_lu_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 32, 32, 256)  1024        add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_108 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 16, 16, 128)  512         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_74 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_87 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 8, 8, 128)    512         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 16, 16, 128)  147584      re_lu_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 32, 32, 128)  512         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_109 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 16, 16, 128)  512         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_75 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_88 (ReLU)                 (None, 16, 16, 128)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 16, 16, 256)  33024       re_lu_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 32, 32, 128)  512         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 16, 16, 256)  0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 16, 16, 256)  0           add_28[0][0]                     \n",
      "                                                                 conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_76 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (None, 16, 16, 256)  0           up_sampling2d_5[0][0]            \n",
      "                                                                 add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 16, 16, 256)  1024        tf.__operators__.add_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, 32, 32, 256)  0           add_24[0][0]                     \n",
      "                                                                 conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_110 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 256)  1024        add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_77 (ReLU)                 (None, 32, 32, 256)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 16, 16, 128)  512         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 32, 32, 128)  32896       re_lu_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 64, 64, 256)  1024        add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_111 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 32, 32, 128)  512         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_65 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_111[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_78 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 16, 16, 128)  512         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 32, 32, 128)  147584      re_lu_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 64, 64, 128)  512         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_112 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 32, 32, 128)  512         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_66 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_79 (ReLU)                 (None, 32, 32, 128)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_5[0][0]     \n",
      "                                                                 conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 32, 32, 256)  33024       re_lu_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 64, 64, 128)  512         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 32, 32, 256)  0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 32, 32, 256)  0           add_25[0][0]                     \n",
      "                                                                 conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_67 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (None, 32, 32, 256)  0           up_sampling2d_6[0][0]            \n",
      "                                                                 add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 32, 32, 256)  1024        tf.__operators__.add_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, 64, 64, 256)  0           add_21[0][0]                     \n",
      "                                                                 conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_113 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 64, 64, 256)  1024        add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_68 (ReLU)                 (None, 64, 64, 256)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 32, 32, 128)  512         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 64, 64, 128)  32896       re_lu_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_114 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 64, 64, 128)  512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_114[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_69 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 32, 32, 128)  512         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 64, 64, 128)  147584      re_lu_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_115 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 64, 64, 128)  512         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_115[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_70 (ReLU)                 (None, 64, 64, 128)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 64, 64, 256)  33024       re_lu_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 64, 64, 256)  0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, 64, 64, 256)  0           add_22[0][0]                     \n",
      "                                                                 conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (None, 64, 64, 256)  0           up_sampling2d_7[0][0]            \n",
      "                                                                 add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 64, 64, 256)  1024        tf.__operators__.add_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_116 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 64, 64, 128)  512         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_117 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_117[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 64, 64, 128)  512         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_118 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_118[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_7[0][0]     \n",
      "                                                                 conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 64, 64, 256)  65792       add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 64, 64, 256)  1024        conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_119 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 64, 64, 16)   4112        re_lu_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 64, 64, 256)  65792       re_lu_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 64, 64, 256)  4352        conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 64, 64, 256)  0           conv2d_126[0][0]                 \n",
      "                                                                 conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 32, 32, 256)  0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 32, 32, 256)  1024        max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_126 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_126[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 32, 32, 128)  512         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_127 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 32, 32, 128)  512         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_128 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 32, 32, 256)  0           max_pooling2d_9[0][0]            \n",
      "                                                                 conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 16, 16, 256)  0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 16, 16, 256)  1024        max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_135 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_135[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 16, 16, 128)  512         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_136 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_136[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 16, 16, 128)  512         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_137 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_137[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 16, 16, 256)  0           max_pooling2d_10[0][0]           \n",
      "                                                                 conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 8, 8, 256)    0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 8, 8, 256)    1024        max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_144 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_144[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 8, 8, 128)    512         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_145 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_145[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 8, 8, 128)    512         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_146 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_146[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_11[0][0]           \n",
      "                                                                 conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling2D) (None, 4, 4, 256)    0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 4, 4, 256)    1024        max_pooling2d_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_153 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_153[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 4, 4, 128)    512         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_154 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_154[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 4, 4, 128)    512         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_155 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_155[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_12[0][0]           \n",
      "                                                                 conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 4, 4, 256)    1024        add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_156 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 8, 8, 256)    1024        add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_156[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_147 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 4, 4, 128)    512         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_147[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_157 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 8, 8, 128)    512         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_157[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_148 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 4, 4, 128)    512         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_148[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_158 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 8, 8, 128)    512         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_158[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_149 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 4, 4, 256)    0           add_52[0][0]                     \n",
      "                                                                 conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_149[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 4, 4, 256)    1024        add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 8, 8, 256)    0           add_49[0][0]                     \n",
      "                                                                 conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_159 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 8, 8, 256)    1024        add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_159[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_150 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 4, 4, 128)    512         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_150[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 16, 16, 256)  1024        add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_160 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 8, 8, 128)    512         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_138 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_160[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_151 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 4, 4, 128)    512         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_151[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 16, 16, 128)  512         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_161 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 8, 8, 128)    512         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_139 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_161[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_152 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 4, 4, 256)    0           add_53[0][0]                     \n",
      "                                                                 conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_152[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 16, 16, 128)  512         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 8, 8, 256)    0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 8, 8, 256)    0           add_50[0][0]                     \n",
      "                                                                 conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_140 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (None, 8, 8, 256)    0           up_sampling2d_8[0][0]            \n",
      "                                                                 add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_140[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 8, 8, 256)    1024        tf.__operators__.add_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 16, 16, 256)  0           add_46[0][0]                     \n",
      "                                                                 conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_162 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 16, 16, 256)  1024        add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_162[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_141 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 8, 8, 128)    512         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_141[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 32, 32, 256)  1024        add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_163 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 16, 16, 128)  512         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_129 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_163[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_142 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_129[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 8, 8, 128)    512         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 32, 32, 128)  512         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_164 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 16, 16, 128)  512         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_130 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_164[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_143 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_130[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add_8[0][0]     \n",
      "                                                                 conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_143[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 32, 32, 128)  512         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2D)  (None, 16, 16, 256)  0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 16, 16, 256)  0           add_47[0][0]                     \n",
      "                                                                 conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_131 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (None, 16, 16, 256)  0           up_sampling2d_9[0][0]            \n",
      "                                                                 add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_131[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 16, 16, 256)  1024        tf.__operators__.add_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 32, 32, 256)  0           add_43[0][0]                     \n",
      "                                                                 conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_165 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 32, 32, 256)  1024        add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_165[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_132 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 16, 16, 128)  512         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_132[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 64, 64, 256)  1024        add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_166 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 32, 32, 128)  512         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_120 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_166[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_133 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 16, 16, 128)  512         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_133[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 64, 64, 128)  512         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_167 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 32, 32, 128)  512         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_121 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_167[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_134 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_9[0][0]     \n",
      "                                                                 conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 64, 64, 128)  512         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_10 (UpSampling2D) (None, 32, 32, 256)  0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 32, 32, 256)  0           add_44[0][0]                     \n",
      "                                                                 conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_122 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (None, 32, 32, 256)  0           up_sampling2d_10[0][0]           \n",
      "                                                                 add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_122[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 32, 32, 256)  1024        tf.__operators__.add_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 64, 64, 256)  0           add_40[0][0]                     \n",
      "                                                                 conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_168 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 64, 64, 256)  1024        add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_168[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_123 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 32, 32, 128)  512         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_123[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_169 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 64, 64, 128)  512         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_169[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_124 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 32, 32, 128)  512         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_170 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 64, 64, 128)  512         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_170[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_125 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_10[0][0]    \n",
      "                                                                 conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_125[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_11 (UpSampling2D) (None, 64, 64, 256)  0           add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 64, 64, 256)  0           add_41[0][0]                     \n",
      "                                                                 conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (None, 64, 64, 256)  0           up_sampling2d_11[0][0]           \n",
      "                                                                 add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 64, 64, 256)  1024        tf.__operators__.add_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_171 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 64, 64, 128)  512         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_172 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_172[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 64, 64, 128)  512         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_173 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_173[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_11[0][0]    \n",
      "                                                                 conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 64, 64, 256)  65792       add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 64, 64, 256)  1024        conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_174 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 64, 64, 16)   4112        re_lu_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 64, 64, 256)  65792       re_lu_174[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 64, 64, 256)  4352        conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 64, 64, 256)  0           conv2d_184[0][0]                 \n",
      "                                                                 conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D) (None, 32, 32, 256)  0           add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 32, 32, 256)  1024        max_pooling2d_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_181 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_181[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 32, 32, 128)  512         conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_182 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 32, 32, 128)  512         conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_183 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_183[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 32, 32, 256)  0           max_pooling2d_13[0][0]           \n",
      "                                                                 conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 16, 16, 256)  0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 16, 16, 256)  1024        max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_190 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_190[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 16, 16, 128)  512         conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_191 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_191[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchN (None, 16, 16, 128)  512         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_192 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_192[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_192[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 16, 16, 256)  0           max_pooling2d_14[0][0]           \n",
      "                                                                 conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D) (None, 8, 8, 256)    0           add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 8, 8, 256)    1024        max_pooling2d_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_199 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_199[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_199[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 8, 8, 128)    512         conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_200 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_200[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 8, 8, 128)    512         conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_201 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_201[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 8, 8, 256)    0           max_pooling2d_15[0][0]           \n",
      "                                                                 conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D) (None, 4, 4, 256)    0           add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 4, 4, 256)    1024        max_pooling2d_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_208 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_208[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 4, 4, 128)    512         conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_209 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_209[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 4, 4, 128)    512         conv2d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_210 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 4, 4, 256)    0           max_pooling2d_16[0][0]           \n",
      "                                                                 conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 4, 4, 256)    1024        add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_211 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 8, 8, 256)    1024        add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_211[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_202 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 4, 4, 128)    512         conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_202[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_212 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 8, 8, 128)    512         conv2d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_203 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_203[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 4, 4, 128)    512         conv2d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_203[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_213 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_213[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 8, 8, 128)    512         conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_204 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 4, 4, 256)    0           add_71[0][0]                     \n",
      "                                                                 conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_204[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 4, 4, 256)    1024        add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 8, 8, 256)    0           add_68[0][0]                     \n",
      "                                                                 conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_214 (ReLU)                (None, 4, 4, 256)    0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 8, 8, 256)    1024        add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 4, 4, 128)    32896       re_lu_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_205 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 4, 4, 128)    512         conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_205[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchN (None, 16, 16, 256)  1024        add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_215 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 8, 8, 128)    512         conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_193 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_193[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 4, 4, 128)    147584      re_lu_215[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_206 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_206[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_193[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 4, 4, 128)    512         conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_206[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchN (None, 16, 16, 128)  512         conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_216 (ReLU)                (None, 4, 4, 128)    0           batch_normalization_216[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 8, 8, 128)    512         conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_194 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_194[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, 4, 4, 256)    33024       re_lu_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_207 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_194[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 4, 4, 256)    0           add_72[0][0]                     \n",
      "                                                                 conv2d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_207[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchN (None, 16, 16, 128)  512         conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling2D) (None, 8, 8, 256)    0           add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 8, 8, 256)    0           add_69[0][0]                     \n",
      "                                                                 conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_195 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_195[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (None, 8, 8, 256)    0           up_sampling2d_12[0][0]           \n",
      "                                                                 add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_195[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 8, 8, 256)    1024        tf.__operators__.add_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 16, 16, 256)  0           add_65[0][0]                     \n",
      "                                                                 conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_217 (ReLU)                (None, 8, 8, 256)    0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 16, 16, 256)  1024        add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 8, 8, 128)    32896       re_lu_217[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_196 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 8, 8, 128)    512         conv2d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 32, 32, 256)  1024        add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_218 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 16, 16, 128)  512         conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_184 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 8, 8, 128)    147584      re_lu_218[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_197 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_184[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 8, 8, 128)    512         conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_197[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 32, 32, 128)  512         conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_219 (ReLU)                (None, 8, 8, 128)    0           batch_normalization_219[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 16, 16, 128)  512         conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_185 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 8, 8, 256)    33024       re_lu_219[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_198 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_185[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 8, 8, 256)    0           tf.__operators__.add_12[0][0]    \n",
      "                                                                 conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_198[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 32, 32, 128)  512         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling2D) (None, 16, 16, 256)  0           add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 16, 16, 256)  0           add_66[0][0]                     \n",
      "                                                                 conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_186 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_13 (TFOpLa (None, 16, 16, 256)  0           up_sampling2d_13[0][0]           \n",
      "                                                                 add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 16, 16, 256)  1024        tf.__operators__.add_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 32, 32, 256)  0           add_62[0][0]                     \n",
      "                                                                 conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_220 (ReLU)                (None, 16, 16, 256)  0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 32, 32, 256)  1024        add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 16, 16, 128)  32896       re_lu_220[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_187 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 16, 16, 128)  512         conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_187[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 64, 64, 256)  1024        add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_221 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 32, 32, 128)  512         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_175 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 16, 16, 128)  147584      re_lu_221[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_188 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 16, 16, 128)  512         conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_188[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 64, 64, 128)  512         conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_222 (ReLU)                (None, 16, 16, 128)  0           batch_normalization_222[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 32, 32, 128)  512         conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_176 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 16, 16, 256)  33024       re_lu_222[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_189 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 16, 16, 256)  0           tf.__operators__.add_13[0][0]    \n",
      "                                                                 conv2d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_189[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 64, 64, 128)  512         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling2D) (None, 32, 32, 256)  0           add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 32, 32, 256)  0           add_63[0][0]                     \n",
      "                                                                 conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_177 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_14 (TFOpLa (None, 32, 32, 256)  0           up_sampling2d_14[0][0]           \n",
      "                                                                 add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_177[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 32, 32, 256)  1024        tf.__operators__.add_14[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 64, 64, 256)  0           add_59[0][0]                     \n",
      "                                                                 conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_223 (ReLU)                (None, 32, 32, 256)  0           batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 64, 64, 256)  1024        add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 32, 32, 128)  32896       re_lu_223[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_178 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 32, 32, 128)  512         conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_224 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 64, 64, 128)  512         conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 32, 32, 128)  147584      re_lu_224[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_179 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 32, 32, 128)  512         conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_225 (ReLU)                (None, 32, 32, 128)  0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 64, 64, 128)  512         conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 32, 32, 256)  33024       re_lu_225[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_180 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 32, 32, 256)  0           tf.__operators__.add_14[0][0]    \n",
      "                                                                 conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_180[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling2D) (None, 64, 64, 256)  0           add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 64, 64, 256)  0           add_60[0][0]                     \n",
      "                                                                 conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (None, 64, 64, 256)  0           up_sampling2d_15[0][0]           \n",
      "                                                                 add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 64, 64, 256)  1024        tf.__operators__.add_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_226 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_226[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 64, 64, 128)  32896       re_lu_226[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 64, 64, 128)  512         conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_227 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 64, 64, 128)  147584      re_lu_227[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 64, 64, 128)  512         conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_228 (ReLU)                (None, 64, 64, 128)  0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 64, 64, 256)  33024       re_lu_228[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 64, 64, 256)  0           tf.__operators__.add_15[0][0]    \n",
      "                                                                 conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, 64, 64, 256)  65792       add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 64, 64, 256)  1024        conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_229 (ReLU)                (None, 64, 64, 256)  0           batch_normalization_229[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 64, 64, 16)   4112        re_lu_229[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,368,320\n",
      "Trainable params: 16,290,752\n",
      "Non-trainable params: 77,568\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# StackedHourglassNetwork 모델 요약하기\n",
    "stacked_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c445f",
   "metadata": {},
   "source": [
    "이제 `StackedHourglassNetwork`만 이용하면 모델을 쉽게 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf804f7",
   "metadata": {},
   "source": [
    "### **Simplebaseline 구조**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4320870",
   "metadata": {},
   "source": [
    "* deconv module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be8709f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86467aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "upconv = _make_deconv_layer(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87129a9a",
   "metadata": {},
   "source": [
    "* 최종 레이어(final layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d288d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = tf.keras.layers.Conv2D(16, kernel_size=(1,1), padding='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabdbfa3",
   "metadata": {},
   "source": [
    "* Backbone : ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18732bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleBaseline(input_shape=(256, 256, 3)):\n",
    "    # resnet backbone\n",
    "    resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "                                                   \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = resnet(inputs)\n",
    "    x = upconv(x)\n",
    "    out = final_layer(x)\n",
    "    model = tf.keras.Model(inputs, out, name='Simple_Baseline')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157b9cf",
   "metadata": {},
   "source": [
    "이제 각각의 요소를 합쳐 모델을 완성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fe6b653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 1s 0us/step\n",
      "94781440/94765736 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Simple Baseline 모델 생성하기\n",
    "simplebase_model = SimpleBaseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c00f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Simple_Baseline\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 64, 64, 256)       10489600  \n",
      "_________________________________________________________________\n",
      "conv2d_244 (Conv2D)          (None, 64, 64, 16)        4112      \n",
      "=================================================================\n",
      "Total params: 34,081,424\n",
      "Trainable params: 34,026,768\n",
      "Non-trainable params: 54,656\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Simple Baseline 모델 요약하기\n",
    "simplebase_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c27c4",
   "metadata": {},
   "source": [
    "## **6. 학습 엔진 만들기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860a47a",
   "metadata": {},
   "source": [
    "### **GPU가 여러 개인 환경**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f142bd6",
   "metadata": {},
   "source": [
    "그런데 학습을 할 수 있는 GPU가 여러 개이고 데이터를 병렬로 학습시키려면 어떻게 해야할까요? \n",
    "\n",
    "여러 GPU를 사용하기 위해서는 약간의 코드를 추가해줘야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc342c",
   "metadata": {},
   "source": [
    "가장 핵심 키워드는 `tf.distribute.MirroredStrategy`입니다. \n",
    "\n",
    "한 컴퓨터에 GPU가 여러 개인 경우 사용할 수 있는 방법입니다. \n",
    "\n",
    "여러 GPU가 모델을 학습한 후 각각의 Loss를 계산하면 CPU가 전체 Loss를 종합합니다. \n",
    "\n",
    "그런 후 모델의 가중치를 업데이트 하도록 하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68ee68",
   "metadata": {},
   "source": [
    "각 GPU에서 계산한 Loss를 토대로 전체 Loss를 종합해주는 역할은 `strategy.reduce` 함수가 담당합니다.\n",
    "\n",
    "이번에도 각 함수를 별개로 만들지 않고 하나의 객체로 만들어 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3163fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate):\n",
    "        \n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.best_model = None\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.is_simple = True\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        \n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def comput_loss_simplebase(self, labels, outputs):\n",
    "        loss = 0\n",
    "        weights = tf.cast(labels > 0, dtype = tf.float32) * 81 + 1\n",
    "        loss += tf.math.reduce_mean(tf.math.square(labels - outputs) * weights) *( 1./self.global_batch_size)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            \n",
    "            if self.is_simple:\n",
    "                loss = self.comput_loss_simplebase(labels, outputs)\n",
    "            else:\n",
    "                loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        \n",
    "        if self.is_simple:\n",
    "                loss = self.comput_loss_simplebase(labels, outputs)\n",
    "        else:\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        \n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed training...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.run(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.lr_decay()\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "            \n",
    "            self.train_loss.append(train_loss)\n",
    "            self.val_loss.append(val_loss)\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "        \n",
    "        \n",
    "        return self.best_model, self.train_loss, self.val_loss\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = os.getenv('HOME') + '/mpii/weights' + '/shg_model-epoch-{}-loss-{:.4f}.h5'.format(epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e709327",
   "metadata": {},
   "source": [
    "**Q. tf.distribute.MirroredStrategy외에 Tensorflow의 GPU 분산 훈련을 위한 전략 메소드는 또 무엇이 있는지 알아볼까요?**\n",
    "\n",
    "1. MirroredStrategy : TensorFlow에서 다중 GPU 환경에서 모델을 훈련하기 위한 분산 전략\n",
    "2. TPUStrategy : 구글의 Tensor Processing Units(TPU)를 사용하여 모델을 훈련하기 위한 분산 전략\n",
    "3. MultiWorkerMirroredStrategy : 다중 워커(worker) 환경에서 모델을 훈련하기 위한 분산 전략\n",
    "4. ParameterServerStrategy : 다중 서버(server) 환경에서 모델을 훈련하기 위한 분산 전략\n",
    "5. CentralStorageStrategy : 중앙 저장소를 사용하여 모델을 훈련하기 위한 분산 전략"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43811847",
   "metadata": {},
   "source": [
    "이제 데이터셋을 만드는 함수를 작성합니다. \n",
    "\n",
    "TFRecord 파일이 여러개이므로 tf.data.Dataset.list_files를 통해 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0acc4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)\n",
    "\n",
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd818f4",
   "metadata": {},
   "source": [
    "이제 데이터셋과 모델, 훈련용 객체를 조립만 하면 됩니다.\n",
    "\n",
    "주의할 점은 `with strategy.scope():` 부분이 반드시 필요하다는 점입니다.\n",
    "\n",
    "또한 데이터셋도 experimental_distribute_dataset를 통해 연결해 줘야 한다는 것도 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c16d7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_hourglass_train(epochs, learning_rate, num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate)\n",
    "\n",
    "        print('Start training...')\n",
    "        trainer\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad216e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_baseline_train(epochs, learning_rate,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(os.path.join(weight_path)):\n",
    "        os.makedirs(os.path.join(weight_path))\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = Simplebaseline(IMAGE_SHAPE)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            )\n",
    "\n",
    "        print('Start training...')\n",
    "        trainer\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46a3ea",
   "metadata": {},
   "source": [
    "모델을 학습시킬 차례입니다. (1 Epoch 학습에만 1 시간 가까이 소요됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0b4a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1834a965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:374: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed training...\n",
      "Trained batch 1 batch loss 1.28931165 epoch total loss 1.28931165\n",
      "Trained batch 2 batch loss 1.28962958 epoch total loss 1.28947067\n",
      "Trained batch 3 batch loss 1.2438786 epoch total loss 1.27427328\n",
      "Trained batch 4 batch loss 1.27144063 epoch total loss 1.27356517\n",
      "Trained batch 5 batch loss 1.27479267 epoch total loss 1.27381063\n",
      "Trained batch 6 batch loss 1.25460792 epoch total loss 1.27061021\n",
      "Trained batch 7 batch loss 1.2231797 epoch total loss 1.26383436\n",
      "Trained batch 8 batch loss 1.21406889 epoch total loss 1.25761366\n",
      "Trained batch 9 batch loss 1.1712184 epoch total loss 1.24801421\n",
      "Trained batch 10 batch loss 1.18406773 epoch total loss 1.24161959\n",
      "Trained batch 11 batch loss 1.15298963 epoch total loss 1.23356235\n",
      "Trained batch 12 batch loss 1.13507807 epoch total loss 1.22535527\n",
      "Trained batch 13 batch loss 1.01909375 epoch total loss 1.20948899\n",
      "Trained batch 14 batch loss 0.973246038 epoch total loss 1.19261456\n",
      "Trained batch 15 batch loss 0.997541189 epoch total loss 1.17960966\n",
      "Trained batch 16 batch loss 1.01099336 epoch total loss 1.1690712\n",
      "Trained batch 17 batch loss 1.04838908 epoch total loss 1.16197228\n",
      "Trained batch 18 batch loss 1.06479132 epoch total loss 1.1565733\n",
      "Trained batch 19 batch loss 1.05633962 epoch total loss 1.15129781\n",
      "Trained batch 20 batch loss 1.05947292 epoch total loss 1.14670658\n",
      "Trained batch 21 batch loss 1.04740775 epoch total loss 1.14197803\n",
      "Trained batch 22 batch loss 0.939974368 epoch total loss 1.13279605\n",
      "Trained batch 23 batch loss 0.97189945 epoch total loss 1.12580049\n",
      "Trained batch 24 batch loss 0.90633744 epoch total loss 1.11665618\n",
      "Trained batch 25 batch loss 0.919826925 epoch total loss 1.10878301\n",
      "Trained batch 26 batch loss 0.903585732 epoch total loss 1.10089087\n",
      "Trained batch 27 batch loss 0.907009661 epoch total loss 1.09371006\n",
      "Trained batch 28 batch loss 0.989602923 epoch total loss 1.08999193\n",
      "Trained batch 29 batch loss 0.991583288 epoch total loss 1.08659852\n",
      "Trained batch 30 batch loss 0.983899951 epoch total loss 1.08317518\n",
      "Trained batch 31 batch loss 0.991241276 epoch total loss 1.08020949\n",
      "Trained batch 32 batch loss 0.962281942 epoch total loss 1.07652426\n",
      "Trained batch 33 batch loss 0.905262947 epoch total loss 1.07133448\n",
      "Trained batch 34 batch loss 0.945141435 epoch total loss 1.0676229\n",
      "Trained batch 35 batch loss 0.860484838 epoch total loss 1.06170464\n",
      "Trained batch 36 batch loss 0.848679662 epoch total loss 1.05578732\n",
      "Trained batch 37 batch loss 0.96938765 epoch total loss 1.05345213\n",
      "Trained batch 38 batch loss 0.937667251 epoch total loss 1.05040514\n",
      "Trained batch 39 batch loss 0.944229603 epoch total loss 1.04768276\n",
      "Trained batch 40 batch loss 0.927715957 epoch total loss 1.04468358\n",
      "Trained batch 41 batch loss 0.903362095 epoch total loss 1.04123664\n",
      "Trained batch 42 batch loss 0.847829282 epoch total loss 1.0366317\n",
      "Trained batch 43 batch loss 0.87139082 epoch total loss 1.03278887\n",
      "Trained batch 44 batch loss 0.892785609 epoch total loss 1.02960694\n",
      "Trained batch 45 batch loss 0.955438197 epoch total loss 1.02795875\n",
      "Trained batch 46 batch loss 0.888669848 epoch total loss 1.02493072\n",
      "Trained batch 47 batch loss 0.875156105 epoch total loss 1.02174401\n",
      "Trained batch 48 batch loss 0.936595798 epoch total loss 1.01997\n",
      "Trained batch 49 batch loss 0.91171658 epoch total loss 1.01776087\n",
      "Trained batch 50 batch loss 0.957110643 epoch total loss 1.0165478\n",
      "Trained batch 51 batch loss 0.910553813 epoch total loss 1.0144695\n",
      "Trained batch 52 batch loss 0.898085415 epoch total loss 1.01223135\n",
      "Trained batch 53 batch loss 0.833728433 epoch total loss 1.00886345\n",
      "Trained batch 54 batch loss 0.792479455 epoch total loss 1.00485635\n",
      "Trained batch 55 batch loss 0.911890328 epoch total loss 1.00316608\n",
      "Trained batch 56 batch loss 0.888984084 epoch total loss 1.00112712\n",
      "Trained batch 57 batch loss 0.886252224 epoch total loss 0.999111772\n",
      "Trained batch 58 batch loss 0.823086321 epoch total loss 0.996076822\n",
      "Trained batch 59 batch loss 0.849842787 epoch total loss 0.993598282\n",
      "Trained batch 60 batch loss 0.811084092 epoch total loss 0.990556419\n",
      "Trained batch 61 batch loss 0.879870176 epoch total loss 0.988741875\n",
      "Trained batch 62 batch loss 0.856069326 epoch total loss 0.986602\n",
      "Trained batch 63 batch loss 0.805454612 epoch total loss 0.983726621\n",
      "Trained batch 64 batch loss 0.846502662 epoch total loss 0.981582522\n",
      "Trained batch 65 batch loss 0.863411188 epoch total loss 0.979764521\n",
      "Trained batch 66 batch loss 0.851481795 epoch total loss 0.977820754\n",
      "Trained batch 67 batch loss 0.873589396 epoch total loss 0.976265073\n",
      "Trained batch 68 batch loss 0.9221766 epoch total loss 0.975469708\n",
      "Trained batch 69 batch loss 0.908341348 epoch total loss 0.974496841\n",
      "Trained batch 70 batch loss 0.895511627 epoch total loss 0.973368406\n",
      "Trained batch 71 batch loss 0.850011706 epoch total loss 0.971631\n",
      "Trained batch 72 batch loss 0.881621 epoch total loss 0.970380902\n",
      "Trained batch 73 batch loss 0.894440293 epoch total loss 0.969340622\n",
      "Trained batch 74 batch loss 0.848640561 epoch total loss 0.967709541\n",
      "Trained batch 75 batch loss 0.832785666 epoch total loss 0.965910554\n",
      "Trained batch 76 batch loss 0.912303209 epoch total loss 0.965205133\n",
      "Trained batch 77 batch loss 0.832098246 epoch total loss 0.963476479\n",
      "Trained batch 78 batch loss 0.868682384 epoch total loss 0.9622612\n",
      "Trained batch 79 batch loss 0.893445134 epoch total loss 0.961390138\n",
      "Trained batch 80 batch loss 0.86752516 epoch total loss 0.96021682\n",
      "Trained batch 81 batch loss 0.85643363 epoch total loss 0.958935499\n",
      "Trained batch 82 batch loss 0.794472575 epoch total loss 0.956929862\n",
      "Trained batch 83 batch loss 0.885660529 epoch total loss 0.956071138\n",
      "Trained batch 84 batch loss 0.863088429 epoch total loss 0.954964221\n",
      "Trained batch 85 batch loss 0.849116743 epoch total loss 0.953718901\n",
      "Trained batch 86 batch loss 0.877233326 epoch total loss 0.952829599\n",
      "Trained batch 87 batch loss 0.891182244 epoch total loss 0.95212096\n",
      "Trained batch 88 batch loss 0.86712414 epoch total loss 0.951155126\n",
      "Trained batch 89 batch loss 0.845461 epoch total loss 0.949967563\n",
      "Trained batch 90 batch loss 0.818004787 epoch total loss 0.948501348\n",
      "Trained batch 91 batch loss 0.856045842 epoch total loss 0.947485387\n",
      "Trained batch 92 batch loss 0.815867245 epoch total loss 0.946054697\n",
      "Trained batch 93 batch loss 0.87026459 epoch total loss 0.945239723\n",
      "Trained batch 94 batch loss 0.872532666 epoch total loss 0.944466293\n",
      "Trained batch 95 batch loss 0.753508866 epoch total loss 0.942456186\n",
      "Trained batch 96 batch loss 0.852128506 epoch total loss 0.941515267\n",
      "Trained batch 97 batch loss 0.841704786 epoch total loss 0.940486312\n",
      "Trained batch 98 batch loss 0.913440526 epoch total loss 0.940210283\n",
      "Trained batch 99 batch loss 0.907643139 epoch total loss 0.939881384\n",
      "Trained batch 100 batch loss 0.951744139 epoch total loss 0.94\n",
      "Trained batch 101 batch loss 0.975743592 epoch total loss 0.94035393\n",
      "Trained batch 102 batch loss 0.796317458 epoch total loss 0.938941777\n",
      "Trained batch 103 batch loss 0.893719792 epoch total loss 0.938502789\n",
      "Trained batch 104 batch loss 0.879381359 epoch total loss 0.937934279\n",
      "Trained batch 105 batch loss 0.922757328 epoch total loss 0.937789798\n",
      "Trained batch 106 batch loss 0.819521785 epoch total loss 0.936674\n",
      "Trained batch 107 batch loss 0.81346041 epoch total loss 0.935522497\n",
      "Trained batch 108 batch loss 0.865301132 epoch total loss 0.934872329\n",
      "Trained batch 109 batch loss 0.878471851 epoch total loss 0.934354842\n",
      "Trained batch 110 batch loss 0.825845 epoch total loss 0.933368385\n",
      "Trained batch 111 batch loss 0.87864691 epoch total loss 0.932875395\n",
      "Trained batch 112 batch loss 0.862742722 epoch total loss 0.932249188\n",
      "Trained batch 113 batch loss 0.797495961 epoch total loss 0.931056678\n",
      "Trained batch 114 batch loss 0.855352163 epoch total loss 0.930392623\n",
      "Trained batch 115 batch loss 0.8814044 epoch total loss 0.929966629\n",
      "Trained batch 116 batch loss 0.882126927 epoch total loss 0.929554164\n",
      "Trained batch 117 batch loss 0.860731125 epoch total loss 0.928966\n",
      "Trained batch 118 batch loss 0.871224046 epoch total loss 0.928476632\n",
      "Trained batch 119 batch loss 0.896553 epoch total loss 0.928208351\n",
      "Trained batch 120 batch loss 0.880800784 epoch total loss 0.927813292\n",
      "Trained batch 121 batch loss 0.907323062 epoch total loss 0.927643955\n",
      "Trained batch 122 batch loss 0.906770885 epoch total loss 0.92747283\n",
      "Trained batch 123 batch loss 0.891610622 epoch total loss 0.927181304\n",
      "Trained batch 124 batch loss 0.856315136 epoch total loss 0.926609755\n",
      "Trained batch 125 batch loss 0.778799415 epoch total loss 0.925427318\n",
      "Trained batch 126 batch loss 0.835038722 epoch total loss 0.924709916\n",
      "Trained batch 127 batch loss 0.833625317 epoch total loss 0.923992753\n",
      "Trained batch 128 batch loss 0.883754492 epoch total loss 0.923678339\n",
      "Trained batch 129 batch loss 0.837888658 epoch total loss 0.92301333\n",
      "Trained batch 130 batch loss 0.807203352 epoch total loss 0.922122478\n",
      "Trained batch 131 batch loss 0.883951187 epoch total loss 0.921831071\n",
      "Trained batch 132 batch loss 0.903093338 epoch total loss 0.921689093\n",
      "Trained batch 133 batch loss 0.894569278 epoch total loss 0.921485186\n",
      "Trained batch 134 batch loss 0.82091713 epoch total loss 0.920734704\n",
      "Trained batch 135 batch loss 0.749560595 epoch total loss 0.919466734\n",
      "Trained batch 136 batch loss 0.768300414 epoch total loss 0.918355227\n",
      "Trained batch 137 batch loss 0.68892163 epoch total loss 0.916680515\n",
      "Trained batch 138 batch loss 0.685773849 epoch total loss 0.915007293\n",
      "Trained batch 139 batch loss 0.755697966 epoch total loss 0.913861156\n",
      "Trained batch 140 batch loss 0.802638113 epoch total loss 0.913066685\n",
      "Trained batch 141 batch loss 0.906913519 epoch total loss 0.913023055\n",
      "Trained batch 142 batch loss 0.847589612 epoch total loss 0.912562311\n",
      "Trained batch 143 batch loss 0.907666206 epoch total loss 0.912528098\n",
      "Trained batch 144 batch loss 0.815361381 epoch total loss 0.911853373\n",
      "Trained batch 145 batch loss 0.843308806 epoch total loss 0.911380649\n",
      "Trained batch 146 batch loss 0.869934857 epoch total loss 0.911096752\n",
      "Trained batch 147 batch loss 0.819519401 epoch total loss 0.910473764\n",
      "Trained batch 148 batch loss 0.856016636 epoch total loss 0.910105824\n",
      "Trained batch 149 batch loss 0.850317061 epoch total loss 0.909704506\n",
      "Trained batch 150 batch loss 0.887781203 epoch total loss 0.909558415\n",
      "Trained batch 151 batch loss 0.879513085 epoch total loss 0.909359455\n",
      "Trained batch 152 batch loss 0.864878893 epoch total loss 0.909066856\n",
      "Trained batch 153 batch loss 0.87843 epoch total loss 0.908866644\n",
      "Trained batch 154 batch loss 0.783083141 epoch total loss 0.908049822\n",
      "Trained batch 155 batch loss 0.836110234 epoch total loss 0.90758568\n",
      "Trained batch 156 batch loss 0.775623322 epoch total loss 0.906739771\n",
      "Trained batch 157 batch loss 0.705730796 epoch total loss 0.905459464\n",
      "Trained batch 158 batch loss 0.841436267 epoch total loss 0.905054212\n",
      "Trained batch 159 batch loss 0.849578738 epoch total loss 0.904705286\n",
      "Trained batch 160 batch loss 0.865410864 epoch total loss 0.904459774\n",
      "Trained batch 161 batch loss 0.814596295 epoch total loss 0.903901577\n",
      "Trained batch 162 batch loss 0.812773108 epoch total loss 0.903339\n",
      "Trained batch 163 batch loss 0.847409606 epoch total loss 0.902995944\n",
      "Trained batch 164 batch loss 0.882356226 epoch total loss 0.902870059\n",
      "Trained batch 165 batch loss 0.839164078 epoch total loss 0.90248394\n",
      "Trained batch 166 batch loss 0.866379917 epoch total loss 0.902266443\n",
      "Trained batch 167 batch loss 0.862409472 epoch total loss 0.902027786\n",
      "Trained batch 168 batch loss 0.834291816 epoch total loss 0.90162456\n",
      "Trained batch 169 batch loss 0.863392353 epoch total loss 0.901398361\n",
      "Trained batch 170 batch loss 0.868744135 epoch total loss 0.901206255\n",
      "Trained batch 171 batch loss 0.864230037 epoch total loss 0.90099\n",
      "Trained batch 172 batch loss 0.841470242 epoch total loss 0.900644\n",
      "Trained batch 173 batch loss 0.862159491 epoch total loss 0.9004215\n",
      "Trained batch 174 batch loss 0.823443949 epoch total loss 0.899979055\n",
      "Trained batch 175 batch loss 0.850987792 epoch total loss 0.899699092\n",
      "Trained batch 176 batch loss 0.846837461 epoch total loss 0.899398744\n",
      "Trained batch 177 batch loss 0.876913786 epoch total loss 0.899271667\n",
      "Trained batch 178 batch loss 0.839647651 epoch total loss 0.898936689\n",
      "Trained batch 179 batch loss 0.741667926 epoch total loss 0.898058057\n",
      "Trained batch 180 batch loss 0.793515921 epoch total loss 0.897477269\n",
      "Trained batch 181 batch loss 0.846396804 epoch total loss 0.897195041\n",
      "Trained batch 182 batch loss 0.861971676 epoch total loss 0.897001505\n",
      "Trained batch 183 batch loss 0.834341407 epoch total loss 0.896659076\n",
      "Trained batch 184 batch loss 0.887337923 epoch total loss 0.896608412\n",
      "Trained batch 185 batch loss 0.798274696 epoch total loss 0.896076918\n",
      "Trained batch 186 batch loss 0.794895291 epoch total loss 0.895532906\n",
      "Trained batch 187 batch loss 0.839171052 epoch total loss 0.895231545\n",
      "Trained batch 188 batch loss 0.833500624 epoch total loss 0.894903123\n",
      "Trained batch 189 batch loss 0.830713272 epoch total loss 0.894563556\n",
      "Trained batch 190 batch loss 0.760600567 epoch total loss 0.893858492\n",
      "Trained batch 191 batch loss 0.836243 epoch total loss 0.893556833\n",
      "Trained batch 192 batch loss 0.733387947 epoch total loss 0.892722607\n",
      "Trained batch 193 batch loss 0.794230044 epoch total loss 0.892212331\n",
      "Trained batch 194 batch loss 0.873583078 epoch total loss 0.892116249\n",
      "Trained batch 195 batch loss 0.836391091 epoch total loss 0.891830504\n",
      "Trained batch 196 batch loss 0.850387633 epoch total loss 0.891619086\n",
      "Trained batch 197 batch loss 0.873168945 epoch total loss 0.891525447\n",
      "Trained batch 198 batch loss 0.882637501 epoch total loss 0.891480565\n",
      "Trained batch 199 batch loss 0.858415365 epoch total loss 0.891314387\n",
      "Trained batch 200 batch loss 0.863487244 epoch total loss 0.89117521\n",
      "Trained batch 201 batch loss 0.888263881 epoch total loss 0.891160727\n",
      "Trained batch 202 batch loss 0.866029561 epoch total loss 0.891036332\n",
      "Trained batch 203 batch loss 0.932960749 epoch total loss 0.891242862\n",
      "Trained batch 204 batch loss 0.902662516 epoch total loss 0.891298831\n",
      "Trained batch 205 batch loss 0.885099649 epoch total loss 0.891268611\n",
      "Trained batch 206 batch loss 0.827610075 epoch total loss 0.890959561\n",
      "Trained batch 207 batch loss 0.878059 epoch total loss 0.890897214\n",
      "Trained batch 208 batch loss 0.766884625 epoch total loss 0.890301049\n",
      "Trained batch 209 batch loss 0.785963178 epoch total loss 0.8898018\n",
      "Trained batch 210 batch loss 0.753174186 epoch total loss 0.889151216\n",
      "Trained batch 211 batch loss 0.730730772 epoch total loss 0.888400376\n",
      "Trained batch 212 batch loss 0.678283 epoch total loss 0.88740927\n",
      "Trained batch 213 batch loss 0.696264327 epoch total loss 0.886511862\n",
      "Trained batch 214 batch loss 0.705334902 epoch total loss 0.885665238\n",
      "Trained batch 215 batch loss 0.753993571 epoch total loss 0.88505286\n",
      "Trained batch 216 batch loss 0.674331307 epoch total loss 0.884077311\n",
      "Trained batch 217 batch loss 0.814197481 epoch total loss 0.883755267\n",
      "Trained batch 218 batch loss 0.802991271 epoch total loss 0.883384764\n",
      "Trained batch 219 batch loss 0.768262923 epoch total loss 0.882859111\n",
      "Trained batch 220 batch loss 0.819642 epoch total loss 0.882571757\n",
      "Trained batch 221 batch loss 0.826562822 epoch total loss 0.882318377\n",
      "Trained batch 222 batch loss 0.849977076 epoch total loss 0.882172644\n",
      "Trained batch 223 batch loss 0.811139226 epoch total loss 0.881854117\n",
      "Trained batch 224 batch loss 0.771596968 epoch total loss 0.881361902\n",
      "Trained batch 225 batch loss 0.750903964 epoch total loss 0.880782068\n",
      "Trained batch 226 batch loss 0.77617991 epoch total loss 0.880319238\n",
      "Trained batch 227 batch loss 0.862261057 epoch total loss 0.880239666\n",
      "Trained batch 228 batch loss 0.790875077 epoch total loss 0.879847765\n",
      "Trained batch 229 batch loss 0.855466425 epoch total loss 0.879741311\n",
      "Trained batch 230 batch loss 0.829798 epoch total loss 0.879524171\n",
      "Trained batch 231 batch loss 0.826720655 epoch total loss 0.879295588\n",
      "Trained batch 232 batch loss 0.845498085 epoch total loss 0.879149914\n",
      "Trained batch 233 batch loss 0.792946398 epoch total loss 0.87878\n",
      "Trained batch 234 batch loss 0.771860778 epoch total loss 0.878323078\n",
      "Trained batch 235 batch loss 0.85751909 epoch total loss 0.878234565\n",
      "Trained batch 236 batch loss 0.839683414 epoch total loss 0.878071129\n",
      "Trained batch 237 batch loss 0.720616877 epoch total loss 0.877406776\n",
      "Trained batch 238 batch loss 0.86286366 epoch total loss 0.877345681\n",
      "Trained batch 239 batch loss 0.786702454 epoch total loss 0.876966417\n",
      "Trained batch 240 batch loss 0.800552607 epoch total loss 0.876648\n",
      "Trained batch 241 batch loss 0.854067206 epoch total loss 0.87655431\n",
      "Trained batch 242 batch loss 0.783810318 epoch total loss 0.876171052\n",
      "Trained batch 243 batch loss 0.828670382 epoch total loss 0.875975609\n",
      "Trained batch 244 batch loss 0.751641929 epoch total loss 0.875466108\n",
      "Trained batch 245 batch loss 0.767726779 epoch total loss 0.875026345\n",
      "Trained batch 246 batch loss 0.794416904 epoch total loss 0.874698699\n",
      "Trained batch 247 batch loss 0.810899258 epoch total loss 0.874440372\n",
      "Trained batch 248 batch loss 0.856773555 epoch total loss 0.874369144\n",
      "Trained batch 249 batch loss 0.732699335 epoch total loss 0.873800218\n",
      "Trained batch 250 batch loss 0.851224482 epoch total loss 0.873709917\n",
      "Trained batch 251 batch loss 0.776961446 epoch total loss 0.873324454\n",
      "Trained batch 252 batch loss 0.855267525 epoch total loss 0.873252809\n",
      "Trained batch 253 batch loss 0.82003814 epoch total loss 0.873042464\n",
      "Trained batch 254 batch loss 0.82028985 epoch total loss 0.872834802\n",
      "Trained batch 255 batch loss 0.784416437 epoch total loss 0.872488081\n",
      "Trained batch 256 batch loss 0.873389482 epoch total loss 0.872491598\n",
      "Trained batch 257 batch loss 0.819916964 epoch total loss 0.872287035\n",
      "Trained batch 258 batch loss 0.848608792 epoch total loss 0.872195244\n",
      "Trained batch 259 batch loss 0.784935117 epoch total loss 0.871858358\n",
      "Trained batch 260 batch loss 0.757724285 epoch total loss 0.87141937\n",
      "Trained batch 261 batch loss 0.769921 epoch total loss 0.871030509\n",
      "Trained batch 262 batch loss 0.880360544 epoch total loss 0.871066093\n",
      "Trained batch 263 batch loss 0.834271073 epoch total loss 0.870926201\n",
      "Trained batch 264 batch loss 0.782354474 epoch total loss 0.870590687\n",
      "Trained batch 265 batch loss 0.859601676 epoch total loss 0.870549202\n",
      "Trained batch 266 batch loss 0.786087692 epoch total loss 0.870231688\n",
      "Trained batch 267 batch loss 0.744383574 epoch total loss 0.869760334\n",
      "Trained batch 268 batch loss 0.760262191 epoch total loss 0.869351804\n",
      "Trained batch 269 batch loss 0.716546297 epoch total loss 0.868783772\n",
      "Trained batch 270 batch loss 0.786329567 epoch total loss 0.868478417\n",
      "Trained batch 271 batch loss 0.825455666 epoch total loss 0.868319631\n",
      "Trained batch 272 batch loss 0.781619847 epoch total loss 0.868000865\n",
      "Trained batch 273 batch loss 0.839746 epoch total loss 0.867897391\n",
      "Trained batch 274 batch loss 0.792031646 epoch total loss 0.867620528\n",
      "Trained batch 275 batch loss 0.685029507 epoch total loss 0.866956592\n",
      "Trained batch 276 batch loss 0.701096416 epoch total loss 0.866355598\n",
      "Trained batch 277 batch loss 0.700093806 epoch total loss 0.865755379\n",
      "Trained batch 278 batch loss 0.672218204 epoch total loss 0.865059197\n",
      "Trained batch 279 batch loss 0.692973256 epoch total loss 0.864442408\n",
      "Trained batch 280 batch loss 0.696077883 epoch total loss 0.863841057\n",
      "Trained batch 281 batch loss 0.759398103 epoch total loss 0.863469422\n",
      "Trained batch 282 batch loss 0.791348219 epoch total loss 0.863213658\n",
      "Trained batch 283 batch loss 0.755779207 epoch total loss 0.862834036\n",
      "Trained batch 284 batch loss 0.668959558 epoch total loss 0.862151384\n",
      "Trained batch 285 batch loss 0.700491071 epoch total loss 0.861584127\n",
      "Trained batch 286 batch loss 0.77593565 epoch total loss 0.861284673\n",
      "Trained batch 287 batch loss 0.860900879 epoch total loss 0.861283362\n",
      "Trained batch 288 batch loss 0.810669243 epoch total loss 0.861107588\n",
      "Trained batch 289 batch loss 0.786205769 epoch total loss 0.860848427\n",
      "Trained batch 290 batch loss 0.768061 epoch total loss 0.860528529\n",
      "Trained batch 291 batch loss 0.84257257 epoch total loss 0.860466838\n",
      "Trained batch 292 batch loss 0.791465938 epoch total loss 0.860230565\n",
      "Trained batch 293 batch loss 0.726025701 epoch total loss 0.859772503\n",
      "Trained batch 294 batch loss 0.725447595 epoch total loss 0.859315634\n",
      "Trained batch 295 batch loss 0.771447718 epoch total loss 0.859017789\n",
      "Trained batch 296 batch loss 0.774870872 epoch total loss 0.858733535\n",
      "Trained batch 297 batch loss 0.802132607 epoch total loss 0.858543\n",
      "Trained batch 298 batch loss 0.826761842 epoch total loss 0.858436346\n",
      "Trained batch 299 batch loss 0.84072715 epoch total loss 0.858377099\n",
      "Trained batch 300 batch loss 0.857488275 epoch total loss 0.858374119\n",
      "Trained batch 301 batch loss 0.859126925 epoch total loss 0.858376622\n",
      "Trained batch 302 batch loss 0.881629229 epoch total loss 0.858453631\n",
      "Trained batch 303 batch loss 0.892740846 epoch total loss 0.858566761\n",
      "Trained batch 304 batch loss 0.879464269 epoch total loss 0.858635426\n",
      "Trained batch 305 batch loss 0.878283918 epoch total loss 0.858699918\n",
      "Trained batch 306 batch loss 0.877041101 epoch total loss 0.85875988\n",
      "Trained batch 307 batch loss 0.869511366 epoch total loss 0.858794868\n",
      "Trained batch 308 batch loss 0.842119634 epoch total loss 0.858740747\n",
      "Trained batch 309 batch loss 0.824899495 epoch total loss 0.858631194\n",
      "Trained batch 310 batch loss 0.882805645 epoch total loss 0.858709216\n",
      "Trained batch 311 batch loss 0.849082232 epoch total loss 0.858678281\n",
      "Trained batch 312 batch loss 0.842482328 epoch total loss 0.858626366\n",
      "Trained batch 313 batch loss 0.818988681 epoch total loss 0.858499765\n",
      "Trained batch 314 batch loss 0.825912714 epoch total loss 0.858396\n",
      "Trained batch 315 batch loss 0.797870338 epoch total loss 0.858203888\n",
      "Trained batch 316 batch loss 0.835513949 epoch total loss 0.858132064\n",
      "Trained batch 317 batch loss 0.844967365 epoch total loss 0.85809058\n",
      "Trained batch 318 batch loss 0.802872539 epoch total loss 0.857916951\n",
      "Trained batch 319 batch loss 0.85108614 epoch total loss 0.857895494\n",
      "Trained batch 320 batch loss 0.843180776 epoch total loss 0.857849479\n",
      "Trained batch 321 batch loss 0.794986129 epoch total loss 0.857653677\n",
      "Trained batch 322 batch loss 0.839325666 epoch total loss 0.857596755\n",
      "Trained batch 323 batch loss 0.852662086 epoch total loss 0.857581437\n",
      "Trained batch 324 batch loss 0.830124855 epoch total loss 0.857496738\n",
      "Trained batch 325 batch loss 0.854363501 epoch total loss 0.857487142\n",
      "Trained batch 326 batch loss 0.828314424 epoch total loss 0.857397616\n",
      "Trained batch 327 batch loss 0.849433959 epoch total loss 0.857373238\n",
      "Trained batch 328 batch loss 0.845395684 epoch total loss 0.85733676\n",
      "Trained batch 329 batch loss 0.79914391 epoch total loss 0.857159853\n",
      "Trained batch 330 batch loss 0.822599292 epoch total loss 0.857055128\n",
      "Trained batch 331 batch loss 0.844174683 epoch total loss 0.857016206\n",
      "Trained batch 332 batch loss 0.847592592 epoch total loss 0.856987834\n",
      "Trained batch 333 batch loss 0.817133844 epoch total loss 0.856868148\n",
      "Trained batch 334 batch loss 0.838677347 epoch total loss 0.856813729\n",
      "Trained batch 335 batch loss 0.833390594 epoch total loss 0.856743813\n",
      "Trained batch 336 batch loss 0.792102 epoch total loss 0.856551468\n",
      "Trained batch 337 batch loss 0.756075084 epoch total loss 0.856253326\n",
      "Trained batch 338 batch loss 0.848633885 epoch total loss 0.856230795\n",
      "Trained batch 339 batch loss 0.838827252 epoch total loss 0.856179476\n",
      "Trained batch 340 batch loss 0.788367152 epoch total loss 0.855980039\n",
      "Trained batch 341 batch loss 0.811863184 epoch total loss 0.855850637\n",
      "Trained batch 342 batch loss 0.807395 epoch total loss 0.855708957\n",
      "Trained batch 343 batch loss 0.777017117 epoch total loss 0.855479538\n",
      "Trained batch 344 batch loss 0.742686808 epoch total loss 0.855151594\n",
      "Trained batch 345 batch loss 0.722256839 epoch total loss 0.854766428\n",
      "Trained batch 346 batch loss 0.693416595 epoch total loss 0.854300082\n",
      "Trained batch 347 batch loss 0.701347351 epoch total loss 0.853859305\n",
      "Trained batch 348 batch loss 0.811791599 epoch total loss 0.853738427\n",
      "Trained batch 349 batch loss 0.871567428 epoch total loss 0.853789568\n",
      "Trained batch 350 batch loss 0.846285462 epoch total loss 0.85376811\n",
      "Trained batch 351 batch loss 0.829578102 epoch total loss 0.853699267\n",
      "Trained batch 352 batch loss 0.84544462 epoch total loss 0.853675842\n",
      "Trained batch 353 batch loss 0.842762589 epoch total loss 0.853644967\n",
      "Trained batch 354 batch loss 0.825899363 epoch total loss 0.853566587\n",
      "Trained batch 355 batch loss 0.824560106 epoch total loss 0.853484869\n",
      "Trained batch 356 batch loss 0.814613879 epoch total loss 0.853375614\n",
      "Trained batch 357 batch loss 0.827146471 epoch total loss 0.853302181\n",
      "Trained batch 358 batch loss 0.838433743 epoch total loss 0.853260636\n",
      "Trained batch 359 batch loss 0.783463538 epoch total loss 0.853066266\n",
      "Trained batch 360 batch loss 0.851210535 epoch total loss 0.85306108\n",
      "Trained batch 361 batch loss 0.858803272 epoch total loss 0.853077\n",
      "Trained batch 362 batch loss 0.884613931 epoch total loss 0.853164077\n",
      "Trained batch 363 batch loss 0.794678688 epoch total loss 0.853002965\n",
      "Trained batch 364 batch loss 0.82889384 epoch total loss 0.852936685\n",
      "Trained batch 365 batch loss 0.806494 epoch total loss 0.852809429\n",
      "Trained batch 366 batch loss 0.807733536 epoch total loss 0.852686286\n",
      "Trained batch 367 batch loss 0.851332366 epoch total loss 0.85268259\n",
      "Trained batch 368 batch loss 0.860496819 epoch total loss 0.852703869\n",
      "Trained batch 369 batch loss 0.871343613 epoch total loss 0.852754354\n",
      "Trained batch 370 batch loss 0.880321085 epoch total loss 0.852828801\n",
      "Trained batch 371 batch loss 0.727613926 epoch total loss 0.85249126\n",
      "Trained batch 372 batch loss 0.816298068 epoch total loss 0.852393925\n",
      "Trained batch 373 batch loss 0.901374698 epoch total loss 0.852525234\n",
      "Trained batch 374 batch loss 0.846849561 epoch total loss 0.852510095\n",
      "Trained batch 375 batch loss 0.812875032 epoch total loss 0.852404356\n",
      "Trained batch 376 batch loss 0.785715282 epoch total loss 0.852227\n",
      "Trained batch 377 batch loss 0.857277095 epoch total loss 0.852240384\n",
      "Trained batch 378 batch loss 0.823566496 epoch total loss 0.852164507\n",
      "Trained batch 379 batch loss 0.849395096 epoch total loss 0.852157235\n",
      "Trained batch 380 batch loss 0.845678806 epoch total loss 0.852140188\n",
      "Trained batch 381 batch loss 0.86419 epoch total loss 0.852171838\n",
      "Trained batch 382 batch loss 0.70162034 epoch total loss 0.851777732\n",
      "Trained batch 383 batch loss 0.81467551 epoch total loss 0.851680815\n",
      "Trained batch 384 batch loss 0.841647625 epoch total loss 0.851654708\n",
      "Trained batch 385 batch loss 0.792925477 epoch total loss 0.85150218\n",
      "Trained batch 386 batch loss 0.844320238 epoch total loss 0.851483583\n",
      "Trained batch 387 batch loss 0.764175057 epoch total loss 0.851258\n",
      "Trained batch 388 batch loss 0.834624648 epoch total loss 0.851215065\n",
      "Trained batch 389 batch loss 0.870930552 epoch total loss 0.851265788\n",
      "Trained batch 390 batch loss 0.844954133 epoch total loss 0.851249576\n",
      "Trained batch 391 batch loss 0.837335289 epoch total loss 0.851214\n",
      "Trained batch 392 batch loss 0.832761 epoch total loss 0.851166964\n",
      "Trained batch 393 batch loss 0.834203601 epoch total loss 0.85112375\n",
      "Trained batch 394 batch loss 0.815531194 epoch total loss 0.85103339\n",
      "Trained batch 395 batch loss 0.79671675 epoch total loss 0.850895882\n",
      "Trained batch 396 batch loss 0.771141708 epoch total loss 0.850694537\n",
      "Trained batch 397 batch loss 0.792746305 epoch total loss 0.850548565\n",
      "Trained batch 398 batch loss 0.800564647 epoch total loss 0.850423\n",
      "Trained batch 399 batch loss 0.795971215 epoch total loss 0.850286484\n",
      "Trained batch 400 batch loss 0.77867794 epoch total loss 0.850107491\n",
      "Trained batch 401 batch loss 0.863166571 epoch total loss 0.850140035\n",
      "Trained batch 402 batch loss 0.769166529 epoch total loss 0.849938631\n",
      "Trained batch 403 batch loss 0.835113 epoch total loss 0.849901855\n",
      "Trained batch 404 batch loss 0.815651178 epoch total loss 0.849817038\n",
      "Trained batch 405 batch loss 0.785560608 epoch total loss 0.84965837\n",
      "Trained batch 406 batch loss 0.745886922 epoch total loss 0.849402726\n",
      "Trained batch 407 batch loss 0.768602371 epoch total loss 0.849204242\n",
      "Trained batch 408 batch loss 0.786852777 epoch total loss 0.849051476\n",
      "Trained batch 409 batch loss 0.834778965 epoch total loss 0.849016547\n",
      "Trained batch 410 batch loss 0.860957 epoch total loss 0.849045694\n",
      "Trained batch 411 batch loss 0.820743382 epoch total loss 0.848976851\n",
      "Trained batch 412 batch loss 0.898833096 epoch total loss 0.849097848\n",
      "Trained batch 413 batch loss 0.854066789 epoch total loss 0.849109888\n",
      "Trained batch 414 batch loss 0.829394937 epoch total loss 0.849062264\n",
      "Trained batch 415 batch loss 0.831474364 epoch total loss 0.849019885\n",
      "Trained batch 416 batch loss 0.80557 epoch total loss 0.848915458\n",
      "Trained batch 417 batch loss 0.853774726 epoch total loss 0.848927081\n",
      "Trained batch 418 batch loss 0.84717828 epoch total loss 0.848922849\n",
      "Trained batch 419 batch loss 0.84111917 epoch total loss 0.848904252\n",
      "Trained batch 420 batch loss 0.846034944 epoch total loss 0.848897457\n",
      "Trained batch 421 batch loss 0.868071616 epoch total loss 0.848943\n",
      "Trained batch 422 batch loss 0.835052252 epoch total loss 0.848910093\n",
      "Trained batch 423 batch loss 0.822276235 epoch total loss 0.848847091\n",
      "Trained batch 424 batch loss 0.787949502 epoch total loss 0.848703504\n",
      "Trained batch 425 batch loss 0.786586344 epoch total loss 0.848557353\n",
      "Trained batch 426 batch loss 0.865090668 epoch total loss 0.848596156\n",
      "Trained batch 427 batch loss 0.842995346 epoch total loss 0.848583\n",
      "Trained batch 428 batch loss 0.869985342 epoch total loss 0.848633051\n",
      "Trained batch 429 batch loss 0.855304718 epoch total loss 0.848648608\n",
      "Trained batch 430 batch loss 0.832662821 epoch total loss 0.848611474\n",
      "Trained batch 431 batch loss 0.831502855 epoch total loss 0.848571777\n",
      "Trained batch 432 batch loss 0.831015646 epoch total loss 0.848531187\n",
      "Trained batch 433 batch loss 0.793039 epoch total loss 0.848403\n",
      "Trained batch 434 batch loss 0.753917038 epoch total loss 0.848185241\n",
      "Trained batch 435 batch loss 0.79063797 epoch total loss 0.848053\n",
      "Trained batch 436 batch loss 0.750743389 epoch total loss 0.847829759\n",
      "Trained batch 437 batch loss 0.692821503 epoch total loss 0.847475052\n",
      "Trained batch 438 batch loss 0.767747045 epoch total loss 0.847293\n",
      "Trained batch 439 batch loss 0.818214774 epoch total loss 0.847226799\n",
      "Trained batch 440 batch loss 0.7590909 epoch total loss 0.847026467\n",
      "Trained batch 441 batch loss 0.81382 epoch total loss 0.846951187\n",
      "Trained batch 442 batch loss 0.816340446 epoch total loss 0.846881926\n",
      "Trained batch 443 batch loss 0.823272288 epoch total loss 0.84682864\n",
      "Trained batch 444 batch loss 0.764371574 epoch total loss 0.846642911\n",
      "Trained batch 445 batch loss 0.833935499 epoch total loss 0.846614361\n",
      "Trained batch 446 batch loss 0.850499928 epoch total loss 0.846623\n",
      "Trained batch 447 batch loss 0.853079617 epoch total loss 0.846637487\n",
      "Trained batch 448 batch loss 0.828228831 epoch total loss 0.84659636\n",
      "Trained batch 449 batch loss 0.786199093 epoch total loss 0.846461833\n",
      "Trained batch 450 batch loss 0.813721895 epoch total loss 0.846389115\n",
      "Trained batch 451 batch loss 0.795986772 epoch total loss 0.846277356\n",
      "Trained batch 452 batch loss 0.843323767 epoch total loss 0.8462708\n",
      "Trained batch 453 batch loss 0.840158045 epoch total loss 0.846257269\n",
      "Trained batch 454 batch loss 0.832205892 epoch total loss 0.846226335\n",
      "Trained batch 455 batch loss 0.833075881 epoch total loss 0.846197426\n",
      "Trained batch 456 batch loss 0.801474035 epoch total loss 0.846099377\n",
      "Trained batch 457 batch loss 0.811137497 epoch total loss 0.846022844\n",
      "Trained batch 458 batch loss 0.802574039 epoch total loss 0.845928\n",
      "Trained batch 459 batch loss 0.816843569 epoch total loss 0.845864594\n",
      "Trained batch 460 batch loss 0.826426327 epoch total loss 0.845822334\n",
      "Trained batch 461 batch loss 0.816866398 epoch total loss 0.845759511\n",
      "Trained batch 462 batch loss 0.915802717 epoch total loss 0.845911145\n",
      "Trained batch 463 batch loss 0.796310902 epoch total loss 0.845804036\n",
      "Trained batch 464 batch loss 0.810527861 epoch total loss 0.845728\n",
      "Trained batch 465 batch loss 0.818490088 epoch total loss 0.845669389\n",
      "Trained batch 466 batch loss 0.81397146 epoch total loss 0.84560138\n",
      "Trained batch 467 batch loss 0.803417742 epoch total loss 0.845511\n",
      "Trained batch 468 batch loss 0.789949417 epoch total loss 0.845392287\n",
      "Trained batch 469 batch loss 0.814387381 epoch total loss 0.845326185\n",
      "Trained batch 470 batch loss 0.805055797 epoch total loss 0.845240474\n",
      "Trained batch 471 batch loss 0.802121639 epoch total loss 0.845148921\n",
      "Trained batch 472 batch loss 0.80571121 epoch total loss 0.845065415\n",
      "Trained batch 473 batch loss 0.787656486 epoch total loss 0.84494406\n",
      "Trained batch 474 batch loss 0.866368949 epoch total loss 0.84498924\n",
      "Trained batch 475 batch loss 0.819795728 epoch total loss 0.844936192\n",
      "Trained batch 476 batch loss 0.817385733 epoch total loss 0.844878316\n",
      "Trained batch 477 batch loss 0.76980722 epoch total loss 0.84472096\n",
      "Trained batch 478 batch loss 0.820293307 epoch total loss 0.844669819\n",
      "Trained batch 479 batch loss 0.774066389 epoch total loss 0.844522417\n",
      "Trained batch 480 batch loss 0.826744795 epoch total loss 0.844485402\n",
      "Trained batch 481 batch loss 0.767548 epoch total loss 0.844325483\n",
      "Trained batch 482 batch loss 0.746836901 epoch total loss 0.844123185\n",
      "Trained batch 483 batch loss 0.689111471 epoch total loss 0.843802273\n",
      "Trained batch 484 batch loss 0.659900188 epoch total loss 0.843422294\n",
      "Trained batch 485 batch loss 0.683417261 epoch total loss 0.843092382\n",
      "Trained batch 486 batch loss 0.701194644 epoch total loss 0.842800438\n",
      "Trained batch 487 batch loss 0.66673243 epoch total loss 0.842438877\n",
      "Trained batch 488 batch loss 0.734421372 epoch total loss 0.842217565\n",
      "Trained batch 489 batch loss 0.851042509 epoch total loss 0.842235625\n",
      "Trained batch 490 batch loss 0.763945341 epoch total loss 0.842075825\n",
      "Trained batch 491 batch loss 0.756383121 epoch total loss 0.841901302\n",
      "Trained batch 492 batch loss 0.853853524 epoch total loss 0.841925561\n",
      "Trained batch 493 batch loss 0.870747 epoch total loss 0.841984093\n",
      "Trained batch 494 batch loss 0.910222769 epoch total loss 0.842122197\n",
      "Trained batch 495 batch loss 0.913019657 epoch total loss 0.842265427\n",
      "Trained batch 496 batch loss 0.822369218 epoch total loss 0.842225313\n",
      "Trained batch 497 batch loss 0.785965741 epoch total loss 0.842112124\n",
      "Trained batch 498 batch loss 0.85947454 epoch total loss 0.842147\n",
      "Trained batch 499 batch loss 0.82341665 epoch total loss 0.842109442\n",
      "Trained batch 500 batch loss 0.827858031 epoch total loss 0.842080951\n",
      "Trained batch 501 batch loss 0.855749726 epoch total loss 0.84210819\n",
      "Trained batch 502 batch loss 0.84102869 epoch total loss 0.842106044\n",
      "Trained batch 503 batch loss 0.802767813 epoch total loss 0.842027843\n",
      "Trained batch 504 batch loss 0.810929775 epoch total loss 0.841966152\n",
      "Trained batch 505 batch loss 0.868777514 epoch total loss 0.84201926\n",
      "Trained batch 506 batch loss 0.859593511 epoch total loss 0.842054\n",
      "Trained batch 507 batch loss 0.82074517 epoch total loss 0.842011929\n",
      "Trained batch 508 batch loss 0.847449958 epoch total loss 0.842022657\n",
      "Trained batch 509 batch loss 0.822383821 epoch total loss 0.841984034\n",
      "Trained batch 510 batch loss 0.896755755 epoch total loss 0.842091441\n",
      "Trained batch 511 batch loss 0.866244555 epoch total loss 0.842138708\n",
      "Trained batch 512 batch loss 0.841318488 epoch total loss 0.842137098\n",
      "Trained batch 513 batch loss 0.79915446 epoch total loss 0.842053354\n",
      "Trained batch 514 batch loss 0.814060509 epoch total loss 0.841998875\n",
      "Trained batch 515 batch loss 0.794730365 epoch total loss 0.841907084\n",
      "Trained batch 516 batch loss 0.819732368 epoch total loss 0.841864109\n",
      "Trained batch 517 batch loss 0.765264034 epoch total loss 0.841715932\n",
      "Trained batch 518 batch loss 0.683238924 epoch total loss 0.84141\n",
      "Trained batch 519 batch loss 0.722757936 epoch total loss 0.841181338\n",
      "Trained batch 520 batch loss 0.730774 epoch total loss 0.840969\n",
      "Trained batch 521 batch loss 0.762438834 epoch total loss 0.840818346\n",
      "Trained batch 522 batch loss 0.812560797 epoch total loss 0.840764165\n",
      "Trained batch 523 batch loss 0.860500336 epoch total loss 0.840801954\n",
      "Trained batch 524 batch loss 0.771304429 epoch total loss 0.840669274\n",
      "Trained batch 525 batch loss 0.72532022 epoch total loss 0.840449572\n",
      "Trained batch 526 batch loss 0.79757452 epoch total loss 0.840368032\n",
      "Trained batch 527 batch loss 0.830597758 epoch total loss 0.840349495\n",
      "Trained batch 528 batch loss 0.772437811 epoch total loss 0.840220869\n",
      "Trained batch 529 batch loss 0.799474955 epoch total loss 0.840143859\n",
      "Trained batch 530 batch loss 0.81944406 epoch total loss 0.840104818\n",
      "Trained batch 531 batch loss 0.834911048 epoch total loss 0.840095043\n",
      "Trained batch 532 batch loss 0.83699 epoch total loss 0.840089142\n",
      "Trained batch 533 batch loss 0.831588447 epoch total loss 0.840073168\n",
      "Trained batch 534 batch loss 0.788004458 epoch total loss 0.839975655\n",
      "Trained batch 535 batch loss 0.82718569 epoch total loss 0.839951754\n",
      "Trained batch 536 batch loss 0.798025668 epoch total loss 0.839873552\n",
      "Trained batch 537 batch loss 0.768036306 epoch total loss 0.83973974\n",
      "Trained batch 538 batch loss 0.813701868 epoch total loss 0.839691341\n",
      "Trained batch 539 batch loss 0.710350931 epoch total loss 0.839451373\n",
      "Trained batch 540 batch loss 0.729832888 epoch total loss 0.839248359\n",
      "Trained batch 541 batch loss 0.794004858 epoch total loss 0.839164734\n",
      "Trained batch 542 batch loss 0.77060324 epoch total loss 0.839038253\n",
      "Trained batch 543 batch loss 0.848659396 epoch total loss 0.839055955\n",
      "Trained batch 544 batch loss 0.848436892 epoch total loss 0.839073241\n",
      "Trained batch 545 batch loss 0.778281689 epoch total loss 0.83896172\n",
      "Trained batch 546 batch loss 0.756715894 epoch total loss 0.8388111\n",
      "Trained batch 547 batch loss 0.77684325 epoch total loss 0.838697791\n",
      "Trained batch 548 batch loss 0.827617645 epoch total loss 0.838677585\n",
      "Trained batch 549 batch loss 0.853999436 epoch total loss 0.83870548\n",
      "Trained batch 550 batch loss 0.782471359 epoch total loss 0.838603258\n",
      "Trained batch 551 batch loss 0.773552418 epoch total loss 0.838485181\n",
      "Trained batch 552 batch loss 0.828269422 epoch total loss 0.838466704\n",
      "Trained batch 553 batch loss 0.852731228 epoch total loss 0.838492453\n",
      "Trained batch 554 batch loss 0.764430702 epoch total loss 0.838358819\n",
      "Trained batch 555 batch loss 0.712080121 epoch total loss 0.838131249\n",
      "Trained batch 556 batch loss 0.658045292 epoch total loss 0.837807357\n",
      "Trained batch 557 batch loss 0.793118834 epoch total loss 0.837727129\n",
      "Trained batch 558 batch loss 0.808533192 epoch total loss 0.837674797\n",
      "Trained batch 559 batch loss 0.75555861 epoch total loss 0.837527931\n",
      "Trained batch 560 batch loss 0.83517 epoch total loss 0.837523699\n",
      "Trained batch 561 batch loss 0.845176935 epoch total loss 0.837537348\n",
      "Trained batch 562 batch loss 0.860211492 epoch total loss 0.837577701\n",
      "Trained batch 563 batch loss 0.837350607 epoch total loss 0.837577283\n",
      "Trained batch 564 batch loss 0.800451458 epoch total loss 0.83751142\n",
      "Trained batch 565 batch loss 0.708005965 epoch total loss 0.83728224\n",
      "Trained batch 566 batch loss 0.810617924 epoch total loss 0.837235093\n",
      "Trained batch 567 batch loss 0.806168437 epoch total loss 0.837180316\n",
      "Trained batch 568 batch loss 0.869057 epoch total loss 0.837236404\n",
      "Trained batch 569 batch loss 0.900949895 epoch total loss 0.837348402\n",
      "Trained batch 570 batch loss 0.879727304 epoch total loss 0.837422729\n",
      "Trained batch 571 batch loss 0.868719101 epoch total loss 0.837477565\n",
      "Trained batch 572 batch loss 0.857155323 epoch total loss 0.837511957\n",
      "Trained batch 573 batch loss 0.852031112 epoch total loss 0.837537229\n",
      "Trained batch 574 batch loss 0.858582616 epoch total loss 0.837573886\n",
      "Trained batch 575 batch loss 0.837165236 epoch total loss 0.837573171\n",
      "Trained batch 576 batch loss 0.881395757 epoch total loss 0.837649286\n",
      "Trained batch 577 batch loss 0.791780114 epoch total loss 0.837569773\n",
      "Trained batch 578 batch loss 0.82153511 epoch total loss 0.837542057\n",
      "Trained batch 579 batch loss 0.805854559 epoch total loss 0.83748728\n",
      "Trained batch 580 batch loss 0.769747317 epoch total loss 0.837370515\n",
      "Trained batch 581 batch loss 0.724643707 epoch total loss 0.837176502\n",
      "Trained batch 582 batch loss 0.79798615 epoch total loss 0.837109149\n",
      "Trained batch 583 batch loss 0.801036835 epoch total loss 0.837047219\n",
      "Trained batch 584 batch loss 0.846762776 epoch total loss 0.837063909\n",
      "Trained batch 585 batch loss 0.802992702 epoch total loss 0.837005615\n",
      "Trained batch 586 batch loss 0.824208856 epoch total loss 0.8369838\n",
      "Trained batch 587 batch loss 0.778867245 epoch total loss 0.836884797\n",
      "Trained batch 588 batch loss 0.780809581 epoch total loss 0.836789429\n",
      "Trained batch 589 batch loss 0.833486378 epoch total loss 0.836783886\n",
      "Trained batch 590 batch loss 0.796946228 epoch total loss 0.836716294\n",
      "Trained batch 591 batch loss 0.852047086 epoch total loss 0.836742282\n",
      "Trained batch 592 batch loss 0.758075 epoch total loss 0.836609423\n",
      "Trained batch 593 batch loss 0.817700207 epoch total loss 0.836577475\n",
      "Trained batch 594 batch loss 0.712520361 epoch total loss 0.83636868\n",
      "Trained batch 595 batch loss 0.843043506 epoch total loss 0.836379886\n",
      "Trained batch 596 batch loss 0.77250272 epoch total loss 0.836272657\n",
      "Trained batch 597 batch loss 0.815684676 epoch total loss 0.836238205\n",
      "Trained batch 598 batch loss 0.861931086 epoch total loss 0.83628118\n",
      "Trained batch 599 batch loss 0.868345261 epoch total loss 0.836334705\n",
      "Trained batch 600 batch loss 0.898843348 epoch total loss 0.836438835\n",
      "Trained batch 601 batch loss 0.809832215 epoch total loss 0.836394608\n",
      "Trained batch 602 batch loss 0.812241137 epoch total loss 0.836354494\n",
      "Trained batch 603 batch loss 0.844810188 epoch total loss 0.836368561\n",
      "Trained batch 604 batch loss 0.862009168 epoch total loss 0.836411\n",
      "Trained batch 605 batch loss 0.862087846 epoch total loss 0.836453438\n",
      "Trained batch 606 batch loss 0.84209913 epoch total loss 0.836462736\n",
      "Trained batch 607 batch loss 0.83284539 epoch total loss 0.836456776\n",
      "Trained batch 608 batch loss 0.797776 epoch total loss 0.836393178\n",
      "Trained batch 609 batch loss 0.824851871 epoch total loss 0.836374283\n",
      "Trained batch 610 batch loss 0.768817425 epoch total loss 0.836263537\n",
      "Trained batch 611 batch loss 0.829860866 epoch total loss 0.836253047\n",
      "Trained batch 612 batch loss 0.791698575 epoch total loss 0.83618027\n",
      "Trained batch 613 batch loss 0.808101296 epoch total loss 0.836134434\n",
      "Trained batch 614 batch loss 0.817724288 epoch total loss 0.836104512\n",
      "Trained batch 615 batch loss 0.822254539 epoch total loss 0.836082\n",
      "Trained batch 616 batch loss 0.777647793 epoch total loss 0.835987151\n",
      "Trained batch 617 batch loss 0.808586836 epoch total loss 0.835942745\n",
      "Trained batch 618 batch loss 0.777208149 epoch total loss 0.835847735\n",
      "Trained batch 619 batch loss 0.805021226 epoch total loss 0.835797906\n",
      "Trained batch 620 batch loss 0.776154 epoch total loss 0.835701704\n",
      "Trained batch 621 batch loss 0.815442443 epoch total loss 0.8356691\n",
      "Trained batch 622 batch loss 0.75902915 epoch total loss 0.835545897\n",
      "Trained batch 623 batch loss 0.810562432 epoch total loss 0.835505724\n",
      "Trained batch 624 batch loss 0.701609671 epoch total loss 0.835291147\n",
      "Trained batch 625 batch loss 0.749700367 epoch total loss 0.835154176\n",
      "Trained batch 626 batch loss 0.786414742 epoch total loss 0.835076392\n",
      "Trained batch 627 batch loss 0.830176473 epoch total loss 0.835068583\n",
      "Trained batch 628 batch loss 0.774305224 epoch total loss 0.834971845\n",
      "Trained batch 629 batch loss 0.825342298 epoch total loss 0.834956467\n",
      "Trained batch 630 batch loss 0.847652555 epoch total loss 0.834976614\n",
      "Trained batch 631 batch loss 0.825597107 epoch total loss 0.834961832\n",
      "Trained batch 632 batch loss 0.832669199 epoch total loss 0.834958136\n",
      "Trained batch 633 batch loss 0.846497953 epoch total loss 0.834976375\n",
      "Trained batch 634 batch loss 0.821989536 epoch total loss 0.834955812\n",
      "Trained batch 635 batch loss 0.824306369 epoch total loss 0.834939\n",
      "Trained batch 636 batch loss 0.841869354 epoch total loss 0.834949911\n",
      "Trained batch 637 batch loss 0.804707289 epoch total loss 0.834902406\n",
      "Trained batch 638 batch loss 0.691473901 epoch total loss 0.834677577\n",
      "Trained batch 639 batch loss 0.809297919 epoch total loss 0.83463788\n",
      "Trained batch 640 batch loss 0.755108178 epoch total loss 0.834513664\n",
      "Trained batch 641 batch loss 0.817249537 epoch total loss 0.834486723\n",
      "Trained batch 642 batch loss 0.803181171 epoch total loss 0.834437966\n",
      "Trained batch 643 batch loss 0.801331699 epoch total loss 0.834386468\n",
      "Trained batch 644 batch loss 0.81197083 epoch total loss 0.834351599\n",
      "Trained batch 645 batch loss 0.747649968 epoch total loss 0.834217131\n",
      "Trained batch 646 batch loss 0.734032512 epoch total loss 0.83406204\n",
      "Trained batch 647 batch loss 0.732967257 epoch total loss 0.833905816\n",
      "Trained batch 648 batch loss 0.786249757 epoch total loss 0.833832264\n",
      "Trained batch 649 batch loss 0.793390691 epoch total loss 0.83377\n",
      "Trained batch 650 batch loss 0.751381934 epoch total loss 0.833643258\n",
      "Trained batch 651 batch loss 0.853518665 epoch total loss 0.833673775\n",
      "Trained batch 652 batch loss 0.904412568 epoch total loss 0.833782256\n",
      "Trained batch 653 batch loss 0.884013772 epoch total loss 0.833859205\n",
      "Trained batch 654 batch loss 0.829167843 epoch total loss 0.833852053\n",
      "Trained batch 655 batch loss 0.843340456 epoch total loss 0.833866477\n",
      "Trained batch 656 batch loss 0.793458343 epoch total loss 0.833804905\n",
      "Trained batch 657 batch loss 0.762993634 epoch total loss 0.83369714\n",
      "Trained batch 658 batch loss 0.775946617 epoch total loss 0.833609343\n",
      "Trained batch 659 batch loss 0.860161901 epoch total loss 0.833649635\n",
      "Trained batch 660 batch loss 0.813823819 epoch total loss 0.833619654\n",
      "Trained batch 661 batch loss 0.832721233 epoch total loss 0.833618283\n",
      "Trained batch 662 batch loss 0.752408 epoch total loss 0.833495557\n",
      "Trained batch 663 batch loss 0.816238761 epoch total loss 0.83346951\n",
      "Trained batch 664 batch loss 0.76971364 epoch total loss 0.833373487\n",
      "Trained batch 665 batch loss 0.789936781 epoch total loss 0.833308101\n",
      "Trained batch 666 batch loss 0.782639384 epoch total loss 0.833232045\n",
      "Trained batch 667 batch loss 0.75839597 epoch total loss 0.833119929\n",
      "Trained batch 668 batch loss 0.757928491 epoch total loss 0.833007336\n",
      "Trained batch 669 batch loss 0.822080433 epoch total loss 0.832991\n",
      "Trained batch 670 batch loss 0.731612146 epoch total loss 0.832839727\n",
      "Trained batch 671 batch loss 0.816314578 epoch total loss 0.832815051\n",
      "Trained batch 672 batch loss 0.818383 epoch total loss 0.832793534\n",
      "Trained batch 673 batch loss 0.816056371 epoch total loss 0.832768679\n",
      "Trained batch 674 batch loss 0.765865743 epoch total loss 0.832669377\n",
      "Trained batch 675 batch loss 0.804986954 epoch total loss 0.832628429\n",
      "Trained batch 676 batch loss 0.858535647 epoch total loss 0.832666695\n",
      "Trained batch 677 batch loss 0.805805802 epoch total loss 0.832627\n",
      "Trained batch 678 batch loss 0.835221291 epoch total loss 0.832630813\n",
      "Trained batch 679 batch loss 0.788453937 epoch total loss 0.832565725\n",
      "Trained batch 680 batch loss 0.786791682 epoch total loss 0.832498431\n",
      "Trained batch 681 batch loss 0.841783404 epoch total loss 0.832512081\n",
      "Trained batch 682 batch loss 0.820085824 epoch total loss 0.832493842\n",
      "Trained batch 683 batch loss 0.806736171 epoch total loss 0.832456172\n",
      "Trained batch 684 batch loss 0.809216619 epoch total loss 0.832422197\n",
      "Trained batch 685 batch loss 0.800371408 epoch total loss 0.832375348\n",
      "Trained batch 686 batch loss 0.798998594 epoch total loss 0.83232671\n",
      "Trained batch 687 batch loss 0.675125122 epoch total loss 0.832097888\n",
      "Trained batch 688 batch loss 0.791659951 epoch total loss 0.832039118\n",
      "Trained batch 689 batch loss 0.706875443 epoch total loss 0.831857443\n",
      "Trained batch 690 batch loss 0.757051885 epoch total loss 0.831749082\n",
      "Trained batch 691 batch loss 0.744357944 epoch total loss 0.83162266\n",
      "Trained batch 692 batch loss 0.776728213 epoch total loss 0.831543326\n",
      "Trained batch 693 batch loss 0.777410507 epoch total loss 0.831465185\n",
      "Trained batch 694 batch loss 0.80103457 epoch total loss 0.831421316\n",
      "Trained batch 695 batch loss 0.819833934 epoch total loss 0.831404626\n",
      "Trained batch 696 batch loss 0.77663523 epoch total loss 0.831325948\n",
      "Trained batch 697 batch loss 0.785689473 epoch total loss 0.831260502\n",
      "Trained batch 698 batch loss 0.777886391 epoch total loss 0.831184\n",
      "Trained batch 699 batch loss 0.784769654 epoch total loss 0.83111763\n",
      "Trained batch 700 batch loss 0.796978 epoch total loss 0.831068873\n",
      "Trained batch 701 batch loss 0.75521636 epoch total loss 0.830960631\n",
      "Trained batch 702 batch loss 0.781825662 epoch total loss 0.830890596\n",
      "Trained batch 703 batch loss 0.770441651 epoch total loss 0.830804646\n",
      "Trained batch 704 batch loss 0.813461065 epoch total loss 0.83078\n",
      "Trained batch 705 batch loss 0.806936145 epoch total loss 0.830746233\n",
      "Trained batch 706 batch loss 0.770473361 epoch total loss 0.83066082\n",
      "Trained batch 707 batch loss 0.776833951 epoch total loss 0.830584705\n",
      "Trained batch 708 batch loss 0.797944307 epoch total loss 0.83053863\n",
      "Trained batch 709 batch loss 0.778163433 epoch total loss 0.830464721\n",
      "Trained batch 710 batch loss 0.808514059 epoch total loss 0.830433846\n",
      "Trained batch 711 batch loss 0.780521214 epoch total loss 0.830363631\n",
      "Trained batch 712 batch loss 0.775349855 epoch total loss 0.830286324\n",
      "Trained batch 713 batch loss 0.775652349 epoch total loss 0.830209672\n",
      "Trained batch 714 batch loss 0.798355699 epoch total loss 0.830165088\n",
      "Trained batch 715 batch loss 0.767240644 epoch total loss 0.830077\n",
      "Trained batch 716 batch loss 0.795048773 epoch total loss 0.830028057\n",
      "Trained batch 717 batch loss 0.782061338 epoch total loss 0.829961181\n",
      "Trained batch 718 batch loss 0.82689774 epoch total loss 0.829956889\n",
      "Trained batch 719 batch loss 0.716031671 epoch total loss 0.8297984\n",
      "Trained batch 720 batch loss 0.711215496 epoch total loss 0.829633772\n",
      "Trained batch 721 batch loss 0.846035 epoch total loss 0.829656482\n",
      "Trained batch 722 batch loss 0.809376121 epoch total loss 0.829628408\n",
      "Trained batch 723 batch loss 0.884172618 epoch total loss 0.829703808\n",
      "Trained batch 724 batch loss 0.805791378 epoch total loss 0.829670787\n",
      "Trained batch 725 batch loss 0.847244263 epoch total loss 0.829695\n",
      "Trained batch 726 batch loss 0.78197068 epoch total loss 0.829629302\n",
      "Trained batch 727 batch loss 0.855904 epoch total loss 0.829665422\n",
      "Trained batch 728 batch loss 0.869507492 epoch total loss 0.82972014\n",
      "Trained batch 729 batch loss 0.933447 epoch total loss 0.829862475\n",
      "Trained batch 730 batch loss 0.854540765 epoch total loss 0.829896271\n",
      "Trained batch 731 batch loss 0.870508 epoch total loss 0.829951823\n",
      "Trained batch 732 batch loss 0.817820132 epoch total loss 0.829935193\n",
      "Trained batch 733 batch loss 0.813123167 epoch total loss 0.829912245\n",
      "Trained batch 734 batch loss 0.840440929 epoch total loss 0.82992661\n",
      "Trained batch 735 batch loss 0.854919195 epoch total loss 0.829960644\n",
      "Trained batch 736 batch loss 0.77871424 epoch total loss 0.829890966\n",
      "Trained batch 737 batch loss 0.759560585 epoch total loss 0.829795539\n",
      "Trained batch 738 batch loss 0.821730077 epoch total loss 0.829784632\n",
      "Trained batch 739 batch loss 0.759942532 epoch total loss 0.829690099\n",
      "Trained batch 740 batch loss 0.699886322 epoch total loss 0.829514682\n",
      "Trained batch 741 batch loss 0.814635634 epoch total loss 0.829494655\n",
      "Trained batch 742 batch loss 0.771376193 epoch total loss 0.829416275\n",
      "Trained batch 743 batch loss 0.874459624 epoch total loss 0.829476893\n",
      "Trained batch 744 batch loss 0.758243799 epoch total loss 0.829381168\n",
      "Trained batch 745 batch loss 0.798442602 epoch total loss 0.829339623\n",
      "Trained batch 746 batch loss 0.778507113 epoch total loss 0.829271495\n",
      "Trained batch 747 batch loss 0.837506115 epoch total loss 0.829282522\n",
      "Trained batch 748 batch loss 0.84059906 epoch total loss 0.829297662\n",
      "Trained batch 749 batch loss 0.837289095 epoch total loss 0.829308271\n",
      "Trained batch 750 batch loss 0.857466936 epoch total loss 0.829345882\n",
      "Trained batch 751 batch loss 0.756704 epoch total loss 0.829249144\n",
      "Trained batch 752 batch loss 0.752051055 epoch total loss 0.829146504\n",
      "Trained batch 753 batch loss 0.722133517 epoch total loss 0.829004347\n",
      "Trained batch 754 batch loss 0.772122502 epoch total loss 0.828928888\n",
      "Trained batch 755 batch loss 0.778379738 epoch total loss 0.828861952\n",
      "Trained batch 756 batch loss 0.744466066 epoch total loss 0.828750312\n",
      "Trained batch 757 batch loss 0.736148775 epoch total loss 0.828627944\n",
      "Trained batch 758 batch loss 0.764841735 epoch total loss 0.828543782\n",
      "Trained batch 759 batch loss 0.807916403 epoch total loss 0.828516603\n",
      "Trained batch 760 batch loss 0.826942205 epoch total loss 0.828514576\n",
      "Trained batch 761 batch loss 0.811419249 epoch total loss 0.828492105\n",
      "Trained batch 762 batch loss 0.832288146 epoch total loss 0.828497052\n",
      "Trained batch 763 batch loss 0.830120444 epoch total loss 0.828499198\n",
      "Trained batch 764 batch loss 0.777592182 epoch total loss 0.82843256\n",
      "Trained batch 765 batch loss 0.827771664 epoch total loss 0.828431666\n",
      "Trained batch 766 batch loss 0.828761458 epoch total loss 0.828432083\n",
      "Trained batch 767 batch loss 0.840324104 epoch total loss 0.82844758\n",
      "Trained batch 768 batch loss 0.805580914 epoch total loss 0.828417838\n",
      "Trained batch 769 batch loss 0.841448128 epoch total loss 0.828434765\n",
      "Trained batch 770 batch loss 0.842111409 epoch total loss 0.828452528\n",
      "Trained batch 771 batch loss 0.839354813 epoch total loss 0.828466654\n",
      "Trained batch 772 batch loss 0.799192 epoch total loss 0.828428745\n",
      "Trained batch 773 batch loss 0.790161371 epoch total loss 0.828379273\n",
      "Trained batch 774 batch loss 0.830485 epoch total loss 0.828382\n",
      "Trained batch 775 batch loss 0.787875 epoch total loss 0.828329742\n",
      "Trained batch 776 batch loss 0.785387695 epoch total loss 0.828274429\n",
      "Trained batch 777 batch loss 0.761529386 epoch total loss 0.828188539\n",
      "Trained batch 778 batch loss 0.820809066 epoch total loss 0.828179061\n",
      "Trained batch 779 batch loss 0.762266874 epoch total loss 0.828094423\n",
      "Trained batch 780 batch loss 0.803472042 epoch total loss 0.828062892\n",
      "Trained batch 781 batch loss 0.830446 epoch total loss 0.828065932\n",
      "Trained batch 782 batch loss 0.782161832 epoch total loss 0.828007221\n",
      "Trained batch 783 batch loss 0.723321 epoch total loss 0.827873528\n",
      "Trained batch 784 batch loss 0.762045205 epoch total loss 0.827789545\n",
      "Trained batch 785 batch loss 0.789939284 epoch total loss 0.827741265\n",
      "Trained batch 786 batch loss 0.741807 epoch total loss 0.827632\n",
      "Trained batch 787 batch loss 0.805035055 epoch total loss 0.827603281\n",
      "Trained batch 788 batch loss 0.81846118 epoch total loss 0.827591717\n",
      "Trained batch 789 batch loss 0.834621429 epoch total loss 0.827600598\n",
      "Trained batch 790 batch loss 0.84954828 epoch total loss 0.827628374\n",
      "Trained batch 791 batch loss 0.820818663 epoch total loss 0.827619731\n",
      "Trained batch 792 batch loss 0.848450541 epoch total loss 0.827646\n",
      "Trained batch 793 batch loss 0.850197852 epoch total loss 0.827674508\n",
      "Trained batch 794 batch loss 0.887058318 epoch total loss 0.827749312\n",
      "Trained batch 795 batch loss 0.85094142 epoch total loss 0.827778518\n",
      "Trained batch 796 batch loss 0.792432606 epoch total loss 0.827734113\n",
      "Trained batch 797 batch loss 0.641891062 epoch total loss 0.827500939\n",
      "Trained batch 798 batch loss 0.723337591 epoch total loss 0.827370405\n",
      "Trained batch 799 batch loss 0.726610601 epoch total loss 0.827244282\n",
      "Trained batch 800 batch loss 0.77839 epoch total loss 0.827183247\n",
      "Trained batch 801 batch loss 0.821361363 epoch total loss 0.827176\n",
      "Trained batch 802 batch loss 0.758491158 epoch total loss 0.827090323\n",
      "Trained batch 803 batch loss 0.747139633 epoch total loss 0.826990724\n",
      "Trained batch 804 batch loss 0.708633065 epoch total loss 0.8268435\n",
      "Trained batch 805 batch loss 0.784796596 epoch total loss 0.826791227\n",
      "Trained batch 806 batch loss 0.776770115 epoch total loss 0.826729238\n",
      "Trained batch 807 batch loss 0.780084252 epoch total loss 0.826671422\n",
      "Trained batch 808 batch loss 0.772819757 epoch total loss 0.826604784\n",
      "Trained batch 809 batch loss 0.797136307 epoch total loss 0.826568365\n",
      "Trained batch 810 batch loss 0.787542462 epoch total loss 0.826520145\n",
      "Trained batch 811 batch loss 0.789590478 epoch total loss 0.826474667\n",
      "Trained batch 812 batch loss 0.78478086 epoch total loss 0.826423287\n",
      "Trained batch 813 batch loss 0.762731314 epoch total loss 0.826344967\n",
      "Trained batch 814 batch loss 0.761049211 epoch total loss 0.826264799\n",
      "Trained batch 815 batch loss 0.764611483 epoch total loss 0.826189101\n",
      "Trained batch 816 batch loss 0.750209153 epoch total loss 0.826095939\n",
      "Trained batch 817 batch loss 0.820477188 epoch total loss 0.826089084\n",
      "Trained batch 818 batch loss 0.79222095 epoch total loss 0.826047719\n",
      "Trained batch 819 batch loss 0.800631762 epoch total loss 0.826016724\n",
      "Trained batch 820 batch loss 0.761844277 epoch total loss 0.825938463\n",
      "Trained batch 821 batch loss 0.754046679 epoch total loss 0.825850844\n",
      "Trained batch 822 batch loss 0.768179059 epoch total loss 0.82578069\n",
      "Trained batch 823 batch loss 0.74532181 epoch total loss 0.825682938\n",
      "Trained batch 824 batch loss 0.759855628 epoch total loss 0.825603\n",
      "Trained batch 825 batch loss 0.821161151 epoch total loss 0.825597644\n",
      "Trained batch 826 batch loss 0.797819376 epoch total loss 0.825563967\n",
      "Trained batch 827 batch loss 0.774456918 epoch total loss 0.825502157\n",
      "Trained batch 828 batch loss 0.757978261 epoch total loss 0.825420678\n",
      "Trained batch 829 batch loss 0.81731981 epoch total loss 0.825410903\n",
      "Trained batch 830 batch loss 0.755617 epoch total loss 0.8253268\n",
      "Trained batch 831 batch loss 0.798017621 epoch total loss 0.825293958\n",
      "Trained batch 832 batch loss 0.794512928 epoch total loss 0.825256944\n",
      "Trained batch 833 batch loss 0.834856749 epoch total loss 0.825268447\n",
      "Trained batch 834 batch loss 0.805227876 epoch total loss 0.825244427\n",
      "Trained batch 835 batch loss 0.778401852 epoch total loss 0.825188279\n",
      "Trained batch 836 batch loss 0.724796057 epoch total loss 0.825068176\n",
      "Trained batch 837 batch loss 0.806871176 epoch total loss 0.82504648\n",
      "Trained batch 838 batch loss 0.7764166 epoch total loss 0.824988484\n",
      "Trained batch 839 batch loss 0.785930276 epoch total loss 0.824941933\n",
      "Trained batch 840 batch loss 0.772248507 epoch total loss 0.824879229\n",
      "Trained batch 841 batch loss 0.781123221 epoch total loss 0.824827194\n",
      "Trained batch 842 batch loss 0.752751052 epoch total loss 0.824741602\n",
      "Trained batch 843 batch loss 0.781812191 epoch total loss 0.82469064\n",
      "Trained batch 844 batch loss 0.796575546 epoch total loss 0.824657321\n",
      "Trained batch 845 batch loss 0.794410527 epoch total loss 0.824621558\n",
      "Trained batch 846 batch loss 0.841752648 epoch total loss 0.824641824\n",
      "Trained batch 847 batch loss 0.855392575 epoch total loss 0.824678123\n",
      "Trained batch 848 batch loss 0.84765029 epoch total loss 0.824705243\n",
      "Trained batch 849 batch loss 0.832738757 epoch total loss 0.82471472\n",
      "Trained batch 850 batch loss 0.822001815 epoch total loss 0.824711561\n",
      "Trained batch 851 batch loss 0.824187696 epoch total loss 0.824710906\n",
      "Trained batch 852 batch loss 0.802325 epoch total loss 0.82468462\n",
      "Trained batch 853 batch loss 0.784926593 epoch total loss 0.824638\n",
      "Trained batch 854 batch loss 0.790387 epoch total loss 0.824597895\n",
      "Trained batch 855 batch loss 0.824946821 epoch total loss 0.824598312\n",
      "Trained batch 856 batch loss 0.818125725 epoch total loss 0.824590743\n",
      "Trained batch 857 batch loss 0.81687969 epoch total loss 0.824581742\n",
      "Trained batch 858 batch loss 0.796626568 epoch total loss 0.824549198\n",
      "Trained batch 859 batch loss 0.779445827 epoch total loss 0.824496627\n",
      "Trained batch 860 batch loss 0.806879222 epoch total loss 0.824476182\n",
      "Trained batch 861 batch loss 0.809320509 epoch total loss 0.824458539\n",
      "Trained batch 862 batch loss 0.80001092 epoch total loss 0.824430168\n",
      "Trained batch 863 batch loss 0.800611496 epoch total loss 0.824402571\n",
      "Trained batch 864 batch loss 0.78522259 epoch total loss 0.824357212\n",
      "Trained batch 865 batch loss 0.763143241 epoch total loss 0.824286401\n",
      "Trained batch 866 batch loss 0.76949203 epoch total loss 0.824223101\n",
      "Trained batch 867 batch loss 0.836811662 epoch total loss 0.824237645\n",
      "Trained batch 868 batch loss 0.851204336 epoch total loss 0.824268699\n",
      "Trained batch 869 batch loss 0.867924094 epoch total loss 0.824318886\n",
      "Trained batch 870 batch loss 0.870129943 epoch total loss 0.824371576\n",
      "Trained batch 871 batch loss 0.859482765 epoch total loss 0.824411869\n",
      "Trained batch 872 batch loss 0.854093313 epoch total loss 0.824445903\n",
      "Trained batch 873 batch loss 0.761457324 epoch total loss 0.824373722\n",
      "Trained batch 874 batch loss 0.674172819 epoch total loss 0.824201941\n",
      "Trained batch 875 batch loss 0.707627475 epoch total loss 0.824068725\n",
      "Trained batch 876 batch loss 0.749544501 epoch total loss 0.823983669\n",
      "Trained batch 877 batch loss 0.70764637 epoch total loss 0.823851\n",
      "Trained batch 878 batch loss 0.707847595 epoch total loss 0.823718846\n",
      "Trained batch 879 batch loss 0.84261179 epoch total loss 0.823740304\n",
      "Trained batch 880 batch loss 0.757153153 epoch total loss 0.823664665\n",
      "Trained batch 881 batch loss 0.698668599 epoch total loss 0.823522747\n",
      "Trained batch 882 batch loss 0.749363244 epoch total loss 0.823438704\n",
      "Trained batch 883 batch loss 0.762009 epoch total loss 0.823369145\n",
      "Trained batch 884 batch loss 0.765278339 epoch total loss 0.823303401\n",
      "Trained batch 885 batch loss 0.78693527 epoch total loss 0.823262334\n",
      "Trained batch 886 batch loss 0.760730267 epoch total loss 0.823191762\n",
      "Trained batch 887 batch loss 0.800972402 epoch total loss 0.823166728\n",
      "Trained batch 888 batch loss 0.82634306 epoch total loss 0.823170304\n",
      "Trained batch 889 batch loss 0.822546244 epoch total loss 0.823169589\n",
      "Trained batch 890 batch loss 0.779721141 epoch total loss 0.823120832\n",
      "Trained batch 891 batch loss 0.82772541 epoch total loss 0.823125958\n",
      "Trained batch 892 batch loss 0.838403702 epoch total loss 0.823143065\n",
      "Trained batch 893 batch loss 0.789676249 epoch total loss 0.823105574\n",
      "Trained batch 894 batch loss 0.777991056 epoch total loss 0.823055148\n",
      "Trained batch 895 batch loss 0.75771749 epoch total loss 0.822982073\n",
      "Trained batch 896 batch loss 0.780356526 epoch total loss 0.822934508\n",
      "Trained batch 897 batch loss 0.738499165 epoch total loss 0.822840393\n",
      "Trained batch 898 batch loss 0.809962213 epoch total loss 0.822826\n",
      "Trained batch 899 batch loss 0.786231577 epoch total loss 0.822785318\n",
      "Trained batch 900 batch loss 0.766222 epoch total loss 0.822722495\n",
      "Trained batch 901 batch loss 0.751863182 epoch total loss 0.822643876\n",
      "Trained batch 902 batch loss 0.785932779 epoch total loss 0.822603226\n",
      "Trained batch 903 batch loss 0.722303271 epoch total loss 0.822492123\n",
      "Trained batch 904 batch loss 0.726399601 epoch total loss 0.822385788\n",
      "Trained batch 905 batch loss 0.743474185 epoch total loss 0.822298586\n",
      "Trained batch 906 batch loss 0.726663589 epoch total loss 0.822193086\n",
      "Trained batch 907 batch loss 0.768132746 epoch total loss 0.822133482\n",
      "Trained batch 908 batch loss 0.746948779 epoch total loss 0.822050631\n",
      "Trained batch 909 batch loss 0.707516372 epoch total loss 0.821924686\n",
      "Trained batch 910 batch loss 0.691295445 epoch total loss 0.821781099\n",
      "Trained batch 911 batch loss 0.718845606 epoch total loss 0.821668148\n",
      "Trained batch 912 batch loss 0.764446139 epoch total loss 0.821605444\n",
      "Trained batch 913 batch loss 0.86063993 epoch total loss 0.82164818\n",
      "Trained batch 914 batch loss 0.818833172 epoch total loss 0.821645141\n",
      "Trained batch 915 batch loss 0.778307259 epoch total loss 0.821597755\n",
      "Trained batch 916 batch loss 0.796889126 epoch total loss 0.821570754\n",
      "Trained batch 917 batch loss 0.797757447 epoch total loss 0.821544766\n",
      "Trained batch 918 batch loss 0.794852912 epoch total loss 0.821515739\n",
      "Trained batch 919 batch loss 0.806455612 epoch total loss 0.821499348\n",
      "Trained batch 920 batch loss 0.783762693 epoch total loss 0.82145828\n",
      "Trained batch 921 batch loss 0.79817456 epoch total loss 0.821433\n",
      "Trained batch 922 batch loss 0.762366176 epoch total loss 0.821369\n",
      "Trained batch 923 batch loss 0.769666553 epoch total loss 0.821312964\n",
      "Trained batch 924 batch loss 0.763044 epoch total loss 0.821249902\n",
      "Trained batch 925 batch loss 0.797485292 epoch total loss 0.821224213\n",
      "Trained batch 926 batch loss 0.790169597 epoch total loss 0.821190655\n",
      "Trained batch 927 batch loss 0.781451583 epoch total loss 0.82114774\n",
      "Trained batch 928 batch loss 0.860232234 epoch total loss 0.82118988\n",
      "Trained batch 929 batch loss 0.776980639 epoch total loss 0.821142316\n",
      "Trained batch 930 batch loss 0.772342741 epoch total loss 0.821089804\n",
      "Trained batch 931 batch loss 0.755072534 epoch total loss 0.821018875\n",
      "Trained batch 932 batch loss 0.753421366 epoch total loss 0.820946336\n",
      "Trained batch 933 batch loss 0.752501667 epoch total loss 0.820873\n",
      "Trained batch 934 batch loss 0.782021403 epoch total loss 0.820831418\n",
      "Trained batch 935 batch loss 0.766161799 epoch total loss 0.820772946\n",
      "Trained batch 936 batch loss 0.756462 epoch total loss 0.820704281\n",
      "Trained batch 937 batch loss 0.792511642 epoch total loss 0.820674241\n",
      "Trained batch 938 batch loss 0.769986868 epoch total loss 0.82062012\n",
      "Trained batch 939 batch loss 0.756536782 epoch total loss 0.820551872\n",
      "Trained batch 940 batch loss 0.778339565 epoch total loss 0.820507\n",
      "Trained batch 941 batch loss 0.765539348 epoch total loss 0.820448577\n",
      "Trained batch 942 batch loss 0.751262128 epoch total loss 0.820375144\n",
      "Trained batch 943 batch loss 0.741335511 epoch total loss 0.82029134\n",
      "Trained batch 944 batch loss 0.762901723 epoch total loss 0.820230544\n",
      "Trained batch 945 batch loss 0.790374756 epoch total loss 0.820198953\n",
      "Trained batch 946 batch loss 0.79883492 epoch total loss 0.820176363\n",
      "Trained batch 947 batch loss 0.779918611 epoch total loss 0.820133865\n",
      "Trained batch 948 batch loss 0.808918 epoch total loss 0.820122\n",
      "Trained batch 949 batch loss 0.758361459 epoch total loss 0.820056915\n",
      "Trained batch 950 batch loss 0.77910006 epoch total loss 0.820013821\n",
      "Trained batch 951 batch loss 0.788419664 epoch total loss 0.819980562\n",
      "Trained batch 952 batch loss 0.802836895 epoch total loss 0.819962561\n",
      "Trained batch 953 batch loss 0.822758198 epoch total loss 0.819965482\n",
      "Trained batch 954 batch loss 0.813578844 epoch total loss 0.819958806\n",
      "Trained batch 955 batch loss 0.805799544 epoch total loss 0.819943964\n",
      "Trained batch 956 batch loss 0.833371341 epoch total loss 0.819958031\n",
      "Trained batch 957 batch loss 0.73100543 epoch total loss 0.819865108\n",
      "Trained batch 958 batch loss 0.755426764 epoch total loss 0.819797814\n",
      "Trained batch 959 batch loss 0.714406192 epoch total loss 0.819687963\n",
      "Trained batch 960 batch loss 0.812397838 epoch total loss 0.819680333\n",
      "Trained batch 961 batch loss 0.795515418 epoch total loss 0.81965524\n",
      "Trained batch 962 batch loss 0.832764447 epoch total loss 0.819668829\n",
      "Trained batch 963 batch loss 0.698832452 epoch total loss 0.819543362\n",
      "Trained batch 964 batch loss 0.687312663 epoch total loss 0.819406211\n",
      "Trained batch 965 batch loss 0.713614225 epoch total loss 0.819296598\n",
      "Trained batch 966 batch loss 0.783010542 epoch total loss 0.819259048\n",
      "Trained batch 967 batch loss 0.754906416 epoch total loss 0.819192469\n",
      "Trained batch 968 batch loss 0.788125336 epoch total loss 0.819160402\n",
      "Trained batch 969 batch loss 0.73524946 epoch total loss 0.819073796\n",
      "Trained batch 970 batch loss 0.781471372 epoch total loss 0.819035053\n",
      "Trained batch 971 batch loss 0.845420778 epoch total loss 0.819062173\n",
      "Trained batch 972 batch loss 0.771734178 epoch total loss 0.819013476\n",
      "Trained batch 973 batch loss 0.736897171 epoch total loss 0.818929076\n",
      "Trained batch 974 batch loss 0.731950879 epoch total loss 0.818839788\n",
      "Trained batch 975 batch loss 0.74701041 epoch total loss 0.818766117\n",
      "Trained batch 976 batch loss 0.756154478 epoch total loss 0.818701923\n",
      "Trained batch 977 batch loss 0.710198283 epoch total loss 0.818590879\n",
      "Trained batch 978 batch loss 0.769185066 epoch total loss 0.818540335\n",
      "Trained batch 979 batch loss 0.780875325 epoch total loss 0.81850189\n",
      "Trained batch 980 batch loss 0.723434508 epoch total loss 0.818404913\n",
      "Trained batch 981 batch loss 0.755597949 epoch total loss 0.818340898\n",
      "Trained batch 982 batch loss 0.721188247 epoch total loss 0.818241954\n",
      "Trained batch 983 batch loss 0.736175299 epoch total loss 0.818158448\n",
      "Trained batch 984 batch loss 0.711598694 epoch total loss 0.818050146\n",
      "Trained batch 985 batch loss 0.771646738 epoch total loss 0.818003058\n",
      "Trained batch 986 batch loss 0.791862845 epoch total loss 0.817976594\n",
      "Trained batch 987 batch loss 0.765667498 epoch total loss 0.817923605\n",
      "Trained batch 988 batch loss 0.834605694 epoch total loss 0.817940474\n",
      "Trained batch 989 batch loss 0.82905966 epoch total loss 0.817951679\n",
      "Trained batch 990 batch loss 0.869688332 epoch total loss 0.818003953\n",
      "Trained batch 991 batch loss 0.798766494 epoch total loss 0.817984521\n",
      "Trained batch 992 batch loss 0.879919052 epoch total loss 0.818047\n",
      "Trained batch 993 batch loss 0.845552087 epoch total loss 0.818074703\n",
      "Trained batch 994 batch loss 0.732165813 epoch total loss 0.817988336\n",
      "Trained batch 995 batch loss 0.790106 epoch total loss 0.817960262\n",
      "Trained batch 996 batch loss 0.803189039 epoch total loss 0.817945421\n",
      "Trained batch 997 batch loss 0.660150707 epoch total loss 0.81778717\n",
      "Trained batch 998 batch loss 0.798243523 epoch total loss 0.81776756\n",
      "Trained batch 999 batch loss 0.783643723 epoch total loss 0.817733407\n",
      "Trained batch 1000 batch loss 0.796852112 epoch total loss 0.817712545\n",
      "Trained batch 1001 batch loss 0.78272289 epoch total loss 0.817677557\n",
      "Trained batch 1002 batch loss 0.814392567 epoch total loss 0.817674279\n",
      "Trained batch 1003 batch loss 0.755429924 epoch total loss 0.817612231\n",
      "Trained batch 1004 batch loss 0.75023663 epoch total loss 0.817545116\n",
      "Trained batch 1005 batch loss 0.798909247 epoch total loss 0.817526579\n",
      "Trained batch 1006 batch loss 0.713469744 epoch total loss 0.817423105\n",
      "Trained batch 1007 batch loss 0.765004 epoch total loss 0.81737107\n",
      "Trained batch 1008 batch loss 0.709622085 epoch total loss 0.81726414\n",
      "Trained batch 1009 batch loss 0.640190303 epoch total loss 0.817088664\n",
      "Trained batch 1010 batch loss 0.6832093 epoch total loss 0.816956103\n",
      "Trained batch 1011 batch loss 0.724013746 epoch total loss 0.816864192\n",
      "Trained batch 1012 batch loss 0.745987952 epoch total loss 0.816794097\n",
      "Trained batch 1013 batch loss 0.745154262 epoch total loss 0.816723406\n",
      "Trained batch 1014 batch loss 0.726718903 epoch total loss 0.816634655\n",
      "Trained batch 1015 batch loss 0.706111431 epoch total loss 0.816525817\n",
      "Trained batch 1016 batch loss 0.761035264 epoch total loss 0.816471219\n",
      "Trained batch 1017 batch loss 0.770881295 epoch total loss 0.816426337\n",
      "Trained batch 1018 batch loss 0.72710216 epoch total loss 0.816338599\n",
      "Trained batch 1019 batch loss 0.711678803 epoch total loss 0.8162359\n",
      "Trained batch 1020 batch loss 0.799817741 epoch total loss 0.816219807\n",
      "Trained batch 1021 batch loss 0.703982234 epoch total loss 0.816109836\n",
      "Trained batch 1022 batch loss 0.750260234 epoch total loss 0.816045403\n",
      "Trained batch 1023 batch loss 0.733351111 epoch total loss 0.81596458\n",
      "Trained batch 1024 batch loss 0.794018328 epoch total loss 0.815943122\n",
      "Trained batch 1025 batch loss 0.725911796 epoch total loss 0.815855265\n",
      "Trained batch 1026 batch loss 0.877193451 epoch total loss 0.815915048\n",
      "Trained batch 1027 batch loss 0.735781848 epoch total loss 0.815837\n",
      "Trained batch 1028 batch loss 0.736936808 epoch total loss 0.815760255\n",
      "Trained batch 1029 batch loss 0.723329 epoch total loss 0.815670431\n",
      "Trained batch 1030 batch loss 0.672387064 epoch total loss 0.815531313\n",
      "Trained batch 1031 batch loss 0.839649498 epoch total loss 0.815554738\n",
      "Trained batch 1032 batch loss 0.736176431 epoch total loss 0.815477848\n",
      "Trained batch 1033 batch loss 0.794538438 epoch total loss 0.815457582\n",
      "Trained batch 1034 batch loss 0.779118657 epoch total loss 0.815422416\n",
      "Trained batch 1035 batch loss 0.779237747 epoch total loss 0.815387487\n",
      "Trained batch 1036 batch loss 0.754322886 epoch total loss 0.815328538\n",
      "Trained batch 1037 batch loss 0.754069448 epoch total loss 0.81526947\n",
      "Trained batch 1038 batch loss 0.826794147 epoch total loss 0.815280557\n",
      "Trained batch 1039 batch loss 0.802026808 epoch total loss 0.815267801\n",
      "Trained batch 1040 batch loss 0.849996448 epoch total loss 0.81530118\n",
      "Trained batch 1041 batch loss 0.784550667 epoch total loss 0.815271616\n",
      "Trained batch 1042 batch loss 0.8033728 epoch total loss 0.815260172\n",
      "Trained batch 1043 batch loss 0.792153418 epoch total loss 0.815238059\n",
      "Trained batch 1044 batch loss 0.808193505 epoch total loss 0.815231264\n",
      "Trained batch 1045 batch loss 0.777899802 epoch total loss 0.81519556\n",
      "Trained batch 1046 batch loss 0.749881 epoch total loss 0.815133095\n",
      "Trained batch 1047 batch loss 0.714789 epoch total loss 0.815037251\n",
      "Trained batch 1048 batch loss 0.738647 epoch total loss 0.814964354\n",
      "Trained batch 1049 batch loss 0.696579456 epoch total loss 0.814851522\n",
      "Trained batch 1050 batch loss 0.766767919 epoch total loss 0.814805746\n",
      "Trained batch 1051 batch loss 0.766019464 epoch total loss 0.814759314\n",
      "Trained batch 1052 batch loss 0.753404617 epoch total loss 0.814700961\n",
      "Trained batch 1053 batch loss 0.748060346 epoch total loss 0.814637661\n",
      "Trained batch 1054 batch loss 0.771120965 epoch total loss 0.814596415\n",
      "Trained batch 1055 batch loss 0.711703241 epoch total loss 0.814498901\n",
      "Trained batch 1056 batch loss 0.74843663 epoch total loss 0.814436316\n",
      "Trained batch 1057 batch loss 0.76596415 epoch total loss 0.814390481\n",
      "Trained batch 1058 batch loss 0.764521897 epoch total loss 0.814343333\n",
      "Trained batch 1059 batch loss 0.800887 epoch total loss 0.814330637\n",
      "Trained batch 1060 batch loss 0.674470246 epoch total loss 0.814198732\n",
      "Trained batch 1061 batch loss 0.733765185 epoch total loss 0.814122915\n",
      "Trained batch 1062 batch loss 0.775209069 epoch total loss 0.814086258\n",
      "Trained batch 1063 batch loss 0.784444034 epoch total loss 0.814058363\n",
      "Trained batch 1064 batch loss 0.769402683 epoch total loss 0.814016402\n",
      "Trained batch 1065 batch loss 0.804943919 epoch total loss 0.814007878\n",
      "Trained batch 1066 batch loss 0.753015637 epoch total loss 0.813950658\n",
      "Trained batch 1067 batch loss 0.782641828 epoch total loss 0.813921332\n",
      "Trained batch 1068 batch loss 0.79608041 epoch total loss 0.813904583\n",
      "Trained batch 1069 batch loss 0.84353435 epoch total loss 0.8139323\n",
      "Trained batch 1070 batch loss 0.848959446 epoch total loss 0.813965\n",
      "Trained batch 1071 batch loss 0.858245909 epoch total loss 0.814006388\n",
      "Trained batch 1072 batch loss 0.864251792 epoch total loss 0.814053237\n",
      "Trained batch 1073 batch loss 0.735335171 epoch total loss 0.813979924\n",
      "Trained batch 1074 batch loss 0.737330616 epoch total loss 0.813908517\n",
      "Trained batch 1075 batch loss 0.621674538 epoch total loss 0.813729703\n",
      "Trained batch 1076 batch loss 0.696079373 epoch total loss 0.813620389\n",
      "Trained batch 1077 batch loss 0.668980837 epoch total loss 0.813486159\n",
      "Trained batch 1078 batch loss 0.762597084 epoch total loss 0.813438892\n",
      "Trained batch 1079 batch loss 0.758936167 epoch total loss 0.813388348\n",
      "Trained batch 1080 batch loss 0.827059329 epoch total loss 0.813401043\n",
      "Trained batch 1081 batch loss 0.824845612 epoch total loss 0.813411653\n",
      "Trained batch 1082 batch loss 0.807323456 epoch total loss 0.813406\n",
      "Trained batch 1083 batch loss 0.802822471 epoch total loss 0.813396215\n",
      "Trained batch 1084 batch loss 0.798159182 epoch total loss 0.813382149\n",
      "Trained batch 1085 batch loss 0.729749322 epoch total loss 0.813305\n",
      "Trained batch 1086 batch loss 0.77610755 epoch total loss 0.813270807\n",
      "Trained batch 1087 batch loss 0.780998707 epoch total loss 0.813241124\n",
      "Trained batch 1088 batch loss 0.766267776 epoch total loss 0.813198\n",
      "Trained batch 1089 batch loss 0.778203726 epoch total loss 0.813165843\n",
      "Trained batch 1090 batch loss 0.811143339 epoch total loss 0.813164\n",
      "Trained batch 1091 batch loss 0.783579886 epoch total loss 0.813136876\n",
      "Trained batch 1092 batch loss 0.788679361 epoch total loss 0.813114464\n",
      "Trained batch 1093 batch loss 0.769520819 epoch total loss 0.813074589\n",
      "Trained batch 1094 batch loss 0.78828311 epoch total loss 0.813051939\n",
      "Trained batch 1095 batch loss 0.754187 epoch total loss 0.812998176\n",
      "Trained batch 1096 batch loss 0.749128222 epoch total loss 0.812939942\n",
      "Trained batch 1097 batch loss 0.748731315 epoch total loss 0.81288141\n",
      "Trained batch 1098 batch loss 0.715497136 epoch total loss 0.812792718\n",
      "Trained batch 1099 batch loss 0.761783838 epoch total loss 0.812746286\n",
      "Trained batch 1100 batch loss 0.76399 epoch total loss 0.812702\n",
      "Trained batch 1101 batch loss 0.800689638 epoch total loss 0.812691033\n",
      "Trained batch 1102 batch loss 0.782249093 epoch total loss 0.812663376\n",
      "Trained batch 1103 batch loss 0.740370035 epoch total loss 0.812597811\n",
      "Trained batch 1104 batch loss 0.787655771 epoch total loss 0.812575221\n",
      "Trained batch 1105 batch loss 0.823913574 epoch total loss 0.812585533\n",
      "Trained batch 1106 batch loss 0.751773953 epoch total loss 0.812530518\n",
      "Trained batch 1107 batch loss 0.699494123 epoch total loss 0.812428415\n",
      "Trained batch 1108 batch loss 0.756639481 epoch total loss 0.812378109\n",
      "Trained batch 1109 batch loss 0.690773129 epoch total loss 0.812268436\n",
      "Trained batch 1110 batch loss 0.680408955 epoch total loss 0.812149704\n",
      "Trained batch 1111 batch loss 0.757944345 epoch total loss 0.812100887\n",
      "Trained batch 1112 batch loss 0.766625226 epoch total loss 0.812059939\n",
      "Trained batch 1113 batch loss 0.820506215 epoch total loss 0.812067568\n",
      "Trained batch 1114 batch loss 0.770601153 epoch total loss 0.812030375\n",
      "Trained batch 1115 batch loss 0.805294573 epoch total loss 0.812024295\n",
      "Trained batch 1116 batch loss 0.741857648 epoch total loss 0.811961472\n",
      "Trained batch 1117 batch loss 0.794461966 epoch total loss 0.811945736\n",
      "Trained batch 1118 batch loss 0.74671638 epoch total loss 0.811887383\n",
      "Trained batch 1119 batch loss 0.820064962 epoch total loss 0.811894715\n",
      "Trained batch 1120 batch loss 0.828393102 epoch total loss 0.811909437\n",
      "Trained batch 1121 batch loss 0.775655091 epoch total loss 0.811877072\n",
      "Trained batch 1122 batch loss 0.785880148 epoch total loss 0.811853886\n",
      "Trained batch 1123 batch loss 0.816191256 epoch total loss 0.81185776\n",
      "Trained batch 1124 batch loss 0.784480453 epoch total loss 0.811833382\n",
      "Trained batch 1125 batch loss 0.770207882 epoch total loss 0.811796367\n",
      "Trained batch 1126 batch loss 0.759668112 epoch total loss 0.811750054\n",
      "Trained batch 1127 batch loss 0.741823196 epoch total loss 0.811688\n",
      "Trained batch 1128 batch loss 0.750949502 epoch total loss 0.811634183\n",
      "Trained batch 1129 batch loss 0.781167865 epoch total loss 0.811607242\n",
      "Trained batch 1130 batch loss 0.774905443 epoch total loss 0.811574757\n",
      "Trained batch 1131 batch loss 0.751548529 epoch total loss 0.811521649\n",
      "Trained batch 1132 batch loss 0.812018514 epoch total loss 0.811522067\n",
      "Trained batch 1133 batch loss 0.853382766 epoch total loss 0.811559\n",
      "Trained batch 1134 batch loss 0.891568124 epoch total loss 0.811629593\n",
      "Trained batch 1135 batch loss 0.843692124 epoch total loss 0.811657846\n",
      "Trained batch 1136 batch loss 0.830539107 epoch total loss 0.811674476\n",
      "Trained batch 1137 batch loss 0.850712 epoch total loss 0.811708808\n",
      "Trained batch 1138 batch loss 0.817175806 epoch total loss 0.811713636\n",
      "Trained batch 1139 batch loss 0.815576077 epoch total loss 0.811717\n",
      "Trained batch 1140 batch loss 0.808348715 epoch total loss 0.811714053\n",
      "Trained batch 1141 batch loss 0.706513643 epoch total loss 0.811621845\n",
      "Trained batch 1142 batch loss 0.736679733 epoch total loss 0.81155628\n",
      "Trained batch 1143 batch loss 0.785557866 epoch total loss 0.811533511\n",
      "Trained batch 1144 batch loss 0.700080454 epoch total loss 0.811436117\n",
      "Trained batch 1145 batch loss 0.745404184 epoch total loss 0.811378419\n",
      "Trained batch 1146 batch loss 0.786027431 epoch total loss 0.811356306\n",
      "Trained batch 1147 batch loss 0.811702 epoch total loss 0.811356604\n",
      "Trained batch 1148 batch loss 0.79735738 epoch total loss 0.811344445\n",
      "Trained batch 1149 batch loss 0.78113085 epoch total loss 0.811318099\n",
      "Trained batch 1150 batch loss 0.809423327 epoch total loss 0.81131649\n",
      "Trained batch 1151 batch loss 0.718349695 epoch total loss 0.811235726\n",
      "Trained batch 1152 batch loss 0.730749 epoch total loss 0.811165869\n",
      "Trained batch 1153 batch loss 0.713987708 epoch total loss 0.811081588\n",
      "Trained batch 1154 batch loss 0.743749082 epoch total loss 0.811023235\n",
      "Trained batch 1155 batch loss 0.745762885 epoch total loss 0.81096679\n",
      "Trained batch 1156 batch loss 0.747052908 epoch total loss 0.810911477\n",
      "Trained batch 1157 batch loss 0.713671148 epoch total loss 0.810827494\n",
      "Trained batch 1158 batch loss 0.746719122 epoch total loss 0.810772121\n",
      "Trained batch 1159 batch loss 0.821053863 epoch total loss 0.810780942\n",
      "Trained batch 1160 batch loss 0.849966288 epoch total loss 0.810814738\n",
      "Trained batch 1161 batch loss 0.878024161 epoch total loss 0.810872674\n",
      "Trained batch 1162 batch loss 0.850980282 epoch total loss 0.810907125\n",
      "Trained batch 1163 batch loss 0.887091517 epoch total loss 0.810972631\n",
      "Trained batch 1164 batch loss 0.829948723 epoch total loss 0.810988963\n",
      "Trained batch 1165 batch loss 0.832606435 epoch total loss 0.8110075\n",
      "Trained batch 1166 batch loss 0.763545752 epoch total loss 0.81096679\n",
      "Trained batch 1167 batch loss 0.763417304 epoch total loss 0.81092608\n",
      "Trained batch 1168 batch loss 0.808404863 epoch total loss 0.810923874\n",
      "Trained batch 1169 batch loss 0.875297189 epoch total loss 0.810978949\n",
      "Trained batch 1170 batch loss 0.860467076 epoch total loss 0.811021268\n",
      "Trained batch 1171 batch loss 0.851544261 epoch total loss 0.811055899\n",
      "Trained batch 1172 batch loss 0.780081511 epoch total loss 0.811029494\n",
      "Trained batch 1173 batch loss 0.726901889 epoch total loss 0.810957789\n",
      "Trained batch 1174 batch loss 0.721038878 epoch total loss 0.810881197\n",
      "Trained batch 1175 batch loss 0.748735964 epoch total loss 0.810828328\n",
      "Trained batch 1176 batch loss 0.732416451 epoch total loss 0.810761631\n",
      "Trained batch 1177 batch loss 0.718720138 epoch total loss 0.810683489\n",
      "Trained batch 1178 batch loss 0.790105939 epoch total loss 0.810665965\n",
      "Trained batch 1179 batch loss 0.786005616 epoch total loss 0.810645103\n",
      "Trained batch 1180 batch loss 0.68621558 epoch total loss 0.810539663\n",
      "Trained batch 1181 batch loss 0.76047051 epoch total loss 0.810497284\n",
      "Trained batch 1182 batch loss 0.731577754 epoch total loss 0.810430467\n",
      "Trained batch 1183 batch loss 0.736164927 epoch total loss 0.810367703\n",
      "Trained batch 1184 batch loss 0.703657269 epoch total loss 0.810277581\n",
      "Trained batch 1185 batch loss 0.752417684 epoch total loss 0.810228765\n",
      "Trained batch 1186 batch loss 0.700735867 epoch total loss 0.810136437\n",
      "Trained batch 1187 batch loss 0.737273455 epoch total loss 0.810075045\n",
      "Trained batch 1188 batch loss 0.753448784 epoch total loss 0.810027421\n",
      "Trained batch 1189 batch loss 0.733401477 epoch total loss 0.809963\n",
      "Trained batch 1190 batch loss 0.704630136 epoch total loss 0.809874475\n",
      "Trained batch 1191 batch loss 0.777011037 epoch total loss 0.809846878\n",
      "Trained batch 1192 batch loss 0.784360766 epoch total loss 0.80982554\n",
      "Trained batch 1193 batch loss 0.764650404 epoch total loss 0.809787631\n",
      "Trained batch 1194 batch loss 0.751497507 epoch total loss 0.809738874\n",
      "Trained batch 1195 batch loss 0.709067345 epoch total loss 0.809654593\n",
      "Trained batch 1196 batch loss 0.653698146 epoch total loss 0.809524179\n",
      "Trained batch 1197 batch loss 0.757236838 epoch total loss 0.809480548\n",
      "Trained batch 1198 batch loss 0.760473 epoch total loss 0.809439659\n",
      "Trained batch 1199 batch loss 0.758627236 epoch total loss 0.809397221\n",
      "Trained batch 1200 batch loss 0.796440423 epoch total loss 0.809386432\n",
      "Trained batch 1201 batch loss 0.798029304 epoch total loss 0.809377\n",
      "Trained batch 1202 batch loss 0.756548524 epoch total loss 0.809333\n",
      "Trained batch 1203 batch loss 0.751239 epoch total loss 0.809284747\n",
      "Trained batch 1204 batch loss 0.768162 epoch total loss 0.809250593\n",
      "Trained batch 1205 batch loss 0.768129289 epoch total loss 0.80921644\n",
      "Trained batch 1206 batch loss 0.739111423 epoch total loss 0.809158325\n",
      "Trained batch 1207 batch loss 0.701310813 epoch total loss 0.809069\n",
      "Trained batch 1208 batch loss 0.74121958 epoch total loss 0.80901283\n",
      "Trained batch 1209 batch loss 0.711379349 epoch total loss 0.808932066\n",
      "Trained batch 1210 batch loss 0.685688376 epoch total loss 0.808830202\n",
      "Trained batch 1211 batch loss 0.781137407 epoch total loss 0.808807313\n",
      "Trained batch 1212 batch loss 0.72826004 epoch total loss 0.808740854\n",
      "Trained batch 1213 batch loss 0.687464178 epoch total loss 0.808640838\n",
      "Trained batch 1214 batch loss 0.665221214 epoch total loss 0.808522701\n",
      "Trained batch 1215 batch loss 0.757723 epoch total loss 0.808480918\n",
      "Trained batch 1216 batch loss 0.707578182 epoch total loss 0.808397949\n",
      "Trained batch 1217 batch loss 0.725011468 epoch total loss 0.808329463\n",
      "Trained batch 1218 batch loss 0.726502538 epoch total loss 0.808262289\n",
      "Trained batch 1219 batch loss 0.731527388 epoch total loss 0.808199286\n",
      "Trained batch 1220 batch loss 0.621917903 epoch total loss 0.808046639\n",
      "Trained batch 1221 batch loss 0.636222124 epoch total loss 0.807905912\n",
      "Trained batch 1222 batch loss 0.653099656 epoch total loss 0.807779193\n",
      "Trained batch 1223 batch loss 0.638284147 epoch total loss 0.807640672\n",
      "Trained batch 1224 batch loss 0.772351503 epoch total loss 0.807611823\n",
      "Trained batch 1225 batch loss 0.704783618 epoch total loss 0.80752784\n",
      "Trained batch 1226 batch loss 0.740782917 epoch total loss 0.807473421\n",
      "Trained batch 1227 batch loss 0.723899 epoch total loss 0.807405293\n",
      "Trained batch 1228 batch loss 0.683649361 epoch total loss 0.807304502\n",
      "Trained batch 1229 batch loss 0.631364048 epoch total loss 0.807161331\n",
      "Trained batch 1230 batch loss 0.736501098 epoch total loss 0.807103872\n",
      "Trained batch 1231 batch loss 0.723375678 epoch total loss 0.807035863\n",
      "Trained batch 1232 batch loss 0.750695288 epoch total loss 0.806990147\n",
      "Trained batch 1233 batch loss 0.822082937 epoch total loss 0.807002366\n",
      "Trained batch 1234 batch loss 0.702482641 epoch total loss 0.806917667\n",
      "Trained batch 1235 batch loss 0.729141891 epoch total loss 0.806854665\n",
      "Trained batch 1236 batch loss 0.702830434 epoch total loss 0.806770504\n",
      "Trained batch 1237 batch loss 0.74571085 epoch total loss 0.806721151\n",
      "Trained batch 1238 batch loss 0.697622836 epoch total loss 0.806633055\n",
      "Trained batch 1239 batch loss 0.735523462 epoch total loss 0.806575656\n",
      "Trained batch 1240 batch loss 0.730384052 epoch total loss 0.806514204\n",
      "Trained batch 1241 batch loss 0.749055445 epoch total loss 0.80646795\n",
      "Trained batch 1242 batch loss 0.783270538 epoch total loss 0.806449234\n",
      "Trained batch 1243 batch loss 0.798755109 epoch total loss 0.806443095\n",
      "Trained batch 1244 batch loss 0.708337665 epoch total loss 0.806364179\n",
      "Trained batch 1245 batch loss 0.531425953 epoch total loss 0.806143343\n",
      "Trained batch 1246 batch loss 0.586042464 epoch total loss 0.805966735\n",
      "Trained batch 1247 batch loss 0.576301873 epoch total loss 0.805782557\n",
      "Trained batch 1248 batch loss 0.54005748 epoch total loss 0.805569649\n",
      "Trained batch 1249 batch loss 0.682411969 epoch total loss 0.805471063\n",
      "Trained batch 1250 batch loss 0.713529468 epoch total loss 0.805397451\n",
      "Trained batch 1251 batch loss 0.832345247 epoch total loss 0.805418968\n",
      "Trained batch 1252 batch loss 0.873290658 epoch total loss 0.805473208\n",
      "Trained batch 1253 batch loss 0.856177807 epoch total loss 0.80551368\n",
      "Trained batch 1254 batch loss 0.877919912 epoch total loss 0.805571437\n",
      "Trained batch 1255 batch loss 0.809050441 epoch total loss 0.805574179\n",
      "Trained batch 1256 batch loss 0.813511968 epoch total loss 0.805580556\n",
      "Trained batch 1257 batch loss 0.812101781 epoch total loss 0.805585682\n",
      "Trained batch 1258 batch loss 0.754072845 epoch total loss 0.805544734\n",
      "Trained batch 1259 batch loss 0.754861116 epoch total loss 0.805504501\n",
      "Trained batch 1260 batch loss 0.834127605 epoch total loss 0.80552721\n",
      "Trained batch 1261 batch loss 0.669770658 epoch total loss 0.805419564\n",
      "Trained batch 1262 batch loss 0.736385167 epoch total loss 0.805364907\n",
      "Trained batch 1263 batch loss 0.671387434 epoch total loss 0.805258811\n",
      "Trained batch 1264 batch loss 0.781991422 epoch total loss 0.805240393\n",
      "Trained batch 1265 batch loss 0.722427368 epoch total loss 0.805174887\n",
      "Trained batch 1266 batch loss 0.751563191 epoch total loss 0.805132568\n",
      "Trained batch 1267 batch loss 0.827559173 epoch total loss 0.80515027\n",
      "Trained batch 1268 batch loss 0.657427132 epoch total loss 0.805033803\n",
      "Trained batch 1269 batch loss 0.616218805 epoch total loss 0.80488497\n",
      "Trained batch 1270 batch loss 0.700945377 epoch total loss 0.804803133\n",
      "Trained batch 1271 batch loss 0.767065287 epoch total loss 0.80477345\n",
      "Trained batch 1272 batch loss 0.768230319 epoch total loss 0.80474472\n",
      "Trained batch 1273 batch loss 0.834294438 epoch total loss 0.804767966\n",
      "Trained batch 1274 batch loss 0.795076609 epoch total loss 0.804760337\n",
      "Trained batch 1275 batch loss 0.810412943 epoch total loss 0.804764748\n",
      "Trained batch 1276 batch loss 0.792200804 epoch total loss 0.804755\n",
      "Trained batch 1277 batch loss 0.762658596 epoch total loss 0.804722\n",
      "Trained batch 1278 batch loss 0.742795229 epoch total loss 0.804673553\n",
      "Trained batch 1279 batch loss 0.731438637 epoch total loss 0.804616272\n",
      "Trained batch 1280 batch loss 0.735780358 epoch total loss 0.804562569\n",
      "Trained batch 1281 batch loss 0.721886516 epoch total loss 0.804498076\n",
      "Trained batch 1282 batch loss 0.739936292 epoch total loss 0.804447711\n",
      "Trained batch 1283 batch loss 0.716328 epoch total loss 0.804379046\n",
      "Trained batch 1284 batch loss 0.685075879 epoch total loss 0.804286122\n",
      "Trained batch 1285 batch loss 0.711207807 epoch total loss 0.804213643\n",
      "Trained batch 1286 batch loss 0.684981704 epoch total loss 0.804120898\n",
      "Trained batch 1287 batch loss 0.721368253 epoch total loss 0.804056585\n",
      "Trained batch 1288 batch loss 0.663538456 epoch total loss 0.803947508\n",
      "Trained batch 1289 batch loss 0.710913777 epoch total loss 0.803875327\n",
      "Trained batch 1290 batch loss 0.729861856 epoch total loss 0.803818\n",
      "Trained batch 1291 batch loss 0.776784301 epoch total loss 0.803797\n",
      "Trained batch 1292 batch loss 0.763663948 epoch total loss 0.803765953\n",
      "Trained batch 1293 batch loss 0.800687253 epoch total loss 0.803763509\n",
      "Trained batch 1294 batch loss 0.758165359 epoch total loss 0.803728282\n",
      "Trained batch 1295 batch loss 0.737155259 epoch total loss 0.803676903\n",
      "Trained batch 1296 batch loss 0.829622447 epoch total loss 0.80369693\n",
      "Trained batch 1297 batch loss 0.775754273 epoch total loss 0.803675354\n",
      "Trained batch 1298 batch loss 0.785871863 epoch total loss 0.803661644\n",
      "Trained batch 1299 batch loss 0.750307202 epoch total loss 0.803620636\n",
      "Trained batch 1300 batch loss 0.7314201 epoch total loss 0.803565085\n",
      "Trained batch 1301 batch loss 0.728077829 epoch total loss 0.80350703\n",
      "Trained batch 1302 batch loss 0.630879819 epoch total loss 0.803374469\n",
      "Trained batch 1303 batch loss 0.601848364 epoch total loss 0.803219736\n",
      "Trained batch 1304 batch loss 0.596279085 epoch total loss 0.803061068\n",
      "Trained batch 1305 batch loss 0.574182272 epoch total loss 0.802885711\n",
      "Trained batch 1306 batch loss 0.543284774 epoch total loss 0.802687\n",
      "Trained batch 1307 batch loss 0.742394507 epoch total loss 0.802640855\n",
      "Trained batch 1308 batch loss 0.764652729 epoch total loss 0.802611828\n",
      "Trained batch 1309 batch loss 0.830399215 epoch total loss 0.802633107\n",
      "Trained batch 1310 batch loss 0.850672722 epoch total loss 0.802669823\n",
      "Trained batch 1311 batch loss 0.919581056 epoch total loss 0.802758932\n",
      "Trained batch 1312 batch loss 0.933904 epoch total loss 0.802858949\n",
      "Trained batch 1313 batch loss 0.887732387 epoch total loss 0.80292356\n",
      "Trained batch 1314 batch loss 0.658443093 epoch total loss 0.80281359\n",
      "Trained batch 1315 batch loss 0.74597168 epoch total loss 0.802770376\n",
      "Trained batch 1316 batch loss 0.74827677 epoch total loss 0.802729\n",
      "Trained batch 1317 batch loss 0.68498075 epoch total loss 0.802639544\n",
      "Trained batch 1318 batch loss 0.733932793 epoch total loss 0.80258739\n",
      "Trained batch 1319 batch loss 0.802034438 epoch total loss 0.802587\n",
      "Trained batch 1320 batch loss 0.821540058 epoch total loss 0.802601278\n",
      "Trained batch 1321 batch loss 0.809819579 epoch total loss 0.802606761\n",
      "Trained batch 1322 batch loss 0.805739284 epoch total loss 0.802609146\n",
      "Trained batch 1323 batch loss 0.802607298 epoch total loss 0.802609146\n",
      "Trained batch 1324 batch loss 0.773926258 epoch total loss 0.802587509\n",
      "Trained batch 1325 batch loss 0.748692155 epoch total loss 0.802546799\n",
      "Trained batch 1326 batch loss 0.759707153 epoch total loss 0.802514553\n",
      "Trained batch 1327 batch loss 0.744250476 epoch total loss 0.802470624\n",
      "Trained batch 1328 batch loss 0.802045047 epoch total loss 0.802470267\n",
      "Trained batch 1329 batch loss 0.827615857 epoch total loss 0.802489221\n",
      "Trained batch 1330 batch loss 0.798326552 epoch total loss 0.802486122\n",
      "Trained batch 1331 batch loss 0.759499907 epoch total loss 0.802453816\n",
      "Trained batch 1332 batch loss 0.730475783 epoch total loss 0.802399755\n",
      "Trained batch 1333 batch loss 0.75327909 epoch total loss 0.802362919\n",
      "Trained batch 1334 batch loss 0.758002877 epoch total loss 0.802329719\n",
      "Trained batch 1335 batch loss 0.727356195 epoch total loss 0.802273631\n",
      "Trained batch 1336 batch loss 0.745174468 epoch total loss 0.802230835\n",
      "Trained batch 1337 batch loss 0.717861176 epoch total loss 0.802167773\n",
      "Trained batch 1338 batch loss 0.778468788 epoch total loss 0.80215\n",
      "Trained batch 1339 batch loss 0.780717313 epoch total loss 0.802134037\n",
      "Trained batch 1340 batch loss 0.794596195 epoch total loss 0.802128375\n",
      "Trained batch 1341 batch loss 0.759356141 epoch total loss 0.802096546\n",
      "Trained batch 1342 batch loss 0.759428263 epoch total loss 0.802064717\n",
      "Trained batch 1343 batch loss 0.695653915 epoch total loss 0.801985502\n",
      "Trained batch 1344 batch loss 0.666854203 epoch total loss 0.801884949\n",
      "Trained batch 1345 batch loss 0.643736601 epoch total loss 0.801767349\n",
      "Trained batch 1346 batch loss 0.71336019 epoch total loss 0.801701665\n",
      "Trained batch 1347 batch loss 0.689985037 epoch total loss 0.801618695\n",
      "Trained batch 1348 batch loss 0.743751 epoch total loss 0.80157578\n",
      "Trained batch 1349 batch loss 0.738405764 epoch total loss 0.801529\n",
      "Trained batch 1350 batch loss 0.773779273 epoch total loss 0.801508427\n",
      "Trained batch 1351 batch loss 0.732559443 epoch total loss 0.801457405\n",
      "Trained batch 1352 batch loss 0.783393264 epoch total loss 0.801444054\n",
      "Trained batch 1353 batch loss 0.640969 epoch total loss 0.80132544\n",
      "Trained batch 1354 batch loss 0.74642241 epoch total loss 0.801284969\n",
      "Trained batch 1355 batch loss 0.707492173 epoch total loss 0.801215768\n",
      "Trained batch 1356 batch loss 0.729268074 epoch total loss 0.80116266\n",
      "Trained batch 1357 batch loss 0.76608634 epoch total loss 0.801136851\n",
      "Trained batch 1358 batch loss 0.726320326 epoch total loss 0.801081777\n",
      "Trained batch 1359 batch loss 0.688278377 epoch total loss 0.800998688\n",
      "Trained batch 1360 batch loss 0.803017 epoch total loss 0.801000178\n",
      "Trained batch 1361 batch loss 0.830610216 epoch total loss 0.801021874\n",
      "Trained batch 1362 batch loss 0.789978325 epoch total loss 0.801013827\n",
      "Trained batch 1363 batch loss 0.648400247 epoch total loss 0.80090189\n",
      "Trained batch 1364 batch loss 0.705075383 epoch total loss 0.800831616\n",
      "Trained batch 1365 batch loss 0.623999596 epoch total loss 0.800702095\n",
      "Trained batch 1366 batch loss 0.609144688 epoch total loss 0.800561845\n",
      "Trained batch 1367 batch loss 0.63468492 epoch total loss 0.80044049\n",
      "Trained batch 1368 batch loss 0.654823184 epoch total loss 0.800334036\n",
      "Trained batch 1369 batch loss 0.733656645 epoch total loss 0.80028528\n",
      "Trained batch 1370 batch loss 0.795969546 epoch total loss 0.80028218\n",
      "Trained batch 1371 batch loss 0.683896899 epoch total loss 0.800197244\n",
      "Trained batch 1372 batch loss 0.690747619 epoch total loss 0.800117493\n",
      "Trained batch 1373 batch loss 0.676814139 epoch total loss 0.800027668\n",
      "Trained batch 1374 batch loss 0.721753299 epoch total loss 0.799970746\n",
      "Trained batch 1375 batch loss 0.736381531 epoch total loss 0.799924433\n",
      "Trained batch 1376 batch loss 0.790206313 epoch total loss 0.79991734\n",
      "Trained batch 1377 batch loss 0.765360057 epoch total loss 0.799892247\n",
      "Trained batch 1378 batch loss 0.733441055 epoch total loss 0.799844\n",
      "Trained batch 1379 batch loss 0.743566394 epoch total loss 0.799803197\n",
      "Trained batch 1380 batch loss 0.779631734 epoch total loss 0.799788594\n",
      "Trained batch 1381 batch loss 0.71202606 epoch total loss 0.799725056\n",
      "Trained batch 1382 batch loss 0.787038386 epoch total loss 0.799715817\n",
      "Trained batch 1383 batch loss 0.66974932 epoch total loss 0.79962188\n",
      "Trained batch 1384 batch loss 0.883683741 epoch total loss 0.799682617\n",
      "Trained batch 1385 batch loss 0.905537784 epoch total loss 0.79975903\n",
      "Trained batch 1386 batch loss 0.81517309 epoch total loss 0.799770176\n",
      "Trained batch 1387 batch loss 0.871444225 epoch total loss 0.799821854\n",
      "Trained batch 1388 batch loss 0.74296397 epoch total loss 0.799780846\n",
      "Trained batch 1389 batch loss 0.708608747 epoch total loss 0.799715221\n",
      "Trained batch 1390 batch loss 0.674048185 epoch total loss 0.79962486\n",
      "Trained batch 1391 batch loss 0.772391677 epoch total loss 0.799605191\n",
      "Trained batch 1392 batch loss 0.825794339 epoch total loss 0.799624\n",
      "Trained batch 1393 batch loss 0.715898037 epoch total loss 0.799563944\n",
      "Trained batch 1394 batch loss 0.79976511 epoch total loss 0.799564123\n",
      "Trained batch 1395 batch loss 0.79732883 epoch total loss 0.799562573\n",
      "Trained batch 1396 batch loss 0.799348354 epoch total loss 0.799562395\n",
      "Trained batch 1397 batch loss 0.777873933 epoch total loss 0.799546838\n",
      "Trained batch 1398 batch loss 0.779444337 epoch total loss 0.799532413\n",
      "Trained batch 1399 batch loss 0.765844822 epoch total loss 0.799508393\n",
      "Trained batch 1400 batch loss 0.744861722 epoch total loss 0.799469352\n",
      "Trained batch 1401 batch loss 0.720187426 epoch total loss 0.799412787\n",
      "Trained batch 1402 batch loss 0.77656424 epoch total loss 0.799396515\n",
      "Trained batch 1403 batch loss 0.705945194 epoch total loss 0.799329877\n",
      "Trained batch 1404 batch loss 0.74660176 epoch total loss 0.799292326\n",
      "Trained batch 1405 batch loss 0.75780493 epoch total loss 0.799262822\n",
      "Trained batch 1406 batch loss 0.792763948 epoch total loss 0.799258173\n",
      "Trained batch 1407 batch loss 0.784332395 epoch total loss 0.799247503\n",
      "Trained batch 1408 batch loss 0.748826146 epoch total loss 0.799211681\n",
      "Trained batch 1409 batch loss 0.744351327 epoch total loss 0.799172759\n",
      "Trained batch 1410 batch loss 0.732356787 epoch total loss 0.799125314\n",
      "Trained batch 1411 batch loss 0.734768271 epoch total loss 0.799079716\n",
      "Trained batch 1412 batch loss 0.725667477 epoch total loss 0.799027741\n",
      "Trained batch 1413 batch loss 0.749834776 epoch total loss 0.798992932\n",
      "Trained batch 1414 batch loss 0.775233567 epoch total loss 0.798976183\n",
      "Trained batch 1415 batch loss 0.653983474 epoch total loss 0.798873663\n",
      "Trained batch 1416 batch loss 0.687962174 epoch total loss 0.798795342\n",
      "Trained batch 1417 batch loss 0.698606968 epoch total loss 0.798724651\n",
      "Trained batch 1418 batch loss 0.725986183 epoch total loss 0.798673332\n",
      "Trained batch 1419 batch loss 0.651495695 epoch total loss 0.79856962\n",
      "Trained batch 1420 batch loss 0.656883895 epoch total loss 0.798469841\n",
      "Trained batch 1421 batch loss 0.627624691 epoch total loss 0.798349619\n",
      "Trained batch 1422 batch loss 0.645199 epoch total loss 0.798241913\n",
      "Trained batch 1423 batch loss 0.701508939 epoch total loss 0.798173964\n",
      "Trained batch 1424 batch loss 0.763797462 epoch total loss 0.798149824\n",
      "Trained batch 1425 batch loss 0.805726349 epoch total loss 0.798155129\n",
      "Trained batch 1426 batch loss 0.77929616 epoch total loss 0.798141956\n",
      "Trained batch 1427 batch loss 0.784091353 epoch total loss 0.798132062\n",
      "Trained batch 1428 batch loss 0.809458315 epoch total loss 0.79814\n",
      "Trained batch 1429 batch loss 0.86226517 epoch total loss 0.798184872\n",
      "Trained batch 1430 batch loss 0.792822182 epoch total loss 0.798181176\n",
      "Trained batch 1431 batch loss 0.812668741 epoch total loss 0.798191249\n",
      "Trained batch 1432 batch loss 0.838236332 epoch total loss 0.798219204\n",
      "Trained batch 1433 batch loss 0.804379225 epoch total loss 0.798223495\n",
      "Trained batch 1434 batch loss 0.817850947 epoch total loss 0.798237205\n",
      "Trained batch 1435 batch loss 0.845299721 epoch total loss 0.79827\n",
      "Trained batch 1436 batch loss 0.813705564 epoch total loss 0.798280776\n",
      "Trained batch 1437 batch loss 0.785873353 epoch total loss 0.798272133\n",
      "Trained batch 1438 batch loss 0.822459579 epoch total loss 0.798289\n",
      "Trained batch 1439 batch loss 0.732880354 epoch total loss 0.798243582\n",
      "Trained batch 1440 batch loss 0.711214185 epoch total loss 0.798183084\n",
      "Trained batch 1441 batch loss 0.710319519 epoch total loss 0.798122108\n",
      "Trained batch 1442 batch loss 0.757158279 epoch total loss 0.798093736\n",
      "Trained batch 1443 batch loss 0.721042216 epoch total loss 0.79804039\n",
      "Trained batch 1444 batch loss 0.751202047 epoch total loss 0.798007965\n",
      "Trained batch 1445 batch loss 0.769767523 epoch total loss 0.797988415\n",
      "Trained batch 1446 batch loss 0.742354393 epoch total loss 0.79794991\n",
      "Trained batch 1447 batch loss 0.713475525 epoch total loss 0.797891557\n",
      "Trained batch 1448 batch loss 0.763798416 epoch total loss 0.797868\n",
      "Trained batch 1449 batch loss 0.717270374 epoch total loss 0.797812402\n",
      "Trained batch 1450 batch loss 0.759632289 epoch total loss 0.797786057\n",
      "Trained batch 1451 batch loss 0.837260783 epoch total loss 0.797813296\n",
      "Trained batch 1452 batch loss 0.772576511 epoch total loss 0.797795892\n",
      "Trained batch 1453 batch loss 0.685803294 epoch total loss 0.797718823\n",
      "Trained batch 1454 batch loss 0.710267961 epoch total loss 0.797658741\n",
      "Trained batch 1455 batch loss 0.724773407 epoch total loss 0.797608614\n",
      "Trained batch 1456 batch loss 0.749142945 epoch total loss 0.797575295\n",
      "Trained batch 1457 batch loss 0.812527716 epoch total loss 0.797585547\n",
      "Trained batch 1458 batch loss 0.776909351 epoch total loss 0.797571361\n",
      "Trained batch 1459 batch loss 0.767614365 epoch total loss 0.797550797\n",
      "Trained batch 1460 batch loss 0.711893082 epoch total loss 0.797492146\n",
      "Trained batch 1461 batch loss 0.742275536 epoch total loss 0.797454357\n",
      "Trained batch 1462 batch loss 0.754217267 epoch total loss 0.797424793\n",
      "Trained batch 1463 batch loss 0.758824706 epoch total loss 0.797398388\n",
      "Trained batch 1464 batch loss 0.742735565 epoch total loss 0.797361\n",
      "Trained batch 1465 batch loss 0.744898 epoch total loss 0.797325194\n",
      "Trained batch 1466 batch loss 0.720845103 epoch total loss 0.79727304\n",
      "Trained batch 1467 batch loss 0.721729219 epoch total loss 0.797221482\n",
      "Trained batch 1468 batch loss 0.702973902 epoch total loss 0.797157288\n",
      "Trained batch 1469 batch loss 0.685311198 epoch total loss 0.797081172\n",
      "Trained batch 1470 batch loss 0.743233681 epoch total loss 0.797044575\n",
      "Trained batch 1471 batch loss 0.708391845 epoch total loss 0.796984315\n",
      "Trained batch 1472 batch loss 0.715042 epoch total loss 0.796928644\n",
      "Trained batch 1473 batch loss 0.761954665 epoch total loss 0.796904922\n",
      "Trained batch 1474 batch loss 0.754004657 epoch total loss 0.796875834\n",
      "Trained batch 1475 batch loss 0.791017115 epoch total loss 0.796871841\n",
      "Trained batch 1476 batch loss 0.826748371 epoch total loss 0.796892107\n",
      "Trained batch 1477 batch loss 0.752294242 epoch total loss 0.796861947\n",
      "Trained batch 1478 batch loss 0.752043 epoch total loss 0.796831667\n",
      "Trained batch 1479 batch loss 0.703114867 epoch total loss 0.796768308\n",
      "Trained batch 1480 batch loss 0.792885602 epoch total loss 0.796765625\n",
      "Trained batch 1481 batch loss 0.746194541 epoch total loss 0.796731472\n",
      "Trained batch 1482 batch loss 0.780982494 epoch total loss 0.796720862\n",
      "Trained batch 1483 batch loss 0.741697669 epoch total loss 0.796683788\n",
      "Trained batch 1484 batch loss 0.766468 epoch total loss 0.796663463\n",
      "Trained batch 1485 batch loss 0.68680191 epoch total loss 0.796589434\n",
      "Trained batch 1486 batch loss 0.65490824 epoch total loss 0.796494067\n",
      "Trained batch 1487 batch loss 0.662454128 epoch total loss 0.796403944\n",
      "Trained batch 1488 batch loss 0.73997581 epoch total loss 0.796366036\n",
      "Trained batch 1489 batch loss 0.779369116 epoch total loss 0.796354651\n",
      "Trained batch 1490 batch loss 0.7447837 epoch total loss 0.79632\n",
      "Trained batch 1491 batch loss 0.70304805 epoch total loss 0.796257436\n",
      "Trained batch 1492 batch loss 0.759637594 epoch total loss 0.796232879\n",
      "Trained batch 1493 batch loss 0.733765841 epoch total loss 0.796191037\n",
      "Trained batch 1494 batch loss 0.772388 epoch total loss 0.796175122\n",
      "Trained batch 1495 batch loss 0.712568104 epoch total loss 0.796119153\n",
      "Trained batch 1496 batch loss 0.72793591 epoch total loss 0.796073556\n",
      "Trained batch 1497 batch loss 0.732012212 epoch total loss 0.79603076\n",
      "Trained batch 1498 batch loss 0.756250143 epoch total loss 0.796004236\n",
      "Trained batch 1499 batch loss 0.752596498 epoch total loss 0.795975208\n",
      "Trained batch 1500 batch loss 0.703764379 epoch total loss 0.795913756\n",
      "Trained batch 1501 batch loss 0.677836776 epoch total loss 0.795835078\n",
      "Trained batch 1502 batch loss 0.714730144 epoch total loss 0.795781076\n",
      "Trained batch 1503 batch loss 0.739252806 epoch total loss 0.795743465\n",
      "Trained batch 1504 batch loss 0.766622603 epoch total loss 0.795724094\n",
      "Trained batch 1505 batch loss 0.769242167 epoch total loss 0.795706511\n",
      "Trained batch 1506 batch loss 0.743816078 epoch total loss 0.795672059\n",
      "Trained batch 1507 batch loss 0.731106758 epoch total loss 0.795629203\n",
      "Trained batch 1508 batch loss 0.75286907 epoch total loss 0.795600891\n",
      "Trained batch 1509 batch loss 0.770515859 epoch total loss 0.795584261\n",
      "Trained batch 1510 batch loss 0.78921771 epoch total loss 0.79558\n",
      "Trained batch 1511 batch loss 0.737143815 epoch total loss 0.795541346\n",
      "Trained batch 1512 batch loss 0.753259301 epoch total loss 0.795513391\n",
      "Trained batch 1513 batch loss 0.709108949 epoch total loss 0.79545629\n",
      "Trained batch 1514 batch loss 0.639190733 epoch total loss 0.795353055\n",
      "Trained batch 1515 batch loss 0.718638957 epoch total loss 0.795302451\n",
      "Trained batch 1516 batch loss 0.706800818 epoch total loss 0.795244038\n",
      "Trained batch 1517 batch loss 0.774054766 epoch total loss 0.795230091\n",
      "Trained batch 1518 batch loss 0.812673688 epoch total loss 0.795241535\n",
      "Trained batch 1519 batch loss 0.870005429 epoch total loss 0.795290709\n",
      "Trained batch 1520 batch loss 0.775630653 epoch total loss 0.795277834\n",
      "Trained batch 1521 batch loss 0.809562 epoch total loss 0.795287192\n",
      "Trained batch 1522 batch loss 0.78445214 epoch total loss 0.795280039\n",
      "Trained batch 1523 batch loss 0.720513523 epoch total loss 0.795230925\n",
      "Trained batch 1524 batch loss 0.726956069 epoch total loss 0.795186102\n",
      "Trained batch 1525 batch loss 0.772133887 epoch total loss 0.795170963\n",
      "Trained batch 1526 batch loss 0.787857175 epoch total loss 0.795166194\n",
      "Trained batch 1527 batch loss 0.830466032 epoch total loss 0.795189261\n",
      "Trained batch 1528 batch loss 0.770803034 epoch total loss 0.795173287\n",
      "Trained batch 1529 batch loss 0.766343772 epoch total loss 0.795154452\n",
      "Trained batch 1530 batch loss 0.801731527 epoch total loss 0.795158744\n",
      "Trained batch 1531 batch loss 0.756584167 epoch total loss 0.795133591\n",
      "Trained batch 1532 batch loss 0.703555822 epoch total loss 0.795073807\n",
      "Trained batch 1533 batch loss 0.709738374 epoch total loss 0.795018137\n",
      "Trained batch 1534 batch loss 0.768625855 epoch total loss 0.795001\n",
      "Trained batch 1535 batch loss 0.77308315 epoch total loss 0.794986665\n",
      "Trained batch 1536 batch loss 0.783081174 epoch total loss 0.794978917\n",
      "Trained batch 1537 batch loss 0.722245216 epoch total loss 0.79493165\n",
      "Trained batch 1538 batch loss 0.684072495 epoch total loss 0.794859588\n",
      "Trained batch 1539 batch loss 0.756381154 epoch total loss 0.794834554\n",
      "Trained batch 1540 batch loss 0.757058263 epoch total loss 0.794810057\n",
      "Trained batch 1541 batch loss 0.734347224 epoch total loss 0.794770837\n",
      "Trained batch 1542 batch loss 0.738789618 epoch total loss 0.794734478\n",
      "Trained batch 1543 batch loss 0.779812336 epoch total loss 0.794724822\n",
      "Trained batch 1544 batch loss 0.771217525 epoch total loss 0.794709623\n",
      "Trained batch 1545 batch loss 0.704910398 epoch total loss 0.794651508\n",
      "Trained batch 1546 batch loss 0.725237429 epoch total loss 0.794606566\n",
      "Trained batch 1547 batch loss 0.704798579 epoch total loss 0.794548571\n",
      "Trained batch 1548 batch loss 0.696079731 epoch total loss 0.794484913\n",
      "Trained batch 1549 batch loss 0.715878189 epoch total loss 0.79443413\n",
      "Trained batch 1550 batch loss 0.684776843 epoch total loss 0.794363439\n",
      "Trained batch 1551 batch loss 0.6908499 epoch total loss 0.794296622\n",
      "Trained batch 1552 batch loss 0.704910219 epoch total loss 0.794239104\n",
      "Trained batch 1553 batch loss 0.693830073 epoch total loss 0.794174433\n",
      "Trained batch 1554 batch loss 0.722597778 epoch total loss 0.794128418\n",
      "Trained batch 1555 batch loss 0.695414722 epoch total loss 0.794064939\n",
      "Trained batch 1556 batch loss 0.7464872 epoch total loss 0.794034362\n",
      "Trained batch 1557 batch loss 0.690529585 epoch total loss 0.793967903\n",
      "Trained batch 1558 batch loss 0.71308744 epoch total loss 0.793916\n",
      "Trained batch 1559 batch loss 0.62124747 epoch total loss 0.793805242\n",
      "Trained batch 1560 batch loss 0.70565927 epoch total loss 0.793748736\n",
      "Trained batch 1561 batch loss 0.67808032 epoch total loss 0.793674648\n",
      "Trained batch 1562 batch loss 0.655624449 epoch total loss 0.793586314\n",
      "Trained batch 1563 batch loss 0.729875207 epoch total loss 0.793545544\n",
      "Trained batch 1564 batch loss 0.720693886 epoch total loss 0.793498933\n",
      "Trained batch 1565 batch loss 0.738820314 epoch total loss 0.793463945\n",
      "Trained batch 1566 batch loss 0.755977213 epoch total loss 0.793440044\n",
      "Trained batch 1567 batch loss 0.75630039 epoch total loss 0.793416381\n",
      "Trained batch 1568 batch loss 0.786969781 epoch total loss 0.793412268\n",
      "Trained batch 1569 batch loss 0.747528374 epoch total loss 0.793383062\n",
      "Trained batch 1570 batch loss 0.740889549 epoch total loss 0.793349564\n",
      "Trained batch 1571 batch loss 0.701755 epoch total loss 0.793291271\n",
      "Trained batch 1572 batch loss 0.677476883 epoch total loss 0.793217599\n",
      "Trained batch 1573 batch loss 0.749559641 epoch total loss 0.793189824\n",
      "Trained batch 1574 batch loss 0.720975399 epoch total loss 0.793143928\n",
      "Trained batch 1575 batch loss 0.727870107 epoch total loss 0.793102503\n",
      "Trained batch 1576 batch loss 0.680450141 epoch total loss 0.793031037\n",
      "Trained batch 1577 batch loss 0.687304 epoch total loss 0.792963922\n",
      "Trained batch 1578 batch loss 0.733009815 epoch total loss 0.792925954\n",
      "Trained batch 1579 batch loss 0.804818034 epoch total loss 0.792933524\n",
      "Trained batch 1580 batch loss 0.703669131 epoch total loss 0.792876959\n",
      "Trained batch 1581 batch loss 0.771485388 epoch total loss 0.792863429\n",
      "Trained batch 1582 batch loss 0.660368443 epoch total loss 0.792779684\n",
      "Trained batch 1583 batch loss 0.704530776 epoch total loss 0.792724\n",
      "Trained batch 1584 batch loss 0.776722968 epoch total loss 0.792713881\n",
      "Trained batch 1585 batch loss 0.720517397 epoch total loss 0.792668283\n",
      "Trained batch 1586 batch loss 0.681350946 epoch total loss 0.792598128\n",
      "Trained batch 1587 batch loss 0.745069742 epoch total loss 0.792568207\n",
      "Trained batch 1588 batch loss 0.739269614 epoch total loss 0.792534649\n",
      "Trained batch 1589 batch loss 0.707011044 epoch total loss 0.792480826\n",
      "Trained batch 1590 batch loss 0.721773 epoch total loss 0.792436421\n",
      "Trained batch 1591 batch loss 0.698478401 epoch total loss 0.792377353\n",
      "Trained batch 1592 batch loss 0.704516113 epoch total loss 0.792322159\n",
      "Trained batch 1593 batch loss 0.718210459 epoch total loss 0.792275667\n",
      "Trained batch 1594 batch loss 0.685337067 epoch total loss 0.792208552\n",
      "Trained batch 1595 batch loss 0.725498617 epoch total loss 0.79216671\n",
      "Trained batch 1596 batch loss 0.648708701 epoch total loss 0.792076766\n",
      "Trained batch 1597 batch loss 0.701343656 epoch total loss 0.792019963\n",
      "Trained batch 1598 batch loss 0.725212634 epoch total loss 0.791978121\n",
      "Trained batch 1599 batch loss 0.705396593 epoch total loss 0.791924\n",
      "Trained batch 1600 batch loss 0.715364516 epoch total loss 0.791876137\n",
      "Trained batch 1601 batch loss 0.731751 epoch total loss 0.791838646\n",
      "Trained batch 1602 batch loss 0.738863468 epoch total loss 0.791805565\n",
      "Trained batch 1603 batch loss 0.751717746 epoch total loss 0.791780591\n",
      "Trained batch 1604 batch loss 0.7942608 epoch total loss 0.791782141\n",
      "Trained batch 1605 batch loss 0.758047819 epoch total loss 0.7917611\n",
      "Trained batch 1606 batch loss 0.727256596 epoch total loss 0.791721\n",
      "Trained batch 1607 batch loss 0.670676172 epoch total loss 0.791645646\n",
      "Trained batch 1608 batch loss 0.716131389 epoch total loss 0.791598737\n",
      "Trained batch 1609 batch loss 0.720989347 epoch total loss 0.791554809\n",
      "Trained batch 1610 batch loss 0.736243963 epoch total loss 0.791520417\n",
      "Trained batch 1611 batch loss 0.735614419 epoch total loss 0.791485727\n",
      "Trained batch 1612 batch loss 0.720424235 epoch total loss 0.791441679\n",
      "Trained batch 1613 batch loss 0.729413748 epoch total loss 0.791403174\n",
      "Trained batch 1614 batch loss 0.683597207 epoch total loss 0.791336358\n",
      "Trained batch 1615 batch loss 0.701680124 epoch total loss 0.791280866\n",
      "Trained batch 1616 batch loss 0.721546412 epoch total loss 0.791237712\n",
      "Trained batch 1617 batch loss 0.755513191 epoch total loss 0.791215599\n",
      "Trained batch 1618 batch loss 0.72396493 epoch total loss 0.791174054\n",
      "Trained batch 1619 batch loss 0.723014474 epoch total loss 0.791132\n",
      "Trained batch 1620 batch loss 0.680996656 epoch total loss 0.791064\n",
      "Trained batch 1621 batch loss 0.615362525 epoch total loss 0.790955603\n",
      "Trained batch 1622 batch loss 0.655758798 epoch total loss 0.790872276\n",
      "Trained batch 1623 batch loss 0.627872944 epoch total loss 0.790771842\n",
      "Trained batch 1624 batch loss 0.535412848 epoch total loss 0.790614605\n",
      "Trained batch 1625 batch loss 0.594312549 epoch total loss 0.790493846\n",
      "Trained batch 1626 batch loss 0.545455217 epoch total loss 0.790343106\n",
      "Trained batch 1627 batch loss 0.701495528 epoch total loss 0.790288508\n",
      "Trained batch 1628 batch loss 0.825511396 epoch total loss 0.790310204\n",
      "Trained batch 1629 batch loss 0.877315283 epoch total loss 0.79036361\n",
      "Trained batch 1630 batch loss 0.857659876 epoch total loss 0.790404916\n",
      "Trained batch 1631 batch loss 0.784550548 epoch total loss 0.79040128\n",
      "Trained batch 1632 batch loss 0.796841085 epoch total loss 0.790405273\n",
      "Trained batch 1633 batch loss 0.78171587 epoch total loss 0.790399969\n",
      "Trained batch 1634 batch loss 0.716179669 epoch total loss 0.79035455\n",
      "Trained batch 1635 batch loss 0.72828716 epoch total loss 0.790316582\n",
      "Trained batch 1636 batch loss 0.706895947 epoch total loss 0.79026562\n",
      "Trained batch 1637 batch loss 0.758384943 epoch total loss 0.790246129\n",
      "Trained batch 1638 batch loss 0.740576804 epoch total loss 0.79021585\n",
      "Trained batch 1639 batch loss 0.679905593 epoch total loss 0.790148556\n",
      "Trained batch 1640 batch loss 0.669994235 epoch total loss 0.790075302\n",
      "Trained batch 1641 batch loss 0.663070917 epoch total loss 0.789997935\n",
      "Trained batch 1642 batch loss 0.693458319 epoch total loss 0.789939165\n",
      "Trained batch 1643 batch loss 0.778639913 epoch total loss 0.789932311\n",
      "Trained batch 1644 batch loss 0.695420444 epoch total loss 0.789874792\n",
      "Trained batch 1645 batch loss 0.682969451 epoch total loss 0.789809823\n",
      "Trained batch 1646 batch loss 0.724844694 epoch total loss 0.789770365\n",
      "Trained batch 1647 batch loss 0.658783674 epoch total loss 0.789690852\n",
      "Trained batch 1648 batch loss 0.79333812 epoch total loss 0.789693058\n",
      "Trained batch 1649 batch loss 0.803141713 epoch total loss 0.789701223\n",
      "Trained batch 1650 batch loss 0.799449146 epoch total loss 0.789707124\n",
      "Trained batch 1651 batch loss 0.758803606 epoch total loss 0.789688349\n",
      "Trained batch 1652 batch loss 0.701297522 epoch total loss 0.789634883\n",
      "Trained batch 1653 batch loss 0.71254 epoch total loss 0.789588213\n",
      "Trained batch 1654 batch loss 0.733407438 epoch total loss 0.789554238\n",
      "Trained batch 1655 batch loss 0.682774663 epoch total loss 0.789489686\n",
      "Trained batch 1656 batch loss 0.765664518 epoch total loss 0.789475322\n",
      "Trained batch 1657 batch loss 0.801038146 epoch total loss 0.789482296\n",
      "Trained batch 1658 batch loss 0.723220229 epoch total loss 0.78944236\n",
      "Trained batch 1659 batch loss 0.749238849 epoch total loss 0.789418101\n",
      "Trained batch 1660 batch loss 0.765434146 epoch total loss 0.789403617\n",
      "Trained batch 1661 batch loss 0.757626235 epoch total loss 0.789384484\n",
      "Trained batch 1662 batch loss 0.731449485 epoch total loss 0.789349616\n",
      "Trained batch 1663 batch loss 0.743611097 epoch total loss 0.789322138\n",
      "Trained batch 1664 batch loss 0.701061845 epoch total loss 0.78926909\n",
      "Trained batch 1665 batch loss 0.752450109 epoch total loss 0.789247\n",
      "Trained batch 1666 batch loss 0.780922174 epoch total loss 0.78924197\n",
      "Trained batch 1667 batch loss 0.777846336 epoch total loss 0.789235115\n",
      "Trained batch 1668 batch loss 0.753758669 epoch total loss 0.789213836\n",
      "Trained batch 1669 batch loss 0.804774702 epoch total loss 0.789223194\n",
      "Trained batch 1670 batch loss 0.782272756 epoch total loss 0.789219\n",
      "Trained batch 1671 batch loss 0.713287711 epoch total loss 0.789173543\n",
      "Trained batch 1672 batch loss 0.728709459 epoch total loss 0.789137423\n",
      "Trained batch 1673 batch loss 0.761904657 epoch total loss 0.789121151\n",
      "Trained batch 1674 batch loss 0.780358613 epoch total loss 0.789115965\n",
      "Trained batch 1675 batch loss 0.765397489 epoch total loss 0.789101779\n",
      "Trained batch 1676 batch loss 0.667458713 epoch total loss 0.789029241\n",
      "Trained batch 1677 batch loss 0.698384762 epoch total loss 0.788975179\n",
      "Trained batch 1678 batch loss 0.792464 epoch total loss 0.788977265\n",
      "Trained batch 1679 batch loss 0.68003273 epoch total loss 0.788912356\n",
      "Trained batch 1680 batch loss 0.721342087 epoch total loss 0.788872123\n",
      "Trained batch 1681 batch loss 0.725236535 epoch total loss 0.788834274\n",
      "Trained batch 1682 batch loss 0.723217368 epoch total loss 0.788795292\n",
      "Trained batch 1683 batch loss 0.702325463 epoch total loss 0.788743854\n",
      "Trained batch 1684 batch loss 0.683290482 epoch total loss 0.788681269\n",
      "Trained batch 1685 batch loss 0.743481517 epoch total loss 0.788654506\n",
      "Trained batch 1686 batch loss 0.777244031 epoch total loss 0.788647711\n",
      "Trained batch 1687 batch loss 0.708319545 epoch total loss 0.788600147\n",
      "Trained batch 1688 batch loss 0.731543899 epoch total loss 0.788566351\n",
      "Trained batch 1689 batch loss 0.740478873 epoch total loss 0.78853786\n",
      "Trained batch 1690 batch loss 0.707111537 epoch total loss 0.788489699\n",
      "Trained batch 1691 batch loss 0.748671234 epoch total loss 0.788466156\n",
      "Trained batch 1692 batch loss 0.774245918 epoch total loss 0.788457751\n",
      "Trained batch 1693 batch loss 0.707618058 epoch total loss 0.788410068\n",
      "Trained batch 1694 batch loss 0.67460835 epoch total loss 0.788342834\n",
      "Trained batch 1695 batch loss 0.750268936 epoch total loss 0.788320363\n",
      "Trained batch 1696 batch loss 0.668399394 epoch total loss 0.788249671\n",
      "Trained batch 1697 batch loss 0.73168534 epoch total loss 0.788216352\n",
      "Trained batch 1698 batch loss 0.743688941 epoch total loss 0.788190126\n",
      "Trained batch 1699 batch loss 0.636657774 epoch total loss 0.788100958\n",
      "Trained batch 1700 batch loss 0.705038726 epoch total loss 0.788052142\n",
      "Trained batch 1701 batch loss 0.730484 epoch total loss 0.788018286\n",
      "Trained batch 1702 batch loss 0.622733176 epoch total loss 0.787921131\n",
      "Trained batch 1703 batch loss 0.763234854 epoch total loss 0.787906587\n",
      "Trained batch 1704 batch loss 0.726794779 epoch total loss 0.787870765\n",
      "Trained batch 1705 batch loss 0.726888299 epoch total loss 0.787835\n",
      "Trained batch 1706 batch loss 0.702595353 epoch total loss 0.787785053\n",
      "Trained batch 1707 batch loss 0.678567886 epoch total loss 0.787721097\n",
      "Trained batch 1708 batch loss 0.696582615 epoch total loss 0.787667692\n",
      "Trained batch 1709 batch loss 0.708593905 epoch total loss 0.787621439\n",
      "Trained batch 1710 batch loss 0.711868405 epoch total loss 0.787577152\n",
      "Trained batch 1711 batch loss 0.595774829 epoch total loss 0.787465096\n",
      "Trained batch 1712 batch loss 0.658240497 epoch total loss 0.787389576\n",
      "Trained batch 1713 batch loss 0.687028766 epoch total loss 0.787331\n",
      "Trained batch 1714 batch loss 0.640833259 epoch total loss 0.787245572\n",
      "Trained batch 1715 batch loss 0.611950636 epoch total loss 0.78714335\n",
      "Trained batch 1716 batch loss 0.679352164 epoch total loss 0.787080467\n",
      "Trained batch 1717 batch loss 0.62024343 epoch total loss 0.786983311\n",
      "Trained batch 1718 batch loss 0.634811044 epoch total loss 0.786894739\n",
      "Trained batch 1719 batch loss 0.72601372 epoch total loss 0.786859334\n",
      "Trained batch 1720 batch loss 0.695477307 epoch total loss 0.786806166\n",
      "Trained batch 1721 batch loss 0.762698114 epoch total loss 0.786792159\n",
      "Trained batch 1722 batch loss 0.709756732 epoch total loss 0.786747396\n",
      "Trained batch 1723 batch loss 0.684495926 epoch total loss 0.786688\n",
      "Trained batch 1724 batch loss 0.704466283 epoch total loss 0.786640346\n",
      "Trained batch 1725 batch loss 0.699121952 epoch total loss 0.786589622\n",
      "Trained batch 1726 batch loss 0.733600259 epoch total loss 0.786558926\n",
      "Trained batch 1727 batch loss 0.782154083 epoch total loss 0.786556363\n",
      "Trained batch 1728 batch loss 0.775740504 epoch total loss 0.786550105\n",
      "Trained batch 1729 batch loss 0.764177144 epoch total loss 0.78653717\n",
      "Trained batch 1730 batch loss 0.787181318 epoch total loss 0.786537528\n",
      "Trained batch 1731 batch loss 0.765248597 epoch total loss 0.786525249\n",
      "Trained batch 1732 batch loss 0.738890648 epoch total loss 0.786497772\n",
      "Trained batch 1733 batch loss 0.731677115 epoch total loss 0.786466122\n",
      "Trained batch 1734 batch loss 0.722982883 epoch total loss 0.786429524\n",
      "Trained batch 1735 batch loss 0.659358144 epoch total loss 0.78635627\n",
      "Trained batch 1736 batch loss 0.768032312 epoch total loss 0.78634572\n",
      "Trained batch 1737 batch loss 0.727551222 epoch total loss 0.786311865\n",
      "Trained batch 1738 batch loss 0.727156103 epoch total loss 0.786277831\n",
      "Trained batch 1739 batch loss 0.689736366 epoch total loss 0.786222339\n",
      "Trained batch 1740 batch loss 0.671715379 epoch total loss 0.786156535\n",
      "Trained batch 1741 batch loss 0.668085754 epoch total loss 0.786088705\n",
      "Trained batch 1742 batch loss 0.698949933 epoch total loss 0.786038697\n",
      "Trained batch 1743 batch loss 0.728683 epoch total loss 0.786005795\n",
      "Trained batch 1744 batch loss 0.759801209 epoch total loss 0.785990715\n",
      "Trained batch 1745 batch loss 0.762083769 epoch total loss 0.785977\n",
      "Trained batch 1746 batch loss 0.770281732 epoch total loss 0.785968\n",
      "Trained batch 1747 batch loss 0.745469749 epoch total loss 0.785944879\n",
      "Trained batch 1748 batch loss 0.746801436 epoch total loss 0.785922468\n",
      "Trained batch 1749 batch loss 0.766742408 epoch total loss 0.7859115\n",
      "Trained batch 1750 batch loss 0.726608932 epoch total loss 0.785877585\n",
      "Trained batch 1751 batch loss 0.72236383 epoch total loss 0.785841346\n",
      "Trained batch 1752 batch loss 0.746528 epoch total loss 0.785818934\n",
      "Trained batch 1753 batch loss 0.708693385 epoch total loss 0.785774946\n",
      "Trained batch 1754 batch loss 0.732047856 epoch total loss 0.785744309\n",
      "Trained batch 1755 batch loss 0.806683719 epoch total loss 0.78575623\n",
      "Trained batch 1756 batch loss 0.822230399 epoch total loss 0.785777032\n",
      "Trained batch 1757 batch loss 0.772679567 epoch total loss 0.785769582\n",
      "Trained batch 1758 batch loss 0.753374577 epoch total loss 0.785751164\n",
      "Trained batch 1759 batch loss 0.716538727 epoch total loss 0.785711825\n",
      "Trained batch 1760 batch loss 0.763463497 epoch total loss 0.785699189\n",
      "Trained batch 1761 batch loss 0.734651744 epoch total loss 0.785670161\n",
      "Trained batch 1762 batch loss 0.730915189 epoch total loss 0.785639107\n",
      "Trained batch 1763 batch loss 0.737549186 epoch total loss 0.785611868\n",
      "Trained batch 1764 batch loss 0.762253761 epoch total loss 0.785598576\n",
      "Trained batch 1765 batch loss 0.648529053 epoch total loss 0.785520911\n",
      "Trained batch 1766 batch loss 0.669209719 epoch total loss 0.785455048\n",
      "Trained batch 1767 batch loss 0.722521365 epoch total loss 0.785419464\n",
      "Trained batch 1768 batch loss 0.671922088 epoch total loss 0.78535521\n",
      "Trained batch 1769 batch loss 0.75662 epoch total loss 0.785339\n",
      "Trained batch 1770 batch loss 0.736635923 epoch total loss 0.78531152\n",
      "Trained batch 1771 batch loss 0.769278884 epoch total loss 0.78530246\n",
      "Trained batch 1772 batch loss 0.752553523 epoch total loss 0.785284\n",
      "Trained batch 1773 batch loss 0.74389869 epoch total loss 0.785260618\n",
      "Trained batch 1774 batch loss 0.71723336 epoch total loss 0.785222292\n",
      "Trained batch 1775 batch loss 0.671136439 epoch total loss 0.785158038\n",
      "Trained batch 1776 batch loss 0.668550491 epoch total loss 0.785092413\n",
      "Trained batch 1777 batch loss 0.672597528 epoch total loss 0.785029113\n",
      "Trained batch 1778 batch loss 0.750743091 epoch total loss 0.785009801\n",
      "Trained batch 1779 batch loss 0.621203721 epoch total loss 0.784917712\n",
      "Trained batch 1780 batch loss 0.663603663 epoch total loss 0.784849584\n",
      "Trained batch 1781 batch loss 0.614978552 epoch total loss 0.784754217\n",
      "Trained batch 1782 batch loss 0.667432606 epoch total loss 0.784688354\n",
      "Trained batch 1783 batch loss 0.683717549 epoch total loss 0.784631729\n",
      "Trained batch 1784 batch loss 0.754259229 epoch total loss 0.784614742\n",
      "Trained batch 1785 batch loss 0.75555867 epoch total loss 0.78459847\n",
      "Trained batch 1786 batch loss 0.787322283 epoch total loss 0.7846\n",
      "Trained batch 1787 batch loss 0.774787366 epoch total loss 0.784594536\n",
      "Trained batch 1788 batch loss 0.719827175 epoch total loss 0.784558296\n",
      "Trained batch 1789 batch loss 0.735009611 epoch total loss 0.78453064\n",
      "Trained batch 1790 batch loss 0.751232147 epoch total loss 0.784512\n",
      "Trained batch 1791 batch loss 0.731684744 epoch total loss 0.784482539\n",
      "Trained batch 1792 batch loss 0.673812151 epoch total loss 0.784420788\n",
      "Trained batch 1793 batch loss 0.676249504 epoch total loss 0.784360468\n",
      "Trained batch 1794 batch loss 0.659363151 epoch total loss 0.784290791\n",
      "Trained batch 1795 batch loss 0.689759851 epoch total loss 0.78423816\n",
      "Trained batch 1796 batch loss 0.756642282 epoch total loss 0.784222782\n",
      "Trained batch 1797 batch loss 0.748427093 epoch total loss 0.784202874\n",
      "Trained batch 1798 batch loss 0.745064616 epoch total loss 0.784181118\n",
      "Trained batch 1799 batch loss 0.791201532 epoch total loss 0.784185052\n",
      "Trained batch 1800 batch loss 0.718174636 epoch total loss 0.784148335\n",
      "Trained batch 1801 batch loss 0.695533395 epoch total loss 0.784099162\n",
      "Trained batch 1802 batch loss 0.677505851 epoch total loss 0.784040034\n",
      "Trained batch 1803 batch loss 0.665157855 epoch total loss 0.783974051\n",
      "Trained batch 1804 batch loss 0.716555238 epoch total loss 0.783936679\n",
      "Trained batch 1805 batch loss 0.679849148 epoch total loss 0.783879\n",
      "Trained batch 1806 batch loss 0.673576176 epoch total loss 0.783817947\n",
      "Trained batch 1807 batch loss 0.703250408 epoch total loss 0.783773363\n",
      "Trained batch 1808 batch loss 0.798560143 epoch total loss 0.783781528\n",
      "Trained batch 1809 batch loss 0.815976679 epoch total loss 0.783799291\n",
      "Trained batch 1810 batch loss 0.800908 epoch total loss 0.783808768\n",
      "Trained batch 1811 batch loss 0.758314788 epoch total loss 0.783794641\n",
      "Trained batch 1812 batch loss 0.735894084 epoch total loss 0.783768177\n",
      "Trained batch 1813 batch loss 0.704110324 epoch total loss 0.783724248\n",
      "Trained batch 1814 batch loss 0.700287104 epoch total loss 0.783678293\n",
      "Trained batch 1815 batch loss 0.692771912 epoch total loss 0.783628166\n",
      "Trained batch 1816 batch loss 0.677041233 epoch total loss 0.783569455\n",
      "Trained batch 1817 batch loss 0.676696539 epoch total loss 0.783510625\n",
      "Trained batch 1818 batch loss 0.677177966 epoch total loss 0.783452094\n",
      "Trained batch 1819 batch loss 0.668883681 epoch total loss 0.783389091\n",
      "Trained batch 1820 batch loss 0.661322415 epoch total loss 0.783322036\n",
      "Trained batch 1821 batch loss 0.710018933 epoch total loss 0.783281744\n",
      "Trained batch 1822 batch loss 0.686308086 epoch total loss 0.783228517\n",
      "Trained batch 1823 batch loss 0.659466565 epoch total loss 0.783160627\n",
      "Trained batch 1824 batch loss 0.72458744 epoch total loss 0.7831285\n",
      "Trained batch 1825 batch loss 0.731639266 epoch total loss 0.783100307\n",
      "Trained batch 1826 batch loss 0.74103713 epoch total loss 0.7830773\n",
      "Trained batch 1827 batch loss 0.853297949 epoch total loss 0.783115745\n",
      "Trained batch 1828 batch loss 0.762252927 epoch total loss 0.7831043\n",
      "Trained batch 1829 batch loss 0.865269125 epoch total loss 0.783149183\n",
      "Trained batch 1830 batch loss 0.898392081 epoch total loss 0.783212185\n",
      "Trained batch 1831 batch loss 0.855338275 epoch total loss 0.783251584\n",
      "Trained batch 1832 batch loss 0.86293757 epoch total loss 0.783295095\n",
      "Trained batch 1833 batch loss 0.766586125 epoch total loss 0.783286\n",
      "Trained batch 1834 batch loss 0.766929328 epoch total loss 0.783277094\n",
      "Trained batch 1835 batch loss 0.79323864 epoch total loss 0.783282459\n",
      "Trained batch 1836 batch loss 0.796553731 epoch total loss 0.783289671\n",
      "Trained batch 1837 batch loss 0.743276179 epoch total loss 0.783267915\n",
      "Trained batch 1838 batch loss 0.760421634 epoch total loss 0.783255458\n",
      "Trained batch 1839 batch loss 0.720758438 epoch total loss 0.783221424\n",
      "Trained batch 1840 batch loss 0.69579196 epoch total loss 0.783173919\n",
      "Trained batch 1841 batch loss 0.624352455 epoch total loss 0.783087671\n",
      "Trained batch 1842 batch loss 0.733575106 epoch total loss 0.783060789\n",
      "Trained batch 1843 batch loss 0.73521018 epoch total loss 0.783034801\n",
      "Trained batch 1844 batch loss 0.787944257 epoch total loss 0.783037484\n",
      "Trained batch 1845 batch loss 0.769776046 epoch total loss 0.783030331\n",
      "Trained batch 1846 batch loss 0.781710148 epoch total loss 0.783029616\n",
      "Trained batch 1847 batch loss 0.802088201 epoch total loss 0.783039927\n",
      "Trained batch 1848 batch loss 0.755025864 epoch total loss 0.783024788\n",
      "Trained batch 1849 batch loss 0.665234804 epoch total loss 0.782961071\n",
      "Trained batch 1850 batch loss 0.6969015 epoch total loss 0.782914579\n",
      "Trained batch 1851 batch loss 0.701594591 epoch total loss 0.782870591\n",
      "Trained batch 1852 batch loss 0.692343354 epoch total loss 0.782821774\n",
      "Trained batch 1853 batch loss 0.683291554 epoch total loss 0.782768071\n",
      "Trained batch 1854 batch loss 0.757641375 epoch total loss 0.78275454\n",
      "Trained batch 1855 batch loss 0.725365102 epoch total loss 0.782723606\n",
      "Trained batch 1856 batch loss 0.667058349 epoch total loss 0.782661319\n",
      "Trained batch 1857 batch loss 0.724020123 epoch total loss 0.782629728\n",
      "Trained batch 1858 batch loss 0.670295715 epoch total loss 0.78256923\n",
      "Trained batch 1859 batch loss 0.717995 epoch total loss 0.78253454\n",
      "Trained batch 1860 batch loss 0.723495781 epoch total loss 0.78250277\n",
      "Trained batch 1861 batch loss 0.668920159 epoch total loss 0.782441795\n",
      "Trained batch 1862 batch loss 0.682135463 epoch total loss 0.782387912\n",
      "Trained batch 1863 batch loss 0.71698463 epoch total loss 0.782352805\n",
      "Trained batch 1864 batch loss 0.737173676 epoch total loss 0.782328606\n",
      "Trained batch 1865 batch loss 0.723169208 epoch total loss 0.782296836\n",
      "Trained batch 1866 batch loss 0.660837054 epoch total loss 0.782231808\n",
      "Trained batch 1867 batch loss 0.791384339 epoch total loss 0.782236695\n",
      "Trained batch 1868 batch loss 0.771511 epoch total loss 0.782230914\n",
      "Trained batch 1869 batch loss 0.7983253 epoch total loss 0.782239556\n",
      "Trained batch 1870 batch loss 0.815862358 epoch total loss 0.782257557\n",
      "Trained batch 1871 batch loss 0.78864342 epoch total loss 0.782261\n",
      "Trained batch 1872 batch loss 0.777785778 epoch total loss 0.78225863\n",
      "Trained batch 1873 batch loss 0.689933896 epoch total loss 0.782209337\n",
      "Trained batch 1874 batch loss 0.676139 epoch total loss 0.782152772\n",
      "Trained batch 1875 batch loss 0.757236481 epoch total loss 0.78213948\n",
      "Trained batch 1876 batch loss 0.758937657 epoch total loss 0.782127082\n",
      "Trained batch 1877 batch loss 0.738594294 epoch total loss 0.782103896\n",
      "Trained batch 1878 batch loss 0.777112424 epoch total loss 0.782101214\n",
      "Trained batch 1879 batch loss 0.712299 epoch total loss 0.78206408\n",
      "Trained batch 1880 batch loss 0.766319 epoch total loss 0.782055736\n",
      "Trained batch 1881 batch loss 0.70818162 epoch total loss 0.782016456\n",
      "Trained batch 1882 batch loss 0.746454895 epoch total loss 0.781997561\n",
      "Trained batch 1883 batch loss 0.790903449 epoch total loss 0.78200227\n",
      "Trained batch 1884 batch loss 0.761675477 epoch total loss 0.781991482\n",
      "Trained batch 1885 batch loss 0.78634876 epoch total loss 0.781993806\n",
      "Trained batch 1886 batch loss 0.718655646 epoch total loss 0.781960249\n",
      "Trained batch 1887 batch loss 0.642813087 epoch total loss 0.781886458\n",
      "Trained batch 1888 batch loss 0.674187601 epoch total loss 0.781829417\n",
      "Trained batch 1889 batch loss 0.727467418 epoch total loss 0.781800628\n",
      "Trained batch 1890 batch loss 0.787333369 epoch total loss 0.781803608\n",
      "Trained batch 1891 batch loss 0.746917665 epoch total loss 0.781785131\n",
      "Trained batch 1892 batch loss 0.665205479 epoch total loss 0.781723499\n",
      "Trained batch 1893 batch loss 0.740529537 epoch total loss 0.781701744\n",
      "Trained batch 1894 batch loss 0.747498751 epoch total loss 0.781683683\n",
      "Trained batch 1895 batch loss 0.739185154 epoch total loss 0.781661212\n",
      "Trained batch 1896 batch loss 0.725565791 epoch total loss 0.781631649\n",
      "Trained batch 1897 batch loss 0.738603294 epoch total loss 0.781609\n",
      "Trained batch 1898 batch loss 0.716608822 epoch total loss 0.781574726\n",
      "Trained batch 1899 batch loss 0.703489482 epoch total loss 0.781533599\n",
      "Trained batch 1900 batch loss 0.748829782 epoch total loss 0.781516373\n",
      "Trained batch 1901 batch loss 0.7626068 epoch total loss 0.781506419\n",
      "Trained batch 1902 batch loss 0.714009345 epoch total loss 0.781470895\n",
      "Trained batch 1903 batch loss 0.769334 epoch total loss 0.781464517\n",
      "Trained batch 1904 batch loss 0.767313719 epoch total loss 0.781457067\n",
      "Trained batch 1905 batch loss 0.835020959 epoch total loss 0.78148514\n",
      "Trained batch 1906 batch loss 0.712978601 epoch total loss 0.781449258\n",
      "Trained batch 1907 batch loss 0.748750627 epoch total loss 0.781432092\n",
      "Trained batch 1908 batch loss 0.691282034 epoch total loss 0.781384885\n",
      "Trained batch 1909 batch loss 0.657396734 epoch total loss 0.781319916\n",
      "Trained batch 1910 batch loss 0.596431136 epoch total loss 0.781223118\n",
      "Trained batch 1911 batch loss 0.553613544 epoch total loss 0.781103969\n",
      "Trained batch 1912 batch loss 0.595494509 epoch total loss 0.781006873\n",
      "Trained batch 1913 batch loss 0.707930446 epoch total loss 0.780968666\n",
      "Trained batch 1914 batch loss 0.728208065 epoch total loss 0.780941069\n",
      "Trained batch 1915 batch loss 0.809036314 epoch total loss 0.780955732\n",
      "Trained batch 1916 batch loss 0.814881206 epoch total loss 0.780973494\n",
      "Trained batch 1917 batch loss 0.744558573 epoch total loss 0.78095448\n",
      "Trained batch 1918 batch loss 0.841892362 epoch total loss 0.780986249\n",
      "Trained batch 1919 batch loss 0.764803171 epoch total loss 0.780977786\n",
      "Trained batch 1920 batch loss 0.730565846 epoch total loss 0.78095156\n",
      "Trained batch 1921 batch loss 0.759010077 epoch total loss 0.780940175\n",
      "Trained batch 1922 batch loss 0.785029829 epoch total loss 0.780942261\n",
      "Trained batch 1923 batch loss 0.858735 epoch total loss 0.780982733\n",
      "Trained batch 1924 batch loss 0.757480383 epoch total loss 0.780970514\n",
      "Trained batch 1925 batch loss 0.745332837 epoch total loss 0.780952036\n",
      "Trained batch 1926 batch loss 0.707046807 epoch total loss 0.780913651\n",
      "Trained batch 1927 batch loss 0.706486702 epoch total loss 0.780875\n",
      "Trained batch 1928 batch loss 0.800538123 epoch total loss 0.78088522\n",
      "Trained batch 1929 batch loss 0.773290813 epoch total loss 0.780881345\n",
      "Trained batch 1930 batch loss 0.826062799 epoch total loss 0.78090471\n",
      "Trained batch 1931 batch loss 0.779678047 epoch total loss 0.780904055\n",
      "Trained batch 1932 batch loss 0.756782711 epoch total loss 0.780891597\n",
      "Trained batch 1933 batch loss 0.700568736 epoch total loss 0.780850053\n",
      "Trained batch 1934 batch loss 0.745389581 epoch total loss 0.780831695\n",
      "Trained batch 1935 batch loss 0.701732218 epoch total loss 0.780790865\n",
      "Trained batch 1936 batch loss 0.731919825 epoch total loss 0.780765653\n",
      "Trained batch 1937 batch loss 0.677648544 epoch total loss 0.780712366\n",
      "Trained batch 1938 batch loss 0.609823883 epoch total loss 0.780624211\n",
      "Trained batch 1939 batch loss 0.640891194 epoch total loss 0.780552149\n",
      "Trained batch 1940 batch loss 0.70064342 epoch total loss 0.780510962\n",
      "Trained batch 1941 batch loss 0.724594831 epoch total loss 0.780482173\n",
      "Trained batch 1942 batch loss 0.711836755 epoch total loss 0.780446827\n",
      "Trained batch 1943 batch loss 0.753019094 epoch total loss 0.780432701\n",
      "Trained batch 1944 batch loss 0.762047827 epoch total loss 0.780423284\n",
      "Trained batch 1945 batch loss 0.753171682 epoch total loss 0.780409276\n",
      "Trained batch 1946 batch loss 0.747018576 epoch total loss 0.78039211\n",
      "Trained batch 1947 batch loss 0.707588 epoch total loss 0.780354738\n",
      "Trained batch 1948 batch loss 0.798886478 epoch total loss 0.780364215\n",
      "Trained batch 1949 batch loss 0.82305485 epoch total loss 0.78038609\n",
      "Trained batch 1950 batch loss 0.726005 epoch total loss 0.780358195\n",
      "Trained batch 1951 batch loss 0.711475313 epoch total loss 0.78032285\n",
      "Trained batch 1952 batch loss 0.720513761 epoch total loss 0.780292213\n",
      "Trained batch 1953 batch loss 0.72757256 epoch total loss 0.780265212\n",
      "Trained batch 1954 batch loss 0.748064876 epoch total loss 0.780248702\n",
      "Trained batch 1955 batch loss 0.716759741 epoch total loss 0.780216217\n",
      "Trained batch 1956 batch loss 0.747514248 epoch total loss 0.780199528\n",
      "Trained batch 1957 batch loss 0.817033291 epoch total loss 0.780218363\n",
      "Trained batch 1958 batch loss 0.823794365 epoch total loss 0.780240655\n",
      "Trained batch 1959 batch loss 0.704041719 epoch total loss 0.780201793\n",
      "Trained batch 1960 batch loss 0.78162086 epoch total loss 0.780202508\n",
      "Trained batch 1961 batch loss 0.754068 epoch total loss 0.780189157\n",
      "Trained batch 1962 batch loss 0.732730389 epoch total loss 0.780165\n",
      "Trained batch 1963 batch loss 0.805374622 epoch total loss 0.780177832\n",
      "Trained batch 1964 batch loss 0.81297195 epoch total loss 0.780194581\n",
      "Trained batch 1965 batch loss 0.729250789 epoch total loss 0.780168653\n",
      "Trained batch 1966 batch loss 0.73901993 epoch total loss 0.780147731\n",
      "Trained batch 1967 batch loss 0.699871123 epoch total loss 0.780106843\n",
      "Trained batch 1968 batch loss 0.700180352 epoch total loss 0.780066252\n",
      "Trained batch 1969 batch loss 0.736894727 epoch total loss 0.780044377\n",
      "Trained batch 1970 batch loss 0.71850884 epoch total loss 0.780013144\n",
      "Trained batch 1971 batch loss 0.766213775 epoch total loss 0.780006111\n",
      "Trained batch 1972 batch loss 0.809262156 epoch total loss 0.780020952\n",
      "Trained batch 1973 batch loss 0.830617309 epoch total loss 0.780046582\n",
      "Trained batch 1974 batch loss 0.810173631 epoch total loss 0.780061841\n",
      "Trained batch 1975 batch loss 0.757495821 epoch total loss 0.780050397\n",
      "Trained batch 1976 batch loss 0.735324204 epoch total loss 0.780027747\n",
      "Trained batch 1977 batch loss 0.694087446 epoch total loss 0.779984295\n",
      "Trained batch 1978 batch loss 0.717069745 epoch total loss 0.779952466\n",
      "Trained batch 1979 batch loss 0.719075322 epoch total loss 0.77992171\n",
      "Trained batch 1980 batch loss 0.777264118 epoch total loss 0.77992034\n",
      "Trained batch 1981 batch loss 0.795452356 epoch total loss 0.779928148\n",
      "Trained batch 1982 batch loss 0.75492537 epoch total loss 0.779915512\n",
      "Trained batch 1983 batch loss 0.714438677 epoch total loss 0.77988255\n",
      "Trained batch 1984 batch loss 0.69939208 epoch total loss 0.779841959\n",
      "Trained batch 1985 batch loss 0.77512145 epoch total loss 0.779839575\n",
      "Trained batch 1986 batch loss 0.78455776 epoch total loss 0.779841959\n",
      "Trained batch 1987 batch loss 0.72164911 epoch total loss 0.779812694\n",
      "Trained batch 1988 batch loss 0.793110371 epoch total loss 0.779819369\n",
      "Trained batch 1989 batch loss 0.793390274 epoch total loss 0.779826164\n",
      "Trained batch 1990 batch loss 0.727424204 epoch total loss 0.779799819\n",
      "Trained batch 1991 batch loss 0.743661702 epoch total loss 0.77978164\n",
      "Trained batch 1992 batch loss 0.648637891 epoch total loss 0.779715836\n",
      "Trained batch 1993 batch loss 0.629285097 epoch total loss 0.779640377\n",
      "Trained batch 1994 batch loss 0.622665107 epoch total loss 0.779561639\n",
      "Trained batch 1995 batch loss 0.657442868 epoch total loss 0.779500425\n",
      "Trained batch 1996 batch loss 0.673544705 epoch total loss 0.779447377\n",
      "Trained batch 1997 batch loss 0.759367049 epoch total loss 0.779437363\n",
      "Trained batch 1998 batch loss 0.762432098 epoch total loss 0.77942884\n",
      "Trained batch 1999 batch loss 0.733545125 epoch total loss 0.779405892\n",
      "Trained batch 2000 batch loss 0.683068812 epoch total loss 0.779357731\n",
      "Trained batch 2001 batch loss 0.759935915 epoch total loss 0.779348\n",
      "Trained batch 2002 batch loss 0.654279292 epoch total loss 0.77928555\n",
      "Trained batch 2003 batch loss 0.647261798 epoch total loss 0.779219627\n",
      "Trained batch 2004 batch loss 0.652745843 epoch total loss 0.779156446\n",
      "Trained batch 2005 batch loss 0.641460776 epoch total loss 0.779087782\n",
      "Trained batch 2006 batch loss 0.631991506 epoch total loss 0.779014468\n",
      "Trained batch 2007 batch loss 0.749418378 epoch total loss 0.778999686\n",
      "Trained batch 2008 batch loss 0.6883623 epoch total loss 0.778954566\n",
      "Trained batch 2009 batch loss 0.644948542 epoch total loss 0.778887808\n",
      "Trained batch 2010 batch loss 0.699003398 epoch total loss 0.778848052\n",
      "Trained batch 2011 batch loss 0.754742682 epoch total loss 0.778836071\n",
      "Trained batch 2012 batch loss 0.769131899 epoch total loss 0.778831303\n",
      "Trained batch 2013 batch loss 0.737752199 epoch total loss 0.778810918\n",
      "Trained batch 2014 batch loss 0.748287678 epoch total loss 0.778795719\n",
      "Trained batch 2015 batch loss 0.791183174 epoch total loss 0.778801858\n",
      "Trained batch 2016 batch loss 0.740002 epoch total loss 0.778782606\n",
      "Trained batch 2017 batch loss 0.724703 epoch total loss 0.778755844\n",
      "Trained batch 2018 batch loss 0.685855448 epoch total loss 0.778709829\n",
      "Trained batch 2019 batch loss 0.700648069 epoch total loss 0.778671145\n",
      "Trained batch 2020 batch loss 0.688938558 epoch total loss 0.77862674\n",
      "Trained batch 2021 batch loss 0.700698137 epoch total loss 0.778588176\n",
      "Trained batch 2022 batch loss 0.705106 epoch total loss 0.778551817\n",
      "Trained batch 2023 batch loss 0.708502233 epoch total loss 0.778517187\n",
      "Trained batch 2024 batch loss 0.73196578 epoch total loss 0.778494179\n",
      "Trained batch 2025 batch loss 0.717740774 epoch total loss 0.778464198\n",
      "Trained batch 2026 batch loss 0.591034651 epoch total loss 0.778371692\n",
      "Trained batch 2027 batch loss 0.676321507 epoch total loss 0.778321326\n",
      "Trained batch 2028 batch loss 0.82721287 epoch total loss 0.778345466\n",
      "Trained batch 2029 batch loss 0.702961564 epoch total loss 0.778308332\n",
      "Trained batch 2030 batch loss 0.750410318 epoch total loss 0.778294563\n",
      "Trained batch 2031 batch loss 0.68619 epoch total loss 0.778249204\n",
      "Trained batch 2032 batch loss 0.723616123 epoch total loss 0.778222322\n",
      "Trained batch 2033 batch loss 0.695192218 epoch total loss 0.778181493\n",
      "Trained batch 2034 batch loss 0.73194927 epoch total loss 0.778158724\n",
      "Trained batch 2035 batch loss 0.695285261 epoch total loss 0.778118\n",
      "Trained batch 2036 batch loss 0.661744237 epoch total loss 0.778060853\n",
      "Trained batch 2037 batch loss 0.661653876 epoch total loss 0.778003693\n",
      "Trained batch 2038 batch loss 0.668798685 epoch total loss 0.777950168\n",
      "Trained batch 2039 batch loss 0.711522937 epoch total loss 0.777917564\n",
      "Trained batch 2040 batch loss 0.725212693 epoch total loss 0.777891755\n",
      "Trained batch 2041 batch loss 0.696306646 epoch total loss 0.77785176\n",
      "Trained batch 2042 batch loss 0.6887784 epoch total loss 0.77780813\n",
      "Trained batch 2043 batch loss 0.694568634 epoch total loss 0.77776736\n",
      "Trained batch 2044 batch loss 0.716795921 epoch total loss 0.777737558\n",
      "Trained batch 2045 batch loss 0.783347726 epoch total loss 0.7777403\n",
      "Trained batch 2046 batch loss 0.776934206 epoch total loss 0.777739882\n",
      "Trained batch 2047 batch loss 0.725297272 epoch total loss 0.777714312\n",
      "Trained batch 2048 batch loss 0.714450359 epoch total loss 0.777683437\n",
      "Trained batch 2049 batch loss 0.686996579 epoch total loss 0.77763921\n",
      "Trained batch 2050 batch loss 0.630546391 epoch total loss 0.777567387\n",
      "Trained batch 2051 batch loss 0.639593601 epoch total loss 0.777500153\n",
      "Trained batch 2052 batch loss 0.672440469 epoch total loss 0.777449\n",
      "Trained batch 2053 batch loss 0.672502637 epoch total loss 0.777397871\n",
      "Trained batch 2054 batch loss 0.58300823 epoch total loss 0.777303219\n",
      "Trained batch 2055 batch loss 0.684266508 epoch total loss 0.777258\n",
      "Trained batch 2056 batch loss 0.693083644 epoch total loss 0.777217031\n",
      "Trained batch 2057 batch loss 0.684150398 epoch total loss 0.77717185\n",
      "Trained batch 2058 batch loss 0.748759627 epoch total loss 0.777158\n",
      "Trained batch 2059 batch loss 0.78554976 epoch total loss 0.777162075\n",
      "Trained batch 2060 batch loss 0.757170498 epoch total loss 0.777152419\n",
      "Trained batch 2061 batch loss 0.75234282 epoch total loss 0.777140379\n",
      "Trained batch 2062 batch loss 0.676872671 epoch total loss 0.777091742\n",
      "Trained batch 2063 batch loss 0.729581356 epoch total loss 0.777068734\n",
      "Trained batch 2064 batch loss 0.673977 epoch total loss 0.777018785\n",
      "Trained batch 2065 batch loss 0.677388191 epoch total loss 0.776970506\n",
      "Trained batch 2066 batch loss 0.637752175 epoch total loss 0.776903093\n",
      "Trained batch 2067 batch loss 0.693956494 epoch total loss 0.776863\n",
      "Trained batch 2068 batch loss 0.720116556 epoch total loss 0.776835501\n",
      "Trained batch 2069 batch loss 0.732392669 epoch total loss 0.776814044\n",
      "Trained batch 2070 batch loss 0.796541035 epoch total loss 0.77682358\n",
      "Trained batch 2071 batch loss 0.767047644 epoch total loss 0.776818871\n",
      "Trained batch 2072 batch loss 0.694402099 epoch total loss 0.776779115\n",
      "Trained batch 2073 batch loss 0.713150263 epoch total loss 0.776748419\n",
      "Trained batch 2074 batch loss 0.749712944 epoch total loss 0.776735425\n",
      "Trained batch 2075 batch loss 0.661175907 epoch total loss 0.776679695\n",
      "Trained batch 2076 batch loss 0.710406899 epoch total loss 0.776647806\n",
      "Trained batch 2077 batch loss 0.653175473 epoch total loss 0.776588321\n",
      "Trained batch 2078 batch loss 0.635035276 epoch total loss 0.776520193\n",
      "Trained batch 2079 batch loss 0.611092 epoch total loss 0.77644062\n",
      "Trained batch 2080 batch loss 0.628075361 epoch total loss 0.776369274\n",
      "Trained batch 2081 batch loss 0.679887176 epoch total loss 0.776322961\n",
      "Trained batch 2082 batch loss 0.689146459 epoch total loss 0.776281059\n",
      "Trained batch 2083 batch loss 0.654507399 epoch total loss 0.776222587\n",
      "Trained batch 2084 batch loss 0.7178967 epoch total loss 0.776194632\n",
      "Trained batch 2085 batch loss 0.801431715 epoch total loss 0.776206732\n",
      "Trained batch 2086 batch loss 0.770218611 epoch total loss 0.776203871\n",
      "Trained batch 2087 batch loss 0.768009365 epoch total loss 0.776199937\n",
      "Trained batch 2088 batch loss 0.770206451 epoch total loss 0.776197135\n",
      "Trained batch 2089 batch loss 0.729863167 epoch total loss 0.776174963\n",
      "Trained batch 2090 batch loss 0.734735131 epoch total loss 0.776155114\n",
      "Trained batch 2091 batch loss 0.845148504 epoch total loss 0.776188076\n",
      "Trained batch 2092 batch loss 0.959512055 epoch total loss 0.776275694\n",
      "Trained batch 2093 batch loss 0.803750694 epoch total loss 0.776288807\n",
      "Trained batch 2094 batch loss 0.82134068 epoch total loss 0.776310265\n",
      "Trained batch 2095 batch loss 0.794988394 epoch total loss 0.776319206\n",
      "Trained batch 2096 batch loss 0.7256338 epoch total loss 0.776295\n",
      "Trained batch 2097 batch loss 0.688791752 epoch total loss 0.776253343\n",
      "Trained batch 2098 batch loss 0.621212184 epoch total loss 0.776179433\n",
      "Trained batch 2099 batch loss 0.709107399 epoch total loss 0.776147485\n",
      "Trained batch 2100 batch loss 0.639087439 epoch total loss 0.776082158\n",
      "Trained batch 2101 batch loss 0.58420527 epoch total loss 0.775990844\n",
      "Trained batch 2102 batch loss 0.588620484 epoch total loss 0.775901735\n",
      "Trained batch 2103 batch loss 0.634591162 epoch total loss 0.77583456\n",
      "Trained batch 2104 batch loss 0.698772728 epoch total loss 0.775797904\n",
      "Trained batch 2105 batch loss 0.770069718 epoch total loss 0.775795162\n",
      "Trained batch 2106 batch loss 0.707345128 epoch total loss 0.775762677\n",
      "Trained batch 2107 batch loss 0.565832138 epoch total loss 0.775663\n",
      "Trained batch 2108 batch loss 0.57166 epoch total loss 0.77556628\n",
      "Trained batch 2109 batch loss 0.554815114 epoch total loss 0.775461614\n",
      "Trained batch 2110 batch loss 0.584291041 epoch total loss 0.775371\n",
      "Trained batch 2111 batch loss 0.599944353 epoch total loss 0.775287926\n",
      "Trained batch 2112 batch loss 0.612626731 epoch total loss 0.775210917\n",
      "Trained batch 2113 batch loss 0.635140777 epoch total loss 0.775144637\n",
      "Trained batch 2114 batch loss 0.635576248 epoch total loss 0.775078654\n",
      "Trained batch 2115 batch loss 0.699474037 epoch total loss 0.775042892\n",
      "Trained batch 2116 batch loss 0.727421939 epoch total loss 0.775020361\n",
      "Trained batch 2117 batch loss 0.726905584 epoch total loss 0.774997652\n",
      "Trained batch 2118 batch loss 0.721220076 epoch total loss 0.77497226\n",
      "Trained batch 2119 batch loss 0.754101098 epoch total loss 0.774962425\n",
      "Trained batch 2120 batch loss 0.763975501 epoch total loss 0.77495724\n",
      "Trained batch 2121 batch loss 0.762289822 epoch total loss 0.774951279\n",
      "Trained batch 2122 batch loss 0.807657301 epoch total loss 0.774966657\n",
      "Trained batch 2123 batch loss 0.85543257 epoch total loss 0.775004566\n",
      "Trained batch 2124 batch loss 0.760629356 epoch total loss 0.77499783\n",
      "Trained batch 2125 batch loss 0.770895123 epoch total loss 0.774995863\n",
      "Trained batch 2126 batch loss 0.731725454 epoch total loss 0.774975479\n",
      "Trained batch 2127 batch loss 0.73264581 epoch total loss 0.77495563\n",
      "Trained batch 2128 batch loss 0.719430745 epoch total loss 0.774929523\n",
      "Trained batch 2129 batch loss 0.719323218 epoch total loss 0.774903417\n",
      "Trained batch 2130 batch loss 0.70001328 epoch total loss 0.774868309\n",
      "Trained batch 2131 batch loss 0.72439158 epoch total loss 0.774844587\n",
      "Trained batch 2132 batch loss 0.648700118 epoch total loss 0.774785399\n",
      "Trained batch 2133 batch loss 0.633295059 epoch total loss 0.774719119\n",
      "Trained batch 2134 batch loss 0.582859218 epoch total loss 0.774629176\n",
      "Trained batch 2135 batch loss 0.633906841 epoch total loss 0.774563313\n",
      "Trained batch 2136 batch loss 0.663667798 epoch total loss 0.774511397\n",
      "Trained batch 2137 batch loss 0.722693205 epoch total loss 0.774487138\n",
      "Trained batch 2138 batch loss 0.683523118 epoch total loss 0.77444458\n",
      "Trained batch 2139 batch loss 0.669308543 epoch total loss 0.774395406\n",
      "Trained batch 2140 batch loss 0.749227107 epoch total loss 0.774383664\n",
      "Trained batch 2141 batch loss 0.75790143 epoch total loss 0.774376\n",
      "Trained batch 2142 batch loss 0.693068922 epoch total loss 0.774338067\n",
      "Trained batch 2143 batch loss 0.733737648 epoch total loss 0.774319112\n",
      "Trained batch 2144 batch loss 0.757909477 epoch total loss 0.774311483\n",
      "Trained batch 2145 batch loss 0.715630054 epoch total loss 0.774284065\n",
      "Trained batch 2146 batch loss 0.637225807 epoch total loss 0.774220228\n",
      "Trained batch 2147 batch loss 0.722292066 epoch total loss 0.774196\n",
      "Trained batch 2148 batch loss 0.74596411 epoch total loss 0.774182856\n",
      "Trained batch 2149 batch loss 0.737336874 epoch total loss 0.77416569\n",
      "Trained batch 2150 batch loss 0.751455545 epoch total loss 0.77415514\n",
      "Trained batch 2151 batch loss 0.73304081 epoch total loss 0.774136066\n",
      "Trained batch 2152 batch loss 0.754435837 epoch total loss 0.774126887\n",
      "Trained batch 2153 batch loss 0.724747 epoch total loss 0.77410394\n",
      "Trained batch 2154 batch loss 0.680484176 epoch total loss 0.774060488\n",
      "Trained batch 2155 batch loss 0.705628693 epoch total loss 0.774028778\n",
      "Trained batch 2156 batch loss 0.735775471 epoch total loss 0.774011\n",
      "Trained batch 2157 batch loss 0.72777456 epoch total loss 0.773989558\n",
      "Trained batch 2158 batch loss 0.690564096 epoch total loss 0.773950875\n",
      "Trained batch 2159 batch loss 0.703160167 epoch total loss 0.773918092\n",
      "Trained batch 2160 batch loss 0.675730109 epoch total loss 0.773872674\n",
      "Trained batch 2161 batch loss 0.574458241 epoch total loss 0.773780406\n",
      "Trained batch 2162 batch loss 0.697848678 epoch total loss 0.773745298\n",
      "Trained batch 2163 batch loss 0.714833617 epoch total loss 0.773718059\n",
      "Trained batch 2164 batch loss 0.725265324 epoch total loss 0.773695648\n",
      "Trained batch 2165 batch loss 0.734177053 epoch total loss 0.773677349\n",
      "Trained batch 2166 batch loss 0.663403511 epoch total loss 0.773626447\n",
      "Trained batch 2167 batch loss 0.676105917 epoch total loss 0.773581505\n",
      "Trained batch 2168 batch loss 0.655122757 epoch total loss 0.773526847\n",
      "Trained batch 2169 batch loss 0.731822371 epoch total loss 0.773507595\n",
      "Trained batch 2170 batch loss 0.660175 epoch total loss 0.773455381\n",
      "Trained batch 2171 batch loss 0.707133591 epoch total loss 0.773424864\n",
      "Trained batch 2172 batch loss 0.685642123 epoch total loss 0.773384452\n",
      "Trained batch 2173 batch loss 0.748606265 epoch total loss 0.773373067\n",
      "Trained batch 2174 batch loss 0.672366619 epoch total loss 0.773326576\n",
      "Trained batch 2175 batch loss 0.655319214 epoch total loss 0.773272336\n",
      "Trained batch 2176 batch loss 0.680999875 epoch total loss 0.773229957\n",
      "Trained batch 2177 batch loss 0.706850171 epoch total loss 0.773199499\n",
      "Trained batch 2178 batch loss 0.691752 epoch total loss 0.773162067\n",
      "Trained batch 2179 batch loss 0.669099331 epoch total loss 0.773114324\n",
      "Trained batch 2180 batch loss 0.66522336 epoch total loss 0.773064852\n",
      "Trained batch 2181 batch loss 0.670249343 epoch total loss 0.773017704\n",
      "Trained batch 2182 batch loss 0.698126674 epoch total loss 0.772983372\n",
      "Trained batch 2183 batch loss 0.654917598 epoch total loss 0.772929311\n",
      "Trained batch 2184 batch loss 0.63107425 epoch total loss 0.772864401\n",
      "Trained batch 2185 batch loss 0.709158361 epoch total loss 0.772835195\n",
      "Trained batch 2186 batch loss 0.638282061 epoch total loss 0.772773623\n",
      "Trained batch 2187 batch loss 0.712717175 epoch total loss 0.772746205\n",
      "Trained batch 2188 batch loss 0.699608922 epoch total loss 0.772712767\n",
      "Trained batch 2189 batch loss 0.745276 epoch total loss 0.77270025\n",
      "Trained batch 2190 batch loss 0.6963799 epoch total loss 0.772665381\n",
      "Trained batch 2191 batch loss 0.719950795 epoch total loss 0.772641361\n",
      "Trained batch 2192 batch loss 0.643700838 epoch total loss 0.772582531\n",
      "Trained batch 2193 batch loss 0.62646836 epoch total loss 0.772515893\n",
      "Trained batch 2194 batch loss 0.676950097 epoch total loss 0.772472322\n",
      "Trained batch 2195 batch loss 0.721576273 epoch total loss 0.772449136\n",
      "Trained batch 2196 batch loss 0.672061086 epoch total loss 0.772403479\n",
      "Trained batch 2197 batch loss 0.748721242 epoch total loss 0.77239269\n",
      "Trained batch 2198 batch loss 0.794172227 epoch total loss 0.772402644\n",
      "Trained batch 2199 batch loss 0.705276847 epoch total loss 0.772372127\n",
      "Trained batch 2200 batch loss 0.806548238 epoch total loss 0.772387624\n",
      "Trained batch 2201 batch loss 0.784762204 epoch total loss 0.772393286\n",
      "Trained batch 2202 batch loss 0.779838 epoch total loss 0.772396624\n",
      "Trained batch 2203 batch loss 0.735567331 epoch total loss 0.772379935\n",
      "Trained batch 2204 batch loss 0.693463624 epoch total loss 0.772344112\n",
      "Trained batch 2205 batch loss 0.729816556 epoch total loss 0.77232486\n",
      "Trained batch 2206 batch loss 0.755273581 epoch total loss 0.772317111\n",
      "Trained batch 2207 batch loss 0.71424818 epoch total loss 0.772290826\n",
      "Trained batch 2208 batch loss 0.708095551 epoch total loss 0.772261739\n",
      "Trained batch 2209 batch loss 0.763188124 epoch total loss 0.772257626\n",
      "Trained batch 2210 batch loss 0.685930967 epoch total loss 0.772218585\n",
      "Trained batch 2211 batch loss 0.726235271 epoch total loss 0.772197723\n",
      "Trained batch 2212 batch loss 0.690514565 epoch total loss 0.772160828\n",
      "Trained batch 2213 batch loss 0.657734096 epoch total loss 0.772109151\n",
      "Trained batch 2214 batch loss 0.699426591 epoch total loss 0.772076309\n",
      "Trained batch 2215 batch loss 0.653834879 epoch total loss 0.772022903\n",
      "Trained batch 2216 batch loss 0.672557116 epoch total loss 0.77197808\n",
      "Trained batch 2217 batch loss 0.735680103 epoch total loss 0.771961689\n",
      "Trained batch 2218 batch loss 0.716543853 epoch total loss 0.771936715\n",
      "Trained batch 2219 batch loss 0.636076033 epoch total loss 0.771875501\n",
      "Trained batch 2220 batch loss 0.661912322 epoch total loss 0.771825969\n",
      "Trained batch 2221 batch loss 0.745102167 epoch total loss 0.771813929\n",
      "Trained batch 2222 batch loss 0.738494933 epoch total loss 0.771798968\n",
      "Trained batch 2223 batch loss 0.713339 epoch total loss 0.771772683\n",
      "Trained batch 2224 batch loss 0.734664917 epoch total loss 0.771755934\n",
      "Trained batch 2225 batch loss 0.699389577 epoch total loss 0.77172339\n",
      "Trained batch 2226 batch loss 0.71858418 epoch total loss 0.771699548\n",
      "Trained batch 2227 batch loss 0.716130614 epoch total loss 0.771674633\n",
      "Trained batch 2228 batch loss 0.695502043 epoch total loss 0.77164048\n",
      "Trained batch 2229 batch loss 0.753513813 epoch total loss 0.771632373\n",
      "Trained batch 2230 batch loss 0.823034763 epoch total loss 0.771655381\n",
      "Trained batch 2231 batch loss 0.747954965 epoch total loss 0.771644771\n",
      "Trained batch 2232 batch loss 0.719235539 epoch total loss 0.771621287\n",
      "Trained batch 2233 batch loss 0.694897771 epoch total loss 0.771586955\n",
      "Trained batch 2234 batch loss 0.802996039 epoch total loss 0.771600962\n",
      "Trained batch 2235 batch loss 0.744210839 epoch total loss 0.771588743\n",
      "Trained batch 2236 batch loss 0.6785146 epoch total loss 0.771547079\n",
      "Trained batch 2237 batch loss 0.629646301 epoch total loss 0.77148366\n",
      "Trained batch 2238 batch loss 0.62603426 epoch total loss 0.771418631\n",
      "Trained batch 2239 batch loss 0.686262846 epoch total loss 0.771380603\n",
      "Trained batch 2240 batch loss 0.621257186 epoch total loss 0.771313608\n",
      "Trained batch 2241 batch loss 0.68166995 epoch total loss 0.771273553\n",
      "Trained batch 2242 batch loss 0.729162216 epoch total loss 0.771254778\n",
      "Trained batch 2243 batch loss 0.798233867 epoch total loss 0.771266818\n",
      "Trained batch 2244 batch loss 0.570067942 epoch total loss 0.771177113\n",
      "Trained batch 2245 batch loss 0.804759085 epoch total loss 0.771192133\n",
      "Trained batch 2246 batch loss 0.807260752 epoch total loss 0.771208167\n",
      "Trained batch 2247 batch loss 0.766667604 epoch total loss 0.7712062\n",
      "Trained batch 2248 batch loss 0.749706089 epoch total loss 0.771196604\n",
      "Trained batch 2249 batch loss 0.738027692 epoch total loss 0.771181881\n",
      "Trained batch 2250 batch loss 0.747226119 epoch total loss 0.771171212\n",
      "Trained batch 2251 batch loss 0.677104414 epoch total loss 0.771129429\n",
      "Trained batch 2252 batch loss 0.606458664 epoch total loss 0.771056294\n",
      "Trained batch 2253 batch loss 0.693798363 epoch total loss 0.771022\n",
      "Trained batch 2254 batch loss 0.69526124 epoch total loss 0.770988464\n",
      "Trained batch 2255 batch loss 0.731741309 epoch total loss 0.77097106\n",
      "Trained batch 2256 batch loss 0.683282852 epoch total loss 0.770932138\n",
      "Trained batch 2257 batch loss 0.683244824 epoch total loss 0.770893276\n",
      "Trained batch 2258 batch loss 0.717265546 epoch total loss 0.770869553\n",
      "Trained batch 2259 batch loss 0.673263431 epoch total loss 0.77082628\n",
      "Trained batch 2260 batch loss 0.643325448 epoch total loss 0.770769894\n",
      "Trained batch 2261 batch loss 0.678581655 epoch total loss 0.770729125\n",
      "Trained batch 2262 batch loss 0.675780118 epoch total loss 0.770687163\n",
      "Trained batch 2263 batch loss 0.701555312 epoch total loss 0.770656586\n",
      "Trained batch 2264 batch loss 0.731078506 epoch total loss 0.770639122\n",
      "Trained batch 2265 batch loss 0.768656373 epoch total loss 0.770638227\n",
      "Trained batch 2266 batch loss 0.689746499 epoch total loss 0.770602524\n",
      "Trained batch 2267 batch loss 0.771239936 epoch total loss 0.770602822\n",
      "Trained batch 2268 batch loss 0.783526659 epoch total loss 0.770608544\n",
      "Trained batch 2269 batch loss 0.716050625 epoch total loss 0.770584464\n",
      "Trained batch 2270 batch loss 0.663508356 epoch total loss 0.770537257\n",
      "Trained batch 2271 batch loss 0.75310564 epoch total loss 0.770529568\n",
      "Trained batch 2272 batch loss 0.685587645 epoch total loss 0.770492196\n",
      "Trained batch 2273 batch loss 0.607199132 epoch total loss 0.770420313\n",
      "Trained batch 2274 batch loss 0.648389399 epoch total loss 0.770366669\n",
      "Trained batch 2275 batch loss 0.630945265 epoch total loss 0.770305395\n",
      "Trained batch 2276 batch loss 0.660109222 epoch total loss 0.770257\n",
      "Trained batch 2277 batch loss 0.684891 epoch total loss 0.770219564\n",
      "Trained batch 2278 batch loss 0.681544721 epoch total loss 0.770180583\n",
      "Trained batch 2279 batch loss 0.713866115 epoch total loss 0.770155907\n",
      "Trained batch 2280 batch loss 0.734161556 epoch total loss 0.770140111\n",
      "Trained batch 2281 batch loss 0.725204468 epoch total loss 0.770120382\n",
      "Trained batch 2282 batch loss 0.796120763 epoch total loss 0.770131826\n",
      "Trained batch 2283 batch loss 0.730853915 epoch total loss 0.770114601\n",
      "Trained batch 2284 batch loss 0.716900229 epoch total loss 0.770091295\n",
      "Trained batch 2285 batch loss 0.704022527 epoch total loss 0.770062387\n",
      "Trained batch 2286 batch loss 0.748600602 epoch total loss 0.770053\n",
      "Trained batch 2287 batch loss 0.691838861 epoch total loss 0.770018816\n",
      "Trained batch 2288 batch loss 0.765885651 epoch total loss 0.770017\n",
      "Trained batch 2289 batch loss 0.7101717 epoch total loss 0.769990921\n",
      "Trained batch 2290 batch loss 0.670827508 epoch total loss 0.769947588\n",
      "Trained batch 2291 batch loss 0.691085339 epoch total loss 0.769913137\n",
      "Trained batch 2292 batch loss 0.67100966 epoch total loss 0.76987\n",
      "Trained batch 2293 batch loss 0.701671302 epoch total loss 0.76984024\n",
      "Trained batch 2294 batch loss 0.683661878 epoch total loss 0.76980269\n",
      "Trained batch 2295 batch loss 0.6594854 epoch total loss 0.769754648\n",
      "Trained batch 2296 batch loss 0.705147386 epoch total loss 0.769726515\n",
      "Trained batch 2297 batch loss 0.733795583 epoch total loss 0.769710898\n",
      "Trained batch 2298 batch loss 0.75512892 epoch total loss 0.769704521\n",
      "Trained batch 2299 batch loss 0.774492919 epoch total loss 0.769706607\n",
      "Trained batch 2300 batch loss 0.716688752 epoch total loss 0.769683599\n",
      "Trained batch 2301 batch loss 0.809262395 epoch total loss 0.769700766\n",
      "Trained batch 2302 batch loss 0.787488878 epoch total loss 0.769708455\n",
      "Trained batch 2303 batch loss 0.720957458 epoch total loss 0.769687295\n",
      "Trained batch 2304 batch loss 0.643503428 epoch total loss 0.769632578\n",
      "Trained batch 2305 batch loss 0.645444 epoch total loss 0.769578636\n",
      "Trained batch 2306 batch loss 0.586584449 epoch total loss 0.769499302\n",
      "Trained batch 2307 batch loss 0.656851232 epoch total loss 0.769450426\n",
      "Trained batch 2308 batch loss 0.647177577 epoch total loss 0.769397497\n",
      "Trained batch 2309 batch loss 0.676025808 epoch total loss 0.769357\n",
      "Trained batch 2310 batch loss 0.695043 epoch total loss 0.769324899\n",
      "Trained batch 2311 batch loss 0.659351 epoch total loss 0.769277275\n",
      "Trained batch 2312 batch loss 0.738730192 epoch total loss 0.769264102\n",
      "Trained batch 2313 batch loss 0.693636596 epoch total loss 0.769231379\n",
      "Trained batch 2314 batch loss 0.684244573 epoch total loss 0.769194663\n",
      "Trained batch 2315 batch loss 0.619630516 epoch total loss 0.769130051\n",
      "Trained batch 2316 batch loss 0.728860497 epoch total loss 0.769112647\n",
      "Trained batch 2317 batch loss 0.699245155 epoch total loss 0.769082487\n",
      "Trained batch 2318 batch loss 0.705640674 epoch total loss 0.769055128\n",
      "Trained batch 2319 batch loss 0.643027842 epoch total loss 0.769000828\n",
      "Trained batch 2320 batch loss 0.774372637 epoch total loss 0.769003153\n",
      "Trained batch 2321 batch loss 0.692414403 epoch total loss 0.768970132\n",
      "Trained batch 2322 batch loss 0.691018939 epoch total loss 0.768936574\n",
      "Trained batch 2323 batch loss 0.74622333 epoch total loss 0.768926799\n",
      "Trained batch 2324 batch loss 0.804116845 epoch total loss 0.768941879\n",
      "Trained batch 2325 batch loss 0.757827282 epoch total loss 0.768937111\n",
      "Trained batch 2326 batch loss 0.594900072 epoch total loss 0.768862307\n",
      "Trained batch 2327 batch loss 0.675206 epoch total loss 0.768822\n",
      "Trained batch 2328 batch loss 0.630111396 epoch total loss 0.76876241\n",
      "Trained batch 2329 batch loss 0.691957831 epoch total loss 0.768729508\n",
      "Trained batch 2330 batch loss 0.694332063 epoch total loss 0.76869756\n",
      "Trained batch 2331 batch loss 0.694920123 epoch total loss 0.76866591\n",
      "Trained batch 2332 batch loss 0.756749 epoch total loss 0.768660784\n",
      "Trained batch 2333 batch loss 0.758921087 epoch total loss 0.768656611\n",
      "Trained batch 2334 batch loss 0.843782961 epoch total loss 0.768688798\n",
      "Trained batch 2335 batch loss 0.778042436 epoch total loss 0.768692791\n",
      "Trained batch 2336 batch loss 0.802019715 epoch total loss 0.768707037\n",
      "Trained batch 2337 batch loss 0.697871447 epoch total loss 0.768676758\n",
      "Trained batch 2338 batch loss 0.729121 epoch total loss 0.76865983\n",
      "Trained batch 2339 batch loss 0.692564785 epoch total loss 0.768627286\n",
      "Trained batch 2340 batch loss 0.661909461 epoch total loss 0.768581629\n",
      "Trained batch 2341 batch loss 0.63139075 epoch total loss 0.768523037\n",
      "Trained batch 2342 batch loss 0.683996439 epoch total loss 0.768486917\n",
      "Trained batch 2343 batch loss 0.647823095 epoch total loss 0.768435419\n",
      "Trained batch 2344 batch loss 0.718941033 epoch total loss 0.768414319\n",
      "Trained batch 2345 batch loss 0.702604473 epoch total loss 0.768386304\n",
      "Trained batch 2346 batch loss 0.671728849 epoch total loss 0.768345118\n",
      "Trained batch 2347 batch loss 0.733880162 epoch total loss 0.768330395\n",
      "Trained batch 2348 batch loss 0.664710462 epoch total loss 0.768286288\n",
      "Trained batch 2349 batch loss 0.700854778 epoch total loss 0.768257558\n",
      "Trained batch 2350 batch loss 0.700422466 epoch total loss 0.76822871\n",
      "Trained batch 2351 batch loss 0.787638605 epoch total loss 0.768236935\n",
      "Trained batch 2352 batch loss 0.780710936 epoch total loss 0.76824224\n",
      "Trained batch 2353 batch loss 0.750808418 epoch total loss 0.768234849\n",
      "Trained batch 2354 batch loss 0.785977185 epoch total loss 0.768242419\n",
      "Trained batch 2355 batch loss 0.785694778 epoch total loss 0.76824981\n",
      "Trained batch 2356 batch loss 0.760946572 epoch total loss 0.76824671\n",
      "Trained batch 2357 batch loss 0.686028898 epoch total loss 0.768211842\n",
      "Trained batch 2358 batch loss 0.727264524 epoch total loss 0.768194497\n",
      "Trained batch 2359 batch loss 0.722129762 epoch total loss 0.768174946\n",
      "Trained batch 2360 batch loss 0.691952288 epoch total loss 0.768142641\n",
      "Trained batch 2361 batch loss 0.718256 epoch total loss 0.768121541\n",
      "Trained batch 2362 batch loss 0.667000115 epoch total loss 0.768078685\n",
      "Trained batch 2363 batch loss 0.713840604 epoch total loss 0.768055737\n",
      "Trained batch 2364 batch loss 0.748659968 epoch total loss 0.768047571\n",
      "Trained batch 2365 batch loss 0.797145128 epoch total loss 0.76805985\n",
      "Trained batch 2366 batch loss 0.753287315 epoch total loss 0.768053591\n",
      "Trained batch 2367 batch loss 0.734322727 epoch total loss 0.768039405\n",
      "Trained batch 2368 batch loss 0.827805102 epoch total loss 0.768064618\n",
      "Trained batch 2369 batch loss 0.794234216 epoch total loss 0.768075645\n",
      "Trained batch 2370 batch loss 0.904457092 epoch total loss 0.768133163\n",
      "Trained batch 2371 batch loss 0.712697744 epoch total loss 0.768109739\n",
      "Trained batch 2372 batch loss 0.792947233 epoch total loss 0.768120229\n",
      "Trained batch 2373 batch loss 0.767697036 epoch total loss 0.76812005\n",
      "Trained batch 2374 batch loss 0.756543517 epoch total loss 0.768115222\n",
      "Trained batch 2375 batch loss 0.828257799 epoch total loss 0.768140495\n",
      "Trained batch 2376 batch loss 0.736190796 epoch total loss 0.768127084\n",
      "Trained batch 2377 batch loss 0.754893363 epoch total loss 0.768121481\n",
      "Trained batch 2378 batch loss 0.832234561 epoch total loss 0.768148482\n",
      "Trained batch 2379 batch loss 0.820761085 epoch total loss 0.768170595\n",
      "Trained batch 2380 batch loss 0.783421159 epoch total loss 0.768177032\n",
      "Trained batch 2381 batch loss 0.822812617 epoch total loss 0.7682\n",
      "Trained batch 2382 batch loss 0.732260704 epoch total loss 0.7681849\n",
      "Trained batch 2383 batch loss 0.752437294 epoch total loss 0.768178284\n",
      "Trained batch 2384 batch loss 0.665453851 epoch total loss 0.76813519\n",
      "Trained batch 2385 batch loss 0.66343379 epoch total loss 0.768091261\n",
      "Trained batch 2386 batch loss 0.714984894 epoch total loss 0.768069\n",
      "Trained batch 2387 batch loss 0.794889212 epoch total loss 0.768080294\n",
      "Trained batch 2388 batch loss 0.785658836 epoch total loss 0.768087626\n",
      "Trained batch 2389 batch loss 0.675527453 epoch total loss 0.768048882\n",
      "Trained batch 2390 batch loss 0.750587881 epoch total loss 0.768041611\n",
      "Trained batch 2391 batch loss 0.774984479 epoch total loss 0.768044531\n",
      "Trained batch 2392 batch loss 0.721184671 epoch total loss 0.768024921\n",
      "Trained batch 2393 batch loss 0.696587086 epoch total loss 0.767995059\n",
      "Trained batch 2394 batch loss 0.739054203 epoch total loss 0.76798296\n",
      "Trained batch 2395 batch loss 0.758227944 epoch total loss 0.767978847\n",
      "Trained batch 2396 batch loss 0.764696956 epoch total loss 0.767977476\n",
      "Trained batch 2397 batch loss 0.77559489 epoch total loss 0.767980635\n",
      "Trained batch 2398 batch loss 0.712690711 epoch total loss 0.767957568\n",
      "Trained batch 2399 batch loss 0.736466706 epoch total loss 0.767944455\n",
      "Trained batch 2400 batch loss 0.728946805 epoch total loss 0.767928183\n",
      "Trained batch 2401 batch loss 0.75409925 epoch total loss 0.767922461\n",
      "Trained batch 2402 batch loss 0.665691 epoch total loss 0.767879903\n",
      "Trained batch 2403 batch loss 0.657494366 epoch total loss 0.767833948\n",
      "Trained batch 2404 batch loss 0.670533478 epoch total loss 0.767793477\n",
      "Trained batch 2405 batch loss 0.635867476 epoch total loss 0.76773864\n",
      "Trained batch 2406 batch loss 0.613863111 epoch total loss 0.767674685\n",
      "Trained batch 2407 batch loss 0.593332112 epoch total loss 0.767602265\n",
      "Trained batch 2408 batch loss 0.633332491 epoch total loss 0.767546475\n",
      "Trained batch 2409 batch loss 0.703377724 epoch total loss 0.767519832\n",
      "Trained batch 2410 batch loss 0.667887866 epoch total loss 0.767478466\n",
      "Trained batch 2411 batch loss 0.610207319 epoch total loss 0.767413259\n",
      "Trained batch 2412 batch loss 0.613721728 epoch total loss 0.767349601\n",
      "Trained batch 2413 batch loss 0.556436777 epoch total loss 0.767262161\n",
      "Trained batch 2414 batch loss 0.561319888 epoch total loss 0.767176807\n",
      "Trained batch 2415 batch loss 0.561213851 epoch total loss 0.767091513\n",
      "Trained batch 2416 batch loss 0.596469581 epoch total loss 0.767020881\n",
      "Trained batch 2417 batch loss 0.52545011 epoch total loss 0.766920924\n",
      "Trained batch 2418 batch loss 0.532297611 epoch total loss 0.766823888\n",
      "Trained batch 2419 batch loss 0.75290072 epoch total loss 0.766818166\n",
      "Trained batch 2420 batch loss 0.723059416 epoch total loss 0.766800046\n",
      "Trained batch 2421 batch loss 0.772078753 epoch total loss 0.766802251\n",
      "Trained batch 2422 batch loss 0.768523693 epoch total loss 0.766802967\n",
      "Trained batch 2423 batch loss 0.739662051 epoch total loss 0.766791761\n",
      "Trained batch 2424 batch loss 0.703104377 epoch total loss 0.766765475\n",
      "Trained batch 2425 batch loss 0.739513874 epoch total loss 0.76675421\n",
      "Trained batch 2426 batch loss 0.700878143 epoch total loss 0.76672709\n",
      "Trained batch 2427 batch loss 0.597019792 epoch total loss 0.766657174\n",
      "Trained batch 2428 batch loss 0.737826586 epoch total loss 0.766645312\n",
      "Trained batch 2429 batch loss 0.699579895 epoch total loss 0.766617715\n",
      "Trained batch 2430 batch loss 0.696702361 epoch total loss 0.766588926\n",
      "Trained batch 2431 batch loss 0.680642903 epoch total loss 0.766553581\n",
      "Trained batch 2432 batch loss 0.689372838 epoch total loss 0.766521811\n",
      "Trained batch 2433 batch loss 0.680209756 epoch total loss 0.766486287\n",
      "Trained batch 2434 batch loss 0.688726783 epoch total loss 0.766454339\n",
      "Trained batch 2435 batch loss 0.725030124 epoch total loss 0.766437352\n",
      "Trained batch 2436 batch loss 0.686626196 epoch total loss 0.766404569\n",
      "Trained batch 2437 batch loss 0.694384396 epoch total loss 0.766375\n",
      "Trained batch 2438 batch loss 0.684035599 epoch total loss 0.766341269\n",
      "Trained batch 2439 batch loss 0.675332904 epoch total loss 0.766303897\n",
      "Trained batch 2440 batch loss 0.685125649 epoch total loss 0.766270697\n",
      "Trained batch 2441 batch loss 0.732245803 epoch total loss 0.76625675\n",
      "Trained batch 2442 batch loss 0.661670387 epoch total loss 0.766213894\n",
      "Trained batch 2443 batch loss 0.782215774 epoch total loss 0.76622045\n",
      "Trained batch 2444 batch loss 0.783618867 epoch total loss 0.766227543\n",
      "Trained batch 2445 batch loss 0.707617 epoch total loss 0.766203582\n",
      "Trained batch 2446 batch loss 0.754793525 epoch total loss 0.766198933\n",
      "Trained batch 2447 batch loss 0.725600481 epoch total loss 0.766182303\n",
      "Trained batch 2448 batch loss 0.723060071 epoch total loss 0.76616472\n",
      "Trained batch 2449 batch loss 0.727976382 epoch total loss 0.766149104\n",
      "Trained batch 2450 batch loss 0.701022923 epoch total loss 0.76612252\n",
      "Trained batch 2451 batch loss 0.696419418 epoch total loss 0.766094089\n",
      "Trained batch 2452 batch loss 0.711083651 epoch total loss 0.766071677\n",
      "Trained batch 2453 batch loss 0.604007661 epoch total loss 0.766005576\n",
      "Trained batch 2454 batch loss 0.615050852 epoch total loss 0.765944064\n",
      "Trained batch 2455 batch loss 0.589330733 epoch total loss 0.765872121\n",
      "Trained batch 2456 batch loss 0.654559255 epoch total loss 0.765826821\n",
      "Trained batch 2457 batch loss 0.659850895 epoch total loss 0.765783608\n",
      "Trained batch 2458 batch loss 0.691176593 epoch total loss 0.765753269\n",
      "Trained batch 2459 batch loss 0.744418621 epoch total loss 0.765744567\n",
      "Trained batch 2460 batch loss 0.737370312 epoch total loss 0.765733063\n",
      "Trained batch 2461 batch loss 0.771961808 epoch total loss 0.765735626\n",
      "Trained batch 2462 batch loss 0.72961247 epoch total loss 0.765720963\n",
      "Trained batch 2463 batch loss 0.685704291 epoch total loss 0.765688419\n",
      "Trained batch 2464 batch loss 0.667942822 epoch total loss 0.765648782\n",
      "Trained batch 2465 batch loss 0.640753269 epoch total loss 0.765598118\n",
      "Trained batch 2466 batch loss 0.652106822 epoch total loss 0.765552104\n",
      "Trained batch 2467 batch loss 0.696684599 epoch total loss 0.765524149\n",
      "Trained batch 2468 batch loss 0.647774398 epoch total loss 0.765476465\n",
      "Trained batch 2469 batch loss 0.718921065 epoch total loss 0.765457571\n",
      "Trained batch 2470 batch loss 0.728806257 epoch total loss 0.765442729\n",
      "Trained batch 2471 batch loss 0.544688463 epoch total loss 0.765353382\n",
      "Trained batch 2472 batch loss 0.646024406 epoch total loss 0.765305102\n",
      "Trained batch 2473 batch loss 0.596130252 epoch total loss 0.765236676\n",
      "Trained batch 2474 batch loss 0.6200881 epoch total loss 0.765178\n",
      "Trained batch 2475 batch loss 0.699590504 epoch total loss 0.765151501\n",
      "Trained batch 2476 batch loss 0.615568519 epoch total loss 0.765091121\n",
      "Trained batch 2477 batch loss 0.7813887 epoch total loss 0.765097678\n",
      "Trained batch 2478 batch loss 0.849245965 epoch total loss 0.765131652\n",
      "Trained batch 2479 batch loss 0.764771223 epoch total loss 0.765131474\n",
      "Trained batch 2480 batch loss 0.74684906 epoch total loss 0.765124142\n",
      "Trained batch 2481 batch loss 0.708958626 epoch total loss 0.765101492\n",
      "Trained batch 2482 batch loss 0.601268888 epoch total loss 0.76503551\n",
      "Trained batch 2483 batch loss 0.660456717 epoch total loss 0.76499337\n",
      "Trained batch 2484 batch loss 0.665245 epoch total loss 0.764953196\n",
      "Trained batch 2485 batch loss 0.681372821 epoch total loss 0.764919579\n",
      "Trained batch 2486 batch loss 0.710421443 epoch total loss 0.764897704\n",
      "Trained batch 2487 batch loss 0.689962268 epoch total loss 0.764867544\n",
      "Trained batch 2488 batch loss 0.698851466 epoch total loss 0.764841\n",
      "Trained batch 2489 batch loss 0.750425577 epoch total loss 0.764835179\n",
      "Trained batch 2490 batch loss 0.781779826 epoch total loss 0.764842\n",
      "Trained batch 2491 batch loss 0.686509192 epoch total loss 0.764810562\n",
      "Trained batch 2492 batch loss 0.756318331 epoch total loss 0.764807165\n",
      "Trained batch 2493 batch loss 0.714270234 epoch total loss 0.764786839\n",
      "Trained batch 2494 batch loss 0.711381912 epoch total loss 0.764765441\n",
      "Trained batch 2495 batch loss 0.717156 epoch total loss 0.764746368\n",
      "Trained batch 2496 batch loss 0.669946671 epoch total loss 0.7647084\n",
      "Trained batch 2497 batch loss 0.713280678 epoch total loss 0.764687777\n",
      "Trained batch 2498 batch loss 0.695369124 epoch total loss 0.76466\n",
      "Trained batch 2499 batch loss 0.662789 epoch total loss 0.764619291\n",
      "Trained batch 2500 batch loss 0.691097498 epoch total loss 0.764589846\n",
      "Trained batch 2501 batch loss 0.683817565 epoch total loss 0.76455754\n",
      "Trained batch 2502 batch loss 0.76497525 epoch total loss 0.764557719\n",
      "Trained batch 2503 batch loss 0.723331153 epoch total loss 0.764541268\n",
      "Trained batch 2504 batch loss 0.768575132 epoch total loss 0.764542878\n",
      "Trained batch 2505 batch loss 0.812698483 epoch total loss 0.76456213\n",
      "Trained batch 2506 batch loss 0.785168946 epoch total loss 0.764570355\n",
      "Trained batch 2507 batch loss 0.758858144 epoch total loss 0.76456809\n",
      "Trained batch 2508 batch loss 0.751173913 epoch total loss 0.764562786\n",
      "Trained batch 2509 batch loss 0.662302136 epoch total loss 0.764522\n",
      "Trained batch 2510 batch loss 0.687416 epoch total loss 0.76449132\n",
      "Trained batch 2511 batch loss 0.686971903 epoch total loss 0.764460444\n",
      "Trained batch 2512 batch loss 0.646733 epoch total loss 0.764413595\n",
      "Trained batch 2513 batch loss 0.642527699 epoch total loss 0.764365077\n",
      "Trained batch 2514 batch loss 0.661868 epoch total loss 0.764324307\n",
      "Trained batch 2515 batch loss 0.748231471 epoch total loss 0.76431793\n",
      "Trained batch 2516 batch loss 0.780528963 epoch total loss 0.764324367\n",
      "Trained batch 2517 batch loss 0.716576099 epoch total loss 0.764305413\n",
      "Trained batch 2518 batch loss 0.694400907 epoch total loss 0.764277697\n",
      "Trained batch 2519 batch loss 0.725507617 epoch total loss 0.764262259\n",
      "Trained batch 2520 batch loss 0.734558761 epoch total loss 0.764250517\n",
      "Trained batch 2521 batch loss 0.750273824 epoch total loss 0.764244914\n",
      "Trained batch 2522 batch loss 0.756625473 epoch total loss 0.764241934\n",
      "Trained batch 2523 batch loss 0.748860896 epoch total loss 0.764235854\n",
      "Trained batch 2524 batch loss 0.680128932 epoch total loss 0.764202535\n",
      "Trained batch 2525 batch loss 0.678833604 epoch total loss 0.764168739\n",
      "Trained batch 2526 batch loss 0.665817261 epoch total loss 0.764129758\n",
      "Trained batch 2527 batch loss 0.567592 epoch total loss 0.764052\n",
      "Trained batch 2528 batch loss 0.599681437 epoch total loss 0.763987\n",
      "Trained batch 2529 batch loss 0.63086015 epoch total loss 0.763934374\n",
      "Trained batch 2530 batch loss 0.648417 epoch total loss 0.763888717\n",
      "Trained batch 2531 batch loss 0.80793488 epoch total loss 0.763906121\n",
      "Trained batch 2532 batch loss 0.759260118 epoch total loss 0.763904274\n",
      "Trained batch 2533 batch loss 0.736278057 epoch total loss 0.763893425\n",
      "Trained batch 2534 batch loss 0.740597785 epoch total loss 0.763884246\n",
      "Trained batch 2535 batch loss 0.752916694 epoch total loss 0.763879895\n",
      "Trained batch 2536 batch loss 0.77611804 epoch total loss 0.763884723\n",
      "Trained batch 2537 batch loss 0.729669452 epoch total loss 0.763871193\n",
      "Trained batch 2538 batch loss 0.780207515 epoch total loss 0.76387763\n",
      "Trained batch 2539 batch loss 0.790360153 epoch total loss 0.763888061\n",
      "Trained batch 2540 batch loss 0.784359515 epoch total loss 0.763896108\n",
      "Trained batch 2541 batch loss 0.691365659 epoch total loss 0.763867617\n",
      "Trained batch 2542 batch loss 0.717385232 epoch total loss 0.763849318\n",
      "Trained batch 2543 batch loss 0.719110608 epoch total loss 0.763831735\n",
      "Trained batch 2544 batch loss 0.720074713 epoch total loss 0.763814509\n",
      "Trained batch 2545 batch loss 0.768293 epoch total loss 0.763816297\n",
      "Trained batch 2546 batch loss 0.659288 epoch total loss 0.763775229\n",
      "Trained batch 2547 batch loss 0.726497829 epoch total loss 0.763760567\n",
      "Trained batch 2548 batch loss 0.710445166 epoch total loss 0.763739645\n",
      "Trained batch 2549 batch loss 0.767581522 epoch total loss 0.763741195\n",
      "Trained batch 2550 batch loss 0.774044454 epoch total loss 0.763745189\n",
      "Trained batch 2551 batch loss 0.659785509 epoch total loss 0.763704479\n",
      "Trained batch 2552 batch loss 0.77261883 epoch total loss 0.763707936\n",
      "Trained batch 2553 batch loss 0.766035616 epoch total loss 0.76370883\n",
      "Trained batch 2554 batch loss 0.710869968 epoch total loss 0.763688147\n",
      "Trained batch 2555 batch loss 0.771481 epoch total loss 0.763691187\n",
      "Trained batch 2556 batch loss 0.702038109 epoch total loss 0.763667047\n",
      "Trained batch 2557 batch loss 0.710548162 epoch total loss 0.763646305\n",
      "Trained batch 2558 batch loss 0.714308321 epoch total loss 0.763627\n",
      "Trained batch 2559 batch loss 0.709036 epoch total loss 0.763605654\n",
      "Trained batch 2560 batch loss 0.708899677 epoch total loss 0.763584256\n",
      "Trained batch 2561 batch loss 0.72225976 epoch total loss 0.763568163\n",
      "Trained batch 2562 batch loss 0.717241406 epoch total loss 0.763550103\n",
      "Trained batch 2563 batch loss 0.711739123 epoch total loss 0.763529897\n",
      "Trained batch 2564 batch loss 0.725614488 epoch total loss 0.763515115\n",
      "Trained batch 2565 batch loss 0.706199706 epoch total loss 0.763492763\n",
      "Trained batch 2566 batch loss 0.707412839 epoch total loss 0.763470888\n",
      "Trained batch 2567 batch loss 0.659333467 epoch total loss 0.763430297\n",
      "Trained batch 2568 batch loss 0.633814 epoch total loss 0.763379812\n",
      "Trained batch 2569 batch loss 0.675696135 epoch total loss 0.763345659\n",
      "Trained batch 2570 batch loss 0.633558869 epoch total loss 0.763295174\n",
      "Trained batch 2571 batch loss 0.617615879 epoch total loss 0.763238549\n",
      "Trained batch 2572 batch loss 0.7210958 epoch total loss 0.763222158\n",
      "Trained batch 2573 batch loss 0.734757662 epoch total loss 0.763211071\n",
      "Trained batch 2574 batch loss 0.705796421 epoch total loss 0.763188779\n",
      "Trained batch 2575 batch loss 0.763940513 epoch total loss 0.763189\n",
      "Trained batch 2576 batch loss 0.77664113 epoch total loss 0.763194263\n",
      "Trained batch 2577 batch loss 0.803671181 epoch total loss 0.76321\n",
      "Trained batch 2578 batch loss 0.838510334 epoch total loss 0.763239205\n",
      "Trained batch 2579 batch loss 0.757781863 epoch total loss 0.763237059\n",
      "Trained batch 2580 batch loss 0.750434279 epoch total loss 0.763232112\n",
      "Trained batch 2581 batch loss 0.788662434 epoch total loss 0.763242\n",
      "Trained batch 2582 batch loss 0.736511469 epoch total loss 0.763231695\n",
      "Trained batch 2583 batch loss 0.715583086 epoch total loss 0.763213217\n",
      "Trained batch 2584 batch loss 0.737769365 epoch total loss 0.763203382\n",
      "Trained batch 2585 batch loss 0.731749177 epoch total loss 0.763191223\n",
      "Trained batch 2586 batch loss 0.670689285 epoch total loss 0.763155401\n",
      "Trained batch 2587 batch loss 0.658631325 epoch total loss 0.763115048\n",
      "Trained batch 2588 batch loss 0.606060863 epoch total loss 0.763054371\n",
      "Trained batch 2589 batch loss 0.723029 epoch total loss 0.763038874\n",
      "Trained batch 2590 batch loss 0.693340421 epoch total loss 0.763012\n",
      "Trained batch 2591 batch loss 0.694714129 epoch total loss 0.762985647\n",
      "Trained batch 2592 batch loss 0.761336863 epoch total loss 0.762985\n",
      "Trained batch 2593 batch loss 0.734083295 epoch total loss 0.762973845\n",
      "Trained batch 2594 batch loss 0.790367723 epoch total loss 0.762984455\n",
      "Trained batch 2595 batch loss 0.695733488 epoch total loss 0.762958527\n",
      "Trained batch 2596 batch loss 0.775599301 epoch total loss 0.762963414\n",
      "Trained batch 2597 batch loss 0.716425121 epoch total loss 0.762945473\n",
      "Trained batch 2598 batch loss 0.706445098 epoch total loss 0.762923717\n",
      "Trained batch 2599 batch loss 0.717222214 epoch total loss 0.762906134\n",
      "Trained batch 2600 batch loss 0.672119617 epoch total loss 0.762871206\n",
      "Trained batch 2601 batch loss 0.709806919 epoch total loss 0.762850821\n",
      "Trained batch 2602 batch loss 0.761593938 epoch total loss 0.762850344\n",
      "Trained batch 2603 batch loss 0.699837804 epoch total loss 0.762826085\n",
      "Trained batch 2604 batch loss 0.681334 epoch total loss 0.762794793\n",
      "Trained batch 2605 batch loss 0.751433551 epoch total loss 0.762790442\n",
      "Trained batch 2606 batch loss 0.738544345 epoch total loss 0.762781143\n",
      "Trained batch 2607 batch loss 0.579212904 epoch total loss 0.76271069\n",
      "Trained batch 2608 batch loss 0.6278916 epoch total loss 0.762659\n",
      "Trained batch 2609 batch loss 0.648681283 epoch total loss 0.762615323\n",
      "Trained batch 2610 batch loss 0.571409106 epoch total loss 0.762542069\n",
      "Trained batch 2611 batch loss 0.610108197 epoch total loss 0.762483716\n",
      "Trained batch 2612 batch loss 0.635956585 epoch total loss 0.762435257\n",
      "Trained batch 2613 batch loss 0.568422854 epoch total loss 0.76236105\n",
      "Trained batch 2614 batch loss 0.690960765 epoch total loss 0.762333751\n",
      "Trained batch 2615 batch loss 0.851864159 epoch total loss 0.762367964\n",
      "Trained batch 2616 batch loss 0.934864223 epoch total loss 0.762433887\n",
      "Trained batch 2617 batch loss 0.79110527 epoch total loss 0.762444854\n",
      "Trained batch 2618 batch loss 0.737666428 epoch total loss 0.762435377\n",
      "Trained batch 2619 batch loss 0.7429263 epoch total loss 0.762427926\n",
      "Trained batch 2620 batch loss 0.670968294 epoch total loss 0.762393\n",
      "Trained batch 2621 batch loss 0.661757648 epoch total loss 0.762354612\n",
      "Trained batch 2622 batch loss 0.695588708 epoch total loss 0.762329161\n",
      "Trained batch 2623 batch loss 0.730571151 epoch total loss 0.762317061\n",
      "Trained batch 2624 batch loss 0.699337304 epoch total loss 0.762293041\n",
      "Trained batch 2625 batch loss 0.667546332 epoch total loss 0.762257\n",
      "Trained batch 2626 batch loss 0.665384352 epoch total loss 0.762220085\n",
      "Trained batch 2627 batch loss 0.694120467 epoch total loss 0.762194157\n",
      "Trained batch 2628 batch loss 0.672964036 epoch total loss 0.762160182\n",
      "Trained batch 2629 batch loss 0.658228695 epoch total loss 0.762120664\n",
      "Trained batch 2630 batch loss 0.68759644 epoch total loss 0.762092352\n",
      "Trained batch 2631 batch loss 0.7234146 epoch total loss 0.76207763\n",
      "Trained batch 2632 batch loss 0.734456778 epoch total loss 0.762067139\n",
      "Trained batch 2633 batch loss 0.718590796 epoch total loss 0.762050629\n",
      "Trained batch 2634 batch loss 0.695041418 epoch total loss 0.762025237\n",
      "Trained batch 2635 batch loss 0.710297 epoch total loss 0.762005627\n",
      "Trained batch 2636 batch loss 0.685512722 epoch total loss 0.7619766\n",
      "Trained batch 2637 batch loss 0.703868806 epoch total loss 0.761954546\n",
      "Trained batch 2638 batch loss 0.762876093 epoch total loss 0.761954904\n",
      "Trained batch 2639 batch loss 0.699387431 epoch total loss 0.761931181\n",
      "Trained batch 2640 batch loss 0.621742845 epoch total loss 0.761878\n",
      "Trained batch 2641 batch loss 0.679795504 epoch total loss 0.76184696\n",
      "Trained batch 2642 batch loss 0.660782754 epoch total loss 0.761808693\n",
      "Trained batch 2643 batch loss 0.660425782 epoch total loss 0.761770368\n",
      "Trained batch 2644 batch loss 0.724685073 epoch total loss 0.761756361\n",
      "Trained batch 2645 batch loss 0.66951263 epoch total loss 0.761721492\n",
      "Trained batch 2646 batch loss 0.665869117 epoch total loss 0.761685252\n",
      "Trained batch 2647 batch loss 0.67799592 epoch total loss 0.761653662\n",
      "Trained batch 2648 batch loss 0.719772 epoch total loss 0.761637807\n",
      "Trained batch 2649 batch loss 0.684843123 epoch total loss 0.761608779\n",
      "Trained batch 2650 batch loss 0.675454736 epoch total loss 0.761576295\n",
      "Trained batch 2651 batch loss 0.710407734 epoch total loss 0.761557\n",
      "Trained batch 2652 batch loss 0.725876808 epoch total loss 0.761543512\n",
      "Trained batch 2653 batch loss 0.707096636 epoch total loss 0.761523\n",
      "Trained batch 2654 batch loss 0.674155 epoch total loss 0.761490107\n",
      "Trained batch 2655 batch loss 0.747921467 epoch total loss 0.761485\n",
      "Trained batch 2656 batch loss 0.711491108 epoch total loss 0.761466205\n",
      "Trained batch 2657 batch loss 0.694760263 epoch total loss 0.761441052\n",
      "Trained batch 2658 batch loss 0.629979 epoch total loss 0.76139164\n",
      "Trained batch 2659 batch loss 0.693937838 epoch total loss 0.761366248\n",
      "Trained batch 2660 batch loss 0.654099107 epoch total loss 0.761325896\n",
      "Trained batch 2661 batch loss 0.706708848 epoch total loss 0.761305392\n",
      "Trained batch 2662 batch loss 0.685061514 epoch total loss 0.761276722\n",
      "Trained batch 2663 batch loss 0.644093454 epoch total loss 0.761232734\n",
      "Trained batch 2664 batch loss 0.653251767 epoch total loss 0.761192143\n",
      "Trained batch 2665 batch loss 0.619298756 epoch total loss 0.761138916\n",
      "Trained batch 2666 batch loss 0.654907286 epoch total loss 0.761099041\n",
      "Trained batch 2667 batch loss 0.631896138 epoch total loss 0.761050582\n",
      "Trained batch 2668 batch loss 0.69470638 epoch total loss 0.761025727\n",
      "Trained batch 2669 batch loss 0.659132302 epoch total loss 0.76098758\n",
      "Trained batch 2670 batch loss 0.665990472 epoch total loss 0.760952\n",
      "Trained batch 2671 batch loss 0.672526598 epoch total loss 0.760918856\n",
      "Trained batch 2672 batch loss 0.690892637 epoch total loss 0.760892689\n",
      "Trained batch 2673 batch loss 0.643465459 epoch total loss 0.760848701\n",
      "Trained batch 2674 batch loss 0.585551262 epoch total loss 0.760783195\n",
      "Trained batch 2675 batch loss 0.673579335 epoch total loss 0.760750592\n",
      "Trained batch 2676 batch loss 0.640629232 epoch total loss 0.760705709\n",
      "Trained batch 2677 batch loss 0.708578467 epoch total loss 0.760686219\n",
      "Trained batch 2678 batch loss 0.709664226 epoch total loss 0.760667205\n",
      "Trained batch 2679 batch loss 0.611397743 epoch total loss 0.760611534\n",
      "Trained batch 2680 batch loss 0.686719835 epoch total loss 0.760583937\n",
      "Trained batch 2681 batch loss 0.610961795 epoch total loss 0.760528147\n",
      "Trained batch 2682 batch loss 0.77529496 epoch total loss 0.760533631\n",
      "Trained batch 2683 batch loss 0.73056972 epoch total loss 0.760522485\n",
      "Trained batch 2684 batch loss 0.692469358 epoch total loss 0.760497153\n",
      "Trained batch 2685 batch loss 0.728418827 epoch total loss 0.760485172\n",
      "Trained batch 2686 batch loss 0.701387525 epoch total loss 0.760463178\n",
      "Trained batch 2687 batch loss 0.703192115 epoch total loss 0.760441899\n",
      "Trained batch 2688 batch loss 0.682353 epoch total loss 0.760412872\n",
      "Trained batch 2689 batch loss 0.685781121 epoch total loss 0.760385096\n",
      "Trained batch 2690 batch loss 0.538680255 epoch total loss 0.760302663\n",
      "Trained batch 2691 batch loss 0.59968704 epoch total loss 0.760243\n",
      "Trained batch 2692 batch loss 0.610962152 epoch total loss 0.760187566\n",
      "Trained batch 2693 batch loss 0.609863281 epoch total loss 0.760131717\n",
      "Trained batch 2694 batch loss 0.63259083 epoch total loss 0.760084391\n",
      "Trained batch 2695 batch loss 0.702227116 epoch total loss 0.760062933\n",
      "Trained batch 2696 batch loss 0.656877 epoch total loss 0.760024726\n",
      "Trained batch 2697 batch loss 0.617213 epoch total loss 0.759971738\n",
      "Trained batch 2698 batch loss 0.634862781 epoch total loss 0.759925365\n",
      "Trained batch 2699 batch loss 0.702520669 epoch total loss 0.759904087\n",
      "Trained batch 2700 batch loss 0.683157444 epoch total loss 0.759875655\n",
      "Trained batch 2701 batch loss 0.649582148 epoch total loss 0.759834886\n",
      "Trained batch 2702 batch loss 0.661567 epoch total loss 0.759798527\n",
      "Trained batch 2703 batch loss 0.655009449 epoch total loss 0.759759784\n",
      "Trained batch 2704 batch loss 0.681097269 epoch total loss 0.759730697\n",
      "Trained batch 2705 batch loss 0.715180457 epoch total loss 0.759714186\n",
      "Trained batch 2706 batch loss 0.706695199 epoch total loss 0.759694636\n",
      "Trained batch 2707 batch loss 0.725140452 epoch total loss 0.759681821\n",
      "Trained batch 2708 batch loss 0.651763439 epoch total loss 0.759642\n",
      "Trained batch 2709 batch loss 0.677894771 epoch total loss 0.759611905\n",
      "Trained batch 2710 batch loss 0.80257076 epoch total loss 0.7596277\n",
      "Trained batch 2711 batch loss 0.750694513 epoch total loss 0.759624422\n",
      "Trained batch 2712 batch loss 0.586311758 epoch total loss 0.759560525\n",
      "Trained batch 2713 batch loss 0.635316 epoch total loss 0.759514749\n",
      "Trained batch 2714 batch loss 0.598859727 epoch total loss 0.759455562\n",
      "Trained batch 2715 batch loss 0.632856727 epoch total loss 0.759408891\n",
      "Trained batch 2716 batch loss 0.643170476 epoch total loss 0.759366035\n",
      "Trained batch 2717 batch loss 0.683651745 epoch total loss 0.7593382\n",
      "Trained batch 2718 batch loss 0.690826714 epoch total loss 0.759313\n",
      "Trained batch 2719 batch loss 0.699148178 epoch total loss 0.759290874\n",
      "Trained batch 2720 batch loss 0.688305318 epoch total loss 0.759264767\n",
      "Trained batch 2721 batch loss 0.722289264 epoch total loss 0.759251118\n",
      "Trained batch 2722 batch loss 0.771009922 epoch total loss 0.759255469\n",
      "Trained batch 2723 batch loss 0.820128441 epoch total loss 0.759277821\n",
      "Trained batch 2724 batch loss 0.750351727 epoch total loss 0.759274483\n",
      "Trained batch 2725 batch loss 0.834064841 epoch total loss 0.759301901\n",
      "Trained batch 2726 batch loss 0.741337895 epoch total loss 0.759295344\n",
      "Trained batch 2727 batch loss 0.74930352 epoch total loss 0.759291649\n",
      "Trained batch 2728 batch loss 0.619999528 epoch total loss 0.759240627\n",
      "Trained batch 2729 batch loss 0.643014252 epoch total loss 0.75919807\n",
      "Trained batch 2730 batch loss 0.69615823 epoch total loss 0.759174943\n",
      "Trained batch 2731 batch loss 0.667736888 epoch total loss 0.759141445\n",
      "Trained batch 2732 batch loss 0.63313359 epoch total loss 0.759095311\n",
      "Trained batch 2733 batch loss 0.703438044 epoch total loss 0.759074926\n",
      "Trained batch 2734 batch loss 0.751098514 epoch total loss 0.759071946\n",
      "Trained batch 2735 batch loss 0.731906295 epoch total loss 0.759062052\n",
      "Trained batch 2736 batch loss 0.644710302 epoch total loss 0.759020269\n",
      "Trained batch 2737 batch loss 0.725990713 epoch total loss 0.759008229\n",
      "Trained batch 2738 batch loss 0.763177752 epoch total loss 0.759009778\n",
      "Trained batch 2739 batch loss 0.729952872 epoch total loss 0.758999169\n",
      "Trained batch 2740 batch loss 0.660243392 epoch total loss 0.758963108\n",
      "Trained batch 2741 batch loss 0.665994823 epoch total loss 0.758929193\n",
      "Trained batch 2742 batch loss 0.66638571 epoch total loss 0.758895457\n",
      "Trained batch 2743 batch loss 0.697820783 epoch total loss 0.758873165\n",
      "Trained batch 2744 batch loss 0.696721673 epoch total loss 0.758850515\n",
      "Trained batch 2745 batch loss 0.71036309 epoch total loss 0.758832932\n",
      "Trained batch 2746 batch loss 0.665984511 epoch total loss 0.758799136\n",
      "Trained batch 2747 batch loss 0.650495708 epoch total loss 0.758759618\n",
      "Trained batch 2748 batch loss 0.653987706 epoch total loss 0.75872153\n",
      "Trained batch 2749 batch loss 0.608712375 epoch total loss 0.758666933\n",
      "Trained batch 2750 batch loss 0.708771229 epoch total loss 0.758648813\n",
      "Trained batch 2751 batch loss 0.728238344 epoch total loss 0.758637726\n",
      "Trained batch 2752 batch loss 0.74729985 epoch total loss 0.758633614\n",
      "Trained batch 2753 batch loss 0.726792812 epoch total loss 0.75862205\n",
      "Trained batch 2754 batch loss 0.70706296 epoch total loss 0.758603334\n",
      "Trained batch 2755 batch loss 0.731694102 epoch total loss 0.758593559\n",
      "Trained batch 2756 batch loss 0.758874297 epoch total loss 0.758593619\n",
      "Trained batch 2757 batch loss 0.744315565 epoch total loss 0.758588493\n",
      "Trained batch 2758 batch loss 0.749078393 epoch total loss 0.758585036\n",
      "Trained batch 2759 batch loss 0.730515838 epoch total loss 0.758574843\n",
      "Trained batch 2760 batch loss 0.718270898 epoch total loss 0.75856024\n",
      "Trained batch 2761 batch loss 0.69221437 epoch total loss 0.75853616\n",
      "Trained batch 2762 batch loss 0.727018595 epoch total loss 0.758524776\n",
      "Trained batch 2763 batch loss 0.743826389 epoch total loss 0.758519471\n",
      "Trained batch 2764 batch loss 0.716335058 epoch total loss 0.758504212\n",
      "Trained batch 2765 batch loss 0.706968367 epoch total loss 0.758485615\n",
      "Trained batch 2766 batch loss 0.698642254 epoch total loss 0.758464\n",
      "Trained batch 2767 batch loss 0.779415488 epoch total loss 0.758471489\n",
      "Trained batch 2768 batch loss 0.660922289 epoch total loss 0.758436263\n",
      "Trained batch 2769 batch loss 0.655705929 epoch total loss 0.758399189\n",
      "Trained batch 2770 batch loss 0.673254251 epoch total loss 0.758368492\n",
      "Trained batch 2771 batch loss 0.677506864 epoch total loss 0.758339286\n",
      "Trained batch 2772 batch loss 0.681028247 epoch total loss 0.758311331\n",
      "Trained batch 2773 batch loss 0.650306106 epoch total loss 0.758272409\n",
      "Trained batch 2774 batch loss 0.618202865 epoch total loss 0.758221924\n",
      "Trained batch 2775 batch loss 0.658334 epoch total loss 0.758186\n",
      "Trained batch 2776 batch loss 0.685415506 epoch total loss 0.758159697\n",
      "Epoch 1 train loss 0.7581596970558167\n",
      "Validated batch 1 batch loss 0.655490398\n",
      "Validated batch 2 batch loss 0.683836937\n",
      "Validated batch 3 batch loss 0.574987411\n",
      "Validated batch 4 batch loss 0.646288753\n",
      "Validated batch 5 batch loss 0.701265097\n",
      "Validated batch 6 batch loss 0.682492197\n",
      "Validated batch 7 batch loss 0.631758392\n",
      "Validated batch 8 batch loss 0.66782403\n",
      "Validated batch 9 batch loss 0.650024\n",
      "Validated batch 10 batch loss 0.717565298\n",
      "Validated batch 11 batch loss 0.674147487\n",
      "Validated batch 12 batch loss 0.719822288\n",
      "Validated batch 13 batch loss 0.684774399\n",
      "Validated batch 14 batch loss 0.702331603\n",
      "Validated batch 15 batch loss 0.700101137\n",
      "Validated batch 16 batch loss 0.800683498\n",
      "Validated batch 17 batch loss 0.704896331\n",
      "Validated batch 18 batch loss 0.766562521\n",
      "Validated batch 19 batch loss 0.694780171\n",
      "Validated batch 20 batch loss 0.702646494\n",
      "Validated batch 21 batch loss 0.690675557\n",
      "Validated batch 22 batch loss 0.669836521\n",
      "Validated batch 23 batch loss 0.73170507\n",
      "Validated batch 24 batch loss 0.68560648\n",
      "Validated batch 25 batch loss 0.713686645\n",
      "Validated batch 26 batch loss 0.709319174\n",
      "Validated batch 27 batch loss 0.71877\n",
      "Validated batch 28 batch loss 0.718929529\n",
      "Validated batch 29 batch loss 0.705980539\n",
      "Validated batch 30 batch loss 0.72658819\n",
      "Validated batch 31 batch loss 0.750233412\n",
      "Validated batch 32 batch loss 0.682493091\n",
      "Validated batch 33 batch loss 0.710991442\n",
      "Validated batch 34 batch loss 0.683439374\n",
      "Validated batch 35 batch loss 0.617214799\n",
      "Validated batch 36 batch loss 0.635647058\n",
      "Validated batch 37 batch loss 0.666903257\n",
      "Validated batch 38 batch loss 0.679456353\n",
      "Validated batch 39 batch loss 0.675226212\n",
      "Validated batch 40 batch loss 0.726545334\n",
      "Validated batch 41 batch loss 0.723319113\n",
      "Validated batch 42 batch loss 0.66048032\n",
      "Validated batch 43 batch loss 0.696652353\n",
      "Validated batch 44 batch loss 0.730691493\n",
      "Validated batch 45 batch loss 0.669178\n",
      "Validated batch 46 batch loss 0.645491779\n",
      "Validated batch 47 batch loss 0.652951241\n",
      "Validated batch 48 batch loss 0.627154827\n",
      "Validated batch 49 batch loss 0.719379723\n",
      "Validated batch 50 batch loss 0.716431141\n",
      "Validated batch 51 batch loss 0.749548078\n",
      "Validated batch 52 batch loss 0.650419593\n",
      "Validated batch 53 batch loss 0.733378708\n",
      "Validated batch 54 batch loss 0.608146906\n",
      "Validated batch 55 batch loss 0.71586597\n",
      "Validated batch 56 batch loss 0.746147275\n",
      "Validated batch 57 batch loss 0.77599293\n",
      "Validated batch 58 batch loss 0.700431406\n",
      "Validated batch 59 batch loss 0.591586053\n",
      "Validated batch 60 batch loss 0.65403676\n",
      "Validated batch 61 batch loss 0.689547837\n",
      "Validated batch 62 batch loss 0.73177743\n",
      "Validated batch 63 batch loss 0.701894462\n",
      "Validated batch 64 batch loss 0.636137426\n",
      "Validated batch 65 batch loss 0.742030859\n",
      "Validated batch 66 batch loss 0.683913469\n",
      "Validated batch 67 batch loss 0.677867293\n",
      "Validated batch 68 batch loss 0.715635836\n",
      "Validated batch 69 batch loss 0.642345309\n",
      "Validated batch 70 batch loss 0.605744\n",
      "Validated batch 71 batch loss 0.491589844\n",
      "Validated batch 72 batch loss 0.749500334\n",
      "Validated batch 73 batch loss 0.678335\n",
      "Validated batch 74 batch loss 0.687146544\n",
      "Validated batch 75 batch loss 0.652008\n",
      "Validated batch 76 batch loss 0.701525569\n",
      "Validated batch 77 batch loss 0.641007662\n",
      "Validated batch 78 batch loss 0.663448453\n",
      "Validated batch 79 batch loss 0.738634706\n",
      "Validated batch 80 batch loss 0.628293812\n",
      "Validated batch 81 batch loss 0.699807584\n",
      "Validated batch 82 batch loss 0.651088238\n",
      "Validated batch 83 batch loss 0.746073246\n",
      "Validated batch 84 batch loss 0.665647268\n",
      "Validated batch 85 batch loss 0.776659846\n",
      "Validated batch 86 batch loss 0.662393093\n",
      "Validated batch 87 batch loss 0.715467453\n",
      "Validated batch 88 batch loss 0.703024507\n",
      "Validated batch 89 batch loss 0.705171227\n",
      "Validated batch 90 batch loss 0.666627586\n",
      "Validated batch 91 batch loss 0.627994418\n",
      "Validated batch 92 batch loss 0.668552577\n",
      "Validated batch 93 batch loss 0.674820423\n",
      "Validated batch 94 batch loss 0.667135835\n",
      "Validated batch 95 batch loss 0.715833\n",
      "Validated batch 96 batch loss 0.664088488\n",
      "Validated batch 97 batch loss 0.671039879\n",
      "Validated batch 98 batch loss 0.602687836\n",
      "Validated batch 99 batch loss 0.676213503\n",
      "Validated batch 100 batch loss 0.74497\n",
      "Validated batch 101 batch loss 0.659918964\n",
      "Validated batch 102 batch loss 0.712605178\n",
      "Validated batch 103 batch loss 0.648824811\n",
      "Validated batch 104 batch loss 0.688785434\n",
      "Validated batch 105 batch loss 0.708134472\n",
      "Validated batch 106 batch loss 0.73060441\n",
      "Validated batch 107 batch loss 0.689890683\n",
      "Validated batch 108 batch loss 0.717213154\n",
      "Validated batch 109 batch loss 0.722563207\n",
      "Validated batch 110 batch loss 0.668073595\n",
      "Validated batch 111 batch loss 0.679224372\n",
      "Validated batch 112 batch loss 0.711961269\n",
      "Validated batch 113 batch loss 0.715450525\n",
      "Validated batch 114 batch loss 0.742584765\n",
      "Validated batch 115 batch loss 0.714173555\n",
      "Validated batch 116 batch loss 0.666425347\n",
      "Validated batch 117 batch loss 0.638615966\n",
      "Validated batch 118 batch loss 0.722901165\n",
      "Validated batch 119 batch loss 0.726439178\n",
      "Validated batch 120 batch loss 0.720214903\n",
      "Validated batch 121 batch loss 0.713385344\n",
      "Validated batch 122 batch loss 0.67972666\n",
      "Validated batch 123 batch loss 0.694587708\n",
      "Validated batch 124 batch loss 0.679409087\n",
      "Validated batch 125 batch loss 0.737872243\n",
      "Validated batch 126 batch loss 0.798440874\n",
      "Validated batch 127 batch loss 0.596684098\n",
      "Validated batch 128 batch loss 0.596878827\n",
      "Validated batch 129 batch loss 0.665920138\n",
      "Validated batch 130 batch loss 0.741875887\n",
      "Validated batch 131 batch loss 0.637021959\n",
      "Validated batch 132 batch loss 0.62255007\n",
      "Validated batch 133 batch loss 0.610997558\n",
      "Validated batch 134 batch loss 0.749379158\n",
      "Validated batch 135 batch loss 0.749705851\n",
      "Validated batch 136 batch loss 0.774355412\n",
      "Validated batch 137 batch loss 0.624962211\n",
      "Validated batch 138 batch loss 0.682612181\n",
      "Validated batch 139 batch loss 0.703437388\n",
      "Validated batch 140 batch loss 0.761040628\n",
      "Validated batch 141 batch loss 0.675723\n",
      "Validated batch 142 batch loss 0.696121454\n",
      "Validated batch 143 batch loss 0.721922278\n",
      "Validated batch 144 batch loss 0.635301\n",
      "Validated batch 145 batch loss 0.6452775\n",
      "Validated batch 146 batch loss 0.694816\n",
      "Validated batch 147 batch loss 0.647263408\n",
      "Validated batch 148 batch loss 0.66875\n",
      "Validated batch 149 batch loss 0.735627055\n",
      "Validated batch 150 batch loss 0.682350814\n",
      "Validated batch 151 batch loss 0.663738191\n",
      "Validated batch 152 batch loss 0.690303564\n",
      "Validated batch 153 batch loss 0.678376138\n",
      "Validated batch 154 batch loss 0.608486235\n",
      "Validated batch 155 batch loss 0.631214499\n",
      "Validated batch 156 batch loss 0.682787359\n",
      "Validated batch 157 batch loss 0.698731899\n",
      "Validated batch 158 batch loss 0.649315536\n",
      "Validated batch 159 batch loss 0.696400404\n",
      "Validated batch 160 batch loss 0.712394238\n",
      "Validated batch 161 batch loss 0.703523636\n",
      "Validated batch 162 batch loss 0.673694253\n",
      "Validated batch 163 batch loss 0.693762898\n",
      "Validated batch 164 batch loss 0.66156137\n",
      "Validated batch 165 batch loss 0.635876179\n",
      "Validated batch 166 batch loss 0.656019688\n",
      "Validated batch 167 batch loss 0.736246824\n",
      "Validated batch 168 batch loss 0.600404561\n",
      "Validated batch 169 batch loss 0.722472429\n",
      "Validated batch 170 batch loss 0.660779715\n",
      "Validated batch 171 batch loss 0.6651088\n",
      "Validated batch 172 batch loss 0.712741256\n",
      "Validated batch 173 batch loss 0.678594887\n",
      "Validated batch 174 batch loss 0.707259595\n",
      "Validated batch 175 batch loss 0.721347809\n",
      "Validated batch 176 batch loss 0.759642363\n",
      "Validated batch 177 batch loss 0.802906334\n",
      "Validated batch 178 batch loss 0.831034303\n",
      "Validated batch 179 batch loss 0.794338822\n",
      "Validated batch 180 batch loss 0.706767797\n",
      "Validated batch 181 batch loss 0.647441268\n",
      "Validated batch 182 batch loss 0.724137068\n",
      "Validated batch 183 batch loss 0.631244659\n",
      "Validated batch 184 batch loss 0.638046265\n",
      "Validated batch 185 batch loss 0.598636746\n",
      "Validated batch 186 batch loss 0.671341836\n",
      "Validated batch 187 batch loss 0.709811807\n",
      "Validated batch 188 batch loss 0.702978969\n",
      "Validated batch 189 batch loss 0.678724647\n",
      "Validated batch 190 batch loss 0.647895098\n",
      "Validated batch 191 batch loss 0.566465378\n",
      "Validated batch 192 batch loss 0.641839\n",
      "Validated batch 193 batch loss 0.674734712\n",
      "Validated batch 194 batch loss 0.62375921\n",
      "Validated batch 195 batch loss 0.747613668\n",
      "Validated batch 196 batch loss 0.707713306\n",
      "Validated batch 197 batch loss 0.643131\n",
      "Validated batch 198 batch loss 0.634981275\n",
      "Validated batch 199 batch loss 0.690820813\n",
      "Validated batch 200 batch loss 0.577399075\n",
      "Validated batch 201 batch loss 0.642546356\n",
      "Validated batch 202 batch loss 0.696642101\n",
      "Validated batch 203 batch loss 0.691825688\n",
      "Validated batch 204 batch loss 0.682121\n",
      "Validated batch 205 batch loss 0.711475492\n",
      "Validated batch 206 batch loss 0.614369333\n",
      "Validated batch 207 batch loss 0.712896228\n",
      "Validated batch 208 batch loss 0.655888736\n",
      "Validated batch 209 batch loss 0.663549066\n",
      "Validated batch 210 batch loss 0.703237712\n",
      "Validated batch 211 batch loss 0.639945149\n",
      "Validated batch 212 batch loss 0.67567122\n",
      "Validated batch 213 batch loss 0.696831524\n",
      "Validated batch 214 batch loss 0.692563534\n",
      "Validated batch 215 batch loss 0.758765\n",
      "Validated batch 216 batch loss 0.734585106\n",
      "Validated batch 217 batch loss 0.658299804\n",
      "Validated batch 218 batch loss 0.640260518\n",
      "Validated batch 219 batch loss 0.781650841\n",
      "Validated batch 220 batch loss 0.687284887\n",
      "Validated batch 221 batch loss 0.663329363\n",
      "Validated batch 222 batch loss 0.586283088\n",
      "Validated batch 223 batch loss 0.624186039\n",
      "Validated batch 224 batch loss 0.686221\n",
      "Validated batch 225 batch loss 0.704187572\n",
      "Validated batch 226 batch loss 0.641392708\n",
      "Validated batch 227 batch loss 0.649531484\n",
      "Validated batch 228 batch loss 0.715517282\n",
      "Validated batch 229 batch loss 0.751887441\n",
      "Validated batch 230 batch loss 0.734065056\n",
      "Validated batch 231 batch loss 0.740727\n",
      "Validated batch 232 batch loss 0.709505796\n",
      "Validated batch 233 batch loss 0.687728\n",
      "Validated batch 234 batch loss 0.673802495\n",
      "Validated batch 235 batch loss 0.705086708\n",
      "Validated batch 236 batch loss 0.631334782\n",
      "Validated batch 237 batch loss 0.642038\n",
      "Validated batch 238 batch loss 0.683538198\n",
      "Validated batch 239 batch loss 0.654753685\n",
      "Validated batch 240 batch loss 0.690861464\n",
      "Validated batch 241 batch loss 0.786070108\n",
      "Validated batch 242 batch loss 0.712258279\n",
      "Validated batch 243 batch loss 0.6429618\n",
      "Validated batch 244 batch loss 0.652752578\n",
      "Validated batch 245 batch loss 0.641756237\n",
      "Validated batch 246 batch loss 0.760433495\n",
      "Validated batch 247 batch loss 0.671242416\n",
      "Validated batch 248 batch loss 0.696675777\n",
      "Validated batch 249 batch loss 0.765743911\n",
      "Validated batch 250 batch loss 0.655809641\n",
      "Validated batch 251 batch loss 0.65420109\n",
      "Validated batch 252 batch loss 0.708295703\n",
      "Validated batch 253 batch loss 0.67193836\n",
      "Validated batch 254 batch loss 0.628869891\n",
      "Validated batch 255 batch loss 0.66435343\n",
      "Validated batch 256 batch loss 0.751625299\n",
      "Validated batch 257 batch loss 0.789074659\n",
      "Validated batch 258 batch loss 0.651710391\n",
      "Validated batch 259 batch loss 0.654419065\n",
      "Validated batch 260 batch loss 0.721056938\n",
      "Validated batch 261 batch loss 0.629968524\n",
      "Validated batch 262 batch loss 0.707285\n",
      "Validated batch 263 batch loss 0.668620586\n",
      "Validated batch 264 batch loss 0.710974634\n",
      "Validated batch 265 batch loss 0.741899908\n",
      "Validated batch 266 batch loss 0.582276642\n",
      "Validated batch 267 batch loss 0.638005912\n",
      "Validated batch 268 batch loss 0.674657464\n",
      "Validated batch 269 batch loss 0.696481586\n",
      "Validated batch 270 batch loss 0.66893208\n",
      "Validated batch 271 batch loss 0.621934295\n",
      "Validated batch 272 batch loss 0.661972284\n",
      "Validated batch 273 batch loss 0.701194644\n",
      "Validated batch 274 batch loss 0.64037329\n",
      "Validated batch 275 batch loss 0.731480122\n",
      "Validated batch 276 batch loss 0.646017969\n",
      "Validated batch 277 batch loss 0.741973758\n",
      "Validated batch 278 batch loss 0.683069229\n",
      "Validated batch 279 batch loss 0.653619289\n",
      "Validated batch 280 batch loss 0.605556309\n",
      "Validated batch 281 batch loss 0.686615884\n",
      "Validated batch 282 batch loss 0.671294272\n",
      "Validated batch 283 batch loss 0.642037451\n",
      "Validated batch 284 batch loss 0.654949\n",
      "Validated batch 285 batch loss 0.640838325\n",
      "Validated batch 286 batch loss 0.657714784\n",
      "Validated batch 287 batch loss 0.69862628\n",
      "Validated batch 288 batch loss 0.658912\n",
      "Validated batch 289 batch loss 0.703004\n",
      "Validated batch 290 batch loss 0.626584709\n",
      "Validated batch 291 batch loss 0.714105487\n",
      "Validated batch 292 batch loss 0.641900182\n",
      "Validated batch 293 batch loss 0.733684242\n",
      "Validated batch 294 batch loss 0.69353956\n",
      "Validated batch 295 batch loss 0.63375175\n",
      "Validated batch 296 batch loss 0.645766497\n",
      "Validated batch 297 batch loss 0.800421357\n",
      "Validated batch 298 batch loss 0.685107112\n",
      "Validated batch 299 batch loss 0.726928174\n",
      "Validated batch 300 batch loss 0.683607042\n",
      "Validated batch 301 batch loss 0.630191505\n",
      "Validated batch 302 batch loss 0.653073609\n",
      "Validated batch 303 batch loss 0.740817368\n",
      "Validated batch 304 batch loss 0.622978389\n",
      "Validated batch 305 batch loss 0.692982376\n",
      "Validated batch 306 batch loss 0.672807097\n",
      "Validated batch 307 batch loss 0.708027\n",
      "Validated batch 308 batch loss 0.627928615\n",
      "Validated batch 309 batch loss 0.778114319\n",
      "Validated batch 310 batch loss 0.729918599\n",
      "Validated batch 311 batch loss 0.677773774\n",
      "Validated batch 312 batch loss 0.709350288\n",
      "Validated batch 313 batch loss 0.747041821\n",
      "Validated batch 314 batch loss 0.663668752\n",
      "Validated batch 315 batch loss 0.649909496\n",
      "Validated batch 316 batch loss 0.709769666\n",
      "Validated batch 317 batch loss 0.681427062\n",
      "Validated batch 318 batch loss 0.671081543\n",
      "Validated batch 319 batch loss 0.66961354\n",
      "Validated batch 320 batch loss 0.715354443\n",
      "Validated batch 321 batch loss 0.625672\n",
      "Validated batch 322 batch loss 0.691781282\n",
      "Validated batch 323 batch loss 0.716738582\n",
      "Validated batch 324 batch loss 0.707719803\n",
      "Validated batch 325 batch loss 0.761121333\n",
      "Validated batch 326 batch loss 0.692625\n",
      "Validated batch 327 batch loss 0.658306181\n",
      "Validated batch 328 batch loss 0.655253351\n",
      "Validated batch 329 batch loss 0.688618183\n",
      "Validated batch 330 batch loss 0.617499352\n",
      "Validated batch 331 batch loss 0.670710742\n",
      "Validated batch 332 batch loss 0.719957948\n",
      "Validated batch 333 batch loss 0.639831543\n",
      "Validated batch 334 batch loss 0.715975761\n",
      "Validated batch 335 batch loss 0.648063302\n",
      "Validated batch 336 batch loss 0.696643531\n",
      "Validated batch 337 batch loss 0.653543115\n",
      "Validated batch 338 batch loss 0.675409675\n",
      "Validated batch 339 batch loss 0.613809288\n",
      "Validated batch 340 batch loss 0.666024208\n",
      "Validated batch 341 batch loss 0.794000268\n",
      "Validated batch 342 batch loss 0.678120434\n",
      "Validated batch 343 batch loss 0.686256468\n",
      "Validated batch 344 batch loss 0.651309967\n",
      "Validated batch 345 batch loss 0.627489865\n",
      "Validated batch 346 batch loss 0.610474408\n",
      "Validated batch 347 batch loss 0.637029588\n",
      "Validated batch 348 batch loss 0.665956318\n",
      "Validated batch 349 batch loss 0.743087411\n",
      "Validated batch 350 batch loss 0.752519488\n",
      "Validated batch 351 batch loss 0.636671424\n",
      "Validated batch 352 batch loss 0.757189631\n",
      "Validated batch 353 batch loss 0.728015184\n",
      "Validated batch 354 batch loss 0.722831428\n",
      "Validated batch 355 batch loss 0.695962369\n",
      "Validated batch 356 batch loss 0.633874416\n",
      "Validated batch 357 batch loss 0.734216273\n",
      "Validated batch 358 batch loss 0.759990573\n",
      "Validated batch 359 batch loss 0.68681\n",
      "Validated batch 360 batch loss 0.668638885\n",
      "Validated batch 361 batch loss 0.768651783\n",
      "Validated batch 362 batch loss 0.673223\n",
      "Validated batch 363 batch loss 0.6827\n",
      "Validated batch 364 batch loss 0.747721136\n",
      "Validated batch 365 batch loss 0.637836218\n",
      "Validated batch 366 batch loss 0.548410773\n",
      "Validated batch 367 batch loss 0.622885048\n",
      "Validated batch 368 batch loss 0.701850474\n",
      "Validated batch 369 batch loss 0.746652722\n",
      "Epoch 1 val loss 0.684732973575592\n",
      "Model /aiffel/mpii/weights/shg_model-epoch-1-loss-0.6847.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 0.664380789 epoch total loss 0.664380789\n",
      "Trained batch 2 batch loss 0.771100938 epoch total loss 0.717740893\n",
      "Trained batch 3 batch loss 0.688704 epoch total loss 0.708061934\n",
      "Trained batch 4 batch loss 0.696616232 epoch total loss 0.705200493\n",
      "Trained batch 5 batch loss 0.649632573 epoch total loss 0.694086909\n",
      "Trained batch 6 batch loss 0.66956532 epoch total loss 0.69\n",
      "Trained batch 7 batch loss 0.675298929 epoch total loss 0.687899888\n",
      "Trained batch 8 batch loss 0.664396 epoch total loss 0.684961855\n",
      "Trained batch 9 batch loss 0.653912902 epoch total loss 0.681512\n",
      "Trained batch 10 batch loss 0.653850615 epoch total loss 0.678745866\n",
      "Trained batch 11 batch loss 0.689189911 epoch total loss 0.679695308\n",
      "Trained batch 12 batch loss 0.716016114 epoch total loss 0.682722032\n",
      "Trained batch 13 batch loss 0.720727742 epoch total loss 0.685645521\n",
      "Trained batch 14 batch loss 0.705250621 epoch total loss 0.687045932\n",
      "Trained batch 15 batch loss 0.709815741 epoch total loss 0.688563943\n",
      "Trained batch 16 batch loss 0.697306752 epoch total loss 0.689110339\n",
      "Trained batch 17 batch loss 0.674123645 epoch total loss 0.688228786\n",
      "Trained batch 18 batch loss 0.680168748 epoch total loss 0.687781036\n",
      "Trained batch 19 batch loss 0.704375 epoch total loss 0.688654423\n",
      "Trained batch 20 batch loss 0.721887589 epoch total loss 0.690316081\n",
      "Trained batch 21 batch loss 0.717787 epoch total loss 0.691624165\n",
      "Trained batch 22 batch loss 0.762424588 epoch total loss 0.694842398\n",
      "Trained batch 23 batch loss 0.774189055 epoch total loss 0.698292255\n",
      "Trained batch 24 batch loss 0.734335661 epoch total loss 0.699794054\n",
      "Trained batch 25 batch loss 0.688259244 epoch total loss 0.699332654\n",
      "Trained batch 26 batch loss 0.76408571 epoch total loss 0.701823175\n",
      "Trained batch 27 batch loss 0.780568719 epoch total loss 0.70473969\n",
      "Trained batch 28 batch loss 0.701211 epoch total loss 0.704613686\n",
      "Trained batch 29 batch loss 0.821146607 epoch total loss 0.708632052\n",
      "Trained batch 30 batch loss 0.701502264 epoch total loss 0.708394349\n",
      "Trained batch 31 batch loss 0.629701734 epoch total loss 0.705855906\n",
      "Trained batch 32 batch loss 0.681856811 epoch total loss 0.705105901\n",
      "Trained batch 33 batch loss 0.6716488 epoch total loss 0.704092\n",
      "Trained batch 34 batch loss 0.710543 epoch total loss 0.704281747\n",
      "Trained batch 35 batch loss 0.672029197 epoch total loss 0.70336026\n",
      "Trained batch 36 batch loss 0.726454794 epoch total loss 0.704001784\n",
      "Trained batch 37 batch loss 0.630601108 epoch total loss 0.702018\n",
      "Trained batch 38 batch loss 0.615022838 epoch total loss 0.699728668\n",
      "Trained batch 39 batch loss 0.653074 epoch total loss 0.698532403\n",
      "Trained batch 40 batch loss 0.647877395 epoch total loss 0.697266\n",
      "Trained batch 41 batch loss 0.684948206 epoch total loss 0.696965575\n",
      "Trained batch 42 batch loss 0.713884 epoch total loss 0.697368383\n",
      "Trained batch 43 batch loss 0.663431823 epoch total loss 0.696579158\n",
      "Trained batch 44 batch loss 0.669840217 epoch total loss 0.695971429\n",
      "Trained batch 45 batch loss 0.711709619 epoch total loss 0.696321189\n",
      "Trained batch 46 batch loss 0.660379887 epoch total loss 0.695539832\n",
      "Trained batch 47 batch loss 0.632248759 epoch total loss 0.694193244\n",
      "Trained batch 48 batch loss 0.663023591 epoch total loss 0.693543911\n",
      "Trained batch 49 batch loss 0.617288172 epoch total loss 0.691987634\n",
      "Trained batch 50 batch loss 0.678095937 epoch total loss 0.691709816\n",
      "Trained batch 51 batch loss 0.627834439 epoch total loss 0.690457344\n",
      "Trained batch 52 batch loss 0.740890205 epoch total loss 0.691427231\n",
      "Trained batch 53 batch loss 0.642299294 epoch total loss 0.690500319\n",
      "Trained batch 54 batch loss 0.756774664 epoch total loss 0.691727579\n",
      "Trained batch 55 batch loss 0.78852427 epoch total loss 0.693487525\n",
      "Trained batch 56 batch loss 0.721947134 epoch total loss 0.693995774\n",
      "Trained batch 57 batch loss 0.649960339 epoch total loss 0.693223178\n",
      "Trained batch 58 batch loss 0.648014843 epoch total loss 0.692443728\n",
      "Trained batch 59 batch loss 0.692767084 epoch total loss 0.692449212\n",
      "Trained batch 60 batch loss 0.636164486 epoch total loss 0.691511154\n",
      "Trained batch 61 batch loss 0.560301304 epoch total loss 0.689360201\n",
      "Trained batch 62 batch loss 0.615169108 epoch total loss 0.688163579\n",
      "Trained batch 63 batch loss 0.647725523 epoch total loss 0.687521696\n",
      "Trained batch 64 batch loss 0.642069399 epoch total loss 0.686811507\n",
      "Trained batch 65 batch loss 0.642227292 epoch total loss 0.686125576\n",
      "Trained batch 66 batch loss 0.635675728 epoch total loss 0.685361207\n",
      "Trained batch 67 batch loss 0.659672797 epoch total loss 0.684977829\n",
      "Trained batch 68 batch loss 0.676030457 epoch total loss 0.684846222\n",
      "Trained batch 69 batch loss 0.73679775 epoch total loss 0.685599148\n",
      "Trained batch 70 batch loss 0.789947152 epoch total loss 0.687089801\n",
      "Trained batch 71 batch loss 0.733690321 epoch total loss 0.687746167\n",
      "Trained batch 72 batch loss 0.700058699 epoch total loss 0.687917173\n",
      "Trained batch 73 batch loss 0.698809803 epoch total loss 0.688066423\n",
      "Trained batch 74 batch loss 0.676899672 epoch total loss 0.687915504\n",
      "Trained batch 75 batch loss 0.698326945 epoch total loss 0.688054323\n",
      "Trained batch 76 batch loss 0.698656201 epoch total loss 0.688193798\n",
      "Trained batch 77 batch loss 0.667412698 epoch total loss 0.687923908\n",
      "Trained batch 78 batch loss 0.671171129 epoch total loss 0.687709153\n",
      "Trained batch 79 batch loss 0.659760833 epoch total loss 0.68735534\n",
      "Trained batch 80 batch loss 0.617208123 epoch total loss 0.686478496\n",
      "Trained batch 81 batch loss 0.679629564 epoch total loss 0.686393917\n",
      "Trained batch 82 batch loss 0.673678 epoch total loss 0.686238885\n",
      "Trained batch 83 batch loss 0.662075758 epoch total loss 0.685947716\n",
      "Trained batch 84 batch loss 0.65771848 epoch total loss 0.685611665\n",
      "Trained batch 85 batch loss 0.712397099 epoch total loss 0.685926795\n",
      "Trained batch 86 batch loss 0.814699 epoch total loss 0.687424123\n",
      "Trained batch 87 batch loss 0.746480286 epoch total loss 0.688102961\n",
      "Trained batch 88 batch loss 0.757018566 epoch total loss 0.688886106\n",
      "Trained batch 89 batch loss 0.67007792 epoch total loss 0.688674748\n",
      "Trained batch 90 batch loss 0.719542205 epoch total loss 0.689017713\n",
      "Trained batch 91 batch loss 0.72757411 epoch total loss 0.689441442\n",
      "Trained batch 92 batch loss 0.647255123 epoch total loss 0.688982904\n",
      "Trained batch 93 batch loss 0.675666809 epoch total loss 0.688839674\n",
      "Trained batch 94 batch loss 0.696567774 epoch total loss 0.688921928\n",
      "Trained batch 95 batch loss 0.665775895 epoch total loss 0.688678324\n",
      "Trained batch 96 batch loss 0.690912604 epoch total loss 0.68870157\n",
      "Trained batch 97 batch loss 0.650828719 epoch total loss 0.6883111\n",
      "Trained batch 98 batch loss 0.581043124 epoch total loss 0.687216461\n",
      "Trained batch 99 batch loss 0.581418335 epoch total loss 0.686147809\n",
      "Trained batch 100 batch loss 0.645731807 epoch total loss 0.68574363\n",
      "Trained batch 101 batch loss 0.647598445 epoch total loss 0.685366\n",
      "Trained batch 102 batch loss 0.611501 epoch total loss 0.684641838\n",
      "Trained batch 103 batch loss 0.638875604 epoch total loss 0.684197485\n",
      "Trained batch 104 batch loss 0.620071769 epoch total loss 0.683580935\n",
      "Trained batch 105 batch loss 0.675315 epoch total loss 0.683502197\n",
      "Trained batch 106 batch loss 0.673301637 epoch total loss 0.683406\n",
      "Trained batch 107 batch loss 0.635487258 epoch total loss 0.682958186\n",
      "Trained batch 108 batch loss 0.580173314 epoch total loss 0.682006419\n",
      "Trained batch 109 batch loss 0.712185502 epoch total loss 0.682283342\n",
      "Trained batch 110 batch loss 0.647612453 epoch total loss 0.681968153\n",
      "Trained batch 111 batch loss 0.597766221 epoch total loss 0.681209505\n",
      "Trained batch 112 batch loss 0.715327203 epoch total loss 0.681514144\n",
      "Trained batch 113 batch loss 0.645269632 epoch total loss 0.681193411\n",
      "Trained batch 114 batch loss 0.670037329 epoch total loss 0.681095541\n",
      "Trained batch 115 batch loss 0.779349506 epoch total loss 0.681949914\n",
      "Trained batch 116 batch loss 0.795680344 epoch total loss 0.68293035\n",
      "Trained batch 117 batch loss 0.650261521 epoch total loss 0.682651103\n",
      "Trained batch 118 batch loss 0.626948059 epoch total loss 0.682179034\n",
      "Trained batch 119 batch loss 0.656212091 epoch total loss 0.681960821\n",
      "Trained batch 120 batch loss 0.712815762 epoch total loss 0.682217896\n",
      "Trained batch 121 batch loss 0.707289338 epoch total loss 0.682425141\n",
      "Trained batch 122 batch loss 0.685796857 epoch total loss 0.682452798\n",
      "Trained batch 123 batch loss 0.663150549 epoch total loss 0.682295799\n",
      "Trained batch 124 batch loss 0.701801538 epoch total loss 0.682453156\n",
      "Trained batch 125 batch loss 0.625129 epoch total loss 0.681994557\n",
      "Trained batch 126 batch loss 0.672404468 epoch total loss 0.681918442\n",
      "Trained batch 127 batch loss 0.776386261 epoch total loss 0.682662249\n",
      "Trained batch 128 batch loss 0.738730788 epoch total loss 0.683100283\n",
      "Trained batch 129 batch loss 0.790991783 epoch total loss 0.683936656\n",
      "Trained batch 130 batch loss 0.68993938 epoch total loss 0.683982849\n",
      "Trained batch 131 batch loss 0.66956526 epoch total loss 0.683872759\n",
      "Trained batch 132 batch loss 0.712752223 epoch total loss 0.684091568\n",
      "Trained batch 133 batch loss 0.667191267 epoch total loss 0.683964491\n",
      "Trained batch 134 batch loss 0.681138575 epoch total loss 0.683943391\n",
      "Trained batch 135 batch loss 0.732145846 epoch total loss 0.684300482\n",
      "Trained batch 136 batch loss 0.731644332 epoch total loss 0.684648573\n",
      "Trained batch 137 batch loss 0.821398 epoch total loss 0.685646713\n",
      "Trained batch 138 batch loss 0.850324154 epoch total loss 0.686840057\n",
      "Trained batch 139 batch loss 0.819759667 epoch total loss 0.687796354\n",
      "Trained batch 140 batch loss 0.781553388 epoch total loss 0.688466072\n",
      "Trained batch 141 batch loss 0.769296646 epoch total loss 0.68903929\n",
      "Trained batch 142 batch loss 0.709445238 epoch total loss 0.689183\n",
      "Trained batch 143 batch loss 0.709274411 epoch total loss 0.689323485\n",
      "Trained batch 144 batch loss 0.754597425 epoch total loss 0.689776778\n",
      "Trained batch 145 batch loss 0.709328711 epoch total loss 0.689911604\n",
      "Trained batch 146 batch loss 0.720277548 epoch total loss 0.690119624\n",
      "Trained batch 147 batch loss 0.692911208 epoch total loss 0.690138578\n",
      "Trained batch 148 batch loss 0.686041415 epoch total loss 0.690110922\n",
      "Trained batch 149 batch loss 0.62477529 epoch total loss 0.68967241\n",
      "Trained batch 150 batch loss 0.639122367 epoch total loss 0.689335406\n",
      "Trained batch 151 batch loss 0.683062553 epoch total loss 0.689293861\n",
      "Trained batch 152 batch loss 0.721525133 epoch total loss 0.689505935\n",
      "Trained batch 153 batch loss 0.729023755 epoch total loss 0.689764261\n",
      "Trained batch 154 batch loss 0.765519 epoch total loss 0.690256119\n",
      "Trained batch 155 batch loss 0.798495293 epoch total loss 0.690954447\n",
      "Trained batch 156 batch loss 0.668120682 epoch total loss 0.690808058\n",
      "Trained batch 157 batch loss 0.690924585 epoch total loss 0.690808833\n",
      "Trained batch 158 batch loss 0.679815888 epoch total loss 0.690739274\n",
      "Trained batch 159 batch loss 0.63380748 epoch total loss 0.690381169\n",
      "Trained batch 160 batch loss 0.635675192 epoch total loss 0.690039277\n",
      "Trained batch 161 batch loss 0.664633214 epoch total loss 0.689881444\n",
      "Trained batch 162 batch loss 0.658216953 epoch total loss 0.689686\n",
      "Trained batch 163 batch loss 0.677020669 epoch total loss 0.689608276\n",
      "Trained batch 164 batch loss 0.5974527 epoch total loss 0.689046323\n",
      "Trained batch 165 batch loss 0.638552547 epoch total loss 0.688740313\n",
      "Trained batch 166 batch loss 0.652373493 epoch total loss 0.688521206\n",
      "Trained batch 167 batch loss 0.646697044 epoch total loss 0.688270807\n",
      "Trained batch 168 batch loss 0.691431284 epoch total loss 0.688289583\n",
      "Trained batch 169 batch loss 0.594382405 epoch total loss 0.687733948\n",
      "Trained batch 170 batch loss 0.602449179 epoch total loss 0.687232256\n",
      "Trained batch 171 batch loss 0.657611907 epoch total loss 0.687059045\n",
      "Trained batch 172 batch loss 0.68356204 epoch total loss 0.68703872\n",
      "Trained batch 173 batch loss 0.675383568 epoch total loss 0.686971366\n",
      "Trained batch 174 batch loss 0.596249104 epoch total loss 0.68645\n",
      "Trained batch 175 batch loss 0.55645144 epoch total loss 0.685707152\n",
      "Trained batch 176 batch loss 0.585649967 epoch total loss 0.685138643\n",
      "Trained batch 177 batch loss 0.688395202 epoch total loss 0.685157\n",
      "Trained batch 178 batch loss 0.740820646 epoch total loss 0.685469747\n",
      "Trained batch 179 batch loss 0.841409504 epoch total loss 0.686340868\n",
      "Trained batch 180 batch loss 0.732440591 epoch total loss 0.686597\n",
      "Trained batch 181 batch loss 0.743945599 epoch total loss 0.686913788\n",
      "Trained batch 182 batch loss 0.763609886 epoch total loss 0.687335193\n",
      "Trained batch 183 batch loss 0.682052612 epoch total loss 0.687306345\n",
      "Trained batch 184 batch loss 0.715556264 epoch total loss 0.687459886\n",
      "Trained batch 185 batch loss 0.73601532 epoch total loss 0.687722325\n",
      "Trained batch 186 batch loss 0.789508224 epoch total loss 0.688269556\n",
      "Trained batch 187 batch loss 0.768545032 epoch total loss 0.688698828\n",
      "Trained batch 188 batch loss 0.708537102 epoch total loss 0.688804388\n",
      "Trained batch 189 batch loss 0.65873915 epoch total loss 0.688645303\n",
      "Trained batch 190 batch loss 0.679021 epoch total loss 0.688594639\n",
      "Trained batch 191 batch loss 0.609342217 epoch total loss 0.688179672\n",
      "Trained batch 192 batch loss 0.641828656 epoch total loss 0.687938273\n",
      "Trained batch 193 batch loss 0.73435241 epoch total loss 0.688178837\n",
      "Trained batch 194 batch loss 0.715723634 epoch total loss 0.688320816\n",
      "Trained batch 195 batch loss 0.773408592 epoch total loss 0.688757181\n",
      "Trained batch 196 batch loss 0.702047288 epoch total loss 0.688824952\n",
      "Trained batch 197 batch loss 0.695857584 epoch total loss 0.688860655\n",
      "Trained batch 198 batch loss 0.738497853 epoch total loss 0.689111352\n",
      "Trained batch 199 batch loss 0.688876 epoch total loss 0.68911016\n",
      "Trained batch 200 batch loss 0.757297635 epoch total loss 0.689451039\n",
      "Trained batch 201 batch loss 0.753899217 epoch total loss 0.689771712\n",
      "Trained batch 202 batch loss 0.71187371 epoch total loss 0.689881146\n",
      "Trained batch 203 batch loss 0.721031427 epoch total loss 0.690034628\n",
      "Trained batch 204 batch loss 0.712637365 epoch total loss 0.690145373\n",
      "Trained batch 205 batch loss 0.753458858 epoch total loss 0.690454245\n",
      "Trained batch 206 batch loss 0.790865541 epoch total loss 0.690941691\n",
      "Trained batch 207 batch loss 0.741892457 epoch total loss 0.691187859\n",
      "Trained batch 208 batch loss 0.738583267 epoch total loss 0.691415727\n",
      "Trained batch 209 batch loss 0.808962703 epoch total loss 0.691978097\n",
      "Trained batch 210 batch loss 0.751408339 epoch total loss 0.6922611\n",
      "Trained batch 211 batch loss 0.735997081 epoch total loss 0.692468345\n",
      "Trained batch 212 batch loss 0.698677778 epoch total loss 0.692497671\n",
      "Trained batch 213 batch loss 0.771297693 epoch total loss 0.692867637\n",
      "Trained batch 214 batch loss 0.746509671 epoch total loss 0.693118274\n",
      "Trained batch 215 batch loss 0.738552809 epoch total loss 0.693329632\n",
      "Trained batch 216 batch loss 0.70517838 epoch total loss 0.693384528\n",
      "Trained batch 217 batch loss 0.786109686 epoch total loss 0.693811774\n",
      "Trained batch 218 batch loss 0.745429 epoch total loss 0.694048524\n",
      "Trained batch 219 batch loss 0.730041504 epoch total loss 0.694212914\n",
      "Trained batch 220 batch loss 0.817194045 epoch total loss 0.694771945\n",
      "Trained batch 221 batch loss 0.820318222 epoch total loss 0.69534\n",
      "Trained batch 222 batch loss 0.736445546 epoch total loss 0.695525169\n",
      "Trained batch 223 batch loss 0.769793749 epoch total loss 0.695858181\n",
      "Trained batch 224 batch loss 0.729577482 epoch total loss 0.696008742\n",
      "Trained batch 225 batch loss 0.68347466 epoch total loss 0.695953\n",
      "Trained batch 226 batch loss 0.702724457 epoch total loss 0.695983\n",
      "Trained batch 227 batch loss 0.685057163 epoch total loss 0.695934892\n",
      "Trained batch 228 batch loss 0.679541409 epoch total loss 0.695862949\n",
      "Trained batch 229 batch loss 0.69336617 epoch total loss 0.695852041\n",
      "Trained batch 230 batch loss 0.692823946 epoch total loss 0.695838869\n",
      "Trained batch 231 batch loss 0.653193831 epoch total loss 0.695654273\n",
      "Trained batch 232 batch loss 0.668821573 epoch total loss 0.69553864\n",
      "Trained batch 233 batch loss 0.728012562 epoch total loss 0.695678\n",
      "Trained batch 234 batch loss 0.671354771 epoch total loss 0.695574045\n",
      "Trained batch 235 batch loss 0.665448725 epoch total loss 0.695445895\n",
      "Trained batch 236 batch loss 0.781328201 epoch total loss 0.695809782\n",
      "Trained batch 237 batch loss 0.720801473 epoch total loss 0.695915163\n",
      "Trained batch 238 batch loss 0.74178 epoch total loss 0.696107864\n",
      "Trained batch 239 batch loss 0.608834624 epoch total loss 0.695742726\n",
      "Trained batch 240 batch loss 0.627909899 epoch total loss 0.695460141\n",
      "Trained batch 241 batch loss 0.742036 epoch total loss 0.695653379\n",
      "Trained batch 242 batch loss 0.704770923 epoch total loss 0.695691049\n",
      "Trained batch 243 batch loss 0.736668 epoch total loss 0.695859671\n",
      "Trained batch 244 batch loss 0.713080049 epoch total loss 0.695930243\n",
      "Trained batch 245 batch loss 0.625777304 epoch total loss 0.695643902\n",
      "Trained batch 246 batch loss 0.626328051 epoch total loss 0.695362151\n",
      "Trained batch 247 batch loss 0.673306584 epoch total loss 0.695272863\n",
      "Trained batch 248 batch loss 0.726430953 epoch total loss 0.69539845\n",
      "Trained batch 249 batch loss 0.679981887 epoch total loss 0.695336521\n",
      "Trained batch 250 batch loss 0.707811952 epoch total loss 0.69538641\n",
      "Trained batch 251 batch loss 0.69487679 epoch total loss 0.695384383\n",
      "Trained batch 252 batch loss 0.648349822 epoch total loss 0.695197701\n",
      "Trained batch 253 batch loss 0.618291497 epoch total loss 0.694893718\n",
      "Trained batch 254 batch loss 0.631286 epoch total loss 0.694643259\n",
      "Trained batch 255 batch loss 0.648364782 epoch total loss 0.694461763\n",
      "Trained batch 256 batch loss 0.65925312 epoch total loss 0.694324255\n",
      "Trained batch 257 batch loss 0.640549302 epoch total loss 0.694115\n",
      "Trained batch 258 batch loss 0.666180968 epoch total loss 0.694006741\n",
      "Trained batch 259 batch loss 0.708188295 epoch total loss 0.694061518\n",
      "Trained batch 260 batch loss 0.747083545 epoch total loss 0.694265425\n",
      "Trained batch 261 batch loss 0.761531353 epoch total loss 0.694523215\n",
      "Trained batch 262 batch loss 0.76898551 epoch total loss 0.69480741\n",
      "Trained batch 263 batch loss 0.744252264 epoch total loss 0.694995344\n",
      "Trained batch 264 batch loss 0.698183179 epoch total loss 0.695007443\n",
      "Trained batch 265 batch loss 0.653288364 epoch total loss 0.69485\n",
      "Trained batch 266 batch loss 0.649956822 epoch total loss 0.694681287\n",
      "Trained batch 267 batch loss 0.698958516 epoch total loss 0.69469732\n",
      "Trained batch 268 batch loss 0.739300191 epoch total loss 0.694863737\n",
      "Trained batch 269 batch loss 0.751116812 epoch total loss 0.69507283\n",
      "Trained batch 270 batch loss 0.710411906 epoch total loss 0.695129693\n",
      "Trained batch 271 batch loss 0.692332864 epoch total loss 0.695119381\n",
      "Trained batch 272 batch loss 0.708389699 epoch total loss 0.695168138\n",
      "Trained batch 273 batch loss 0.688049138 epoch total loss 0.69514209\n",
      "Trained batch 274 batch loss 0.639926553 epoch total loss 0.694940567\n",
      "Trained batch 275 batch loss 0.688238859 epoch total loss 0.694916189\n",
      "Trained batch 276 batch loss 0.69440043 epoch total loss 0.694914281\n",
      "Trained batch 277 batch loss 0.685094118 epoch total loss 0.694878817\n",
      "Trained batch 278 batch loss 0.659028709 epoch total loss 0.694749832\n",
      "Trained batch 279 batch loss 0.620268345 epoch total loss 0.694482863\n",
      "Trained batch 280 batch loss 0.684521 epoch total loss 0.694447339\n",
      "Trained batch 281 batch loss 0.686050892 epoch total loss 0.694417417\n",
      "Trained batch 282 batch loss 0.724739671 epoch total loss 0.694525\n",
      "Trained batch 283 batch loss 0.770860434 epoch total loss 0.694794714\n",
      "Trained batch 284 batch loss 0.757117689 epoch total loss 0.695014119\n",
      "Trained batch 285 batch loss 0.797452331 epoch total loss 0.695373595\n",
      "Trained batch 286 batch loss 0.689557433 epoch total loss 0.69535327\n",
      "Trained batch 287 batch loss 0.751339376 epoch total loss 0.695548356\n",
      "Trained batch 288 batch loss 0.699571967 epoch total loss 0.695562303\n",
      "Trained batch 289 batch loss 0.70232439 epoch total loss 0.695585728\n",
      "Trained batch 290 batch loss 0.735835135 epoch total loss 0.695724547\n",
      "Trained batch 291 batch loss 0.692207754 epoch total loss 0.695712507\n",
      "Trained batch 292 batch loss 0.704408944 epoch total loss 0.695742249\n",
      "Trained batch 293 batch loss 0.726247549 epoch total loss 0.695846379\n",
      "Trained batch 294 batch loss 0.691643357 epoch total loss 0.695832074\n",
      "Trained batch 295 batch loss 0.699154377 epoch total loss 0.695843339\n",
      "Trained batch 296 batch loss 0.654978216 epoch total loss 0.695705295\n",
      "Trained batch 297 batch loss 0.676620662 epoch total loss 0.695641041\n",
      "Trained batch 298 batch loss 0.716615438 epoch total loss 0.695711434\n",
      "Trained batch 299 batch loss 0.698819757 epoch total loss 0.695721805\n",
      "Trained batch 300 batch loss 0.694599867 epoch total loss 0.69571805\n",
      "Trained batch 301 batch loss 0.637157142 epoch total loss 0.69552356\n",
      "Trained batch 302 batch loss 0.665973246 epoch total loss 0.695425689\n",
      "Trained batch 303 batch loss 0.713595748 epoch total loss 0.695485651\n",
      "Trained batch 304 batch loss 0.751823306 epoch total loss 0.695670903\n",
      "Trained batch 305 batch loss 0.658472359 epoch total loss 0.695549\n",
      "Trained batch 306 batch loss 0.718057156 epoch total loss 0.695622563\n",
      "Trained batch 307 batch loss 0.647571802 epoch total loss 0.695466042\n",
      "Trained batch 308 batch loss 0.658584833 epoch total loss 0.695346296\n",
      "Trained batch 309 batch loss 0.693435 epoch total loss 0.695340097\n",
      "Trained batch 310 batch loss 0.62993753 epoch total loss 0.695129156\n",
      "Trained batch 311 batch loss 0.70451349 epoch total loss 0.695159316\n",
      "Trained batch 312 batch loss 0.616295815 epoch total loss 0.694906592\n",
      "Trained batch 313 batch loss 0.666701376 epoch total loss 0.69481647\n",
      "Trained batch 314 batch loss 0.652230084 epoch total loss 0.69468087\n",
      "Trained batch 315 batch loss 0.580379486 epoch total loss 0.694318\n",
      "Trained batch 316 batch loss 0.73621273 epoch total loss 0.694450557\n",
      "Trained batch 317 batch loss 0.732770443 epoch total loss 0.694571435\n",
      "Trained batch 318 batch loss 0.652681649 epoch total loss 0.694439709\n",
      "Trained batch 319 batch loss 0.621078 epoch total loss 0.694209754\n",
      "Trained batch 320 batch loss 0.62994349 epoch total loss 0.694008946\n",
      "Trained batch 321 batch loss 0.611859918 epoch total loss 0.693753\n",
      "Trained batch 322 batch loss 0.556036234 epoch total loss 0.693325281\n",
      "Trained batch 323 batch loss 0.538323522 epoch total loss 0.692845464\n",
      "Trained batch 324 batch loss 0.484948516 epoch total loss 0.69220382\n",
      "Trained batch 325 batch loss 0.553836703 epoch total loss 0.691778064\n",
      "Trained batch 326 batch loss 0.700804591 epoch total loss 0.69180572\n",
      "Trained batch 327 batch loss 0.754269481 epoch total loss 0.691996753\n",
      "Trained batch 328 batch loss 0.853740156 epoch total loss 0.692489922\n",
      "Trained batch 329 batch loss 0.738203824 epoch total loss 0.69262886\n",
      "Trained batch 330 batch loss 0.721000373 epoch total loss 0.69271481\n",
      "Trained batch 331 batch loss 0.743021131 epoch total loss 0.692866802\n",
      "Trained batch 332 batch loss 0.636928141 epoch total loss 0.6926983\n",
      "Trained batch 333 batch loss 0.709657788 epoch total loss 0.692749262\n",
      "Trained batch 334 batch loss 0.682226 epoch total loss 0.692717731\n",
      "Trained batch 335 batch loss 0.744757235 epoch total loss 0.692873061\n",
      "Trained batch 336 batch loss 0.69596988 epoch total loss 0.69288224\n",
      "Trained batch 337 batch loss 0.64247036 epoch total loss 0.692732692\n",
      "Trained batch 338 batch loss 0.603890657 epoch total loss 0.692469835\n",
      "Trained batch 339 batch loss 0.62060982 epoch total loss 0.692257881\n",
      "Trained batch 340 batch loss 0.6489622 epoch total loss 0.692130506\n",
      "Trained batch 341 batch loss 0.680400193 epoch total loss 0.692096114\n",
      "Trained batch 342 batch loss 0.677865505 epoch total loss 0.69205451\n",
      "Trained batch 343 batch loss 0.641456664 epoch total loss 0.691907048\n",
      "Trained batch 344 batch loss 0.675116777 epoch total loss 0.691858232\n",
      "Trained batch 345 batch loss 0.674135387 epoch total loss 0.691806853\n",
      "Trained batch 346 batch loss 0.710482836 epoch total loss 0.691860795\n",
      "Trained batch 347 batch loss 0.673255265 epoch total loss 0.691807151\n",
      "Trained batch 348 batch loss 0.657098532 epoch total loss 0.691707432\n",
      "Trained batch 349 batch loss 0.644344926 epoch total loss 0.691571712\n",
      "Trained batch 350 batch loss 0.594755054 epoch total loss 0.691295147\n",
      "Trained batch 351 batch loss 0.620870471 epoch total loss 0.691094458\n",
      "Trained batch 352 batch loss 0.718322814 epoch total loss 0.691171825\n",
      "Trained batch 353 batch loss 0.698722184 epoch total loss 0.691193163\n",
      "Trained batch 354 batch loss 0.578720093 epoch total loss 0.690875471\n",
      "Trained batch 355 batch loss 0.644045472 epoch total loss 0.690743566\n",
      "Trained batch 356 batch loss 0.7125718 epoch total loss 0.690804839\n",
      "Trained batch 357 batch loss 0.722640395 epoch total loss 0.690894\n",
      "Trained batch 358 batch loss 0.702448189 epoch total loss 0.690926313\n",
      "Trained batch 359 batch loss 0.70842123 epoch total loss 0.69097507\n",
      "Trained batch 360 batch loss 0.697075427 epoch total loss 0.690992\n",
      "Trained batch 361 batch loss 0.683698177 epoch total loss 0.690971792\n",
      "Trained batch 362 batch loss 0.659154952 epoch total loss 0.690883934\n",
      "Trained batch 363 batch loss 0.640856326 epoch total loss 0.690746069\n",
      "Trained batch 364 batch loss 0.781576693 epoch total loss 0.690995634\n",
      "Trained batch 365 batch loss 0.771630466 epoch total loss 0.691216528\n",
      "Trained batch 366 batch loss 0.694821 epoch total loss 0.691226423\n",
      "Trained batch 367 batch loss 0.646072507 epoch total loss 0.691103339\n",
      "Trained batch 368 batch loss 0.728881061 epoch total loss 0.691206038\n",
      "Trained batch 369 batch loss 0.755503178 epoch total loss 0.691380262\n",
      "Trained batch 370 batch loss 0.703763485 epoch total loss 0.69141376\n",
      "Trained batch 371 batch loss 0.660283566 epoch total loss 0.691329837\n",
      "Trained batch 372 batch loss 0.620144963 epoch total loss 0.691138506\n",
      "Trained batch 373 batch loss 0.641168535 epoch total loss 0.691004515\n",
      "Trained batch 374 batch loss 0.578961432 epoch total loss 0.690704942\n",
      "Trained batch 375 batch loss 0.631579399 epoch total loss 0.690547287\n",
      "Trained batch 376 batch loss 0.712679744 epoch total loss 0.690606117\n",
      "Trained batch 377 batch loss 0.711472929 epoch total loss 0.69066155\n",
      "Trained batch 378 batch loss 0.671387434 epoch total loss 0.690610528\n",
      "Trained batch 379 batch loss 0.624338865 epoch total loss 0.690435648\n",
      "Trained batch 380 batch loss 0.741406918 epoch total loss 0.690569758\n",
      "Trained batch 381 batch loss 0.716555178 epoch total loss 0.690637946\n",
      "Trained batch 382 batch loss 0.7736485 epoch total loss 0.690855265\n",
      "Trained batch 383 batch loss 0.712930322 epoch total loss 0.690912843\n",
      "Trained batch 384 batch loss 0.689898968 epoch total loss 0.69091028\n",
      "Trained batch 385 batch loss 0.767332792 epoch total loss 0.691108763\n",
      "Trained batch 386 batch loss 0.628479898 epoch total loss 0.690946519\n",
      "Trained batch 387 batch loss 0.557208836 epoch total loss 0.690601\n",
      "Trained batch 388 batch loss 0.694030344 epoch total loss 0.690609813\n",
      "Trained batch 389 batch loss 0.678122401 epoch total loss 0.690577745\n",
      "Trained batch 390 batch loss 0.684821725 epoch total loss 0.690562963\n",
      "Trained batch 391 batch loss 0.638674855 epoch total loss 0.690430224\n",
      "Trained batch 392 batch loss 0.539706886 epoch total loss 0.690045714\n",
      "Trained batch 393 batch loss 0.53038317 epoch total loss 0.689639509\n",
      "Trained batch 394 batch loss 0.490849972 epoch total loss 0.689134955\n",
      "Trained batch 395 batch loss 0.594726 epoch total loss 0.688895941\n",
      "Trained batch 396 batch loss 0.607005596 epoch total loss 0.688689113\n",
      "Trained batch 397 batch loss 0.697187781 epoch total loss 0.688710451\n",
      "Trained batch 398 batch loss 0.736608505 epoch total loss 0.688830793\n",
      "Trained batch 399 batch loss 0.786578596 epoch total loss 0.689075828\n",
      "Trained batch 400 batch loss 0.784271359 epoch total loss 0.689313829\n",
      "Trained batch 401 batch loss 0.902798772 epoch total loss 0.689846218\n",
      "Trained batch 402 batch loss 0.923502505 epoch total loss 0.690427423\n",
      "Trained batch 403 batch loss 0.600595474 epoch total loss 0.690204501\n",
      "Trained batch 404 batch loss 0.659166694 epoch total loss 0.690127671\n",
      "Trained batch 405 batch loss 0.672505796 epoch total loss 0.690084219\n",
      "Trained batch 406 batch loss 0.615953326 epoch total loss 0.68990165\n",
      "Trained batch 407 batch loss 0.672666132 epoch total loss 0.689859331\n",
      "Trained batch 408 batch loss 0.72355324 epoch total loss 0.689941883\n",
      "Trained batch 409 batch loss 0.707605 epoch total loss 0.689985037\n",
      "Trained batch 410 batch loss 0.729771435 epoch total loss 0.690082073\n",
      "Trained batch 411 batch loss 0.72907 epoch total loss 0.690176904\n",
      "Trained batch 412 batch loss 0.732105 epoch total loss 0.690278709\n",
      "Trained batch 413 batch loss 0.69389987 epoch total loss 0.69028753\n",
      "Trained batch 414 batch loss 0.675555885 epoch total loss 0.690251946\n",
      "Trained batch 415 batch loss 0.734401822 epoch total loss 0.690358341\n",
      "Trained batch 416 batch loss 0.669691384 epoch total loss 0.69030863\n",
      "Trained batch 417 batch loss 0.771506906 epoch total loss 0.690503359\n",
      "Trained batch 418 batch loss 0.750849187 epoch total loss 0.690647781\n",
      "Trained batch 419 batch loss 0.7359007 epoch total loss 0.690755785\n",
      "Trained batch 420 batch loss 0.696375728 epoch total loss 0.690769136\n",
      "Trained batch 421 batch loss 0.683810651 epoch total loss 0.690752625\n",
      "Trained batch 422 batch loss 0.699252725 epoch total loss 0.690772772\n",
      "Trained batch 423 batch loss 0.657501817 epoch total loss 0.690694094\n",
      "Trained batch 424 batch loss 0.655802429 epoch total loss 0.69061178\n",
      "Trained batch 425 batch loss 0.616454661 epoch total loss 0.690437317\n",
      "Trained batch 426 batch loss 0.631329536 epoch total loss 0.690298498\n",
      "Trained batch 427 batch loss 0.713445425 epoch total loss 0.690352738\n",
      "Trained batch 428 batch loss 0.665034711 epoch total loss 0.69029355\n",
      "Trained batch 429 batch loss 0.786374688 epoch total loss 0.690517545\n",
      "Trained batch 430 batch loss 0.670470953 epoch total loss 0.690470934\n",
      "Trained batch 431 batch loss 0.658849 epoch total loss 0.690397561\n",
      "Trained batch 432 batch loss 0.650865078 epoch total loss 0.690306067\n",
      "Trained batch 433 batch loss 0.580534577 epoch total loss 0.690052569\n",
      "Trained batch 434 batch loss 0.65781796 epoch total loss 0.689978242\n",
      "Trained batch 435 batch loss 0.626039803 epoch total loss 0.689831257\n",
      "Trained batch 436 batch loss 0.68817687 epoch total loss 0.689827442\n",
      "Trained batch 437 batch loss 0.736063778 epoch total loss 0.68993324\n",
      "Trained batch 438 batch loss 0.681564212 epoch total loss 0.689914107\n",
      "Trained batch 439 batch loss 0.6627056 epoch total loss 0.689852118\n",
      "Trained batch 440 batch loss 0.691414595 epoch total loss 0.689855695\n",
      "Trained batch 441 batch loss 0.679484606 epoch total loss 0.689832151\n",
      "Trained batch 442 batch loss 0.649000049 epoch total loss 0.689739704\n",
      "Trained batch 443 batch loss 0.719644725 epoch total loss 0.689807236\n",
      "Trained batch 444 batch loss 0.833565891 epoch total loss 0.690130949\n",
      "Trained batch 445 batch loss 0.71054095 epoch total loss 0.690176845\n",
      "Trained batch 446 batch loss 0.732432187 epoch total loss 0.690271556\n",
      "Trained batch 447 batch loss 0.653319776 epoch total loss 0.690188885\n",
      "Trained batch 448 batch loss 0.639764965 epoch total loss 0.690076351\n",
      "Trained batch 449 batch loss 0.778930843 epoch total loss 0.690274239\n",
      "Trained batch 450 batch loss 0.730088711 epoch total loss 0.690362751\n",
      "Trained batch 451 batch loss 0.780859292 epoch total loss 0.690563381\n",
      "Trained batch 452 batch loss 0.760005653 epoch total loss 0.690717041\n",
      "Trained batch 453 batch loss 0.688332438 epoch total loss 0.690711737\n",
      "Trained batch 454 batch loss 0.736382365 epoch total loss 0.690812349\n",
      "Trained batch 455 batch loss 0.659430087 epoch total loss 0.690743387\n",
      "Trained batch 456 batch loss 0.685542822 epoch total loss 0.690732\n",
      "Trained batch 457 batch loss 0.691651881 epoch total loss 0.690733969\n",
      "Trained batch 458 batch loss 0.652302742 epoch total loss 0.690650105\n",
      "Trained batch 459 batch loss 0.660933733 epoch total loss 0.690585315\n",
      "Trained batch 460 batch loss 0.637392104 epoch total loss 0.690469682\n",
      "Trained batch 461 batch loss 0.666666746 epoch total loss 0.690418065\n",
      "Trained batch 462 batch loss 0.701941 epoch total loss 0.690443\n",
      "Trained batch 463 batch loss 0.718092084 epoch total loss 0.690502644\n",
      "Trained batch 464 batch loss 0.696747899 epoch total loss 0.690516114\n",
      "Trained batch 465 batch loss 0.709629893 epoch total loss 0.690557182\n",
      "Trained batch 466 batch loss 0.697732329 epoch total loss 0.69057256\n",
      "Trained batch 467 batch loss 0.686590552 epoch total loss 0.690564036\n",
      "Trained batch 468 batch loss 0.698346198 epoch total loss 0.690580666\n",
      "Trained batch 469 batch loss 0.605331063 epoch total loss 0.690398872\n",
      "Trained batch 470 batch loss 0.632118344 epoch total loss 0.690274835\n",
      "Trained batch 471 batch loss 0.641070366 epoch total loss 0.690170407\n",
      "Trained batch 472 batch loss 0.621851683 epoch total loss 0.690025628\n",
      "Trained batch 473 batch loss 0.676952958 epoch total loss 0.689998\n",
      "Trained batch 474 batch loss 0.60584265 epoch total loss 0.689820409\n",
      "Trained batch 475 batch loss 0.628249 epoch total loss 0.689690769\n",
      "Trained batch 476 batch loss 0.686333179 epoch total loss 0.689683735\n",
      "Trained batch 477 batch loss 0.640431821 epoch total loss 0.6895805\n",
      "Trained batch 478 batch loss 0.641195118 epoch total loss 0.689479291\n",
      "Trained batch 479 batch loss 0.690701902 epoch total loss 0.689481854\n",
      "Trained batch 480 batch loss 0.775257051 epoch total loss 0.689660609\n",
      "Trained batch 481 batch loss 0.699617803 epoch total loss 0.689681292\n",
      "Trained batch 482 batch loss 0.667019844 epoch total loss 0.689634264\n",
      "Trained batch 483 batch loss 0.73394537 epoch total loss 0.689726\n",
      "Trained batch 484 batch loss 0.72081852 epoch total loss 0.689790249\n",
      "Trained batch 485 batch loss 0.645455539 epoch total loss 0.689698815\n",
      "Trained batch 486 batch loss 0.762565792 epoch total loss 0.689848781\n",
      "Trained batch 487 batch loss 0.881753 epoch total loss 0.690242827\n",
      "Trained batch 488 batch loss 0.799917 epoch total loss 0.690467596\n",
      "Trained batch 489 batch loss 0.802144885 epoch total loss 0.690696\n",
      "Trained batch 490 batch loss 0.776777625 epoch total loss 0.690871656\n",
      "Trained batch 491 batch loss 0.678878784 epoch total loss 0.690847218\n",
      "Trained batch 492 batch loss 0.651902378 epoch total loss 0.690768123\n",
      "Trained batch 493 batch loss 0.675418 epoch total loss 0.690736949\n",
      "Trained batch 494 batch loss 0.672193408 epoch total loss 0.690699399\n",
      "Trained batch 495 batch loss 0.634729505 epoch total loss 0.690586329\n",
      "Trained batch 496 batch loss 0.544901669 epoch total loss 0.690292597\n",
      "Trained batch 497 batch loss 0.502945304 epoch total loss 0.689915657\n",
      "Trained batch 498 batch loss 0.548077047 epoch total loss 0.689630866\n",
      "Trained batch 499 batch loss 0.660369039 epoch total loss 0.689572215\n",
      "Trained batch 500 batch loss 0.721868396 epoch total loss 0.689636767\n",
      "Trained batch 501 batch loss 0.685560286 epoch total loss 0.689628601\n",
      "Trained batch 502 batch loss 0.552531958 epoch total loss 0.689355493\n",
      "Trained batch 503 batch loss 0.542945504 epoch total loss 0.689064384\n",
      "Trained batch 504 batch loss 0.522682905 epoch total loss 0.688734293\n",
      "Trained batch 505 batch loss 0.560401917 epoch total loss 0.688480139\n",
      "Trained batch 506 batch loss 0.569382846 epoch total loss 0.68824476\n",
      "Trained batch 507 batch loss 0.566845596 epoch total loss 0.688005328\n",
      "Trained batch 508 batch loss 0.60207969 epoch total loss 0.68783617\n",
      "Trained batch 509 batch loss 0.595441401 epoch total loss 0.687654614\n",
      "Trained batch 510 batch loss 0.615061 epoch total loss 0.687512279\n",
      "Trained batch 511 batch loss 0.682153523 epoch total loss 0.687501788\n",
      "Trained batch 512 batch loss 0.647658408 epoch total loss 0.687423944\n",
      "Trained batch 513 batch loss 0.68710649 epoch total loss 0.687423348\n",
      "Trained batch 514 batch loss 0.708268642 epoch total loss 0.68746388\n",
      "Trained batch 515 batch loss 0.7104671 epoch total loss 0.687508583\n",
      "Trained batch 516 batch loss 0.712285876 epoch total loss 0.687556624\n",
      "Trained batch 517 batch loss 0.712463796 epoch total loss 0.687604785\n",
      "Trained batch 518 batch loss 0.762522101 epoch total loss 0.687749386\n",
      "Trained batch 519 batch loss 0.795418322 epoch total loss 0.68795681\n",
      "Trained batch 520 batch loss 0.697244287 epoch total loss 0.687974691\n",
      "Trained batch 521 batch loss 0.720043898 epoch total loss 0.688036203\n",
      "Trained batch 522 batch loss 0.738279819 epoch total loss 0.688132465\n",
      "Trained batch 523 batch loss 0.794340372 epoch total loss 0.688335538\n",
      "Trained batch 524 batch loss 0.685633898 epoch total loss 0.688330352\n",
      "Trained batch 525 batch loss 0.68460232 epoch total loss 0.688323259\n",
      "Trained batch 526 batch loss 0.763990581 epoch total loss 0.688467085\n",
      "Trained batch 527 batch loss 0.642891705 epoch total loss 0.688380599\n",
      "Trained batch 528 batch loss 0.577751756 epoch total loss 0.688171089\n",
      "Trained batch 529 batch loss 0.644275129 epoch total loss 0.688088119\n",
      "Trained batch 530 batch loss 0.648994446 epoch total loss 0.688014388\n",
      "Trained batch 531 batch loss 0.651009619 epoch total loss 0.687944651\n",
      "Trained batch 532 batch loss 0.678266108 epoch total loss 0.687926471\n",
      "Trained batch 533 batch loss 0.663553238 epoch total loss 0.687880695\n",
      "Trained batch 534 batch loss 0.669151366 epoch total loss 0.687845647\n",
      "Trained batch 535 batch loss 0.618866265 epoch total loss 0.687716722\n",
      "Trained batch 536 batch loss 0.719741464 epoch total loss 0.687776446\n",
      "Trained batch 537 batch loss 0.700503051 epoch total loss 0.687800109\n",
      "Trained batch 538 batch loss 0.650009334 epoch total loss 0.687729895\n",
      "Trained batch 539 batch loss 0.702744365 epoch total loss 0.68775779\n",
      "Trained batch 540 batch loss 0.711143076 epoch total loss 0.687801123\n",
      "Trained batch 541 batch loss 0.69505167 epoch total loss 0.687814474\n",
      "Trained batch 542 batch loss 0.694345176 epoch total loss 0.687826514\n",
      "Trained batch 543 batch loss 0.672502339 epoch total loss 0.687798321\n",
      "Trained batch 544 batch loss 0.646427333 epoch total loss 0.687722266\n",
      "Trained batch 545 batch loss 0.614584088 epoch total loss 0.687588096\n",
      "Trained batch 546 batch loss 0.656532645 epoch total loss 0.687531173\n",
      "Trained batch 547 batch loss 0.625050724 epoch total loss 0.687417\n",
      "Trained batch 548 batch loss 0.657151043 epoch total loss 0.687361777\n",
      "Trained batch 549 batch loss 0.674423337 epoch total loss 0.687338233\n",
      "Trained batch 550 batch loss 0.687235475 epoch total loss 0.687338054\n",
      "Trained batch 551 batch loss 0.557214558 epoch total loss 0.687101901\n",
      "Trained batch 552 batch loss 0.607632101 epoch total loss 0.686957955\n",
      "Trained batch 553 batch loss 0.731619298 epoch total loss 0.68703872\n",
      "Trained batch 554 batch loss 0.650685668 epoch total loss 0.686973095\n",
      "Trained batch 555 batch loss 0.698677182 epoch total loss 0.686994195\n",
      "Trained batch 556 batch loss 0.675282 epoch total loss 0.686973155\n",
      "Trained batch 557 batch loss 0.673329234 epoch total loss 0.686948657\n",
      "Trained batch 558 batch loss 0.72119242 epoch total loss 0.68701005\n",
      "Trained batch 559 batch loss 0.651776433 epoch total loss 0.686947\n",
      "Trained batch 560 batch loss 0.652536869 epoch total loss 0.686885536\n",
      "Trained batch 561 batch loss 0.64269793 epoch total loss 0.686806738\n",
      "Trained batch 562 batch loss 0.61935389 epoch total loss 0.686686695\n",
      "Trained batch 563 batch loss 0.615164936 epoch total loss 0.686559677\n",
      "Trained batch 564 batch loss 0.643390894 epoch total loss 0.686483204\n",
      "Trained batch 565 batch loss 0.726136148 epoch total loss 0.686553359\n",
      "Trained batch 566 batch loss 0.788835287 epoch total loss 0.68673408\n",
      "Trained batch 567 batch loss 0.64960593 epoch total loss 0.686668575\n",
      "Trained batch 568 batch loss 0.622719347 epoch total loss 0.686556\n",
      "Trained batch 569 batch loss 0.674262524 epoch total loss 0.686534405\n",
      "Trained batch 570 batch loss 0.667862654 epoch total loss 0.686501622\n",
      "Trained batch 571 batch loss 0.666728437 epoch total loss 0.686467\n",
      "Trained batch 572 batch loss 0.673467338 epoch total loss 0.686444283\n",
      "Trained batch 573 batch loss 0.73022449 epoch total loss 0.686520696\n",
      "Trained batch 574 batch loss 0.690291166 epoch total loss 0.686527193\n",
      "Trained batch 575 batch loss 0.634275615 epoch total loss 0.686436355\n",
      "Trained batch 576 batch loss 0.705391049 epoch total loss 0.686469257\n",
      "Trained batch 577 batch loss 0.719531357 epoch total loss 0.686526537\n",
      "Trained batch 578 batch loss 0.677491248 epoch total loss 0.686510921\n",
      "Trained batch 579 batch loss 0.642372668 epoch total loss 0.686434686\n",
      "Trained batch 580 batch loss 0.674047232 epoch total loss 0.686413288\n",
      "Trained batch 581 batch loss 0.728932619 epoch total loss 0.686486483\n",
      "Trained batch 582 batch loss 0.794529319 epoch total loss 0.686672151\n",
      "Trained batch 583 batch loss 0.711338043 epoch total loss 0.68671447\n",
      "Trained batch 584 batch loss 0.751791835 epoch total loss 0.686825871\n",
      "Trained batch 585 batch loss 0.748743892 epoch total loss 0.686931729\n",
      "Trained batch 586 batch loss 0.693987131 epoch total loss 0.686943829\n",
      "Trained batch 587 batch loss 0.709425032 epoch total loss 0.686982095\n",
      "Trained batch 588 batch loss 0.662223399 epoch total loss 0.68694\n",
      "Trained batch 589 batch loss 0.743148208 epoch total loss 0.687035382\n",
      "Trained batch 590 batch loss 0.664901674 epoch total loss 0.686997831\n",
      "Trained batch 591 batch loss 0.713323474 epoch total loss 0.687042415\n",
      "Trained batch 592 batch loss 0.602887213 epoch total loss 0.686900198\n",
      "Trained batch 593 batch loss 0.667834401 epoch total loss 0.686868072\n",
      "Trained batch 594 batch loss 0.679200828 epoch total loss 0.686855197\n",
      "Trained batch 595 batch loss 0.629241705 epoch total loss 0.686758339\n",
      "Trained batch 596 batch loss 0.706784 epoch total loss 0.686791956\n",
      "Trained batch 597 batch loss 0.67505312 epoch total loss 0.686772287\n",
      "Trained batch 598 batch loss 0.664546192 epoch total loss 0.686735094\n",
      "Trained batch 599 batch loss 0.655310631 epoch total loss 0.686682642\n",
      "Trained batch 600 batch loss 0.682732701 epoch total loss 0.686676085\n",
      "Trained batch 601 batch loss 0.73069346 epoch total loss 0.686749279\n",
      "Trained batch 602 batch loss 0.673863411 epoch total loss 0.686727881\n",
      "Trained batch 603 batch loss 0.666099548 epoch total loss 0.686693668\n",
      "Trained batch 604 batch loss 0.719998598 epoch total loss 0.686748862\n",
      "Trained batch 605 batch loss 0.673738718 epoch total loss 0.686727345\n",
      "Trained batch 606 batch loss 0.638944507 epoch total loss 0.686648488\n",
      "Trained batch 607 batch loss 0.69831419 epoch total loss 0.686667681\n",
      "Trained batch 608 batch loss 0.678412437 epoch total loss 0.686654091\n",
      "Trained batch 609 batch loss 0.69793 epoch total loss 0.686672628\n",
      "Trained batch 610 batch loss 0.593800485 epoch total loss 0.686520398\n",
      "Trained batch 611 batch loss 0.69231081 epoch total loss 0.686529875\n",
      "Trained batch 612 batch loss 0.644544 epoch total loss 0.68646127\n",
      "Trained batch 613 batch loss 0.742303431 epoch total loss 0.686552346\n",
      "Trained batch 614 batch loss 0.669201672 epoch total loss 0.686524093\n",
      "Trained batch 615 batch loss 0.66741395 epoch total loss 0.686493039\n",
      "Trained batch 616 batch loss 0.70128113 epoch total loss 0.68651706\n",
      "Trained batch 617 batch loss 0.691677868 epoch total loss 0.686525404\n",
      "Trained batch 618 batch loss 0.714944482 epoch total loss 0.68657136\n",
      "Trained batch 619 batch loss 0.618101954 epoch total loss 0.686460793\n",
      "Trained batch 620 batch loss 0.620749354 epoch total loss 0.686354816\n",
      "Trained batch 621 batch loss 0.646249652 epoch total loss 0.686290205\n",
      "Trained batch 622 batch loss 0.61030215 epoch total loss 0.686168\n",
      "Trained batch 623 batch loss 0.632679403 epoch total loss 0.686082184\n",
      "Trained batch 624 batch loss 0.660878778 epoch total loss 0.686041832\n",
      "Trained batch 625 batch loss 0.609480143 epoch total loss 0.685919285\n",
      "Trained batch 626 batch loss 0.606770396 epoch total loss 0.685792863\n",
      "Trained batch 627 batch loss 0.592465878 epoch total loss 0.685644031\n",
      "Trained batch 628 batch loss 0.605247498 epoch total loss 0.685516\n",
      "Trained batch 629 batch loss 0.618257344 epoch total loss 0.685409069\n",
      "Trained batch 630 batch loss 0.654285908 epoch total loss 0.685359716\n",
      "Trained batch 631 batch loss 0.644919872 epoch total loss 0.685295641\n",
      "Trained batch 632 batch loss 0.659487188 epoch total loss 0.685254812\n",
      "Trained batch 633 batch loss 0.68527931 epoch total loss 0.685254812\n",
      "Trained batch 634 batch loss 0.667903781 epoch total loss 0.685227454\n",
      "Trained batch 635 batch loss 0.608303905 epoch total loss 0.685106337\n",
      "Trained batch 636 batch loss 0.592642903 epoch total loss 0.684960961\n",
      "Trained batch 637 batch loss 0.643037915 epoch total loss 0.684895158\n",
      "Trained batch 638 batch loss 0.672971606 epoch total loss 0.684876442\n",
      "Trained batch 639 batch loss 0.697008491 epoch total loss 0.684895456\n",
      "Trained batch 640 batch loss 0.636185169 epoch total loss 0.684819341\n",
      "Trained batch 641 batch loss 0.584325075 epoch total loss 0.68466258\n",
      "Trained batch 642 batch loss 0.548256457 epoch total loss 0.68445009\n",
      "Trained batch 643 batch loss 0.786453724 epoch total loss 0.684608757\n",
      "Trained batch 644 batch loss 0.688389 epoch total loss 0.684614599\n",
      "Trained batch 645 batch loss 0.683981 epoch total loss 0.684613645\n",
      "Trained batch 646 batch loss 0.702326953 epoch total loss 0.684641063\n",
      "Trained batch 647 batch loss 0.705724776 epoch total loss 0.684673667\n",
      "Trained batch 648 batch loss 0.691110134 epoch total loss 0.684683561\n",
      "Trained batch 649 batch loss 0.64934665 epoch total loss 0.684629142\n",
      "Trained batch 650 batch loss 0.724355221 epoch total loss 0.684690297\n",
      "Trained batch 651 batch loss 0.685514033 epoch total loss 0.684691548\n",
      "Trained batch 652 batch loss 0.911726654 epoch total loss 0.685039759\n",
      "Trained batch 653 batch loss 0.753372 epoch total loss 0.685144365\n",
      "Trained batch 654 batch loss 0.761368275 epoch total loss 0.685260952\n",
      "Trained batch 655 batch loss 0.728564918 epoch total loss 0.685327053\n",
      "Trained batch 656 batch loss 0.668879 epoch total loss 0.685302\n",
      "Trained batch 657 batch loss 0.649539113 epoch total loss 0.68524754\n",
      "Trained batch 658 batch loss 0.780984521 epoch total loss 0.685393035\n",
      "Trained batch 659 batch loss 0.775066 epoch total loss 0.685529113\n",
      "Trained batch 660 batch loss 0.716296196 epoch total loss 0.685575724\n",
      "Trained batch 661 batch loss 0.711355448 epoch total loss 0.685614765\n",
      "Trained batch 662 batch loss 0.694100499 epoch total loss 0.68562752\n",
      "Trained batch 663 batch loss 0.696757555 epoch total loss 0.685644329\n",
      "Trained batch 664 batch loss 0.650213718 epoch total loss 0.685590923\n",
      "Trained batch 665 batch loss 0.672980547 epoch total loss 0.685571969\n",
      "Trained batch 666 batch loss 0.669521213 epoch total loss 0.685547888\n",
      "Trained batch 667 batch loss 0.599460602 epoch total loss 0.685418785\n",
      "Trained batch 668 batch loss 0.619992256 epoch total loss 0.685320854\n",
      "Trained batch 669 batch loss 0.732322693 epoch total loss 0.685391128\n",
      "Trained batch 670 batch loss 0.617681384 epoch total loss 0.685290098\n",
      "Trained batch 671 batch loss 0.688413 epoch total loss 0.685294747\n",
      "Trained batch 672 batch loss 0.763608694 epoch total loss 0.685411274\n",
      "Trained batch 673 batch loss 0.660925388 epoch total loss 0.685374856\n",
      "Trained batch 674 batch loss 0.698313177 epoch total loss 0.685394049\n",
      "Trained batch 675 batch loss 0.743331254 epoch total loss 0.685479879\n",
      "Trained batch 676 batch loss 0.740538299 epoch total loss 0.685561299\n",
      "Trained batch 677 batch loss 0.724651098 epoch total loss 0.685619056\n",
      "Trained batch 678 batch loss 0.70717752 epoch total loss 0.685650826\n",
      "Trained batch 679 batch loss 0.677047 epoch total loss 0.68563813\n",
      "Trained batch 680 batch loss 0.65004319 epoch total loss 0.685585856\n",
      "Trained batch 681 batch loss 0.789596915 epoch total loss 0.685738564\n",
      "Trained batch 682 batch loss 0.770901918 epoch total loss 0.685863435\n",
      "Trained batch 683 batch loss 0.629964888 epoch total loss 0.685781658\n",
      "Trained batch 684 batch loss 0.622041166 epoch total loss 0.685688436\n",
      "Trained batch 685 batch loss 0.614483416 epoch total loss 0.685584486\n",
      "Trained batch 686 batch loss 0.667272091 epoch total loss 0.685557783\n",
      "Trained batch 687 batch loss 0.63698566 epoch total loss 0.685487092\n",
      "Trained batch 688 batch loss 0.613011122 epoch total loss 0.685381711\n",
      "Trained batch 689 batch loss 0.708277345 epoch total loss 0.68541497\n",
      "Trained batch 690 batch loss 0.734618425 epoch total loss 0.685486257\n",
      "Trained batch 691 batch loss 0.681613863 epoch total loss 0.685480654\n",
      "Trained batch 692 batch loss 0.610254705 epoch total loss 0.685372\n",
      "Trained batch 693 batch loss 0.679526925 epoch total loss 0.685363531\n",
      "Trained batch 694 batch loss 0.600533664 epoch total loss 0.685241282\n",
      "Trained batch 695 batch loss 0.719691873 epoch total loss 0.685290873\n",
      "Trained batch 696 batch loss 0.694696784 epoch total loss 0.685304403\n",
      "Trained batch 697 batch loss 0.61590445 epoch total loss 0.685204804\n",
      "Trained batch 698 batch loss 0.662826777 epoch total loss 0.685172796\n",
      "Trained batch 699 batch loss 0.740708 epoch total loss 0.685252249\n",
      "Trained batch 700 batch loss 0.758910716 epoch total loss 0.685357511\n",
      "Trained batch 701 batch loss 0.652914107 epoch total loss 0.685311198\n",
      "Trained batch 702 batch loss 0.685342193 epoch total loss 0.685311258\n",
      "Trained batch 703 batch loss 0.652187705 epoch total loss 0.68526417\n",
      "Trained batch 704 batch loss 0.667382598 epoch total loss 0.685238779\n",
      "Trained batch 705 batch loss 0.692069769 epoch total loss 0.685248435\n",
      "Trained batch 706 batch loss 0.67518276 epoch total loss 0.685234189\n",
      "Trained batch 707 batch loss 0.712316513 epoch total loss 0.685272455\n",
      "Trained batch 708 batch loss 0.647411 epoch total loss 0.685219\n",
      "Trained batch 709 batch loss 0.71781081 epoch total loss 0.685264945\n",
      "Trained batch 710 batch loss 0.647844315 epoch total loss 0.685212255\n",
      "Trained batch 711 batch loss 0.687304 epoch total loss 0.685215235\n",
      "Trained batch 712 batch loss 0.698985219 epoch total loss 0.685234547\n",
      "Trained batch 713 batch loss 0.671220541 epoch total loss 0.685214877\n",
      "Trained batch 714 batch loss 0.668475628 epoch total loss 0.685191453\n",
      "Trained batch 715 batch loss 0.637861848 epoch total loss 0.685125291\n",
      "Trained batch 716 batch loss 0.642498493 epoch total loss 0.685065687\n",
      "Trained batch 717 batch loss 0.682572246 epoch total loss 0.68506223\n",
      "Trained batch 718 batch loss 0.735828578 epoch total loss 0.685133\n",
      "Trained batch 719 batch loss 0.724790335 epoch total loss 0.685188115\n",
      "Trained batch 720 batch loss 0.720018506 epoch total loss 0.685236514\n",
      "Trained batch 721 batch loss 0.630918 epoch total loss 0.685161173\n",
      "Trained batch 722 batch loss 0.672045 epoch total loss 0.685143054\n",
      "Trained batch 723 batch loss 0.706250608 epoch total loss 0.6851722\n",
      "Trained batch 724 batch loss 0.661019802 epoch total loss 0.685138822\n",
      "Trained batch 725 batch loss 0.598465264 epoch total loss 0.685019314\n",
      "Trained batch 726 batch loss 0.611626387 epoch total loss 0.684918225\n",
      "Trained batch 727 batch loss 0.560973465 epoch total loss 0.684747756\n",
      "Trained batch 728 batch loss 0.621625721 epoch total loss 0.684661031\n",
      "Trained batch 729 batch loss 0.681319296 epoch total loss 0.684656441\n",
      "Trained batch 730 batch loss 0.637873054 epoch total loss 0.684592366\n",
      "Trained batch 731 batch loss 0.648947358 epoch total loss 0.68454361\n",
      "Trained batch 732 batch loss 0.665824652 epoch total loss 0.684518039\n",
      "Trained batch 733 batch loss 0.589793801 epoch total loss 0.684388816\n",
      "Trained batch 734 batch loss 0.772448421 epoch total loss 0.684508801\n",
      "Trained batch 735 batch loss 0.718087614 epoch total loss 0.684554458\n",
      "Trained batch 736 batch loss 0.699849784 epoch total loss 0.68457526\n",
      "Trained batch 737 batch loss 0.754541516 epoch total loss 0.68467021\n",
      "Trained batch 738 batch loss 0.785029531 epoch total loss 0.684806168\n",
      "Trained batch 739 batch loss 0.722401381 epoch total loss 0.68485707\n",
      "Trained batch 740 batch loss 0.737134 epoch total loss 0.684927702\n",
      "Trained batch 741 batch loss 0.714986622 epoch total loss 0.684968293\n",
      "Trained batch 742 batch loss 0.664847612 epoch total loss 0.684941173\n",
      "Trained batch 743 batch loss 0.68823868 epoch total loss 0.684945583\n",
      "Trained batch 744 batch loss 0.685656965 epoch total loss 0.684946597\n",
      "Trained batch 745 batch loss 0.719695091 epoch total loss 0.684993207\n",
      "Trained batch 746 batch loss 0.789782703 epoch total loss 0.685133696\n",
      "Trained batch 747 batch loss 0.740870297 epoch total loss 0.685208321\n",
      "Trained batch 748 batch loss 0.644520104 epoch total loss 0.685153902\n",
      "Trained batch 749 batch loss 0.708835065 epoch total loss 0.685185552\n",
      "Trained batch 750 batch loss 0.69179523 epoch total loss 0.685194314\n",
      "Trained batch 751 batch loss 0.723253727 epoch total loss 0.685245037\n",
      "Trained batch 752 batch loss 0.680615246 epoch total loss 0.685238838\n",
      "Trained batch 753 batch loss 0.725295901 epoch total loss 0.685292\n",
      "Trained batch 754 batch loss 0.71330452 epoch total loss 0.685329199\n",
      "Trained batch 755 batch loss 0.749332428 epoch total loss 0.685413957\n",
      "Trained batch 756 batch loss 0.658556759 epoch total loss 0.685378492\n",
      "Trained batch 757 batch loss 0.714683294 epoch total loss 0.685417116\n",
      "Trained batch 758 batch loss 0.630635 epoch total loss 0.685344875\n",
      "Trained batch 759 batch loss 0.683913589 epoch total loss 0.685342968\n",
      "Trained batch 760 batch loss 0.684847116 epoch total loss 0.685342312\n",
      "Trained batch 761 batch loss 0.678781927 epoch total loss 0.685333669\n",
      "Trained batch 762 batch loss 0.772653759 epoch total loss 0.685448289\n",
      "Trained batch 763 batch loss 0.686601 epoch total loss 0.685449779\n",
      "Trained batch 764 batch loss 0.709844947 epoch total loss 0.685481668\n",
      "Trained batch 765 batch loss 0.665315628 epoch total loss 0.685455382\n",
      "Trained batch 766 batch loss 0.651638269 epoch total loss 0.685411155\n",
      "Trained batch 767 batch loss 0.652814329 epoch total loss 0.685368717\n",
      "Trained batch 768 batch loss 0.604691863 epoch total loss 0.685263634\n",
      "Trained batch 769 batch loss 0.637651086 epoch total loss 0.685201705\n",
      "Trained batch 770 batch loss 0.589523 epoch total loss 0.685077488\n",
      "Trained batch 771 batch loss 0.591631174 epoch total loss 0.684956253\n",
      "Trained batch 772 batch loss 0.666045845 epoch total loss 0.684931695\n",
      "Trained batch 773 batch loss 0.682108819 epoch total loss 0.68492806\n",
      "Trained batch 774 batch loss 0.637305558 epoch total loss 0.684866607\n",
      "Trained batch 775 batch loss 0.642831564 epoch total loss 0.684812307\n",
      "Trained batch 776 batch loss 0.634973407 epoch total loss 0.684748054\n",
      "Trained batch 777 batch loss 0.600215554 epoch total loss 0.684639275\n",
      "Trained batch 778 batch loss 0.574051738 epoch total loss 0.684497118\n",
      "Trained batch 779 batch loss 0.582375944 epoch total loss 0.684366047\n",
      "Trained batch 780 batch loss 0.630802691 epoch total loss 0.684297383\n",
      "Trained batch 781 batch loss 0.696424127 epoch total loss 0.68431288\n",
      "Trained batch 782 batch loss 0.697408736 epoch total loss 0.684329629\n",
      "Trained batch 783 batch loss 0.744251549 epoch total loss 0.684406161\n",
      "Trained batch 784 batch loss 0.632690966 epoch total loss 0.684340179\n",
      "Trained batch 785 batch loss 0.64098382 epoch total loss 0.684285\n",
      "Trained batch 786 batch loss 0.639629066 epoch total loss 0.684228182\n",
      "Trained batch 787 batch loss 0.709050894 epoch total loss 0.684259713\n",
      "Trained batch 788 batch loss 0.670711398 epoch total loss 0.684242547\n",
      "Trained batch 789 batch loss 0.618249118 epoch total loss 0.684158862\n",
      "Trained batch 790 batch loss 0.656386256 epoch total loss 0.684123695\n",
      "Trained batch 791 batch loss 0.685425699 epoch total loss 0.684125304\n",
      "Trained batch 792 batch loss 0.655922174 epoch total loss 0.68408972\n",
      "Trained batch 793 batch loss 0.650262117 epoch total loss 0.684047103\n",
      "Trained batch 794 batch loss 0.676199734 epoch total loss 0.684037209\n",
      "Trained batch 795 batch loss 0.609473944 epoch total loss 0.68394345\n",
      "Trained batch 796 batch loss 0.65311265 epoch total loss 0.683904767\n",
      "Trained batch 797 batch loss 0.629570365 epoch total loss 0.683836579\n",
      "Trained batch 798 batch loss 0.659725487 epoch total loss 0.68380636\n",
      "Trained batch 799 batch loss 0.723968506 epoch total loss 0.683856666\n",
      "Trained batch 800 batch loss 0.703783631 epoch total loss 0.683881581\n",
      "Trained batch 801 batch loss 0.703952789 epoch total loss 0.683906674\n",
      "Trained batch 802 batch loss 0.708298862 epoch total loss 0.683937132\n",
      "Trained batch 803 batch loss 0.719883919 epoch total loss 0.683981955\n",
      "Trained batch 804 batch loss 0.73055768 epoch total loss 0.684039831\n",
      "Trained batch 805 batch loss 0.723337412 epoch total loss 0.684088647\n",
      "Trained batch 806 batch loss 0.743065298 epoch total loss 0.684161782\n",
      "Trained batch 807 batch loss 0.654817343 epoch total loss 0.684125423\n",
      "Trained batch 808 batch loss 0.692785144 epoch total loss 0.684136212\n",
      "Trained batch 809 batch loss 0.715548515 epoch total loss 0.684175074\n",
      "Trained batch 810 batch loss 0.683788121 epoch total loss 0.684174538\n",
      "Trained batch 811 batch loss 0.695127606 epoch total loss 0.684188068\n",
      "Trained batch 812 batch loss 0.704050899 epoch total loss 0.684212506\n",
      "Trained batch 813 batch loss 0.666581392 epoch total loss 0.68419081\n",
      "Trained batch 814 batch loss 0.746622503 epoch total loss 0.684267521\n",
      "Trained batch 815 batch loss 0.656957567 epoch total loss 0.684234083\n",
      "Trained batch 816 batch loss 0.695446432 epoch total loss 0.684247792\n",
      "Trained batch 817 batch loss 0.654058576 epoch total loss 0.684210837\n",
      "Trained batch 818 batch loss 0.640079618 epoch total loss 0.684156895\n",
      "Trained batch 819 batch loss 0.65616 epoch total loss 0.684122741\n",
      "Trained batch 820 batch loss 0.63038826 epoch total loss 0.684057176\n",
      "Trained batch 821 batch loss 0.627483845 epoch total loss 0.683988273\n",
      "Trained batch 822 batch loss 0.605125964 epoch total loss 0.68389231\n",
      "Trained batch 823 batch loss 0.634692907 epoch total loss 0.683832526\n",
      "Trained batch 824 batch loss 0.564714432 epoch total loss 0.683688\n",
      "Trained batch 825 batch loss 0.759186566 epoch total loss 0.683779538\n",
      "Trained batch 826 batch loss 0.653598309 epoch total loss 0.683743\n",
      "Trained batch 827 batch loss 0.692713857 epoch total loss 0.683753848\n",
      "Trained batch 828 batch loss 0.715566814 epoch total loss 0.683792233\n",
      "Trained batch 829 batch loss 0.78878665 epoch total loss 0.683918893\n",
      "Trained batch 830 batch loss 0.745568633 epoch total loss 0.683993101\n",
      "Trained batch 831 batch loss 0.712799072 epoch total loss 0.684027851\n",
      "Trained batch 832 batch loss 0.670054913 epoch total loss 0.684011042\n",
      "Trained batch 833 batch loss 0.664637387 epoch total loss 0.683987737\n",
      "Trained batch 834 batch loss 0.670246601 epoch total loss 0.683971226\n",
      "Trained batch 835 batch loss 0.741445422 epoch total loss 0.68404007\n",
      "Trained batch 836 batch loss 0.757105112 epoch total loss 0.68412745\n",
      "Trained batch 837 batch loss 0.687413931 epoch total loss 0.684131384\n",
      "Trained batch 838 batch loss 0.702051759 epoch total loss 0.684152782\n",
      "Trained batch 839 batch loss 0.655555248 epoch total loss 0.684118688\n",
      "Trained batch 840 batch loss 0.631949067 epoch total loss 0.68405658\n",
      "Trained batch 841 batch loss 0.622362494 epoch total loss 0.683983266\n",
      "Trained batch 842 batch loss 0.65134 epoch total loss 0.683944523\n",
      "Trained batch 843 batch loss 0.733119607 epoch total loss 0.684002817\n",
      "Trained batch 844 batch loss 0.806093276 epoch total loss 0.684147477\n",
      "Trained batch 845 batch loss 0.70736593 epoch total loss 0.684174895\n",
      "Trained batch 846 batch loss 0.756905675 epoch total loss 0.684260905\n",
      "Trained batch 847 batch loss 0.722385108 epoch total loss 0.684305906\n",
      "Trained batch 848 batch loss 0.645586193 epoch total loss 0.684260249\n",
      "Trained batch 849 batch loss 0.689299345 epoch total loss 0.68426615\n",
      "Trained batch 850 batch loss 0.63777703 epoch total loss 0.684211433\n",
      "Trained batch 851 batch loss 0.683323383 epoch total loss 0.68421042\n",
      "Trained batch 852 batch loss 0.703057706 epoch total loss 0.684232533\n",
      "Trained batch 853 batch loss 0.70297277 epoch total loss 0.684254527\n",
      "Trained batch 854 batch loss 0.700483084 epoch total loss 0.684273541\n",
      "Trained batch 855 batch loss 0.647925735 epoch total loss 0.684231102\n",
      "Trained batch 856 batch loss 0.715402305 epoch total loss 0.684267461\n",
      "Trained batch 857 batch loss 0.72995472 epoch total loss 0.684320807\n",
      "Trained batch 858 batch loss 0.712317407 epoch total loss 0.684353471\n",
      "Trained batch 859 batch loss 0.74764663 epoch total loss 0.684427142\n",
      "Trained batch 860 batch loss 0.754193425 epoch total loss 0.684508264\n",
      "Trained batch 861 batch loss 0.737475753 epoch total loss 0.684569836\n",
      "Trained batch 862 batch loss 0.67559582 epoch total loss 0.684559405\n",
      "Trained batch 863 batch loss 0.627132356 epoch total loss 0.684492886\n",
      "Trained batch 864 batch loss 0.628382325 epoch total loss 0.684427917\n",
      "Trained batch 865 batch loss 0.583333373 epoch total loss 0.684311\n",
      "Trained batch 866 batch loss 0.650781393 epoch total loss 0.68427223\n",
      "Trained batch 867 batch loss 0.551065922 epoch total loss 0.684118629\n",
      "Trained batch 868 batch loss 0.64886415 epoch total loss 0.684078038\n",
      "Trained batch 869 batch loss 0.724791586 epoch total loss 0.684124887\n",
      "Trained batch 870 batch loss 0.661544561 epoch total loss 0.684098959\n",
      "Trained batch 871 batch loss 0.699376285 epoch total loss 0.684116483\n",
      "Trained batch 872 batch loss 0.665139437 epoch total loss 0.684094787\n",
      "Trained batch 873 batch loss 0.709813356 epoch total loss 0.684124231\n",
      "Trained batch 874 batch loss 0.676390946 epoch total loss 0.68411541\n",
      "Trained batch 875 batch loss 0.812568545 epoch total loss 0.684262216\n",
      "Trained batch 876 batch loss 0.653828621 epoch total loss 0.684227467\n",
      "Trained batch 877 batch loss 0.590298057 epoch total loss 0.684120297\n",
      "Trained batch 878 batch loss 0.674511969 epoch total loss 0.68410933\n",
      "Trained batch 879 batch loss 0.642228484 epoch total loss 0.684061706\n",
      "Trained batch 880 batch loss 0.634961486 epoch total loss 0.684005857\n",
      "Trained batch 881 batch loss 0.673767924 epoch total loss 0.683994234\n",
      "Trained batch 882 batch loss 0.66037178 epoch total loss 0.683967531\n",
      "Trained batch 883 batch loss 0.601563454 epoch total loss 0.68387419\n",
      "Trained batch 884 batch loss 0.684323132 epoch total loss 0.683874667\n",
      "Trained batch 885 batch loss 0.656913161 epoch total loss 0.683844209\n",
      "Trained batch 886 batch loss 0.598350585 epoch total loss 0.683747709\n",
      "Trained batch 887 batch loss 0.588128 epoch total loss 0.683639944\n",
      "Trained batch 888 batch loss 0.611645818 epoch total loss 0.683558822\n",
      "Trained batch 889 batch loss 0.605297327 epoch total loss 0.683470786\n",
      "Trained batch 890 batch loss 0.600151241 epoch total loss 0.683377206\n",
      "Trained batch 891 batch loss 0.661943078 epoch total loss 0.683353126\n",
      "Trained batch 892 batch loss 0.722722471 epoch total loss 0.683397233\n",
      "Trained batch 893 batch loss 0.620302856 epoch total loss 0.683326602\n",
      "Trained batch 894 batch loss 0.647269428 epoch total loss 0.68328625\n",
      "Trained batch 895 batch loss 0.574342 epoch total loss 0.683164537\n",
      "Trained batch 896 batch loss 0.646226346 epoch total loss 0.683123291\n",
      "Trained batch 897 batch loss 0.619464576 epoch total loss 0.683052301\n",
      "Trained batch 898 batch loss 0.677593172 epoch total loss 0.683046281\n",
      "Trained batch 899 batch loss 0.681785703 epoch total loss 0.683044851\n",
      "Trained batch 900 batch loss 0.614183247 epoch total loss 0.682968318\n",
      "Trained batch 901 batch loss 0.632905483 epoch total loss 0.682912827\n",
      "Trained batch 902 batch loss 0.715597689 epoch total loss 0.682949\n",
      "Trained batch 903 batch loss 0.795291841 epoch total loss 0.683073401\n",
      "Trained batch 904 batch loss 0.780993 epoch total loss 0.683181763\n",
      "Trained batch 905 batch loss 0.741611958 epoch total loss 0.683246374\n",
      "Trained batch 906 batch loss 0.764750242 epoch total loss 0.683336318\n",
      "Trained batch 907 batch loss 0.666740656 epoch total loss 0.683318079\n",
      "Trained batch 908 batch loss 0.675824463 epoch total loss 0.683309793\n",
      "Trained batch 909 batch loss 0.735050857 epoch total loss 0.683366716\n",
      "Trained batch 910 batch loss 0.667642117 epoch total loss 0.68334949\n",
      "Trained batch 911 batch loss 0.616894 epoch total loss 0.683276534\n",
      "Trained batch 912 batch loss 0.619723 epoch total loss 0.683206856\n",
      "Trained batch 913 batch loss 0.658469141 epoch total loss 0.683179736\n",
      "Trained batch 914 batch loss 0.687959492 epoch total loss 0.683185\n",
      "Trained batch 915 batch loss 0.6803509 epoch total loss 0.683181942\n",
      "Trained batch 916 batch loss 0.696807861 epoch total loss 0.683196783\n",
      "Trained batch 917 batch loss 0.718374252 epoch total loss 0.683235109\n",
      "Trained batch 918 batch loss 0.662907958 epoch total loss 0.683213\n",
      "Trained batch 919 batch loss 0.690274358 epoch total loss 0.683220625\n",
      "Trained batch 920 batch loss 0.609899223 epoch total loss 0.683140934\n",
      "Trained batch 921 batch loss 0.638361216 epoch total loss 0.683092356\n",
      "Trained batch 922 batch loss 0.676537752 epoch total loss 0.683085203\n",
      "Trained batch 923 batch loss 0.65227288 epoch total loss 0.683051825\n",
      "Trained batch 924 batch loss 0.632153034 epoch total loss 0.68299675\n",
      "Trained batch 925 batch loss 0.640371323 epoch total loss 0.682950675\n",
      "Trained batch 926 batch loss 0.566425681 epoch total loss 0.68282479\n",
      "Trained batch 927 batch loss 0.739179969 epoch total loss 0.682885647\n",
      "Trained batch 928 batch loss 0.656159163 epoch total loss 0.682856858\n",
      "Trained batch 929 batch loss 0.720345259 epoch total loss 0.68289721\n",
      "Trained batch 930 batch loss 0.669526041 epoch total loss 0.682882845\n",
      "Trained batch 931 batch loss 0.63937676 epoch total loss 0.682836175\n",
      "Trained batch 932 batch loss 0.590413511 epoch total loss 0.682737\n",
      "Trained batch 933 batch loss 0.677711487 epoch total loss 0.682731628\n",
      "Trained batch 934 batch loss 0.678922534 epoch total loss 0.682727516\n",
      "Trained batch 935 batch loss 0.635697126 epoch total loss 0.68267715\n",
      "Trained batch 936 batch loss 0.67160809 epoch total loss 0.682665348\n",
      "Trained batch 937 batch loss 0.617736697 epoch total loss 0.682596087\n",
      "Trained batch 938 batch loss 0.591460288 epoch total loss 0.682498872\n",
      "Trained batch 939 batch loss 0.663894713 epoch total loss 0.682479084\n",
      "Trained batch 940 batch loss 0.707890809 epoch total loss 0.682506084\n",
      "Trained batch 941 batch loss 0.625881791 epoch total loss 0.682445884\n",
      "Trained batch 942 batch loss 0.656538486 epoch total loss 0.682418406\n",
      "Trained batch 943 batch loss 0.697021484 epoch total loss 0.682433903\n",
      "Trained batch 944 batch loss 0.641966462 epoch total loss 0.682391047\n",
      "Trained batch 945 batch loss 0.620551944 epoch total loss 0.682325602\n",
      "Trained batch 946 batch loss 0.647279322 epoch total loss 0.682288527\n",
      "Trained batch 947 batch loss 0.662373602 epoch total loss 0.682267487\n",
      "Trained batch 948 batch loss 0.663025796 epoch total loss 0.682247162\n",
      "Trained batch 949 batch loss 0.60806179 epoch total loss 0.682168961\n",
      "Trained batch 950 batch loss 0.641386509 epoch total loss 0.682126\n",
      "Trained batch 951 batch loss 0.655993402 epoch total loss 0.682098567\n",
      "Trained batch 952 batch loss 0.686701715 epoch total loss 0.682103395\n",
      "Trained batch 953 batch loss 0.735887468 epoch total loss 0.682159841\n",
      "Trained batch 954 batch loss 0.645503342 epoch total loss 0.682121396\n",
      "Trained batch 955 batch loss 0.615436196 epoch total loss 0.682051599\n",
      "Trained batch 956 batch loss 0.594617069 epoch total loss 0.681960106\n",
      "Trained batch 957 batch loss 0.569861948 epoch total loss 0.681843\n",
      "Trained batch 958 batch loss 0.570850849 epoch total loss 0.681727171\n",
      "Trained batch 959 batch loss 0.604811907 epoch total loss 0.681646943\n",
      "Trained batch 960 batch loss 0.669003963 epoch total loss 0.68163377\n",
      "Trained batch 961 batch loss 0.682954311 epoch total loss 0.681635141\n",
      "Trained batch 962 batch loss 0.729774714 epoch total loss 0.681685209\n",
      "Trained batch 963 batch loss 0.645589411 epoch total loss 0.681647718\n",
      "Trained batch 964 batch loss 0.67940414 epoch total loss 0.681645393\n",
      "Trained batch 965 batch loss 0.634992659 epoch total loss 0.681597054\n",
      "Trained batch 966 batch loss 0.722039282 epoch total loss 0.681638896\n",
      "Trained batch 967 batch loss 0.705082536 epoch total loss 0.681663156\n",
      "Trained batch 968 batch loss 0.674081326 epoch total loss 0.681655288\n",
      "Trained batch 969 batch loss 0.715642273 epoch total loss 0.681690395\n",
      "Trained batch 970 batch loss 0.654355466 epoch total loss 0.681662202\n",
      "Trained batch 971 batch loss 0.752721071 epoch total loss 0.681735396\n",
      "Trained batch 972 batch loss 0.722081065 epoch total loss 0.681776941\n",
      "Trained batch 973 batch loss 0.723577082 epoch total loss 0.681819916\n",
      "Trained batch 974 batch loss 0.700529456 epoch total loss 0.681839108\n",
      "Trained batch 975 batch loss 0.716574907 epoch total loss 0.681874692\n",
      "Trained batch 976 batch loss 0.682744265 epoch total loss 0.681875587\n",
      "Trained batch 977 batch loss 0.649570704 epoch total loss 0.681842506\n",
      "Trained batch 978 batch loss 0.631740212 epoch total loss 0.681791306\n",
      "Trained batch 979 batch loss 0.657891393 epoch total loss 0.681766868\n",
      "Trained batch 980 batch loss 0.690728724 epoch total loss 0.681776047\n",
      "Trained batch 981 batch loss 0.677252114 epoch total loss 0.681771398\n",
      "Trained batch 982 batch loss 0.677839935 epoch total loss 0.681767404\n",
      "Trained batch 983 batch loss 0.697310388 epoch total loss 0.681783259\n",
      "Trained batch 984 batch loss 0.639945686 epoch total loss 0.681740761\n",
      "Trained batch 985 batch loss 0.701784909 epoch total loss 0.681761086\n",
      "Trained batch 986 batch loss 0.620480478 epoch total loss 0.681698918\n",
      "Trained batch 987 batch loss 0.643637717 epoch total loss 0.681660354\n",
      "Trained batch 988 batch loss 0.627249181 epoch total loss 0.681605279\n",
      "Trained batch 989 batch loss 0.642811358 epoch total loss 0.68156606\n",
      "Trained batch 990 batch loss 0.623005331 epoch total loss 0.681506932\n",
      "Trained batch 991 batch loss 0.650067627 epoch total loss 0.681475222\n",
      "Trained batch 992 batch loss 0.634639442 epoch total loss 0.681428\n",
      "Trained batch 993 batch loss 0.616551578 epoch total loss 0.681362689\n",
      "Trained batch 994 batch loss 0.640882432 epoch total loss 0.681321919\n",
      "Trained batch 995 batch loss 0.658626676 epoch total loss 0.68129915\n",
      "Trained batch 996 batch loss 0.657006502 epoch total loss 0.681274712\n",
      "Trained batch 997 batch loss 0.636876404 epoch total loss 0.681230187\n",
      "Trained batch 998 batch loss 0.526843369 epoch total loss 0.681075513\n",
      "Trained batch 999 batch loss 0.558869 epoch total loss 0.680953205\n",
      "Trained batch 1000 batch loss 0.631774843 epoch total loss 0.680904031\n",
      "Trained batch 1001 batch loss 0.517360747 epoch total loss 0.680740654\n",
      "Trained batch 1002 batch loss 0.607523322 epoch total loss 0.680667579\n",
      "Trained batch 1003 batch loss 0.640062 epoch total loss 0.680627108\n",
      "Trained batch 1004 batch loss 0.695767403 epoch total loss 0.680642188\n",
      "Trained batch 1005 batch loss 0.634130538 epoch total loss 0.680595934\n",
      "Trained batch 1006 batch loss 0.646181941 epoch total loss 0.680561721\n",
      "Trained batch 1007 batch loss 0.629328966 epoch total loss 0.680510819\n",
      "Trained batch 1008 batch loss 0.670247078 epoch total loss 0.680500627\n",
      "Trained batch 1009 batch loss 0.636951566 epoch total loss 0.680457473\n",
      "Trained batch 1010 batch loss 0.646453381 epoch total loss 0.680423796\n",
      "Trained batch 1011 batch loss 0.595248342 epoch total loss 0.680339575\n",
      "Trained batch 1012 batch loss 0.672876179 epoch total loss 0.680332184\n",
      "Trained batch 1013 batch loss 0.605998337 epoch total loss 0.680258811\n",
      "Trained batch 1014 batch loss 0.663911819 epoch total loss 0.680242717\n",
      "Trained batch 1015 batch loss 0.692154408 epoch total loss 0.680254459\n",
      "Trained batch 1016 batch loss 0.622763455 epoch total loss 0.680197835\n",
      "Trained batch 1017 batch loss 0.634411216 epoch total loss 0.680152774\n",
      "Trained batch 1018 batch loss 0.698777914 epoch total loss 0.680171132\n",
      "Trained batch 1019 batch loss 0.721575916 epoch total loss 0.680211723\n",
      "Trained batch 1020 batch loss 0.583163857 epoch total loss 0.680116594\n",
      "Trained batch 1021 batch loss 0.594314277 epoch total loss 0.680032551\n",
      "Trained batch 1022 batch loss 0.617621839 epoch total loss 0.679971457\n",
      "Trained batch 1023 batch loss 0.600525081 epoch total loss 0.679893792\n",
      "Trained batch 1024 batch loss 0.64520818 epoch total loss 0.679859936\n",
      "Trained batch 1025 batch loss 0.633014202 epoch total loss 0.679814219\n",
      "Trained batch 1026 batch loss 0.670159 epoch total loss 0.679804802\n",
      "Trained batch 1027 batch loss 0.638539553 epoch total loss 0.679764628\n",
      "Trained batch 1028 batch loss 0.70306617 epoch total loss 0.679787278\n",
      "Trained batch 1029 batch loss 0.574160695 epoch total loss 0.679684639\n",
      "Trained batch 1030 batch loss 0.716189921 epoch total loss 0.679720104\n",
      "Trained batch 1031 batch loss 0.775468528 epoch total loss 0.679812968\n",
      "Trained batch 1032 batch loss 0.801378965 epoch total loss 0.679930747\n",
      "Trained batch 1033 batch loss 0.771696925 epoch total loss 0.680019557\n",
      "Trained batch 1034 batch loss 0.768808722 epoch total loss 0.680105448\n",
      "Trained batch 1035 batch loss 0.690301299 epoch total loss 0.680115283\n",
      "Trained batch 1036 batch loss 0.637180746 epoch total loss 0.680073857\n",
      "Trained batch 1037 batch loss 0.61913693 epoch total loss 0.680015087\n",
      "Trained batch 1038 batch loss 0.661667407 epoch total loss 0.679997444\n",
      "Trained batch 1039 batch loss 0.529515564 epoch total loss 0.679852605\n",
      "Trained batch 1040 batch loss 0.6236552 epoch total loss 0.679798603\n",
      "Trained batch 1041 batch loss 0.713269532 epoch total loss 0.67983073\n",
      "Trained batch 1042 batch loss 0.719965339 epoch total loss 0.679869235\n",
      "Trained batch 1043 batch loss 0.836529911 epoch total loss 0.680019498\n",
      "Trained batch 1044 batch loss 0.808002889 epoch total loss 0.680142045\n",
      "Trained batch 1045 batch loss 0.720595062 epoch total loss 0.680180728\n",
      "Trained batch 1046 batch loss 0.631397724 epoch total loss 0.680134118\n",
      "Trained batch 1047 batch loss 0.692277491 epoch total loss 0.680145681\n",
      "Trained batch 1048 batch loss 0.634429693 epoch total loss 0.68010205\n",
      "Trained batch 1049 batch loss 0.585905969 epoch total loss 0.680012226\n",
      "Trained batch 1050 batch loss 0.62968564 epoch total loss 0.679964304\n",
      "Trained batch 1051 batch loss 0.690038681 epoch total loss 0.6799739\n",
      "Trained batch 1052 batch loss 0.669130385 epoch total loss 0.679963589\n",
      "Trained batch 1053 batch loss 0.683156848 epoch total loss 0.679966629\n",
      "Trained batch 1054 batch loss 0.683511198 epoch total loss 0.67997\n",
      "Trained batch 1055 batch loss 0.669199526 epoch total loss 0.679959834\n",
      "Trained batch 1056 batch loss 0.593746543 epoch total loss 0.679878175\n",
      "Trained batch 1057 batch loss 0.588411 epoch total loss 0.679791689\n",
      "Trained batch 1058 batch loss 0.587883055 epoch total loss 0.679704785\n",
      "Trained batch 1059 batch loss 0.56212604 epoch total loss 0.679593801\n",
      "Trained batch 1060 batch loss 0.584422469 epoch total loss 0.679504\n",
      "Trained batch 1061 batch loss 0.61238414 epoch total loss 0.679440737\n",
      "Trained batch 1062 batch loss 0.608955801 epoch total loss 0.679374337\n",
      "Trained batch 1063 batch loss 0.680755615 epoch total loss 0.679375589\n",
      "Trained batch 1064 batch loss 0.547806382 epoch total loss 0.679251909\n",
      "Trained batch 1065 batch loss 0.559434116 epoch total loss 0.679139435\n",
      "Trained batch 1066 batch loss 0.592247963 epoch total loss 0.679057896\n",
      "Trained batch 1067 batch loss 0.626516 epoch total loss 0.679008663\n",
      "Trained batch 1068 batch loss 0.622774243 epoch total loss 0.678956032\n",
      "Trained batch 1069 batch loss 0.689769924 epoch total loss 0.678966165\n",
      "Trained batch 1070 batch loss 0.684492469 epoch total loss 0.67897135\n",
      "Trained batch 1071 batch loss 0.639293671 epoch total loss 0.678934276\n",
      "Trained batch 1072 batch loss 0.663316548 epoch total loss 0.678919733\n",
      "Trained batch 1073 batch loss 0.6728 epoch total loss 0.678914\n",
      "Trained batch 1074 batch loss 0.583318472 epoch total loss 0.678825\n",
      "Trained batch 1075 batch loss 0.527380466 epoch total loss 0.678684115\n",
      "Trained batch 1076 batch loss 0.668474197 epoch total loss 0.678674638\n",
      "Trained batch 1077 batch loss 0.581061959 epoch total loss 0.678584\n",
      "Trained batch 1078 batch loss 0.645244956 epoch total loss 0.678553104\n",
      "Trained batch 1079 batch loss 0.716030717 epoch total loss 0.678587794\n",
      "Trained batch 1080 batch loss 0.757340789 epoch total loss 0.678660691\n",
      "Trained batch 1081 batch loss 0.727172375 epoch total loss 0.678705573\n",
      "Trained batch 1082 batch loss 0.703300238 epoch total loss 0.678728282\n",
      "Trained batch 1083 batch loss 0.717307 epoch total loss 0.678763926\n",
      "Trained batch 1084 batch loss 0.617225885 epoch total loss 0.678707182\n",
      "Trained batch 1085 batch loss 0.644061685 epoch total loss 0.678675234\n",
      "Trained batch 1086 batch loss 0.718765795 epoch total loss 0.67871213\n",
      "Trained batch 1087 batch loss 0.677547812 epoch total loss 0.678711057\n",
      "Trained batch 1088 batch loss 0.626559138 epoch total loss 0.678663135\n",
      "Trained batch 1089 batch loss 0.586007357 epoch total loss 0.678578079\n",
      "Trained batch 1090 batch loss 0.564635694 epoch total loss 0.678473532\n",
      "Trained batch 1091 batch loss 0.6218 epoch total loss 0.678421617\n",
      "Trained batch 1092 batch loss 0.651061833 epoch total loss 0.678396523\n",
      "Trained batch 1093 batch loss 0.762050927 epoch total loss 0.678473055\n",
      "Trained batch 1094 batch loss 0.710199416 epoch total loss 0.678502083\n",
      "Trained batch 1095 batch loss 0.668408394 epoch total loss 0.678492844\n",
      "Trained batch 1096 batch loss 0.677207 epoch total loss 0.678491652\n",
      "Trained batch 1097 batch loss 0.68883729 epoch total loss 0.67850107\n",
      "Trained batch 1098 batch loss 0.672302783 epoch total loss 0.678495407\n",
      "Trained batch 1099 batch loss 0.655810356 epoch total loss 0.678474784\n",
      "Trained batch 1100 batch loss 0.66180253 epoch total loss 0.678459644\n",
      "Trained batch 1101 batch loss 0.654755771 epoch total loss 0.678438127\n",
      "Trained batch 1102 batch loss 0.659745753 epoch total loss 0.67842114\n",
      "Trained batch 1103 batch loss 0.680711806 epoch total loss 0.678423226\n",
      "Trained batch 1104 batch loss 0.698691964 epoch total loss 0.678441584\n",
      "Trained batch 1105 batch loss 0.716480553 epoch total loss 0.678476036\n",
      "Trained batch 1106 batch loss 0.763230264 epoch total loss 0.678552687\n",
      "Trained batch 1107 batch loss 0.652522385 epoch total loss 0.678529143\n",
      "Trained batch 1108 batch loss 0.639404178 epoch total loss 0.678493857\n",
      "Trained batch 1109 batch loss 0.685382783 epoch total loss 0.678500056\n",
      "Trained batch 1110 batch loss 0.608688354 epoch total loss 0.678437173\n",
      "Trained batch 1111 batch loss 0.662967682 epoch total loss 0.678423226\n",
      "Trained batch 1112 batch loss 0.634766519 epoch total loss 0.678383946\n",
      "Trained batch 1113 batch loss 0.668110132 epoch total loss 0.678374708\n",
      "Trained batch 1114 batch loss 0.642657042 epoch total loss 0.67834264\n",
      "Trained batch 1115 batch loss 0.594736934 epoch total loss 0.678267658\n",
      "Trained batch 1116 batch loss 0.611408591 epoch total loss 0.678207755\n",
      "Trained batch 1117 batch loss 0.653542876 epoch total loss 0.678185642\n",
      "Trained batch 1118 batch loss 0.701131225 epoch total loss 0.678206146\n",
      "Trained batch 1119 batch loss 0.687851071 epoch total loss 0.678214788\n",
      "Trained batch 1120 batch loss 0.60954082 epoch total loss 0.678153515\n",
      "Trained batch 1121 batch loss 0.647226334 epoch total loss 0.678125918\n",
      "Trained batch 1122 batch loss 0.699439 epoch total loss 0.678144932\n",
      "Trained batch 1123 batch loss 0.656325579 epoch total loss 0.678125501\n",
      "Trained batch 1124 batch loss 0.61296612 epoch total loss 0.678067505\n",
      "Trained batch 1125 batch loss 0.584545791 epoch total loss 0.677984357\n",
      "Trained batch 1126 batch loss 0.630976796 epoch total loss 0.677942634\n",
      "Trained batch 1127 batch loss 0.66757226 epoch total loss 0.677933455\n",
      "Trained batch 1128 batch loss 0.597407043 epoch total loss 0.677862048\n",
      "Trained batch 1129 batch loss 0.536757708 epoch total loss 0.677737057\n",
      "Trained batch 1130 batch loss 0.516803443 epoch total loss 0.677594662\n",
      "Trained batch 1131 batch loss 0.485626131 epoch total loss 0.677424908\n",
      "Trained batch 1132 batch loss 0.65333271 epoch total loss 0.677403569\n",
      "Trained batch 1133 batch loss 0.68422842 epoch total loss 0.677409589\n",
      "Trained batch 1134 batch loss 0.760666549 epoch total loss 0.677483\n",
      "Trained batch 1135 batch loss 0.754354298 epoch total loss 0.677550733\n",
      "Trained batch 1136 batch loss 0.684898615 epoch total loss 0.67755717\n",
      "Trained batch 1137 batch loss 0.674767494 epoch total loss 0.677554727\n",
      "Trained batch 1138 batch loss 0.683330894 epoch total loss 0.677559793\n",
      "Trained batch 1139 batch loss 0.680463791 epoch total loss 0.677562356\n",
      "Trained batch 1140 batch loss 0.657112777 epoch total loss 0.677544415\n",
      "Trained batch 1141 batch loss 0.718676448 epoch total loss 0.677580476\n",
      "Trained batch 1142 batch loss 0.825098455 epoch total loss 0.677709639\n",
      "Trained batch 1143 batch loss 0.687250614 epoch total loss 0.677718\n",
      "Trained batch 1144 batch loss 0.695790112 epoch total loss 0.677733779\n",
      "Trained batch 1145 batch loss 0.679528892 epoch total loss 0.677735329\n",
      "Trained batch 1146 batch loss 0.639533401 epoch total loss 0.677702\n",
      "Trained batch 1147 batch loss 0.709402561 epoch total loss 0.677729666\n",
      "Trained batch 1148 batch loss 0.778028369 epoch total loss 0.677817\n",
      "Trained batch 1149 batch loss 0.752296388 epoch total loss 0.677881837\n",
      "Trained batch 1150 batch loss 0.735638499 epoch total loss 0.677932084\n",
      "Trained batch 1151 batch loss 0.670053244 epoch total loss 0.677925229\n",
      "Trained batch 1152 batch loss 0.660862446 epoch total loss 0.677910447\n",
      "Trained batch 1153 batch loss 0.668343365 epoch total loss 0.677902102\n",
      "Trained batch 1154 batch loss 0.642914414 epoch total loss 0.677871823\n",
      "Trained batch 1155 batch loss 0.669928312 epoch total loss 0.677864969\n",
      "Trained batch 1156 batch loss 0.605402648 epoch total loss 0.677802265\n",
      "Trained batch 1157 batch loss 0.54992938 epoch total loss 0.677691758\n",
      "Trained batch 1158 batch loss 0.555745244 epoch total loss 0.677586436\n",
      "Trained batch 1159 batch loss 0.58806 epoch total loss 0.677509189\n",
      "Trained batch 1160 batch loss 0.634766161 epoch total loss 0.677472353\n",
      "Trained batch 1161 batch loss 0.699228346 epoch total loss 0.677491069\n",
      "Trained batch 1162 batch loss 0.715026438 epoch total loss 0.677523375\n",
      "Trained batch 1163 batch loss 0.687740862 epoch total loss 0.677532196\n",
      "Trained batch 1164 batch loss 0.675314784 epoch total loss 0.677530229\n",
      "Trained batch 1165 batch loss 0.64508605 epoch total loss 0.677502394\n",
      "Trained batch 1166 batch loss 0.735767901 epoch total loss 0.677552342\n",
      "Trained batch 1167 batch loss 0.737754 epoch total loss 0.67760396\n",
      "Trained batch 1168 batch loss 0.729754388 epoch total loss 0.677648544\n",
      "Trained batch 1169 batch loss 0.704649627 epoch total loss 0.677671671\n",
      "Trained batch 1170 batch loss 0.760096669 epoch total loss 0.677742124\n",
      "Trained batch 1171 batch loss 0.65554738 epoch total loss 0.67772311\n",
      "Trained batch 1172 batch loss 0.699639738 epoch total loss 0.677741826\n",
      "Trained batch 1173 batch loss 0.711595416 epoch total loss 0.677770674\n",
      "Trained batch 1174 batch loss 0.646319091 epoch total loss 0.677743912\n",
      "Trained batch 1175 batch loss 0.654929757 epoch total loss 0.677724481\n",
      "Trained batch 1176 batch loss 0.664374769 epoch total loss 0.677713096\n",
      "Trained batch 1177 batch loss 0.523010969 epoch total loss 0.677581668\n",
      "Trained batch 1178 batch loss 0.573185384 epoch total loss 0.677493036\n",
      "Trained batch 1179 batch loss 0.597627699 epoch total loss 0.677425325\n",
      "Trained batch 1180 batch loss 0.554539204 epoch total loss 0.677321196\n",
      "Trained batch 1181 batch loss 0.54624784 epoch total loss 0.677210212\n",
      "Trained batch 1182 batch loss 0.591754735 epoch total loss 0.677137911\n",
      "Trained batch 1183 batch loss 0.73244065 epoch total loss 0.677184641\n",
      "Trained batch 1184 batch loss 0.791193962 epoch total loss 0.677280962\n",
      "Trained batch 1185 batch loss 0.801601052 epoch total loss 0.677385807\n",
      "Trained batch 1186 batch loss 0.773266256 epoch total loss 0.677466691\n",
      "Trained batch 1187 batch loss 0.680773199 epoch total loss 0.677469432\n",
      "Trained batch 1188 batch loss 0.62647444 epoch total loss 0.677426517\n",
      "Trained batch 1189 batch loss 0.649175048 epoch total loss 0.677402735\n",
      "Trained batch 1190 batch loss 0.623882651 epoch total loss 0.677357793\n",
      "Trained batch 1191 batch loss 0.673083246 epoch total loss 0.677354217\n",
      "Trained batch 1192 batch loss 0.701267958 epoch total loss 0.677374303\n",
      "Trained batch 1193 batch loss 0.692759335 epoch total loss 0.677387178\n",
      "Trained batch 1194 batch loss 0.591336489 epoch total loss 0.677315116\n",
      "Trained batch 1195 batch loss 0.65622133 epoch total loss 0.677297473\n",
      "Trained batch 1196 batch loss 0.606915295 epoch total loss 0.677238643\n",
      "Trained batch 1197 batch loss 0.628358603 epoch total loss 0.677197814\n",
      "Trained batch 1198 batch loss 0.705700815 epoch total loss 0.677221596\n",
      "Trained batch 1199 batch loss 0.64058 epoch total loss 0.677191\n",
      "Trained batch 1200 batch loss 0.659361184 epoch total loss 0.677176178\n",
      "Trained batch 1201 batch loss 0.688719273 epoch total loss 0.677185774\n",
      "Trained batch 1202 batch loss 0.611884773 epoch total loss 0.677131414\n",
      "Trained batch 1203 batch loss 0.705193639 epoch total loss 0.677154779\n",
      "Trained batch 1204 batch loss 0.679314792 epoch total loss 0.677156568\n",
      "Trained batch 1205 batch loss 0.672601 epoch total loss 0.677152812\n",
      "Trained batch 1206 batch loss 0.675770164 epoch total loss 0.67715168\n",
      "Trained batch 1207 batch loss 0.658231497 epoch total loss 0.677135944\n",
      "Trained batch 1208 batch loss 0.635146797 epoch total loss 0.677101195\n",
      "Trained batch 1209 batch loss 0.645938337 epoch total loss 0.677075386\n",
      "Trained batch 1210 batch loss 0.620683908 epoch total loss 0.677028775\n",
      "Trained batch 1211 batch loss 0.619058609 epoch total loss 0.676980913\n",
      "Trained batch 1212 batch loss 0.662324607 epoch total loss 0.676968873\n",
      "Trained batch 1213 batch loss 0.598473 epoch total loss 0.676904142\n",
      "Trained batch 1214 batch loss 0.612127244 epoch total loss 0.676850796\n",
      "Trained batch 1215 batch loss 0.689601302 epoch total loss 0.676861227\n",
      "Trained batch 1216 batch loss 0.668202281 epoch total loss 0.676854134\n",
      "Trained batch 1217 batch loss 0.648777843 epoch total loss 0.676831067\n",
      "Trained batch 1218 batch loss 0.607721448 epoch total loss 0.676774323\n",
      "Trained batch 1219 batch loss 0.666884899 epoch total loss 0.676766217\n",
      "Trained batch 1220 batch loss 0.697976589 epoch total loss 0.676783621\n",
      "Trained batch 1221 batch loss 0.724850655 epoch total loss 0.676823\n",
      "Trained batch 1222 batch loss 0.710670114 epoch total loss 0.676850736\n",
      "Trained batch 1223 batch loss 0.57999748 epoch total loss 0.676771522\n",
      "Trained batch 1224 batch loss 0.572877288 epoch total loss 0.676686645\n",
      "Trained batch 1225 batch loss 0.647826135 epoch total loss 0.676663101\n",
      "Trained batch 1226 batch loss 0.637029588 epoch total loss 0.676630735\n",
      "Trained batch 1227 batch loss 0.644256 epoch total loss 0.676604331\n",
      "Trained batch 1228 batch loss 0.701744139 epoch total loss 0.676624835\n",
      "Trained batch 1229 batch loss 0.667234361 epoch total loss 0.676617146\n",
      "Trained batch 1230 batch loss 0.724931121 epoch total loss 0.676656425\n",
      "Trained batch 1231 batch loss 0.69687295 epoch total loss 0.676672876\n",
      "Trained batch 1232 batch loss 0.746400714 epoch total loss 0.6767295\n",
      "Trained batch 1233 batch loss 0.629737496 epoch total loss 0.676691353\n",
      "Trained batch 1234 batch loss 0.536342859 epoch total loss 0.676577628\n",
      "Trained batch 1235 batch loss 0.533064365 epoch total loss 0.676461458\n",
      "Trained batch 1236 batch loss 0.554525256 epoch total loss 0.676362753\n",
      "Trained batch 1237 batch loss 0.566158593 epoch total loss 0.676273704\n",
      "Trained batch 1238 batch loss 0.623888135 epoch total loss 0.676231384\n",
      "Trained batch 1239 batch loss 0.634918571 epoch total loss 0.676198065\n",
      "Trained batch 1240 batch loss 0.672191262 epoch total loss 0.676194787\n",
      "Trained batch 1241 batch loss 0.613548279 epoch total loss 0.676144302\n",
      "Trained batch 1242 batch loss 0.568186224 epoch total loss 0.676057398\n",
      "Trained batch 1243 batch loss 0.552118361 epoch total loss 0.67595768\n",
      "Trained batch 1244 batch loss 0.632894754 epoch total loss 0.675923049\n",
      "Trained batch 1245 batch loss 0.729828477 epoch total loss 0.675966382\n",
      "Trained batch 1246 batch loss 0.678909779 epoch total loss 0.675968707\n",
      "Trained batch 1247 batch loss 0.683257878 epoch total loss 0.675974548\n",
      "Trained batch 1248 batch loss 0.620005667 epoch total loss 0.675929666\n",
      "Trained batch 1249 batch loss 0.740864158 epoch total loss 0.675981641\n",
      "Trained batch 1250 batch loss 0.717329681 epoch total loss 0.676014721\n",
      "Trained batch 1251 batch loss 0.714866936 epoch total loss 0.676045775\n",
      "Trained batch 1252 batch loss 0.664263129 epoch total loss 0.676036358\n",
      "Trained batch 1253 batch loss 0.648024321 epoch total loss 0.676014\n",
      "Trained batch 1254 batch loss 0.848320603 epoch total loss 0.676151395\n",
      "Trained batch 1255 batch loss 0.804251 epoch total loss 0.676253498\n",
      "Trained batch 1256 batch loss 0.743163347 epoch total loss 0.676306784\n",
      "Trained batch 1257 batch loss 0.862359881 epoch total loss 0.676454782\n",
      "Trained batch 1258 batch loss 0.718135118 epoch total loss 0.676487923\n",
      "Trained batch 1259 batch loss 0.682647109 epoch total loss 0.676492751\n",
      "Trained batch 1260 batch loss 0.633975327 epoch total loss 0.676459\n",
      "Trained batch 1261 batch loss 0.661205232 epoch total loss 0.676446915\n",
      "Trained batch 1262 batch loss 0.675257921 epoch total loss 0.676445961\n",
      "Trained batch 1263 batch loss 0.610709071 epoch total loss 0.676393926\n",
      "Trained batch 1264 batch loss 0.638590455 epoch total loss 0.676364\n",
      "Trained batch 1265 batch loss 0.631055117 epoch total loss 0.676328182\n",
      "Trained batch 1266 batch loss 0.608665466 epoch total loss 0.676274717\n",
      "Trained batch 1267 batch loss 0.662026 epoch total loss 0.676263511\n",
      "Trained batch 1268 batch loss 0.621769369 epoch total loss 0.676220536\n",
      "Trained batch 1269 batch loss 0.555916905 epoch total loss 0.676125705\n",
      "Trained batch 1270 batch loss 0.569273829 epoch total loss 0.676041603\n",
      "Trained batch 1271 batch loss 0.558492303 epoch total loss 0.675949097\n",
      "Trained batch 1272 batch loss 0.634382844 epoch total loss 0.675916433\n",
      "Trained batch 1273 batch loss 0.608039558 epoch total loss 0.675863087\n",
      "Trained batch 1274 batch loss 0.648176074 epoch total loss 0.675841391\n",
      "Trained batch 1275 batch loss 0.624886036 epoch total loss 0.675801396\n",
      "Trained batch 1276 batch loss 0.642288923 epoch total loss 0.675775111\n",
      "Trained batch 1277 batch loss 0.579733729 epoch total loss 0.67569989\n",
      "Trained batch 1278 batch loss 0.620102644 epoch total loss 0.675656438\n",
      "Trained batch 1279 batch loss 0.630868316 epoch total loss 0.67562139\n",
      "Trained batch 1280 batch loss 0.596612096 epoch total loss 0.67555964\n",
      "Trained batch 1281 batch loss 0.619131446 epoch total loss 0.675515592\n",
      "Trained batch 1282 batch loss 0.675624549 epoch total loss 0.675515711\n",
      "Trained batch 1283 batch loss 0.642314374 epoch total loss 0.675489843\n",
      "Trained batch 1284 batch loss 0.614179 epoch total loss 0.6754421\n",
      "Trained batch 1285 batch loss 0.763161063 epoch total loss 0.675510347\n",
      "Trained batch 1286 batch loss 0.796847522 epoch total loss 0.675604761\n",
      "Trained batch 1287 batch loss 0.780608177 epoch total loss 0.6756863\n",
      "Trained batch 1288 batch loss 0.736078858 epoch total loss 0.675733209\n",
      "Trained batch 1289 batch loss 0.717073798 epoch total loss 0.675765276\n",
      "Trained batch 1290 batch loss 0.600272775 epoch total loss 0.675706804\n",
      "Trained batch 1291 batch loss 0.682226896 epoch total loss 0.67571187\n",
      "Trained batch 1292 batch loss 0.821680307 epoch total loss 0.675824821\n",
      "Trained batch 1293 batch loss 0.758729696 epoch total loss 0.675888896\n",
      "Trained batch 1294 batch loss 0.788816 epoch total loss 0.675976217\n",
      "Trained batch 1295 batch loss 0.676189661 epoch total loss 0.675976396\n",
      "Trained batch 1296 batch loss 0.658161223 epoch total loss 0.675962627\n",
      "Trained batch 1297 batch loss 0.689844906 epoch total loss 0.675973296\n",
      "Trained batch 1298 batch loss 0.753365397 epoch total loss 0.676032901\n",
      "Trained batch 1299 batch loss 0.640148461 epoch total loss 0.676005304\n",
      "Trained batch 1300 batch loss 0.620351136 epoch total loss 0.675962448\n",
      "Trained batch 1301 batch loss 0.635698676 epoch total loss 0.675931513\n",
      "Trained batch 1302 batch loss 0.629709601 epoch total loss 0.675896\n",
      "Trained batch 1303 batch loss 0.660718799 epoch total loss 0.675884366\n",
      "Trained batch 1304 batch loss 0.610636473 epoch total loss 0.675834358\n",
      "Trained batch 1305 batch loss 0.732578 epoch total loss 0.67587781\n",
      "Trained batch 1306 batch loss 0.678530633 epoch total loss 0.675879836\n",
      "Trained batch 1307 batch loss 0.744871616 epoch total loss 0.675932646\n",
      "Trained batch 1308 batch loss 0.72659719 epoch total loss 0.675971389\n",
      "Trained batch 1309 batch loss 0.702120483 epoch total loss 0.675991416\n",
      "Trained batch 1310 batch loss 0.714177 epoch total loss 0.676020563\n",
      "Trained batch 1311 batch loss 0.703123391 epoch total loss 0.676041245\n",
      "Trained batch 1312 batch loss 0.641977131 epoch total loss 0.676015258\n",
      "Trained batch 1313 batch loss 0.70909673 epoch total loss 0.676040471\n",
      "Trained batch 1314 batch loss 0.637596846 epoch total loss 0.676011205\n",
      "Trained batch 1315 batch loss 0.688772917 epoch total loss 0.676020861\n",
      "Trained batch 1316 batch loss 0.701215744 epoch total loss 0.676040053\n",
      "Trained batch 1317 batch loss 0.708853245 epoch total loss 0.676064968\n",
      "Trained batch 1318 batch loss 0.719537079 epoch total loss 0.676097929\n",
      "Trained batch 1319 batch loss 0.676784396 epoch total loss 0.676098466\n",
      "Trained batch 1320 batch loss 0.655030489 epoch total loss 0.676082492\n",
      "Trained batch 1321 batch loss 0.684525251 epoch total loss 0.67608887\n",
      "Trained batch 1322 batch loss 0.714267 epoch total loss 0.676117778\n",
      "Trained batch 1323 batch loss 0.687709689 epoch total loss 0.67612654\n",
      "Trained batch 1324 batch loss 0.704610586 epoch total loss 0.676148\n",
      "Trained batch 1325 batch loss 0.63765204 epoch total loss 0.67611897\n",
      "Trained batch 1326 batch loss 0.706348777 epoch total loss 0.676141739\n",
      "Trained batch 1327 batch loss 0.657637596 epoch total loss 0.676127791\n",
      "Trained batch 1328 batch loss 0.781655192 epoch total loss 0.676207304\n",
      "Trained batch 1329 batch loss 0.720549166 epoch total loss 0.676240623\n",
      "Trained batch 1330 batch loss 0.690365791 epoch total loss 0.676251292\n",
      "Trained batch 1331 batch loss 0.659876823 epoch total loss 0.676238954\n",
      "Trained batch 1332 batch loss 0.643061697 epoch total loss 0.676214039\n",
      "Trained batch 1333 batch loss 0.657775402 epoch total loss 0.676200211\n",
      "Trained batch 1334 batch loss 0.727545261 epoch total loss 0.676238716\n",
      "Trained batch 1335 batch loss 0.699165046 epoch total loss 0.676255882\n",
      "Trained batch 1336 batch loss 0.588364244 epoch total loss 0.676190078\n",
      "Trained batch 1337 batch loss 0.741331 epoch total loss 0.676238835\n",
      "Trained batch 1338 batch loss 0.730485678 epoch total loss 0.676279366\n",
      "Trained batch 1339 batch loss 0.672395289 epoch total loss 0.676276445\n",
      "Trained batch 1340 batch loss 0.700416505 epoch total loss 0.676294506\n",
      "Trained batch 1341 batch loss 0.615471184 epoch total loss 0.676249146\n",
      "Trained batch 1342 batch loss 0.637705326 epoch total loss 0.676220417\n",
      "Trained batch 1343 batch loss 0.568831384 epoch total loss 0.676140487\n",
      "Trained batch 1344 batch loss 0.589500129 epoch total loss 0.676076\n",
      "Trained batch 1345 batch loss 0.666877508 epoch total loss 0.67606914\n",
      "Trained batch 1346 batch loss 0.760274529 epoch total loss 0.676131666\n",
      "Trained batch 1347 batch loss 0.748699486 epoch total loss 0.676185548\n",
      "Trained batch 1348 batch loss 0.657707632 epoch total loss 0.676171839\n",
      "Trained batch 1349 batch loss 0.606953859 epoch total loss 0.67612052\n",
      "Trained batch 1350 batch loss 0.610625327 epoch total loss 0.676072\n",
      "Trained batch 1351 batch loss 0.555133104 epoch total loss 0.675982475\n",
      "Trained batch 1352 batch loss 0.537448645 epoch total loss 0.67588\n",
      "Trained batch 1353 batch loss 0.577564597 epoch total loss 0.675807357\n",
      "Trained batch 1354 batch loss 0.650561512 epoch total loss 0.67578876\n",
      "Trained batch 1355 batch loss 0.700873435 epoch total loss 0.675807238\n",
      "Trained batch 1356 batch loss 0.71618247 epoch total loss 0.67583704\n",
      "Trained batch 1357 batch loss 0.754591 epoch total loss 0.675895035\n",
      "Trained batch 1358 batch loss 0.73212868 epoch total loss 0.67593646\n",
      "Trained batch 1359 batch loss 0.697653294 epoch total loss 0.675952435\n",
      "Trained batch 1360 batch loss 0.624145746 epoch total loss 0.675914288\n",
      "Trained batch 1361 batch loss 0.652155817 epoch total loss 0.675896883\n",
      "Trained batch 1362 batch loss 0.650096416 epoch total loss 0.675877929\n",
      "Trained batch 1363 batch loss 0.63482672 epoch total loss 0.675847769\n",
      "Trained batch 1364 batch loss 0.68896389 epoch total loss 0.675857425\n",
      "Trained batch 1365 batch loss 0.71386224 epoch total loss 0.67588526\n",
      "Trained batch 1366 batch loss 0.651395 epoch total loss 0.675867319\n",
      "Trained batch 1367 batch loss 0.657186449 epoch total loss 0.67585361\n",
      "Trained batch 1368 batch loss 0.62446177 epoch total loss 0.675816059\n",
      "Trained batch 1369 batch loss 0.733447433 epoch total loss 0.67585814\n",
      "Trained batch 1370 batch loss 0.730885625 epoch total loss 0.675898314\n",
      "Trained batch 1371 batch loss 0.581213832 epoch total loss 0.675829291\n",
      "Trained batch 1372 batch loss 0.611231089 epoch total loss 0.675782204\n",
      "Trained batch 1373 batch loss 0.672161758 epoch total loss 0.675779581\n",
      "Trained batch 1374 batch loss 0.609931 epoch total loss 0.675731659\n",
      "Trained batch 1375 batch loss 0.667167 epoch total loss 0.6757254\n",
      "Trained batch 1376 batch loss 0.654554725 epoch total loss 0.67571\n",
      "Trained batch 1377 batch loss 0.673182845 epoch total loss 0.675708175\n",
      "Trained batch 1378 batch loss 0.648402214 epoch total loss 0.675688326\n",
      "Trained batch 1379 batch loss 0.643897891 epoch total loss 0.675665259\n",
      "Trained batch 1380 batch loss 0.695780814 epoch total loss 0.675679862\n",
      "Trained batch 1381 batch loss 0.665823 epoch total loss 0.67567277\n",
      "Trained batch 1382 batch loss 0.679978788 epoch total loss 0.675675869\n",
      "Trained batch 1383 batch loss 0.691502213 epoch total loss 0.675687313\n",
      "Trained batch 1384 batch loss 0.684367418 epoch total loss 0.675693631\n",
      "Trained batch 1385 batch loss 0.670374215 epoch total loss 0.675689757\n",
      "Trained batch 1386 batch loss 0.659195721 epoch total loss 0.675677836\n",
      "Trained batch 1387 batch loss 0.696159422 epoch total loss 0.675692618\n",
      "Trained batch 1388 batch loss 0.700070739 epoch total loss 0.675710201\n",
      "Trained batch 1389 batch loss 0.67213434 epoch total loss 0.675707579\n",
      "Trained batch 1390 batch loss 0.678031862 epoch total loss 0.675709307\n",
      "Trained batch 1391 batch loss 0.71107924 epoch total loss 0.675734699\n",
      "Trained batch 1392 batch loss 0.679337204 epoch total loss 0.675737262\n",
      "Trained batch 1393 batch loss 0.625916719 epoch total loss 0.675701499\n",
      "Trained batch 1394 batch loss 0.6662606 epoch total loss 0.675694704\n",
      "Trained batch 1395 batch loss 0.629362643 epoch total loss 0.675661504\n",
      "Trained batch 1396 batch loss 0.69381392 epoch total loss 0.675674498\n",
      "Trained batch 1397 batch loss 0.695820093 epoch total loss 0.675688863\n",
      "Trained batch 1398 batch loss 0.761842906 epoch total loss 0.675750494\n",
      "Trained batch 1399 batch loss 0.6500054 epoch total loss 0.675732136\n",
      "Trained batch 1400 batch loss 0.596226752 epoch total loss 0.675675333\n",
      "Trained batch 1401 batch loss 0.684315622 epoch total loss 0.675681531\n",
      "Trained batch 1402 batch loss 0.735626221 epoch total loss 0.675724328\n",
      "Trained batch 1403 batch loss 0.620637894 epoch total loss 0.675685048\n",
      "Trained batch 1404 batch loss 0.660118103 epoch total loss 0.675673962\n",
      "Trained batch 1405 batch loss 0.602488458 epoch total loss 0.675621867\n",
      "Trained batch 1406 batch loss 0.638601243 epoch total loss 0.675595522\n",
      "Trained batch 1407 batch loss 0.67200762 epoch total loss 0.675592959\n",
      "Trained batch 1408 batch loss 0.573199391 epoch total loss 0.675520241\n",
      "Trained batch 1409 batch loss 0.611718893 epoch total loss 0.675474942\n",
      "Trained batch 1410 batch loss 0.633869767 epoch total loss 0.675445437\n",
      "Trained batch 1411 batch loss 0.639414489 epoch total loss 0.675419867\n",
      "Trained batch 1412 batch loss 0.613865912 epoch total loss 0.675376296\n",
      "Trained batch 1413 batch loss 0.706354737 epoch total loss 0.675398231\n",
      "Trained batch 1414 batch loss 0.741896629 epoch total loss 0.675445259\n",
      "Trained batch 1415 batch loss 0.716727376 epoch total loss 0.675474465\n",
      "Trained batch 1416 batch loss 0.737359345 epoch total loss 0.675518155\n",
      "Trained batch 1417 batch loss 0.81043154 epoch total loss 0.675613344\n",
      "Trained batch 1418 batch loss 0.790262699 epoch total loss 0.675694227\n",
      "Trained batch 1419 batch loss 0.68467772 epoch total loss 0.675700545\n",
      "Trained batch 1420 batch loss 0.782498658 epoch total loss 0.675775766\n",
      "Trained batch 1421 batch loss 0.744911373 epoch total loss 0.675824404\n",
      "Trained batch 1422 batch loss 0.667534173 epoch total loss 0.675818563\n",
      "Trained batch 1423 batch loss 0.668545425 epoch total loss 0.675813437\n",
      "Trained batch 1424 batch loss 0.679438531 epoch total loss 0.675816\n",
      "Trained batch 1425 batch loss 0.590765297 epoch total loss 0.675756335\n",
      "Trained batch 1426 batch loss 0.594706774 epoch total loss 0.675699472\n",
      "Trained batch 1427 batch loss 0.536584675 epoch total loss 0.675602\n",
      "Trained batch 1428 batch loss 0.501340628 epoch total loss 0.675479949\n",
      "Trained batch 1429 batch loss 0.527240634 epoch total loss 0.675376236\n",
      "Trained batch 1430 batch loss 0.542691 epoch total loss 0.675283432\n",
      "Trained batch 1431 batch loss 0.594633341 epoch total loss 0.675227046\n",
      "Trained batch 1432 batch loss 0.581587136 epoch total loss 0.67516166\n",
      "Trained batch 1433 batch loss 0.637764037 epoch total loss 0.675135553\n",
      "Trained batch 1434 batch loss 0.664149523 epoch total loss 0.675127864\n",
      "Trained batch 1435 batch loss 0.683257341 epoch total loss 0.675133526\n",
      "Trained batch 1436 batch loss 0.679661155 epoch total loss 0.675136685\n",
      "Trained batch 1437 batch loss 0.692034245 epoch total loss 0.675148427\n",
      "Trained batch 1438 batch loss 0.678490222 epoch total loss 0.675150752\n",
      "Trained batch 1439 batch loss 0.633427501 epoch total loss 0.675121725\n",
      "Trained batch 1440 batch loss 0.629392564 epoch total loss 0.675089955\n",
      "Trained batch 1441 batch loss 0.648411632 epoch total loss 0.675071478\n",
      "Trained batch 1442 batch loss 0.620038092 epoch total loss 0.675033331\n",
      "Trained batch 1443 batch loss 0.598490655 epoch total loss 0.674980283\n",
      "Trained batch 1444 batch loss 0.675258279 epoch total loss 0.674980462\n",
      "Trained batch 1445 batch loss 0.660790324 epoch total loss 0.674970627\n",
      "Trained batch 1446 batch loss 0.740322 epoch total loss 0.675015807\n",
      "Trained batch 1447 batch loss 0.650951684 epoch total loss 0.674999177\n",
      "Trained batch 1448 batch loss 0.723591745 epoch total loss 0.675032735\n",
      "Trained batch 1449 batch loss 0.616062641 epoch total loss 0.674992\n",
      "Trained batch 1450 batch loss 0.652395666 epoch total loss 0.674976468\n",
      "Trained batch 1451 batch loss 0.682275593 epoch total loss 0.674981475\n",
      "Trained batch 1452 batch loss 0.718932033 epoch total loss 0.675011754\n",
      "Trained batch 1453 batch loss 0.69640696 epoch total loss 0.675026476\n",
      "Trained batch 1454 batch loss 0.63670361 epoch total loss 0.675000131\n",
      "Trained batch 1455 batch loss 0.633555412 epoch total loss 0.67497164\n",
      "Trained batch 1456 batch loss 0.648816645 epoch total loss 0.674953699\n",
      "Trained batch 1457 batch loss 0.673388 epoch total loss 0.674952626\n",
      "Trained batch 1458 batch loss 0.725253046 epoch total loss 0.674987137\n",
      "Trained batch 1459 batch loss 0.677628458 epoch total loss 0.674988925\n",
      "Trained batch 1460 batch loss 0.650281608 epoch total loss 0.674972\n",
      "Trained batch 1461 batch loss 0.652227 epoch total loss 0.674956441\n",
      "Trained batch 1462 batch loss 0.665062666 epoch total loss 0.674949646\n",
      "Trained batch 1463 batch loss 0.712013483 epoch total loss 0.674975\n",
      "Trained batch 1464 batch loss 0.804275751 epoch total loss 0.675063312\n",
      "Trained batch 1465 batch loss 0.743081927 epoch total loss 0.675109744\n",
      "Trained batch 1466 batch loss 0.640970707 epoch total loss 0.675086439\n",
      "Trained batch 1467 batch loss 0.674328566 epoch total loss 0.675085962\n",
      "Trained batch 1468 batch loss 0.670500696 epoch total loss 0.675082803\n",
      "Trained batch 1469 batch loss 0.645019174 epoch total loss 0.675062358\n",
      "Trained batch 1470 batch loss 0.704592347 epoch total loss 0.675082445\n",
      "Trained batch 1471 batch loss 0.593067765 epoch total loss 0.675026655\n",
      "Trained batch 1472 batch loss 0.630080938 epoch total loss 0.674996138\n",
      "Trained batch 1473 batch loss 0.651001453 epoch total loss 0.674979866\n",
      "Trained batch 1474 batch loss 0.657322168 epoch total loss 0.674967885\n",
      "Trained batch 1475 batch loss 0.800375223 epoch total loss 0.675052881\n",
      "Trained batch 1476 batch loss 0.799027681 epoch total loss 0.675136864\n",
      "Trained batch 1477 batch loss 0.720347166 epoch total loss 0.675167441\n",
      "Trained batch 1478 batch loss 0.807791948 epoch total loss 0.675257206\n",
      "Trained batch 1479 batch loss 0.820489705 epoch total loss 0.675355434\n",
      "Trained batch 1480 batch loss 0.728830338 epoch total loss 0.675391555\n",
      "Trained batch 1481 batch loss 0.530648351 epoch total loss 0.675293803\n",
      "Trained batch 1482 batch loss 0.574646115 epoch total loss 0.675225854\n",
      "Trained batch 1483 batch loss 0.618802905 epoch total loss 0.675187826\n",
      "Trained batch 1484 batch loss 0.62057519 epoch total loss 0.67515105\n",
      "Trained batch 1485 batch loss 0.569906592 epoch total loss 0.675080121\n",
      "Trained batch 1486 batch loss 0.712990642 epoch total loss 0.675105691\n",
      "Trained batch 1487 batch loss 0.618629932 epoch total loss 0.675067723\n",
      "Trained batch 1488 batch loss 0.683152139 epoch total loss 0.675073147\n",
      "Trained batch 1489 batch loss 0.661823 epoch total loss 0.675064266\n",
      "Trained batch 1490 batch loss 0.681473196 epoch total loss 0.675068557\n",
      "Trained batch 1491 batch loss 0.687041938 epoch total loss 0.675076544\n",
      "Trained batch 1492 batch loss 0.675160766 epoch total loss 0.675076604\n",
      "Trained batch 1493 batch loss 0.684922218 epoch total loss 0.67508322\n",
      "Trained batch 1494 batch loss 0.66068846 epoch total loss 0.675073564\n",
      "Trained batch 1495 batch loss 0.744964302 epoch total loss 0.675120294\n",
      "Trained batch 1496 batch loss 0.734335303 epoch total loss 0.675159872\n",
      "Trained batch 1497 batch loss 0.730276525 epoch total loss 0.675196707\n",
      "Trained batch 1498 batch loss 0.751370192 epoch total loss 0.67524755\n",
      "Trained batch 1499 batch loss 0.739558935 epoch total loss 0.675290465\n",
      "Trained batch 1500 batch loss 0.674431622 epoch total loss 0.675289869\n",
      "Trained batch 1501 batch loss 0.663942337 epoch total loss 0.6752823\n",
      "Trained batch 1502 batch loss 0.661165357 epoch total loss 0.675272942\n",
      "Trained batch 1503 batch loss 0.677620888 epoch total loss 0.675274491\n",
      "Trained batch 1504 batch loss 0.617951512 epoch total loss 0.675236404\n",
      "Trained batch 1505 batch loss 0.643774271 epoch total loss 0.675215483\n",
      "Trained batch 1506 batch loss 0.628145 epoch total loss 0.67518425\n",
      "Trained batch 1507 batch loss 0.670708299 epoch total loss 0.675181329\n",
      "Trained batch 1508 batch loss 0.74168545 epoch total loss 0.675225437\n",
      "Trained batch 1509 batch loss 0.65919888 epoch total loss 0.675214767\n",
      "Trained batch 1510 batch loss 0.612048864 epoch total loss 0.675173\n",
      "Trained batch 1511 batch loss 0.661951482 epoch total loss 0.675164223\n",
      "Trained batch 1512 batch loss 0.624443054 epoch total loss 0.675130665\n",
      "Trained batch 1513 batch loss 0.625240088 epoch total loss 0.675097704\n",
      "Trained batch 1514 batch loss 0.582935333 epoch total loss 0.675036788\n",
      "Trained batch 1515 batch loss 0.595603 epoch total loss 0.674984396\n",
      "Trained batch 1516 batch loss 0.543890834 epoch total loss 0.674897909\n",
      "Trained batch 1517 batch loss 0.670885921 epoch total loss 0.674895227\n",
      "Trained batch 1518 batch loss 0.66677916 epoch total loss 0.674889922\n",
      "Trained batch 1519 batch loss 0.656756043 epoch total loss 0.674878\n",
      "Trained batch 1520 batch loss 0.588264227 epoch total loss 0.674821\n",
      "Trained batch 1521 batch loss 0.667459309 epoch total loss 0.674816191\n",
      "Trained batch 1522 batch loss 0.712137818 epoch total loss 0.674840689\n",
      "Trained batch 1523 batch loss 0.771238923 epoch total loss 0.674904\n",
      "Trained batch 1524 batch loss 0.805076897 epoch total loss 0.674989402\n",
      "Trained batch 1525 batch loss 0.642765522 epoch total loss 0.674968302\n",
      "Trained batch 1526 batch loss 0.587813497 epoch total loss 0.674911141\n",
      "Trained batch 1527 batch loss 0.657423496 epoch total loss 0.674899757\n",
      "Trained batch 1528 batch loss 0.582489669 epoch total loss 0.674839258\n",
      "Trained batch 1529 batch loss 0.566251755 epoch total loss 0.674768269\n",
      "Trained batch 1530 batch loss 0.598524928 epoch total loss 0.67471844\n",
      "Trained batch 1531 batch loss 0.596164465 epoch total loss 0.67466712\n",
      "Trained batch 1532 batch loss 0.600819349 epoch total loss 0.674618959\n",
      "Trained batch 1533 batch loss 0.675679207 epoch total loss 0.674619615\n",
      "Trained batch 1534 batch loss 0.66296488 epoch total loss 0.674612045\n",
      "Trained batch 1535 batch loss 0.751573682 epoch total loss 0.674662173\n",
      "Trained batch 1536 batch loss 0.761850595 epoch total loss 0.674718916\n",
      "Trained batch 1537 batch loss 0.6241768 epoch total loss 0.674686\n",
      "Trained batch 1538 batch loss 0.650575 epoch total loss 0.674670398\n",
      "Trained batch 1539 batch loss 0.66270715 epoch total loss 0.67466265\n",
      "Trained batch 1540 batch loss 0.690656424 epoch total loss 0.674673\n",
      "Trained batch 1541 batch loss 0.691364348 epoch total loss 0.674683869\n",
      "Trained batch 1542 batch loss 0.635140538 epoch total loss 0.674658239\n",
      "Trained batch 1543 batch loss 0.647414565 epoch total loss 0.674640596\n",
      "Trained batch 1544 batch loss 0.646301746 epoch total loss 0.674622297\n",
      "Trained batch 1545 batch loss 0.581914246 epoch total loss 0.674562275\n",
      "Trained batch 1546 batch loss 0.660532415 epoch total loss 0.674553216\n",
      "Trained batch 1547 batch loss 0.674695313 epoch total loss 0.674553275\n",
      "Trained batch 1548 batch loss 0.655481339 epoch total loss 0.674541\n",
      "Trained batch 1549 batch loss 0.748539 epoch total loss 0.67458874\n",
      "Trained batch 1550 batch loss 0.727050841 epoch total loss 0.674622595\n",
      "Trained batch 1551 batch loss 0.771535277 epoch total loss 0.674685061\n",
      "Trained batch 1552 batch loss 0.738675058 epoch total loss 0.674726248\n",
      "Trained batch 1553 batch loss 0.716583073 epoch total loss 0.674753189\n",
      "Trained batch 1554 batch loss 0.624771893 epoch total loss 0.674721\n",
      "Trained batch 1555 batch loss 0.636460245 epoch total loss 0.674696445\n",
      "Trained batch 1556 batch loss 0.579619408 epoch total loss 0.674635291\n",
      "Trained batch 1557 batch loss 0.644041121 epoch total loss 0.674615681\n",
      "Trained batch 1558 batch loss 0.573806047 epoch total loss 0.674551\n",
      "Trained batch 1559 batch loss 0.616729677 epoch total loss 0.674513876\n",
      "Trained batch 1560 batch loss 0.644642293 epoch total loss 0.674494743\n",
      "Trained batch 1561 batch loss 0.727765 epoch total loss 0.674528897\n",
      "Trained batch 1562 batch loss 0.777277172 epoch total loss 0.674594641\n",
      "Trained batch 1563 batch loss 0.629463911 epoch total loss 0.674565792\n",
      "Trained batch 1564 batch loss 0.649052322 epoch total loss 0.67454946\n",
      "Trained batch 1565 batch loss 0.701920867 epoch total loss 0.674566925\n",
      "Trained batch 1566 batch loss 0.586791337 epoch total loss 0.674510896\n",
      "Trained batch 1567 batch loss 0.676273108 epoch total loss 0.674512\n",
      "Trained batch 1568 batch loss 0.646909 epoch total loss 0.674494386\n",
      "Trained batch 1569 batch loss 0.607303917 epoch total loss 0.67445153\n",
      "Trained batch 1570 batch loss 0.607464135 epoch total loss 0.674408853\n",
      "Trained batch 1571 batch loss 0.62820667 epoch total loss 0.674379408\n",
      "Trained batch 1572 batch loss 0.595694065 epoch total loss 0.6743294\n",
      "Trained batch 1573 batch loss 0.672599912 epoch total loss 0.674328268\n",
      "Trained batch 1574 batch loss 0.696751535 epoch total loss 0.674342513\n",
      "Trained batch 1575 batch loss 0.598560274 epoch total loss 0.674294412\n",
      "Trained batch 1576 batch loss 0.638197482 epoch total loss 0.674271464\n",
      "Trained batch 1577 batch loss 0.539275408 epoch total loss 0.674185872\n",
      "Trained batch 1578 batch loss 0.640092254 epoch total loss 0.674164295\n",
      "Trained batch 1579 batch loss 0.652703106 epoch total loss 0.674150705\n",
      "Trained batch 1580 batch loss 0.629411817 epoch total loss 0.674122393\n",
      "Trained batch 1581 batch loss 0.63584286 epoch total loss 0.674098194\n",
      "Trained batch 1582 batch loss 0.602360547 epoch total loss 0.674052894\n",
      "Trained batch 1583 batch loss 0.570548475 epoch total loss 0.673987508\n",
      "Trained batch 1584 batch loss 0.599026144 epoch total loss 0.673940182\n",
      "Trained batch 1585 batch loss 0.539781392 epoch total loss 0.673855543\n",
      "Trained batch 1586 batch loss 0.604587913 epoch total loss 0.673811913\n",
      "Trained batch 1587 batch loss 0.573322117 epoch total loss 0.673748612\n",
      "Trained batch 1588 batch loss 0.585718334 epoch total loss 0.67369312\n",
      "Trained batch 1589 batch loss 0.640661061 epoch total loss 0.673672318\n",
      "Trained batch 1590 batch loss 0.643431604 epoch total loss 0.673653305\n",
      "Trained batch 1591 batch loss 0.716665447 epoch total loss 0.673680365\n",
      "Trained batch 1592 batch loss 0.644802928 epoch total loss 0.673662186\n",
      "Trained batch 1593 batch loss 0.608415127 epoch total loss 0.673621237\n",
      "Trained batch 1594 batch loss 0.662013412 epoch total loss 0.673613906\n",
      "Trained batch 1595 batch loss 0.630823851 epoch total loss 0.673587143\n",
      "Trained batch 1596 batch loss 0.695775807 epoch total loss 0.673601031\n",
      "Trained batch 1597 batch loss 0.697358727 epoch total loss 0.673615932\n",
      "Trained batch 1598 batch loss 0.685955167 epoch total loss 0.673623621\n",
      "Trained batch 1599 batch loss 0.69704628 epoch total loss 0.673638284\n",
      "Trained batch 1600 batch loss 0.666230142 epoch total loss 0.673633635\n",
      "Trained batch 1601 batch loss 0.704390168 epoch total loss 0.673652828\n",
      "Trained batch 1602 batch loss 0.677575707 epoch total loss 0.673655331\n",
      "Trained batch 1603 batch loss 0.667922676 epoch total loss 0.673651755\n",
      "Trained batch 1604 batch loss 0.668890059 epoch total loss 0.673648834\n",
      "Trained batch 1605 batch loss 0.629596531 epoch total loss 0.673621416\n",
      "Trained batch 1606 batch loss 0.68151176 epoch total loss 0.673626304\n",
      "Trained batch 1607 batch loss 0.667081 epoch total loss 0.673622251\n",
      "Trained batch 1608 batch loss 0.684462607 epoch total loss 0.673629\n",
      "Trained batch 1609 batch loss 0.662083 epoch total loss 0.673621833\n",
      "Trained batch 1610 batch loss 0.67025882 epoch total loss 0.673619747\n",
      "Trained batch 1611 batch loss 0.706610501 epoch total loss 0.673640311\n",
      "Trained batch 1612 batch loss 0.73218751 epoch total loss 0.67367661\n",
      "Trained batch 1613 batch loss 0.652589858 epoch total loss 0.673663497\n",
      "Trained batch 1614 batch loss 0.611706 epoch total loss 0.673625112\n",
      "Trained batch 1615 batch loss 0.630612671 epoch total loss 0.673598468\n",
      "Trained batch 1616 batch loss 0.575243413 epoch total loss 0.673537612\n",
      "Trained batch 1617 batch loss 0.537102222 epoch total loss 0.673453212\n",
      "Trained batch 1618 batch loss 0.562087059 epoch total loss 0.673384428\n",
      "Trained batch 1619 batch loss 0.602262259 epoch total loss 0.673340499\n",
      "Trained batch 1620 batch loss 0.669657707 epoch total loss 0.673338234\n",
      "Trained batch 1621 batch loss 0.744104505 epoch total loss 0.673381925\n",
      "Trained batch 1622 batch loss 0.720315218 epoch total loss 0.673410892\n",
      "Trained batch 1623 batch loss 0.711807966 epoch total loss 0.673434556\n",
      "Trained batch 1624 batch loss 0.706227958 epoch total loss 0.673454702\n",
      "Trained batch 1625 batch loss 0.738887191 epoch total loss 0.673495\n",
      "Trained batch 1626 batch loss 0.687099695 epoch total loss 0.673503339\n",
      "Trained batch 1627 batch loss 0.703177214 epoch total loss 0.673521578\n",
      "Trained batch 1628 batch loss 0.717120767 epoch total loss 0.673548341\n",
      "Trained batch 1629 batch loss 0.734224439 epoch total loss 0.673585653\n",
      "Trained batch 1630 batch loss 0.667937696 epoch total loss 0.673582196\n",
      "Trained batch 1631 batch loss 0.648497224 epoch total loss 0.673566759\n",
      "Trained batch 1632 batch loss 0.710647225 epoch total loss 0.673589528\n",
      "Trained batch 1633 batch loss 0.682286739 epoch total loss 0.673594832\n",
      "Trained batch 1634 batch loss 0.758549392 epoch total loss 0.673646808\n",
      "Trained batch 1635 batch loss 0.598556757 epoch total loss 0.673600852\n",
      "Trained batch 1636 batch loss 0.682504892 epoch total loss 0.673606277\n",
      "Trained batch 1637 batch loss 0.674381 epoch total loss 0.673606813\n",
      "Trained batch 1638 batch loss 0.660743356 epoch total loss 0.673598945\n",
      "Trained batch 1639 batch loss 0.691513956 epoch total loss 0.673609912\n",
      "Trained batch 1640 batch loss 0.658005357 epoch total loss 0.673600376\n",
      "Trained batch 1641 batch loss 0.699349225 epoch total loss 0.673616052\n",
      "Trained batch 1642 batch loss 0.797688484 epoch total loss 0.67369163\n",
      "Trained batch 1643 batch loss 0.711209178 epoch total loss 0.673714459\n",
      "Trained batch 1644 batch loss 0.733958662 epoch total loss 0.673751116\n",
      "Trained batch 1645 batch loss 0.650252342 epoch total loss 0.67373687\n",
      "Trained batch 1646 batch loss 0.639866889 epoch total loss 0.673716307\n",
      "Trained batch 1647 batch loss 0.704679668 epoch total loss 0.673735082\n",
      "Trained batch 1648 batch loss 0.630259752 epoch total loss 0.673708737\n",
      "Trained batch 1649 batch loss 0.609420121 epoch total loss 0.673669696\n",
      "Trained batch 1650 batch loss 0.677645624 epoch total loss 0.67367208\n",
      "Trained batch 1651 batch loss 0.617029309 epoch total loss 0.673637807\n",
      "Trained batch 1652 batch loss 0.634961367 epoch total loss 0.673614442\n",
      "Trained batch 1653 batch loss 0.674205542 epoch total loss 0.6736148\n",
      "Trained batch 1654 batch loss 0.703691721 epoch total loss 0.673633\n",
      "Trained batch 1655 batch loss 0.657033145 epoch total loss 0.673622906\n",
      "Trained batch 1656 batch loss 0.654988945 epoch total loss 0.673611701\n",
      "Trained batch 1657 batch loss 0.601862371 epoch total loss 0.673568368\n",
      "Trained batch 1658 batch loss 0.67409128 epoch total loss 0.673568666\n",
      "Trained batch 1659 batch loss 0.633872807 epoch total loss 0.673544765\n",
      "Trained batch 1660 batch loss 0.640930355 epoch total loss 0.673525155\n",
      "Trained batch 1661 batch loss 0.645493031 epoch total loss 0.673508286\n",
      "Trained batch 1662 batch loss 0.590927 epoch total loss 0.673458636\n",
      "Trained batch 1663 batch loss 0.641562104 epoch total loss 0.673439443\n",
      "Trained batch 1664 batch loss 0.629234672 epoch total loss 0.673412919\n",
      "Trained batch 1665 batch loss 0.6703583 epoch total loss 0.673411131\n",
      "Trained batch 1666 batch loss 0.678936958 epoch total loss 0.673414409\n",
      "Trained batch 1667 batch loss 0.663361251 epoch total loss 0.673408389\n",
      "Trained batch 1668 batch loss 0.712782264 epoch total loss 0.673432\n",
      "Trained batch 1669 batch loss 0.71053648 epoch total loss 0.673454225\n",
      "Trained batch 1670 batch loss 0.647199094 epoch total loss 0.673438549\n",
      "Trained batch 1671 batch loss 0.6759395 epoch total loss 0.67344\n",
      "Trained batch 1672 batch loss 0.614102602 epoch total loss 0.673404515\n",
      "Trained batch 1673 batch loss 0.623696566 epoch total loss 0.673374772\n",
      "Trained batch 1674 batch loss 0.611839294 epoch total loss 0.673338\n",
      "Trained batch 1675 batch loss 0.596748829 epoch total loss 0.673292339\n",
      "Trained batch 1676 batch loss 0.651892483 epoch total loss 0.673279524\n",
      "Trained batch 1677 batch loss 0.690837801 epoch total loss 0.673289955\n",
      "Trained batch 1678 batch loss 0.707554638 epoch total loss 0.673310399\n",
      "Trained batch 1679 batch loss 0.702594817 epoch total loss 0.673327863\n",
      "Trained batch 1680 batch loss 0.723516464 epoch total loss 0.673357725\n",
      "Trained batch 1681 batch loss 0.648848176 epoch total loss 0.673343122\n",
      "Trained batch 1682 batch loss 0.713873 epoch total loss 0.673367202\n",
      "Trained batch 1683 batch loss 0.718723834 epoch total loss 0.673394144\n",
      "Trained batch 1684 batch loss 0.718587518 epoch total loss 0.673421\n",
      "Trained batch 1685 batch loss 0.735160887 epoch total loss 0.673457623\n",
      "Trained batch 1686 batch loss 0.662333965 epoch total loss 0.673451066\n",
      "Trained batch 1687 batch loss 0.697004855 epoch total loss 0.673465\n",
      "Trained batch 1688 batch loss 0.75093925 epoch total loss 0.673510969\n",
      "Trained batch 1689 batch loss 0.620629251 epoch total loss 0.673479617\n",
      "Trained batch 1690 batch loss 0.65484941 epoch total loss 0.673468649\n",
      "Trained batch 1691 batch loss 0.629527211 epoch total loss 0.673442662\n",
      "Trained batch 1692 batch loss 0.644608498 epoch total loss 0.673425615\n",
      "Trained batch 1693 batch loss 0.645406246 epoch total loss 0.673409045\n",
      "Trained batch 1694 batch loss 0.561346591 epoch total loss 0.673342943\n",
      "Trained batch 1695 batch loss 0.700823069 epoch total loss 0.673359156\n",
      "Trained batch 1696 batch loss 0.776369095 epoch total loss 0.673419893\n",
      "Trained batch 1697 batch loss 0.756998599 epoch total loss 0.673469126\n",
      "Trained batch 1698 batch loss 0.832686543 epoch total loss 0.673562825\n",
      "Trained batch 1699 batch loss 0.746568859 epoch total loss 0.6736058\n",
      "Trained batch 1700 batch loss 0.717961669 epoch total loss 0.673631966\n",
      "Trained batch 1701 batch loss 0.718165576 epoch total loss 0.673658133\n",
      "Trained batch 1702 batch loss 0.665445328 epoch total loss 0.673653245\n",
      "Trained batch 1703 batch loss 0.716288626 epoch total loss 0.673678339\n",
      "Trained batch 1704 batch loss 0.653251827 epoch total loss 0.673666298\n",
      "Trained batch 1705 batch loss 0.600490332 epoch total loss 0.673623383\n",
      "Trained batch 1706 batch loss 0.695726216 epoch total loss 0.673636317\n",
      "Trained batch 1707 batch loss 0.662828088 epoch total loss 0.67362994\n",
      "Trained batch 1708 batch loss 0.52166605 epoch total loss 0.67354095\n",
      "Trained batch 1709 batch loss 0.590297222 epoch total loss 0.673492253\n",
      "Trained batch 1710 batch loss 0.669313848 epoch total loss 0.673489809\n",
      "Trained batch 1711 batch loss 0.745488167 epoch total loss 0.67353189\n",
      "Trained batch 1712 batch loss 0.629044652 epoch total loss 0.673505902\n",
      "Trained batch 1713 batch loss 0.717673242 epoch total loss 0.673531651\n",
      "Trained batch 1714 batch loss 0.603473902 epoch total loss 0.673490822\n",
      "Trained batch 1715 batch loss 0.598647058 epoch total loss 0.673447192\n",
      "Trained batch 1716 batch loss 0.61505872 epoch total loss 0.673413157\n",
      "Trained batch 1717 batch loss 0.526888251 epoch total loss 0.673327804\n",
      "Trained batch 1718 batch loss 0.575336099 epoch total loss 0.673270762\n",
      "Trained batch 1719 batch loss 0.61518532 epoch total loss 0.673237\n",
      "Trained batch 1720 batch loss 0.563837886 epoch total loss 0.673173428\n",
      "Trained batch 1721 batch loss 0.539218 epoch total loss 0.673095584\n",
      "Trained batch 1722 batch loss 0.665522754 epoch total loss 0.673091173\n",
      "Trained batch 1723 batch loss 0.749689519 epoch total loss 0.673135579\n",
      "Trained batch 1724 batch loss 0.724502563 epoch total loss 0.673165381\n",
      "Trained batch 1725 batch loss 0.837184489 epoch total loss 0.67326045\n",
      "Trained batch 1726 batch loss 0.791837454 epoch total loss 0.673329175\n",
      "Trained batch 1727 batch loss 0.807313442 epoch total loss 0.67340678\n",
      "Trained batch 1728 batch loss 0.84943068 epoch total loss 0.673508704\n",
      "Trained batch 1729 batch loss 0.712389767 epoch total loss 0.673531175\n",
      "Trained batch 1730 batch loss 0.655658662 epoch total loss 0.673520803\n",
      "Trained batch 1731 batch loss 0.732042789 epoch total loss 0.673554659\n",
      "Trained batch 1732 batch loss 0.759812951 epoch total loss 0.673604429\n",
      "Trained batch 1733 batch loss 0.76581943 epoch total loss 0.673657656\n",
      "Trained batch 1734 batch loss 0.793333054 epoch total loss 0.673726678\n",
      "Trained batch 1735 batch loss 0.716755033 epoch total loss 0.673751533\n",
      "Trained batch 1736 batch loss 0.707797229 epoch total loss 0.673771083\n",
      "Trained batch 1737 batch loss 0.649061739 epoch total loss 0.673756838\n",
      "Trained batch 1738 batch loss 0.608034611 epoch total loss 0.673719049\n",
      "Trained batch 1739 batch loss 0.609224498 epoch total loss 0.673682\n",
      "Trained batch 1740 batch loss 0.605008602 epoch total loss 0.673642516\n",
      "Trained batch 1741 batch loss 0.647675216 epoch total loss 0.673627615\n",
      "Trained batch 1742 batch loss 0.647259116 epoch total loss 0.673612416\n",
      "Trained batch 1743 batch loss 0.662510514 epoch total loss 0.673606038\n",
      "Trained batch 1744 batch loss 0.658058524 epoch total loss 0.673597157\n",
      "Trained batch 1745 batch loss 0.73593843 epoch total loss 0.67363286\n",
      "Trained batch 1746 batch loss 0.634820819 epoch total loss 0.673610628\n",
      "Trained batch 1747 batch loss 0.615828335 epoch total loss 0.673577547\n",
      "Trained batch 1748 batch loss 0.762312949 epoch total loss 0.67362833\n",
      "Trained batch 1749 batch loss 0.717636406 epoch total loss 0.673653483\n",
      "Trained batch 1750 batch loss 0.662917316 epoch total loss 0.673647404\n",
      "Trained batch 1751 batch loss 0.740020096 epoch total loss 0.673685253\n",
      "Trained batch 1752 batch loss 0.727130353 epoch total loss 0.67371583\n",
      "Trained batch 1753 batch loss 0.62098968 epoch total loss 0.67368573\n",
      "Trained batch 1754 batch loss 0.637790918 epoch total loss 0.673665285\n",
      "Trained batch 1755 batch loss 0.743019819 epoch total loss 0.673704803\n",
      "Trained batch 1756 batch loss 0.681229174 epoch total loss 0.673709095\n",
      "Trained batch 1757 batch loss 0.698481321 epoch total loss 0.673723221\n",
      "Trained batch 1758 batch loss 0.663439035 epoch total loss 0.67371738\n",
      "Trained batch 1759 batch loss 0.671361864 epoch total loss 0.673716068\n",
      "Trained batch 1760 batch loss 0.62867403 epoch total loss 0.673690438\n",
      "Trained batch 1761 batch loss 0.693238 epoch total loss 0.673701525\n",
      "Trained batch 1762 batch loss 0.710025609 epoch total loss 0.673722208\n",
      "Trained batch 1763 batch loss 0.715733886 epoch total loss 0.673746\n",
      "Trained batch 1764 batch loss 0.699762583 epoch total loss 0.673760712\n",
      "Trained batch 1765 batch loss 0.706246793 epoch total loss 0.67377913\n",
      "Trained batch 1766 batch loss 0.705016255 epoch total loss 0.673796833\n",
      "Trained batch 1767 batch loss 0.670646548 epoch total loss 0.673795044\n",
      "Trained batch 1768 batch loss 0.653770328 epoch total loss 0.67378372\n",
      "Trained batch 1769 batch loss 0.638834894 epoch total loss 0.673763931\n",
      "Trained batch 1770 batch loss 0.675397098 epoch total loss 0.673764884\n",
      "Trained batch 1771 batch loss 0.790214837 epoch total loss 0.673830628\n",
      "Trained batch 1772 batch loss 0.742564559 epoch total loss 0.673869371\n",
      "Trained batch 1773 batch loss 0.812502623 epoch total loss 0.673947573\n",
      "Trained batch 1774 batch loss 0.762023 epoch total loss 0.673997164\n",
      "Trained batch 1775 batch loss 0.667162061 epoch total loss 0.673993289\n",
      "Trained batch 1776 batch loss 0.638638914 epoch total loss 0.673973441\n",
      "Trained batch 1777 batch loss 0.656544805 epoch total loss 0.673963606\n",
      "Trained batch 1778 batch loss 0.658029437 epoch total loss 0.673954666\n",
      "Trained batch 1779 batch loss 0.600003362 epoch total loss 0.673913062\n",
      "Trained batch 1780 batch loss 0.696910143 epoch total loss 0.673926\n",
      "Trained batch 1781 batch loss 0.729515553 epoch total loss 0.673957169\n",
      "Trained batch 1782 batch loss 0.672026753 epoch total loss 0.673956096\n",
      "Trained batch 1783 batch loss 0.572740436 epoch total loss 0.673899353\n",
      "Trained batch 1784 batch loss 0.565229058 epoch total loss 0.673838377\n",
      "Trained batch 1785 batch loss 0.532415628 epoch total loss 0.673759162\n",
      "Trained batch 1786 batch loss 0.526601 epoch total loss 0.673676789\n",
      "Trained batch 1787 batch loss 0.584661603 epoch total loss 0.673627\n",
      "Trained batch 1788 batch loss 0.586596131 epoch total loss 0.673578322\n",
      "Trained batch 1789 batch loss 0.562578 epoch total loss 0.673516273\n",
      "Trained batch 1790 batch loss 0.634487927 epoch total loss 0.673494518\n",
      "Trained batch 1791 batch loss 0.584228158 epoch total loss 0.673444688\n",
      "Trained batch 1792 batch loss 0.659403384 epoch total loss 0.673436821\n",
      "Trained batch 1793 batch loss 0.602905452 epoch total loss 0.673397481\n",
      "Trained batch 1794 batch loss 0.656142831 epoch total loss 0.673387885\n",
      "Trained batch 1795 batch loss 0.625214398 epoch total loss 0.673361063\n",
      "Trained batch 1796 batch loss 0.648628354 epoch total loss 0.673347294\n",
      "Trained batch 1797 batch loss 0.600756526 epoch total loss 0.673306882\n",
      "Trained batch 1798 batch loss 0.614847302 epoch total loss 0.673274398\n",
      "Trained batch 1799 batch loss 0.710864067 epoch total loss 0.673295259\n",
      "Trained batch 1800 batch loss 0.71252811 epoch total loss 0.673317075\n",
      "Trained batch 1801 batch loss 0.677076221 epoch total loss 0.673319161\n",
      "Trained batch 1802 batch loss 0.744211555 epoch total loss 0.67335856\n",
      "Trained batch 1803 batch loss 0.704518735 epoch total loss 0.673375785\n",
      "Trained batch 1804 batch loss 0.709096551 epoch total loss 0.673395574\n",
      "Trained batch 1805 batch loss 0.638271749 epoch total loss 0.673376143\n",
      "Trained batch 1806 batch loss 0.588901937 epoch total loss 0.673329353\n",
      "Trained batch 1807 batch loss 0.544942558 epoch total loss 0.673258305\n",
      "Trained batch 1808 batch loss 0.614586055 epoch total loss 0.67322588\n",
      "Trained batch 1809 batch loss 0.633860946 epoch total loss 0.673204124\n",
      "Trained batch 1810 batch loss 0.615277946 epoch total loss 0.673172116\n",
      "Trained batch 1811 batch loss 0.712279677 epoch total loss 0.673193693\n",
      "Trained batch 1812 batch loss 0.684196353 epoch total loss 0.673199773\n",
      "Trained batch 1813 batch loss 0.710140169 epoch total loss 0.673220158\n",
      "Trained batch 1814 batch loss 0.726361871 epoch total loss 0.673249424\n",
      "Trained batch 1815 batch loss 0.743058741 epoch total loss 0.673287868\n",
      "Trained batch 1816 batch loss 0.6959095 epoch total loss 0.673300326\n",
      "Trained batch 1817 batch loss 0.661422491 epoch total loss 0.673293769\n",
      "Trained batch 1818 batch loss 0.680027664 epoch total loss 0.673297465\n",
      "Trained batch 1819 batch loss 0.645424604 epoch total loss 0.673282146\n",
      "Trained batch 1820 batch loss 0.574187517 epoch total loss 0.673227727\n",
      "Trained batch 1821 batch loss 0.585538685 epoch total loss 0.673179567\n",
      "Trained batch 1822 batch loss 0.617546797 epoch total loss 0.673149049\n",
      "Trained batch 1823 batch loss 0.639840305 epoch total loss 0.67313081\n",
      "Trained batch 1824 batch loss 0.639013886 epoch total loss 0.673112094\n",
      "Trained batch 1825 batch loss 0.587709904 epoch total loss 0.673065364\n",
      "Trained batch 1826 batch loss 0.579880774 epoch total loss 0.673014283\n",
      "Trained batch 1827 batch loss 0.587920606 epoch total loss 0.672967672\n",
      "Trained batch 1828 batch loss 0.643370926 epoch total loss 0.67295146\n",
      "Trained batch 1829 batch loss 0.607731402 epoch total loss 0.672915816\n",
      "Trained batch 1830 batch loss 0.612419963 epoch total loss 0.672882795\n",
      "Trained batch 1831 batch loss 0.626493633 epoch total loss 0.672857404\n",
      "Trained batch 1832 batch loss 0.594459534 epoch total loss 0.672814667\n",
      "Trained batch 1833 batch loss 0.5981 epoch total loss 0.672773898\n",
      "Trained batch 1834 batch loss 0.692021251 epoch total loss 0.672784388\n",
      "Trained batch 1835 batch loss 0.629714727 epoch total loss 0.672760963\n",
      "Trained batch 1836 batch loss 0.764038622 epoch total loss 0.672810674\n",
      "Trained batch 1837 batch loss 0.740170181 epoch total loss 0.672847331\n",
      "Trained batch 1838 batch loss 0.669550657 epoch total loss 0.672845542\n",
      "Trained batch 1839 batch loss 0.752919316 epoch total loss 0.672889054\n",
      "Trained batch 1840 batch loss 0.660686195 epoch total loss 0.672882438\n",
      "Trained batch 1841 batch loss 0.672323048 epoch total loss 0.67288214\n",
      "Trained batch 1842 batch loss 0.668650866 epoch total loss 0.672879875\n",
      "Trained batch 1843 batch loss 0.619391263 epoch total loss 0.672850847\n",
      "Trained batch 1844 batch loss 0.665385365 epoch total loss 0.672846794\n",
      "Trained batch 1845 batch loss 0.635877669 epoch total loss 0.672826767\n",
      "Trained batch 1846 batch loss 0.560987 epoch total loss 0.672766209\n",
      "Trained batch 1847 batch loss 0.558907628 epoch total loss 0.672704577\n",
      "Trained batch 1848 batch loss 0.610788524 epoch total loss 0.67267108\n",
      "Trained batch 1849 batch loss 0.597743094 epoch total loss 0.672630608\n",
      "Trained batch 1850 batch loss 0.615453541 epoch total loss 0.672599673\n",
      "Trained batch 1851 batch loss 0.668825567 epoch total loss 0.672597647\n",
      "Trained batch 1852 batch loss 0.697010219 epoch total loss 0.672610879\n",
      "Trained batch 1853 batch loss 0.751269102 epoch total loss 0.672653258\n",
      "Trained batch 1854 batch loss 0.743528962 epoch total loss 0.672691524\n",
      "Trained batch 1855 batch loss 0.665212393 epoch total loss 0.672687471\n",
      "Trained batch 1856 batch loss 0.608070374 epoch total loss 0.672652602\n",
      "Trained batch 1857 batch loss 0.652145386 epoch total loss 0.672641516\n",
      "Trained batch 1858 batch loss 0.622417092 epoch total loss 0.672614515\n",
      "Trained batch 1859 batch loss 0.622733474 epoch total loss 0.672587633\n",
      "Trained batch 1860 batch loss 0.634152055 epoch total loss 0.672567\n",
      "Trained batch 1861 batch loss 0.630449 epoch total loss 0.67254436\n",
      "Trained batch 1862 batch loss 0.725027382 epoch total loss 0.672572553\n",
      "Trained batch 1863 batch loss 0.605127811 epoch total loss 0.672536314\n",
      "Trained batch 1864 batch loss 0.514717281 epoch total loss 0.672451675\n",
      "Trained batch 1865 batch loss 0.543435752 epoch total loss 0.672382534\n",
      "Trained batch 1866 batch loss 0.542735875 epoch total loss 0.672313035\n",
      "Trained batch 1867 batch loss 0.606097281 epoch total loss 0.67227757\n",
      "Trained batch 1868 batch loss 0.660095632 epoch total loss 0.672271073\n",
      "Trained batch 1869 batch loss 0.780119956 epoch total loss 0.67232877\n",
      "Trained batch 1870 batch loss 0.766646147 epoch total loss 0.672379196\n",
      "Trained batch 1871 batch loss 0.723638237 epoch total loss 0.672406614\n",
      "Trained batch 1872 batch loss 0.638013899 epoch total loss 0.672388256\n",
      "Trained batch 1873 batch loss 0.728965163 epoch total loss 0.672418475\n",
      "Trained batch 1874 batch loss 0.724098861 epoch total loss 0.672446072\n",
      "Trained batch 1875 batch loss 0.671684623 epoch total loss 0.672445655\n",
      "Trained batch 1876 batch loss 0.676163673 epoch total loss 0.672447622\n",
      "Trained batch 1877 batch loss 0.701361418 epoch total loss 0.672463059\n",
      "Trained batch 1878 batch loss 0.703775465 epoch total loss 0.672479689\n",
      "Trained batch 1879 batch loss 0.66126579 epoch total loss 0.672473729\n",
      "Trained batch 1880 batch loss 0.657640874 epoch total loss 0.672465801\n",
      "Trained batch 1881 batch loss 0.655365646 epoch total loss 0.672456741\n",
      "Trained batch 1882 batch loss 0.63583231 epoch total loss 0.67243731\n",
      "Trained batch 1883 batch loss 0.690415 epoch total loss 0.672446847\n",
      "Trained batch 1884 batch loss 0.708978653 epoch total loss 0.672466218\n",
      "Trained batch 1885 batch loss 0.609793425 epoch total loss 0.672432959\n",
      "Trained batch 1886 batch loss 0.637753367 epoch total loss 0.672414541\n",
      "Trained batch 1887 batch loss 0.608352661 epoch total loss 0.672380626\n",
      "Trained batch 1888 batch loss 0.711203337 epoch total loss 0.67240119\n",
      "Trained batch 1889 batch loss 0.63320595 epoch total loss 0.672380388\n",
      "Trained batch 1890 batch loss 0.635862947 epoch total loss 0.672361076\n",
      "Trained batch 1891 batch loss 0.601437569 epoch total loss 0.672323585\n",
      "Trained batch 1892 batch loss 0.670424759 epoch total loss 0.672322571\n",
      "Trained batch 1893 batch loss 0.588350594 epoch total loss 0.672278225\n",
      "Trained batch 1894 batch loss 0.654526114 epoch total loss 0.672268867\n",
      "Trained batch 1895 batch loss 0.604852557 epoch total loss 0.672233284\n",
      "Trained batch 1896 batch loss 0.583012402 epoch total loss 0.672186255\n",
      "Trained batch 1897 batch loss 0.517063 epoch total loss 0.672104478\n",
      "Trained batch 1898 batch loss 0.561085939 epoch total loss 0.672045946\n",
      "Trained batch 1899 batch loss 0.550805271 epoch total loss 0.67198211\n",
      "Trained batch 1900 batch loss 0.587452948 epoch total loss 0.671937585\n",
      "Trained batch 1901 batch loss 0.67436707 epoch total loss 0.671938837\n",
      "Trained batch 1902 batch loss 0.729251325 epoch total loss 0.671968937\n",
      "Trained batch 1903 batch loss 0.755640566 epoch total loss 0.672012925\n",
      "Trained batch 1904 batch loss 0.649658918 epoch total loss 0.672001183\n",
      "Trained batch 1905 batch loss 0.76071763 epoch total loss 0.672047734\n",
      "Trained batch 1906 batch loss 0.800451338 epoch total loss 0.672115088\n",
      "Trained batch 1907 batch loss 0.666384 epoch total loss 0.672112107\n",
      "Trained batch 1908 batch loss 0.749337196 epoch total loss 0.672152579\n",
      "Trained batch 1909 batch loss 0.644109309 epoch total loss 0.672137916\n",
      "Trained batch 1910 batch loss 0.701156437 epoch total loss 0.672153115\n",
      "Trained batch 1911 batch loss 0.681232095 epoch total loss 0.672157943\n",
      "Trained batch 1912 batch loss 0.664367557 epoch total loss 0.672153831\n",
      "Trained batch 1913 batch loss 0.658587933 epoch total loss 0.672146678\n",
      "Trained batch 1914 batch loss 0.674712777 epoch total loss 0.672148049\n",
      "Trained batch 1915 batch loss 0.620152354 epoch total loss 0.672120869\n",
      "Trained batch 1916 batch loss 0.584298 epoch total loss 0.672075033\n",
      "Trained batch 1917 batch loss 0.622635186 epoch total loss 0.672049284\n",
      "Trained batch 1918 batch loss 0.591801465 epoch total loss 0.672007442\n",
      "Trained batch 1919 batch loss 0.590352297 epoch total loss 0.671964884\n",
      "Trained batch 1920 batch loss 0.56802249 epoch total loss 0.671910703\n",
      "Trained batch 1921 batch loss 0.601800084 epoch total loss 0.671874225\n",
      "Trained batch 1922 batch loss 0.724509 epoch total loss 0.671901584\n",
      "Trained batch 1923 batch loss 0.721477389 epoch total loss 0.671927392\n",
      "Trained batch 1924 batch loss 0.691369474 epoch total loss 0.671937466\n",
      "Trained batch 1925 batch loss 0.667534232 epoch total loss 0.671935201\n",
      "Trained batch 1926 batch loss 0.74443835 epoch total loss 0.671972811\n",
      "Trained batch 1927 batch loss 0.722578466 epoch total loss 0.671999037\n",
      "Trained batch 1928 batch loss 0.730982542 epoch total loss 0.672029614\n",
      "Trained batch 1929 batch loss 0.777902961 epoch total loss 0.67208451\n",
      "Trained batch 1930 batch loss 0.727431953 epoch total loss 0.67211318\n",
      "Trained batch 1931 batch loss 0.72850281 epoch total loss 0.672142386\n",
      "Trained batch 1932 batch loss 0.664955318 epoch total loss 0.672138691\n",
      "Trained batch 1933 batch loss 0.664523482 epoch total loss 0.672134757\n",
      "Trained batch 1934 batch loss 0.694462657 epoch total loss 0.672146261\n",
      "Trained batch 1935 batch loss 0.628601313 epoch total loss 0.67212379\n",
      "Trained batch 1936 batch loss 0.589108825 epoch total loss 0.672080934\n",
      "Trained batch 1937 batch loss 0.595976114 epoch total loss 0.672041655\n",
      "Trained batch 1938 batch loss 0.61649549 epoch total loss 0.672012925\n",
      "Trained batch 1939 batch loss 0.705331802 epoch total loss 0.672030151\n",
      "Trained batch 1940 batch loss 0.596169412 epoch total loss 0.67199105\n",
      "Trained batch 1941 batch loss 0.702924967 epoch total loss 0.672006965\n",
      "Trained batch 1942 batch loss 0.683096886 epoch total loss 0.672012687\n",
      "Trained batch 1943 batch loss 0.643959761 epoch total loss 0.671998203\n",
      "Trained batch 1944 batch loss 0.692239106 epoch total loss 0.672008634\n",
      "Trained batch 1945 batch loss 0.669022202 epoch total loss 0.672007084\n",
      "Trained batch 1946 batch loss 0.677222848 epoch total loss 0.672009826\n",
      "Trained batch 1947 batch loss 0.653002441 epoch total loss 0.672\n",
      "Trained batch 1948 batch loss 0.638178766 epoch total loss 0.671982646\n",
      "Trained batch 1949 batch loss 0.623112857 epoch total loss 0.671957612\n",
      "Trained batch 1950 batch loss 0.687836051 epoch total loss 0.671965778\n",
      "Trained batch 1951 batch loss 0.665365934 epoch total loss 0.67196238\n",
      "Trained batch 1952 batch loss 0.628795624 epoch total loss 0.671940267\n",
      "Trained batch 1953 batch loss 0.636069059 epoch total loss 0.671921968\n",
      "Trained batch 1954 batch loss 0.615961313 epoch total loss 0.671893299\n",
      "Trained batch 1955 batch loss 0.636787713 epoch total loss 0.671875358\n",
      "Trained batch 1956 batch loss 0.725190938 epoch total loss 0.671902657\n",
      "Trained batch 1957 batch loss 0.703098297 epoch total loss 0.671918631\n",
      "Trained batch 1958 batch loss 0.594710529 epoch total loss 0.671879172\n",
      "Trained batch 1959 batch loss 0.735552 epoch total loss 0.671911716\n",
      "Trained batch 1960 batch loss 0.705297709 epoch total loss 0.671928763\n",
      "Trained batch 1961 batch loss 0.731344581 epoch total loss 0.671959043\n",
      "Trained batch 1962 batch loss 0.787740111 epoch total loss 0.672018051\n",
      "Trained batch 1963 batch loss 0.70565033 epoch total loss 0.672035217\n",
      "Trained batch 1964 batch loss 0.74747175 epoch total loss 0.672073603\n",
      "Trained batch 1965 batch loss 0.792735517 epoch total loss 0.672135\n",
      "Trained batch 1966 batch loss 0.732514918 epoch total loss 0.672165692\n",
      "Trained batch 1967 batch loss 0.733397722 epoch total loss 0.672196865\n",
      "Trained batch 1968 batch loss 0.597051501 epoch total loss 0.672158659\n",
      "Trained batch 1969 batch loss 0.609290183 epoch total loss 0.67212671\n",
      "Trained batch 1970 batch loss 0.596521854 epoch total loss 0.672088325\n",
      "Trained batch 1971 batch loss 0.636352956 epoch total loss 0.672070205\n",
      "Trained batch 1972 batch loss 0.625194967 epoch total loss 0.672046483\n",
      "Trained batch 1973 batch loss 0.693944275 epoch total loss 0.672057569\n",
      "Trained batch 1974 batch loss 0.644868 epoch total loss 0.6720438\n",
      "Trained batch 1975 batch loss 0.617156088 epoch total loss 0.672016\n",
      "Trained batch 1976 batch loss 0.555182278 epoch total loss 0.671956897\n",
      "Trained batch 1977 batch loss 0.602376342 epoch total loss 0.67192173\n",
      "Trained batch 1978 batch loss 0.583214104 epoch total loss 0.671876907\n",
      "Trained batch 1979 batch loss 0.596356153 epoch total loss 0.671838701\n",
      "Trained batch 1980 batch loss 0.600334227 epoch total loss 0.67180264\n",
      "Trained batch 1981 batch loss 0.645968497 epoch total loss 0.671789587\n",
      "Trained batch 1982 batch loss 0.620700359 epoch total loss 0.671763837\n",
      "Trained batch 1983 batch loss 0.580218315 epoch total loss 0.671717644\n",
      "Trained batch 1984 batch loss 0.535684586 epoch total loss 0.671649098\n",
      "Trained batch 1985 batch loss 0.5892061 epoch total loss 0.671607554\n",
      "Trained batch 1986 batch loss 0.535273373 epoch total loss 0.671538889\n",
      "Trained batch 1987 batch loss 0.521749318 epoch total loss 0.67146349\n",
      "Trained batch 1988 batch loss 0.728942931 epoch total loss 0.671492457\n",
      "Trained batch 1989 batch loss 0.660859704 epoch total loss 0.671487153\n",
      "Trained batch 1990 batch loss 0.605334401 epoch total loss 0.671453893\n",
      "Trained batch 1991 batch loss 0.583867073 epoch total loss 0.671409905\n",
      "Trained batch 1992 batch loss 0.550875545 epoch total loss 0.671349406\n",
      "Trained batch 1993 batch loss 0.570440531 epoch total loss 0.671298742\n",
      "Trained batch 1994 batch loss 0.591385305 epoch total loss 0.671258688\n",
      "Trained batch 1995 batch loss 0.59481883 epoch total loss 0.671220422\n",
      "Trained batch 1996 batch loss 0.600016296 epoch total loss 0.671184719\n",
      "Trained batch 1997 batch loss 0.654425859 epoch total loss 0.671176314\n",
      "Trained batch 1998 batch loss 0.587726235 epoch total loss 0.671134591\n",
      "Trained batch 1999 batch loss 0.574594438 epoch total loss 0.671086252\n",
      "Trained batch 2000 batch loss 0.665056825 epoch total loss 0.671083272\n",
      "Trained batch 2001 batch loss 0.705728173 epoch total loss 0.671100557\n",
      "Trained batch 2002 batch loss 0.668872476 epoch total loss 0.671099424\n",
      "Trained batch 2003 batch loss 0.681669056 epoch total loss 0.67110467\n",
      "Trained batch 2004 batch loss 0.652735472 epoch total loss 0.67109549\n",
      "Trained batch 2005 batch loss 0.696136475 epoch total loss 0.671108\n",
      "Trained batch 2006 batch loss 0.685288131 epoch total loss 0.6711151\n",
      "Trained batch 2007 batch loss 0.645075619 epoch total loss 0.671102047\n",
      "Trained batch 2008 batch loss 0.612248302 epoch total loss 0.671072781\n",
      "Trained batch 2009 batch loss 0.628271461 epoch total loss 0.671051502\n",
      "Trained batch 2010 batch loss 0.684712052 epoch total loss 0.671058297\n",
      "Trained batch 2011 batch loss 0.618404806 epoch total loss 0.671032131\n",
      "Trained batch 2012 batch loss 0.610747576 epoch total loss 0.67100215\n",
      "Trained batch 2013 batch loss 0.646846175 epoch total loss 0.670990109\n",
      "Trained batch 2014 batch loss 0.689943433 epoch total loss 0.670999527\n",
      "Trained batch 2015 batch loss 0.690391839 epoch total loss 0.671009183\n",
      "Trained batch 2016 batch loss 0.619831145 epoch total loss 0.670983791\n",
      "Trained batch 2017 batch loss 0.605321586 epoch total loss 0.670951247\n",
      "Trained batch 2018 batch loss 0.579848111 epoch total loss 0.670906126\n",
      "Trained batch 2019 batch loss 0.616247058 epoch total loss 0.670879\n",
      "Trained batch 2020 batch loss 0.636553109 epoch total loss 0.670862079\n",
      "Trained batch 2021 batch loss 0.602828264 epoch total loss 0.670828402\n",
      "Trained batch 2022 batch loss 0.646180034 epoch total loss 0.670816183\n",
      "Trained batch 2023 batch loss 0.651850164 epoch total loss 0.670806825\n",
      "Trained batch 2024 batch loss 0.722978175 epoch total loss 0.670832634\n",
      "Trained batch 2025 batch loss 0.671710849 epoch total loss 0.670833111\n",
      "Trained batch 2026 batch loss 0.751760721 epoch total loss 0.670873\n",
      "Trained batch 2027 batch loss 0.650939465 epoch total loss 0.670863152\n",
      "Trained batch 2028 batch loss 0.688315511 epoch total loss 0.670871794\n",
      "Trained batch 2029 batch loss 0.616521 epoch total loss 0.670845032\n",
      "Trained batch 2030 batch loss 0.581175447 epoch total loss 0.670800865\n",
      "Trained batch 2031 batch loss 0.588566303 epoch total loss 0.670760393\n",
      "Trained batch 2032 batch loss 0.59476167 epoch total loss 0.670722961\n",
      "Trained batch 2033 batch loss 0.54902184 epoch total loss 0.670663118\n",
      "Trained batch 2034 batch loss 0.553378224 epoch total loss 0.670605421\n",
      "Trained batch 2035 batch loss 0.582047 epoch total loss 0.67056191\n",
      "Trained batch 2036 batch loss 0.529999852 epoch total loss 0.670492887\n",
      "Trained batch 2037 batch loss 0.596223 epoch total loss 0.670456409\n",
      "Trained batch 2038 batch loss 0.60599947 epoch total loss 0.670424759\n",
      "Trained batch 2039 batch loss 0.66775924 epoch total loss 0.670423448\n",
      "Trained batch 2040 batch loss 0.696191907 epoch total loss 0.670436084\n",
      "Trained batch 2041 batch loss 0.664438367 epoch total loss 0.670433104\n",
      "Trained batch 2042 batch loss 0.660512328 epoch total loss 0.670428276\n",
      "Trained batch 2043 batch loss 0.6997298 epoch total loss 0.670442581\n",
      "Trained batch 2044 batch loss 0.613570273 epoch total loss 0.670414746\n",
      "Trained batch 2045 batch loss 0.537462354 epoch total loss 0.670349777\n",
      "Trained batch 2046 batch loss 0.511669338 epoch total loss 0.670272231\n",
      "Trained batch 2047 batch loss 0.582952797 epoch total loss 0.670229614\n",
      "Trained batch 2048 batch loss 0.579287469 epoch total loss 0.670185208\n",
      "Trained batch 2049 batch loss 0.624163032 epoch total loss 0.670162737\n",
      "Trained batch 2050 batch loss 0.598096669 epoch total loss 0.67012763\n",
      "Trained batch 2051 batch loss 0.61630857 epoch total loss 0.670101404\n",
      "Trained batch 2052 batch loss 0.616272032 epoch total loss 0.670075178\n",
      "Trained batch 2053 batch loss 0.639663219 epoch total loss 0.670060337\n",
      "Trained batch 2054 batch loss 0.610382617 epoch total loss 0.670031309\n",
      "Trained batch 2055 batch loss 0.62492764 epoch total loss 0.670009315\n",
      "Trained batch 2056 batch loss 0.76557076 epoch total loss 0.670055807\n",
      "Trained batch 2057 batch loss 0.715169668 epoch total loss 0.670077801\n",
      "Trained batch 2058 batch loss 0.571152627 epoch total loss 0.6700297\n",
      "Trained batch 2059 batch loss 0.634979904 epoch total loss 0.670012712\n",
      "Trained batch 2060 batch loss 0.654283345 epoch total loss 0.670005083\n",
      "Trained batch 2061 batch loss 0.660066366 epoch total loss 0.670000255\n",
      "Trained batch 2062 batch loss 0.664710164 epoch total loss 0.669997633\n",
      "Trained batch 2063 batch loss 0.721754134 epoch total loss 0.670022786\n",
      "Trained batch 2064 batch loss 0.649543166 epoch total loss 0.670012832\n",
      "Trained batch 2065 batch loss 0.719572425 epoch total loss 0.670036852\n",
      "Trained batch 2066 batch loss 0.736915946 epoch total loss 0.670069218\n",
      "Trained batch 2067 batch loss 0.804225087 epoch total loss 0.670134127\n",
      "Trained batch 2068 batch loss 0.762337685 epoch total loss 0.670178711\n",
      "Trained batch 2069 batch loss 0.791024864 epoch total loss 0.670237124\n",
      "Trained batch 2070 batch loss 0.759226143 epoch total loss 0.670280099\n",
      "Trained batch 2071 batch loss 0.673875272 epoch total loss 0.670281827\n",
      "Trained batch 2072 batch loss 0.657809317 epoch total loss 0.670275807\n",
      "Trained batch 2073 batch loss 0.578574538 epoch total loss 0.67023164\n",
      "Trained batch 2074 batch loss 0.522574842 epoch total loss 0.670160413\n",
      "Trained batch 2075 batch loss 0.672715068 epoch total loss 0.670161664\n",
      "Trained batch 2076 batch loss 0.72683394 epoch total loss 0.670188963\n",
      "Trained batch 2077 batch loss 0.645200372 epoch total loss 0.670176864\n",
      "Trained batch 2078 batch loss 0.755310833 epoch total loss 0.670217872\n",
      "Trained batch 2079 batch loss 0.779519916 epoch total loss 0.670270443\n",
      "Trained batch 2080 batch loss 0.751705587 epoch total loss 0.670309603\n",
      "Trained batch 2081 batch loss 0.682104886 epoch total loss 0.670315325\n",
      "Trained batch 2082 batch loss 0.695446789 epoch total loss 0.670327365\n",
      "Trained batch 2083 batch loss 0.660598695 epoch total loss 0.670322716\n",
      "Trained batch 2084 batch loss 0.721499443 epoch total loss 0.670347333\n",
      "Trained batch 2085 batch loss 0.717628598 epoch total loss 0.67037\n",
      "Trained batch 2086 batch loss 0.612558305 epoch total loss 0.670342267\n",
      "Trained batch 2087 batch loss 0.657722473 epoch total loss 0.670336246\n",
      "Trained batch 2088 batch loss 0.691093326 epoch total loss 0.670346141\n",
      "Trained batch 2089 batch loss 0.747279525 epoch total loss 0.670383\n",
      "Trained batch 2090 batch loss 0.654120505 epoch total loss 0.670375228\n",
      "Trained batch 2091 batch loss 0.620649159 epoch total loss 0.670351446\n",
      "Trained batch 2092 batch loss 0.658850372 epoch total loss 0.670345902\n",
      "Trained batch 2093 batch loss 0.6832546 epoch total loss 0.670352042\n",
      "Trained batch 2094 batch loss 0.693880796 epoch total loss 0.670363307\n",
      "Trained batch 2095 batch loss 0.695151389 epoch total loss 0.670375109\n",
      "Trained batch 2096 batch loss 0.621791065 epoch total loss 0.670352\n",
      "Trained batch 2097 batch loss 0.697582126 epoch total loss 0.670365\n",
      "Trained batch 2098 batch loss 0.61784929 epoch total loss 0.670339942\n",
      "Trained batch 2099 batch loss 0.634795845 epoch total loss 0.670322955\n",
      "Trained batch 2100 batch loss 0.616710901 epoch total loss 0.670297444\n",
      "Trained batch 2101 batch loss 0.58875072 epoch total loss 0.670258641\n",
      "Trained batch 2102 batch loss 0.629612327 epoch total loss 0.670239329\n",
      "Trained batch 2103 batch loss 0.737092853 epoch total loss 0.670271099\n",
      "Trained batch 2104 batch loss 0.653586149 epoch total loss 0.670263112\n",
      "Trained batch 2105 batch loss 0.688818038 epoch total loss 0.670272\n",
      "Trained batch 2106 batch loss 0.756036401 epoch total loss 0.670312643\n",
      "Trained batch 2107 batch loss 0.718759298 epoch total loss 0.67033565\n",
      "Trained batch 2108 batch loss 0.642636299 epoch total loss 0.670322478\n",
      "Trained batch 2109 batch loss 0.667849541 epoch total loss 0.670321286\n",
      "Trained batch 2110 batch loss 0.661354721 epoch total loss 0.670317054\n",
      "Trained batch 2111 batch loss 0.707687378 epoch total loss 0.670334756\n",
      "Trained batch 2112 batch loss 0.641560912 epoch total loss 0.670321167\n",
      "Trained batch 2113 batch loss 0.589420319 epoch total loss 0.6702829\n",
      "Trained batch 2114 batch loss 0.647165358 epoch total loss 0.670272\n",
      "Trained batch 2115 batch loss 0.729701519 epoch total loss 0.670300066\n",
      "Trained batch 2116 batch loss 0.605936766 epoch total loss 0.670269668\n",
      "Trained batch 2117 batch loss 0.520994782 epoch total loss 0.670199156\n",
      "Trained batch 2118 batch loss 0.590190411 epoch total loss 0.670161426\n",
      "Trained batch 2119 batch loss 0.598966658 epoch total loss 0.670127809\n",
      "Trained batch 2120 batch loss 0.739993 epoch total loss 0.67016077\n",
      "Trained batch 2121 batch loss 0.699772537 epoch total loss 0.670174778\n",
      "Trained batch 2122 batch loss 0.656110406 epoch total loss 0.670168161\n",
      "Trained batch 2123 batch loss 0.676963449 epoch total loss 0.67017138\n",
      "Trained batch 2124 batch loss 0.673287511 epoch total loss 0.67017287\n",
      "Trained batch 2125 batch loss 0.684398413 epoch total loss 0.670179546\n",
      "Trained batch 2126 batch loss 0.672489405 epoch total loss 0.670180678\n",
      "Trained batch 2127 batch loss 0.630754054 epoch total loss 0.670162141\n",
      "Trained batch 2128 batch loss 0.540992916 epoch total loss 0.670101404\n",
      "Trained batch 2129 batch loss 0.636591375 epoch total loss 0.670085669\n",
      "Trained batch 2130 batch loss 0.621144891 epoch total loss 0.670062661\n",
      "Trained batch 2131 batch loss 0.740775 epoch total loss 0.670095861\n",
      "Trained batch 2132 batch loss 0.691293836 epoch total loss 0.670105755\n",
      "Trained batch 2133 batch loss 0.646976113 epoch total loss 0.670094907\n",
      "Trained batch 2134 batch loss 0.700791359 epoch total loss 0.670109332\n",
      "Trained batch 2135 batch loss 0.745964587 epoch total loss 0.670144856\n",
      "Trained batch 2136 batch loss 0.769469678 epoch total loss 0.670191348\n",
      "Trained batch 2137 batch loss 0.819263458 epoch total loss 0.670261085\n",
      "Trained batch 2138 batch loss 0.745614529 epoch total loss 0.670296311\n",
      "Trained batch 2139 batch loss 0.620727539 epoch total loss 0.670273125\n",
      "Trained batch 2140 batch loss 0.677788734 epoch total loss 0.670276642\n",
      "Trained batch 2141 batch loss 0.631116033 epoch total loss 0.670258343\n",
      "Trained batch 2142 batch loss 0.627398968 epoch total loss 0.670238316\n",
      "Trained batch 2143 batch loss 0.686147332 epoch total loss 0.670245767\n",
      "Trained batch 2144 batch loss 0.63878113 epoch total loss 0.670231104\n",
      "Trained batch 2145 batch loss 0.603337109 epoch total loss 0.670199931\n",
      "Trained batch 2146 batch loss 0.699711323 epoch total loss 0.670213699\n",
      "Trained batch 2147 batch loss 0.626306534 epoch total loss 0.670193255\n",
      "Trained batch 2148 batch loss 0.669874847 epoch total loss 0.670193136\n",
      "Trained batch 2149 batch loss 0.675243 epoch total loss 0.67019552\n",
      "Trained batch 2150 batch loss 0.735905766 epoch total loss 0.670226097\n",
      "Trained batch 2151 batch loss 0.687904298 epoch total loss 0.670234263\n",
      "Trained batch 2152 batch loss 0.713914514 epoch total loss 0.670254588\n",
      "Trained batch 2153 batch loss 0.703104854 epoch total loss 0.670269847\n",
      "Trained batch 2154 batch loss 0.773867548 epoch total loss 0.670317948\n",
      "Trained batch 2155 batch loss 0.702558219 epoch total loss 0.670332909\n",
      "Trained batch 2156 batch loss 0.704549432 epoch total loss 0.670348763\n",
      "Trained batch 2157 batch loss 0.716623545 epoch total loss 0.670370281\n",
      "Trained batch 2158 batch loss 0.758672476 epoch total loss 0.67041117\n",
      "Trained batch 2159 batch loss 0.758436322 epoch total loss 0.670451939\n",
      "Trained batch 2160 batch loss 0.715283692 epoch total loss 0.670472741\n",
      "Trained batch 2161 batch loss 0.734179735 epoch total loss 0.670502186\n",
      "Trained batch 2162 batch loss 0.731898 epoch total loss 0.670530617\n",
      "Trained batch 2163 batch loss 0.713188529 epoch total loss 0.670550287\n",
      "Trained batch 2164 batch loss 0.708387613 epoch total loss 0.670567751\n",
      "Trained batch 2165 batch loss 0.676357567 epoch total loss 0.670570433\n",
      "Trained batch 2166 batch loss 0.696466684 epoch total loss 0.670582414\n",
      "Trained batch 2167 batch loss 0.696433902 epoch total loss 0.670594275\n",
      "Trained batch 2168 batch loss 0.712250292 epoch total loss 0.670613527\n",
      "Trained batch 2169 batch loss 0.659757555 epoch total loss 0.670608521\n",
      "Trained batch 2170 batch loss 0.68550086 epoch total loss 0.670615435\n",
      "Trained batch 2171 batch loss 0.68066448 epoch total loss 0.670620084\n",
      "Trained batch 2172 batch loss 0.682029188 epoch total loss 0.670625269\n",
      "Trained batch 2173 batch loss 0.71811223 epoch total loss 0.670647144\n",
      "Trained batch 2174 batch loss 0.625783503 epoch total loss 0.670626521\n",
      "Trained batch 2175 batch loss 0.601749599 epoch total loss 0.670594871\n",
      "Trained batch 2176 batch loss 0.652186871 epoch total loss 0.670586407\n",
      "Trained batch 2177 batch loss 0.596785307 epoch total loss 0.670552552\n",
      "Trained batch 2178 batch loss 0.571208656 epoch total loss 0.670506895\n",
      "Trained batch 2179 batch loss 0.613450944 epoch total loss 0.670480669\n",
      "Trained batch 2180 batch loss 0.583149552 epoch total loss 0.670440614\n",
      "Trained batch 2181 batch loss 0.60289079 epoch total loss 0.670409679\n",
      "Trained batch 2182 batch loss 0.674932122 epoch total loss 0.670411706\n",
      "Trained batch 2183 batch loss 0.53785491 epoch total loss 0.670350969\n",
      "Trained batch 2184 batch loss 0.529576957 epoch total loss 0.670286536\n",
      "Trained batch 2185 batch loss 0.604923666 epoch total loss 0.670256615\n",
      "Trained batch 2186 batch loss 0.665089369 epoch total loss 0.67025423\n",
      "Trained batch 2187 batch loss 0.600240529 epoch total loss 0.670222223\n",
      "Trained batch 2188 batch loss 0.605208158 epoch total loss 0.67019254\n",
      "Trained batch 2189 batch loss 0.619151771 epoch total loss 0.670169175\n",
      "Trained batch 2190 batch loss 0.640797377 epoch total loss 0.670155764\n",
      "Trained batch 2191 batch loss 0.628113925 epoch total loss 0.670136571\n",
      "Trained batch 2192 batch loss 0.576173782 epoch total loss 0.670093715\n",
      "Trained batch 2193 batch loss 0.560745776 epoch total loss 0.670043886\n",
      "Trained batch 2194 batch loss 0.648192048 epoch total loss 0.670033932\n",
      "Trained batch 2195 batch loss 0.636377811 epoch total loss 0.670018613\n",
      "Trained batch 2196 batch loss 0.664081395 epoch total loss 0.670015872\n",
      "Trained batch 2197 batch loss 0.672101498 epoch total loss 0.670016825\n",
      "Trained batch 2198 batch loss 0.673949242 epoch total loss 0.670018613\n",
      "Trained batch 2199 batch loss 0.625065148 epoch total loss 0.669998229\n",
      "Trained batch 2200 batch loss 0.726190031 epoch total loss 0.670023739\n",
      "Trained batch 2201 batch loss 0.784153759 epoch total loss 0.670075595\n",
      "Trained batch 2202 batch loss 0.729996383 epoch total loss 0.670102835\n",
      "Trained batch 2203 batch loss 0.6134758 epoch total loss 0.670077145\n",
      "Trained batch 2204 batch loss 0.642174602 epoch total loss 0.670064509\n",
      "Trained batch 2205 batch loss 0.643137395 epoch total loss 0.67005229\n",
      "Trained batch 2206 batch loss 0.718502939 epoch total loss 0.670074284\n",
      "Trained batch 2207 batch loss 0.651619375 epoch total loss 0.67006588\n",
      "Trained batch 2208 batch loss 0.625812054 epoch total loss 0.670045853\n",
      "Trained batch 2209 batch loss 0.663316369 epoch total loss 0.670042813\n",
      "Trained batch 2210 batch loss 0.638388157 epoch total loss 0.670028508\n",
      "Trained batch 2211 batch loss 0.613873661 epoch total loss 0.670003116\n",
      "Trained batch 2212 batch loss 0.595636725 epoch total loss 0.669969499\n",
      "Trained batch 2213 batch loss 0.654085696 epoch total loss 0.669962287\n",
      "Trained batch 2214 batch loss 0.681745112 epoch total loss 0.669967651\n",
      "Trained batch 2215 batch loss 0.677405238 epoch total loss 0.669971\n",
      "Trained batch 2216 batch loss 0.633557796 epoch total loss 0.669954538\n",
      "Trained batch 2217 batch loss 0.571824312 epoch total loss 0.669910252\n",
      "Trained batch 2218 batch loss 0.536694586 epoch total loss 0.66985023\n",
      "Trained batch 2219 batch loss 0.514142871 epoch total loss 0.669780076\n",
      "Trained batch 2220 batch loss 0.539006114 epoch total loss 0.669721186\n",
      "Trained batch 2221 batch loss 0.510584831 epoch total loss 0.669649541\n",
      "Trained batch 2222 batch loss 0.56482619 epoch total loss 0.669602334\n",
      "Trained batch 2223 batch loss 0.635501742 epoch total loss 0.669587\n",
      "Trained batch 2224 batch loss 0.625485599 epoch total loss 0.669567168\n",
      "Trained batch 2225 batch loss 0.707160413 epoch total loss 0.669584095\n",
      "Trained batch 2226 batch loss 0.667423546 epoch total loss 0.669583142\n",
      "Trained batch 2227 batch loss 0.73851639 epoch total loss 0.669614077\n",
      "Trained batch 2228 batch loss 0.762601256 epoch total loss 0.6696558\n",
      "Trained batch 2229 batch loss 0.698187232 epoch total loss 0.669668615\n",
      "Trained batch 2230 batch loss 0.71657604 epoch total loss 0.669689655\n",
      "Trained batch 2231 batch loss 0.660337448 epoch total loss 0.669685423\n",
      "Trained batch 2232 batch loss 0.572620332 epoch total loss 0.669642\n",
      "Trained batch 2233 batch loss 0.605277419 epoch total loss 0.669613123\n",
      "Trained batch 2234 batch loss 0.663465619 epoch total loss 0.669610381\n",
      "Trained batch 2235 batch loss 0.570623517 epoch total loss 0.669566095\n",
      "Trained batch 2236 batch loss 0.72223413 epoch total loss 0.669589698\n",
      "Trained batch 2237 batch loss 0.702837288 epoch total loss 0.66960454\n",
      "Trained batch 2238 batch loss 0.66114068 epoch total loss 0.669600785\n",
      "Trained batch 2239 batch loss 0.621214569 epoch total loss 0.669579148\n",
      "Trained batch 2240 batch loss 0.702733874 epoch total loss 0.669594\n",
      "Trained batch 2241 batch loss 0.770231128 epoch total loss 0.669638872\n",
      "Trained batch 2242 batch loss 0.692910671 epoch total loss 0.669649243\n",
      "Trained batch 2243 batch loss 0.704159617 epoch total loss 0.669664621\n",
      "Trained batch 2244 batch loss 0.709704936 epoch total loss 0.669682443\n",
      "Trained batch 2245 batch loss 0.713858306 epoch total loss 0.669702172\n",
      "Trained batch 2246 batch loss 0.775040686 epoch total loss 0.669749\n",
      "Trained batch 2247 batch loss 0.726994634 epoch total loss 0.669774532\n",
      "Trained batch 2248 batch loss 0.626285493 epoch total loss 0.66975522\n",
      "Trained batch 2249 batch loss 0.692751229 epoch total loss 0.669765472\n",
      "Trained batch 2250 batch loss 0.641485333 epoch total loss 0.669752896\n",
      "Trained batch 2251 batch loss 0.735963404 epoch total loss 0.669782281\n",
      "Trained batch 2252 batch loss 0.596807063 epoch total loss 0.669749856\n",
      "Trained batch 2253 batch loss 0.566729248 epoch total loss 0.669704199\n",
      "Trained batch 2254 batch loss 0.577900887 epoch total loss 0.669663429\n",
      "Trained batch 2255 batch loss 0.592718601 epoch total loss 0.669629335\n",
      "Trained batch 2256 batch loss 0.659613311 epoch total loss 0.669624925\n",
      "Trained batch 2257 batch loss 0.613355815 epoch total loss 0.6696\n",
      "Trained batch 2258 batch loss 0.6088503 epoch total loss 0.669573128\n",
      "Trained batch 2259 batch loss 0.653571367 epoch total loss 0.669566035\n",
      "Trained batch 2260 batch loss 0.546171 epoch total loss 0.669511437\n",
      "Trained batch 2261 batch loss 0.608385146 epoch total loss 0.669484377\n",
      "Trained batch 2262 batch loss 0.647193074 epoch total loss 0.669474542\n",
      "Trained batch 2263 batch loss 0.70395124 epoch total loss 0.669489801\n",
      "Trained batch 2264 batch loss 0.657254755 epoch total loss 0.669484377\n",
      "Trained batch 2265 batch loss 0.669466376 epoch total loss 0.669484377\n",
      "Trained batch 2266 batch loss 0.608948529 epoch total loss 0.669457674\n",
      "Trained batch 2267 batch loss 0.607479 epoch total loss 0.669430315\n",
      "Trained batch 2268 batch loss 0.606739879 epoch total loss 0.669402659\n",
      "Trained batch 2269 batch loss 0.600787401 epoch total loss 0.669372439\n",
      "Trained batch 2270 batch loss 0.558967 epoch total loss 0.669323802\n",
      "Trained batch 2271 batch loss 0.673959613 epoch total loss 0.669325829\n",
      "Trained batch 2272 batch loss 0.631586313 epoch total loss 0.669309199\n",
      "Trained batch 2273 batch loss 0.644805551 epoch total loss 0.66929841\n",
      "Trained batch 2274 batch loss 0.622936 epoch total loss 0.669278\n",
      "Trained batch 2275 batch loss 0.627196074 epoch total loss 0.669259548\n",
      "Trained batch 2276 batch loss 0.762657166 epoch total loss 0.669300556\n",
      "Trained batch 2277 batch loss 0.631694913 epoch total loss 0.669284046\n",
      "Trained batch 2278 batch loss 0.64174664 epoch total loss 0.669272\n",
      "Trained batch 2279 batch loss 0.5646258 epoch total loss 0.66922605\n",
      "Trained batch 2280 batch loss 0.619821429 epoch total loss 0.669204414\n",
      "Trained batch 2281 batch loss 0.620687962 epoch total loss 0.669183135\n",
      "Trained batch 2282 batch loss 0.621792197 epoch total loss 0.669162393\n",
      "Trained batch 2283 batch loss 0.685819507 epoch total loss 0.669169664\n",
      "Trained batch 2284 batch loss 0.672886252 epoch total loss 0.669171274\n",
      "Trained batch 2285 batch loss 0.670942247 epoch total loss 0.669172049\n",
      "Trained batch 2286 batch loss 0.632581 epoch total loss 0.669156\n",
      "Trained batch 2287 batch loss 0.645567358 epoch total loss 0.669145703\n",
      "Trained batch 2288 batch loss 0.674975812 epoch total loss 0.669148207\n",
      "Trained batch 2289 batch loss 0.645363331 epoch total loss 0.669137836\n",
      "Trained batch 2290 batch loss 0.692730665 epoch total loss 0.669148147\n",
      "Trained batch 2291 batch loss 0.595322132 epoch total loss 0.669115901\n",
      "Trained batch 2292 batch loss 0.607503057 epoch total loss 0.669089079\n",
      "Trained batch 2293 batch loss 0.605143726 epoch total loss 0.669061184\n",
      "Trained batch 2294 batch loss 0.572714686 epoch total loss 0.669019163\n",
      "Trained batch 2295 batch loss 0.668219805 epoch total loss 0.669018805\n",
      "Trained batch 2296 batch loss 0.650344849 epoch total loss 0.669010699\n",
      "Trained batch 2297 batch loss 0.644562721 epoch total loss 0.669\n",
      "Trained batch 2298 batch loss 0.548374891 epoch total loss 0.668947518\n",
      "Trained batch 2299 batch loss 0.638091147 epoch total loss 0.668934107\n",
      "Trained batch 2300 batch loss 0.722178578 epoch total loss 0.668957233\n",
      "Trained batch 2301 batch loss 0.746321559 epoch total loss 0.66899091\n",
      "Trained batch 2302 batch loss 0.689962149 epoch total loss 0.66899997\n",
      "Trained batch 2303 batch loss 0.690673709 epoch total loss 0.669009387\n",
      "Trained batch 2304 batch loss 0.679453254 epoch total loss 0.669013917\n",
      "Trained batch 2305 batch loss 0.721715689 epoch total loss 0.669036746\n",
      "Trained batch 2306 batch loss 0.657835603 epoch total loss 0.669031918\n",
      "Trained batch 2307 batch loss 0.718632042 epoch total loss 0.669053435\n",
      "Trained batch 2308 batch loss 0.683097839 epoch total loss 0.669059515\n",
      "Trained batch 2309 batch loss 0.690096557 epoch total loss 0.669068575\n",
      "Trained batch 2310 batch loss 0.708592772 epoch total loss 0.669085741\n",
      "Trained batch 2311 batch loss 0.67611444 epoch total loss 0.669088781\n",
      "Trained batch 2312 batch loss 0.590897918 epoch total loss 0.669055\n",
      "Trained batch 2313 batch loss 0.634537399 epoch total loss 0.66904\n",
      "Trained batch 2314 batch loss 0.637043834 epoch total loss 0.669026256\n",
      "Trained batch 2315 batch loss 0.681498349 epoch total loss 0.66903162\n",
      "Trained batch 2316 batch loss 0.589797199 epoch total loss 0.668997467\n",
      "Trained batch 2317 batch loss 0.569652498 epoch total loss 0.668954611\n",
      "Trained batch 2318 batch loss 0.523675203 epoch total loss 0.668891907\n",
      "Trained batch 2319 batch loss 0.496260792 epoch total loss 0.668817461\n",
      "Trained batch 2320 batch loss 0.597994924 epoch total loss 0.668786943\n",
      "Trained batch 2321 batch loss 0.551025033 epoch total loss 0.668736219\n",
      "Trained batch 2322 batch loss 0.617816269 epoch total loss 0.668714285\n",
      "Trained batch 2323 batch loss 0.614924192 epoch total loss 0.668691099\n",
      "Trained batch 2324 batch loss 0.58409822 epoch total loss 0.66865468\n",
      "Trained batch 2325 batch loss 0.525226235 epoch total loss 0.668593\n",
      "Trained batch 2326 batch loss 0.478344113 epoch total loss 0.668511271\n",
      "Trained batch 2327 batch loss 0.470644772 epoch total loss 0.668426216\n",
      "Trained batch 2328 batch loss 0.515503049 epoch total loss 0.668360531\n",
      "Trained batch 2329 batch loss 0.507325351 epoch total loss 0.66829139\n",
      "Trained batch 2330 batch loss 0.479352564 epoch total loss 0.668210328\n",
      "Trained batch 2331 batch loss 0.432610542 epoch total loss 0.668109238\n",
      "Trained batch 2332 batch loss 0.620322883 epoch total loss 0.668088794\n",
      "Trained batch 2333 batch loss 0.592825234 epoch total loss 0.668056488\n",
      "Trained batch 2334 batch loss 0.701334357 epoch total loss 0.668070734\n",
      "Trained batch 2335 batch loss 0.671101809 epoch total loss 0.668072045\n",
      "Trained batch 2336 batch loss 0.648903847 epoch total loss 0.668063879\n",
      "Trained batch 2337 batch loss 0.605672717 epoch total loss 0.668037176\n",
      "Trained batch 2338 batch loss 0.633164048 epoch total loss 0.668022275\n",
      "Trained batch 2339 batch loss 0.682818949 epoch total loss 0.668028593\n",
      "Trained batch 2340 batch loss 0.608536363 epoch total loss 0.668003201\n",
      "Trained batch 2341 batch loss 0.610156834 epoch total loss 0.667978466\n",
      "Trained batch 2342 batch loss 0.692765951 epoch total loss 0.667989\n",
      "Trained batch 2343 batch loss 0.655373 epoch total loss 0.667983651\n",
      "Trained batch 2344 batch loss 0.664001882 epoch total loss 0.667982\n",
      "Trained batch 2345 batch loss 0.629553318 epoch total loss 0.667965591\n",
      "Trained batch 2346 batch loss 0.64226532 epoch total loss 0.667954624\n",
      "Trained batch 2347 batch loss 0.663536072 epoch total loss 0.667952716\n",
      "Trained batch 2348 batch loss 0.717919528 epoch total loss 0.667974\n",
      "Trained batch 2349 batch loss 0.649421334 epoch total loss 0.667966127\n",
      "Trained batch 2350 batch loss 0.788114429 epoch total loss 0.668017209\n",
      "Trained batch 2351 batch loss 0.678631544 epoch total loss 0.668021739\n",
      "Trained batch 2352 batch loss 0.652409136 epoch total loss 0.668015122\n",
      "Trained batch 2353 batch loss 0.633613706 epoch total loss 0.668000519\n",
      "Trained batch 2354 batch loss 0.583153546 epoch total loss 0.667964458\n",
      "Trained batch 2355 batch loss 0.605483115 epoch total loss 0.667937934\n",
      "Trained batch 2356 batch loss 0.577469587 epoch total loss 0.667899549\n",
      "Trained batch 2357 batch loss 0.5936203 epoch total loss 0.667868\n",
      "Trained batch 2358 batch loss 0.532450318 epoch total loss 0.667810619\n",
      "Trained batch 2359 batch loss 0.562615335 epoch total loss 0.667766035\n",
      "Trained batch 2360 batch loss 0.645596683 epoch total loss 0.667756617\n",
      "Trained batch 2361 batch loss 0.633873 epoch total loss 0.667742312\n",
      "Trained batch 2362 batch loss 0.656486392 epoch total loss 0.667737544\n",
      "Trained batch 2363 batch loss 0.668977261 epoch total loss 0.667738\n",
      "Trained batch 2364 batch loss 0.703680158 epoch total loss 0.667753279\n",
      "Trained batch 2365 batch loss 0.696734786 epoch total loss 0.667765558\n",
      "Trained batch 2366 batch loss 0.577685535 epoch total loss 0.66772747\n",
      "Trained batch 2367 batch loss 0.621841073 epoch total loss 0.667708039\n",
      "Trained batch 2368 batch loss 0.617733 epoch total loss 0.667686939\n",
      "Trained batch 2369 batch loss 0.565754414 epoch total loss 0.667643905\n",
      "Trained batch 2370 batch loss 0.640607953 epoch total loss 0.66763252\n",
      "Trained batch 2371 batch loss 0.602426 epoch total loss 0.667605\n",
      "Trained batch 2372 batch loss 0.680630267 epoch total loss 0.667610526\n",
      "Trained batch 2373 batch loss 0.823927283 epoch total loss 0.667676389\n",
      "Trained batch 2374 batch loss 0.676134348 epoch total loss 0.667679965\n",
      "Trained batch 2375 batch loss 0.644789815 epoch total loss 0.66767031\n",
      "Trained batch 2376 batch loss 0.613308549 epoch total loss 0.667647421\n",
      "Trained batch 2377 batch loss 0.633673072 epoch total loss 0.667633176\n",
      "Trained batch 2378 batch loss 0.648568451 epoch total loss 0.667625129\n",
      "Trained batch 2379 batch loss 0.558199286 epoch total loss 0.667579114\n",
      "Trained batch 2380 batch loss 0.553944826 epoch total loss 0.667531371\n",
      "Trained batch 2381 batch loss 0.532828152 epoch total loss 0.667474806\n",
      "Trained batch 2382 batch loss 0.527176142 epoch total loss 0.667416\n",
      "Trained batch 2383 batch loss 0.453551292 epoch total loss 0.667326152\n",
      "Trained batch 2384 batch loss 0.586027086 epoch total loss 0.667292118\n",
      "Trained batch 2385 batch loss 0.494288206 epoch total loss 0.66721952\n",
      "Trained batch 2386 batch loss 0.634080291 epoch total loss 0.667205632\n",
      "Trained batch 2387 batch loss 0.667076349 epoch total loss 0.667205572\n",
      "Trained batch 2388 batch loss 0.685391665 epoch total loss 0.667213202\n",
      "Trained batch 2389 batch loss 0.663894713 epoch total loss 0.667211831\n",
      "Trained batch 2390 batch loss 0.635204077 epoch total loss 0.667198479\n",
      "Trained batch 2391 batch loss 0.715580046 epoch total loss 0.667218685\n",
      "Trained batch 2392 batch loss 0.676590562 epoch total loss 0.667222679\n",
      "Trained batch 2393 batch loss 0.719052792 epoch total loss 0.667244315\n",
      "Trained batch 2394 batch loss 0.683099329 epoch total loss 0.667250931\n",
      "Trained batch 2395 batch loss 0.693056226 epoch total loss 0.66726172\n",
      "Trained batch 2396 batch loss 0.656387448 epoch total loss 0.66725719\n",
      "Trained batch 2397 batch loss 0.602151453 epoch total loss 0.66723\n",
      "Trained batch 2398 batch loss 0.671553433 epoch total loss 0.667231798\n",
      "Trained batch 2399 batch loss 0.659577131 epoch total loss 0.66722858\n",
      "Trained batch 2400 batch loss 0.641193092 epoch total loss 0.667217791\n",
      "Trained batch 2401 batch loss 0.649537206 epoch total loss 0.6672104\n",
      "Trained batch 2402 batch loss 0.710728288 epoch total loss 0.66722852\n",
      "Trained batch 2403 batch loss 0.643623114 epoch total loss 0.667218685\n",
      "Trained batch 2404 batch loss 0.674524486 epoch total loss 0.667221785\n",
      "Trained batch 2405 batch loss 0.643840671 epoch total loss 0.667212\n",
      "Trained batch 2406 batch loss 0.662249804 epoch total loss 0.667209923\n",
      "Trained batch 2407 batch loss 0.600241542 epoch total loss 0.667182088\n",
      "Trained batch 2408 batch loss 0.642819941 epoch total loss 0.667172\n",
      "Trained batch 2409 batch loss 0.643165231 epoch total loss 0.667162061\n",
      "Trained batch 2410 batch loss 0.643678665 epoch total loss 0.667152286\n",
      "Trained batch 2411 batch loss 0.620574 epoch total loss 0.667133\n",
      "Trained batch 2412 batch loss 0.644693792 epoch total loss 0.667123675\n",
      "Trained batch 2413 batch loss 0.623839 epoch total loss 0.667105734\n",
      "Trained batch 2414 batch loss 0.633450329 epoch total loss 0.667091727\n",
      "Trained batch 2415 batch loss 0.58184284 epoch total loss 0.667056441\n",
      "Trained batch 2416 batch loss 0.534386 epoch total loss 0.667001545\n",
      "Trained batch 2417 batch loss 0.60557276 epoch total loss 0.666976154\n",
      "Trained batch 2418 batch loss 0.593744934 epoch total loss 0.666945815\n",
      "Trained batch 2419 batch loss 0.65771091 epoch total loss 0.666942\n",
      "Trained batch 2420 batch loss 0.602384686 epoch total loss 0.666915357\n",
      "Trained batch 2421 batch loss 0.701764464 epoch total loss 0.666929781\n",
      "Trained batch 2422 batch loss 0.654123843 epoch total loss 0.666924477\n",
      "Trained batch 2423 batch loss 0.699533284 epoch total loss 0.666938\n",
      "Trained batch 2424 batch loss 0.595210791 epoch total loss 0.666908383\n",
      "Trained batch 2425 batch loss 0.644911051 epoch total loss 0.666899323\n",
      "Trained batch 2426 batch loss 0.622333229 epoch total loss 0.666880965\n",
      "Trained batch 2427 batch loss 0.682473123 epoch total loss 0.666887343\n",
      "Trained batch 2428 batch loss 0.713071644 epoch total loss 0.666906357\n",
      "Trained batch 2429 batch loss 0.656712413 epoch total loss 0.666902184\n",
      "Trained batch 2430 batch loss 0.67844528 epoch total loss 0.666906953\n",
      "Trained batch 2431 batch loss 0.77051729 epoch total loss 0.66694957\n",
      "Trained batch 2432 batch loss 0.668301582 epoch total loss 0.666950107\n",
      "Trained batch 2433 batch loss 0.576838 epoch total loss 0.666913092\n",
      "Trained batch 2434 batch loss 0.641533554 epoch total loss 0.666902602\n",
      "Trained batch 2435 batch loss 0.633949876 epoch total loss 0.666889071\n",
      "Trained batch 2436 batch loss 0.585852563 epoch total loss 0.666855812\n",
      "Trained batch 2437 batch loss 0.620878816 epoch total loss 0.666836917\n",
      "Trained batch 2438 batch loss 0.713510394 epoch total loss 0.66685605\n",
      "Trained batch 2439 batch loss 0.587569058 epoch total loss 0.666823506\n",
      "Trained batch 2440 batch loss 0.533333719 epoch total loss 0.666768789\n",
      "Trained batch 2441 batch loss 0.551059365 epoch total loss 0.666721404\n",
      "Trained batch 2442 batch loss 0.690401196 epoch total loss 0.666731119\n",
      "Trained batch 2443 batch loss 0.751260579 epoch total loss 0.66676569\n",
      "Trained batch 2444 batch loss 0.682256222 epoch total loss 0.666772\n",
      "Trained batch 2445 batch loss 0.801884353 epoch total loss 0.666827261\n",
      "Trained batch 2446 batch loss 0.741696358 epoch total loss 0.666857898\n",
      "Trained batch 2447 batch loss 0.600048363 epoch total loss 0.666830599\n",
      "Trained batch 2448 batch loss 0.605549395 epoch total loss 0.666805565\n",
      "Trained batch 2449 batch loss 0.590630591 epoch total loss 0.666774452\n",
      "Trained batch 2450 batch loss 0.558111191 epoch total loss 0.666730106\n",
      "Trained batch 2451 batch loss 0.538445652 epoch total loss 0.666677773\n",
      "Trained batch 2452 batch loss 0.603736877 epoch total loss 0.666652083\n",
      "Trained batch 2453 batch loss 0.640541077 epoch total loss 0.666641474\n",
      "Trained batch 2454 batch loss 0.519568086 epoch total loss 0.666581511\n",
      "Trained batch 2455 batch loss 0.551576257 epoch total loss 0.666534662\n",
      "Trained batch 2456 batch loss 0.537879705 epoch total loss 0.66648227\n",
      "Trained batch 2457 batch loss 0.544190884 epoch total loss 0.6664325\n",
      "Trained batch 2458 batch loss 0.594186604 epoch total loss 0.666403115\n",
      "Trained batch 2459 batch loss 0.532619834 epoch total loss 0.666348696\n",
      "Trained batch 2460 batch loss 0.597274125 epoch total loss 0.666320682\n",
      "Trained batch 2461 batch loss 0.638558865 epoch total loss 0.666309357\n",
      "Trained batch 2462 batch loss 0.603055358 epoch total loss 0.666283667\n",
      "Trained batch 2463 batch loss 0.694390178 epoch total loss 0.666295052\n",
      "Trained batch 2464 batch loss 0.571677208 epoch total loss 0.666256666\n",
      "Trained batch 2465 batch loss 0.629243433 epoch total loss 0.666241646\n",
      "Trained batch 2466 batch loss 0.614046454 epoch total loss 0.666220486\n",
      "Trained batch 2467 batch loss 0.739664435 epoch total loss 0.666250229\n",
      "Trained batch 2468 batch loss 0.666600823 epoch total loss 0.666250348\n",
      "Trained batch 2469 batch loss 0.660484433 epoch total loss 0.666248\n",
      "Trained batch 2470 batch loss 0.67695421 epoch total loss 0.666252375\n",
      "Trained batch 2471 batch loss 0.64906013 epoch total loss 0.666245461\n",
      "Trained batch 2472 batch loss 0.613793135 epoch total loss 0.666224182\n",
      "Trained batch 2473 batch loss 0.645986497 epoch total loss 0.666216\n",
      "Trained batch 2474 batch loss 0.653808773 epoch total loss 0.666211\n",
      "Trained batch 2475 batch loss 0.651541352 epoch total loss 0.666205049\n",
      "Trained batch 2476 batch loss 0.629643142 epoch total loss 0.666190326\n",
      "Trained batch 2477 batch loss 0.678602755 epoch total loss 0.666195333\n",
      "Trained batch 2478 batch loss 0.633172572 epoch total loss 0.666182\n",
      "Trained batch 2479 batch loss 0.633937061 epoch total loss 0.666169\n",
      "Trained batch 2480 batch loss 0.549262404 epoch total loss 0.66612184\n",
      "Trained batch 2481 batch loss 0.567144275 epoch total loss 0.666081965\n",
      "Trained batch 2482 batch loss 0.568403602 epoch total loss 0.666042566\n",
      "Trained batch 2483 batch loss 0.62452209 epoch total loss 0.666025877\n",
      "Trained batch 2484 batch loss 0.613536477 epoch total loss 0.666004717\n",
      "Trained batch 2485 batch loss 0.699023724 epoch total loss 0.666018\n",
      "Trained batch 2486 batch loss 0.608470857 epoch total loss 0.665994883\n",
      "Trained batch 2487 batch loss 0.676198721 epoch total loss 0.665998936\n",
      "Trained batch 2488 batch loss 0.639573276 epoch total loss 0.665988326\n",
      "Trained batch 2489 batch loss 0.666352 epoch total loss 0.665988445\n",
      "Trained batch 2490 batch loss 0.615094423 epoch total loss 0.665968\n",
      "Trained batch 2491 batch loss 0.680380225 epoch total loss 0.665973842\n",
      "Trained batch 2492 batch loss 0.652702034 epoch total loss 0.665968478\n",
      "Trained batch 2493 batch loss 0.555262804 epoch total loss 0.665924132\n",
      "Trained batch 2494 batch loss 0.596782923 epoch total loss 0.665896416\n",
      "Trained batch 2495 batch loss 0.65726006 epoch total loss 0.665892899\n",
      "Trained batch 2496 batch loss 0.746109486 epoch total loss 0.665925\n",
      "Trained batch 2497 batch loss 0.675124466 epoch total loss 0.665928781\n",
      "Trained batch 2498 batch loss 0.64363569 epoch total loss 0.66591984\n",
      "Trained batch 2499 batch loss 0.645644605 epoch total loss 0.665911734\n",
      "Trained batch 2500 batch loss 0.674650371 epoch total loss 0.665915251\n",
      "Trained batch 2501 batch loss 0.645552 epoch total loss 0.665907085\n",
      "Trained batch 2502 batch loss 0.65186727 epoch total loss 0.665901482\n",
      "Trained batch 2503 batch loss 0.619821548 epoch total loss 0.665883064\n",
      "Trained batch 2504 batch loss 0.672509909 epoch total loss 0.665885687\n",
      "Trained batch 2505 batch loss 0.641846597 epoch total loss 0.665876091\n",
      "Trained batch 2506 batch loss 0.62057662 epoch total loss 0.66585803\n",
      "Trained batch 2507 batch loss 0.612559438 epoch total loss 0.665836751\n",
      "Trained batch 2508 batch loss 0.579056203 epoch total loss 0.665802181\n",
      "Trained batch 2509 batch loss 0.544859588 epoch total loss 0.665753961\n",
      "Trained batch 2510 batch loss 0.618267715 epoch total loss 0.665735066\n",
      "Trained batch 2511 batch loss 0.723487318 epoch total loss 0.665758073\n",
      "Trained batch 2512 batch loss 0.681667566 epoch total loss 0.665764391\n",
      "Trained batch 2513 batch loss 0.57025969 epoch total loss 0.665726423\n",
      "Trained batch 2514 batch loss 0.560869455 epoch total loss 0.6656847\n",
      "Trained batch 2515 batch loss 0.636069119 epoch total loss 0.665672958\n",
      "Trained batch 2516 batch loss 0.633588374 epoch total loss 0.665660203\n",
      "Trained batch 2517 batch loss 0.628507137 epoch total loss 0.665645421\n",
      "Trained batch 2518 batch loss 0.623980582 epoch total loss 0.66562891\n",
      "Trained batch 2519 batch loss 0.586411476 epoch total loss 0.665597439\n",
      "Trained batch 2520 batch loss 0.574282527 epoch total loss 0.665561259\n",
      "Trained batch 2521 batch loss 0.60497129 epoch total loss 0.665537238\n",
      "Trained batch 2522 batch loss 0.641882658 epoch total loss 0.665527821\n",
      "Trained batch 2523 batch loss 0.606496394 epoch total loss 0.665504396\n",
      "Trained batch 2524 batch loss 0.584192812 epoch total loss 0.665472209\n",
      "Trained batch 2525 batch loss 0.654684305 epoch total loss 0.665467918\n",
      "Trained batch 2526 batch loss 0.732698 epoch total loss 0.665494502\n",
      "Trained batch 2527 batch loss 0.631525636 epoch total loss 0.665481091\n",
      "Trained batch 2528 batch loss 0.72754097 epoch total loss 0.665505588\n",
      "Trained batch 2529 batch loss 0.862956285 epoch total loss 0.66558367\n",
      "Trained batch 2530 batch loss 0.799393117 epoch total loss 0.665636599\n",
      "Trained batch 2531 batch loss 0.694498539 epoch total loss 0.665648\n",
      "Trained batch 2532 batch loss 0.708127856 epoch total loss 0.665664732\n",
      "Trained batch 2533 batch loss 0.768358588 epoch total loss 0.665705264\n",
      "Trained batch 2534 batch loss 0.666984797 epoch total loss 0.6657058\n",
      "Trained batch 2535 batch loss 0.719754934 epoch total loss 0.665727079\n",
      "Trained batch 2536 batch loss 0.66745156 epoch total loss 0.665727794\n",
      "Trained batch 2537 batch loss 0.730879307 epoch total loss 0.665753424\n",
      "Trained batch 2538 batch loss 0.705342531 epoch total loss 0.665769041\n",
      "Trained batch 2539 batch loss 0.682908297 epoch total loss 0.665775776\n",
      "Trained batch 2540 batch loss 0.681755781 epoch total loss 0.665782034\n",
      "Trained batch 2541 batch loss 0.669099808 epoch total loss 0.665783346\n",
      "Trained batch 2542 batch loss 0.67355442 epoch total loss 0.665786445\n",
      "Trained batch 2543 batch loss 0.666346967 epoch total loss 0.665786624\n",
      "Trained batch 2544 batch loss 0.624995172 epoch total loss 0.66577065\n",
      "Trained batch 2545 batch loss 0.605449259 epoch total loss 0.665746927\n",
      "Trained batch 2546 batch loss 0.617102265 epoch total loss 0.665727794\n",
      "Trained batch 2547 batch loss 0.658550441 epoch total loss 0.665725\n",
      "Trained batch 2548 batch loss 0.651162684 epoch total loss 0.665719271\n",
      "Trained batch 2549 batch loss 0.75277549 epoch total loss 0.665753424\n",
      "Trained batch 2550 batch loss 0.701895416 epoch total loss 0.66576761\n",
      "Trained batch 2551 batch loss 0.660509408 epoch total loss 0.665765524\n",
      "Trained batch 2552 batch loss 0.634345412 epoch total loss 0.665753245\n",
      "Trained batch 2553 batch loss 0.676993668 epoch total loss 0.665757656\n",
      "Trained batch 2554 batch loss 0.584188581 epoch total loss 0.665725768\n",
      "Trained batch 2555 batch loss 0.603292584 epoch total loss 0.66570133\n",
      "Trained batch 2556 batch loss 0.59652853 epoch total loss 0.665674269\n",
      "Trained batch 2557 batch loss 0.712007284 epoch total loss 0.665692389\n",
      "Trained batch 2558 batch loss 0.613767684 epoch total loss 0.665672064\n",
      "Trained batch 2559 batch loss 0.623690605 epoch total loss 0.665655673\n",
      "Trained batch 2560 batch loss 0.622876644 epoch total loss 0.665639\n",
      "Trained batch 2561 batch loss 0.66424191 epoch total loss 0.665638387\n",
      "Trained batch 2562 batch loss 0.658404291 epoch total loss 0.665635586\n",
      "Trained batch 2563 batch loss 0.662939429 epoch total loss 0.665634573\n",
      "Trained batch 2564 batch loss 0.616523862 epoch total loss 0.665615439\n",
      "Trained batch 2565 batch loss 0.626831293 epoch total loss 0.6656003\n",
      "Trained batch 2566 batch loss 0.680330455 epoch total loss 0.665606\n",
      "Trained batch 2567 batch loss 0.581253052 epoch total loss 0.66557318\n",
      "Trained batch 2568 batch loss 0.636998355 epoch total loss 0.665562034\n",
      "Trained batch 2569 batch loss 0.604990065 epoch total loss 0.66553849\n",
      "Trained batch 2570 batch loss 0.702558219 epoch total loss 0.665552855\n",
      "Trained batch 2571 batch loss 0.766062 epoch total loss 0.665591955\n",
      "Trained batch 2572 batch loss 0.628346384 epoch total loss 0.665577471\n",
      "Trained batch 2573 batch loss 0.72397 epoch total loss 0.665600181\n",
      "Trained batch 2574 batch loss 0.710291505 epoch total loss 0.665617526\n",
      "Trained batch 2575 batch loss 0.60162425 epoch total loss 0.66559273\n",
      "Trained batch 2576 batch loss 0.578607917 epoch total loss 0.665558934\n",
      "Trained batch 2577 batch loss 0.6579265 epoch total loss 0.665556\n",
      "Trained batch 2578 batch loss 0.630827844 epoch total loss 0.665542543\n",
      "Trained batch 2579 batch loss 0.594977915 epoch total loss 0.665515184\n",
      "Trained batch 2580 batch loss 0.653243423 epoch total loss 0.665510416\n",
      "Trained batch 2581 batch loss 0.656951666 epoch total loss 0.665507078\n",
      "Trained batch 2582 batch loss 0.65614152 epoch total loss 0.665503442\n",
      "Trained batch 2583 batch loss 0.630930245 epoch total loss 0.665490091\n",
      "Trained batch 2584 batch loss 0.675303578 epoch total loss 0.665493906\n",
      "Trained batch 2585 batch loss 0.634941518 epoch total loss 0.665482044\n",
      "Trained batch 2586 batch loss 0.631217957 epoch total loss 0.665468812\n",
      "Trained batch 2587 batch loss 0.608587384 epoch total loss 0.665446818\n",
      "Trained batch 2588 batch loss 0.638842463 epoch total loss 0.665436566\n",
      "Trained batch 2589 batch loss 0.644524276 epoch total loss 0.66542846\n",
      "Trained batch 2590 batch loss 0.690817237 epoch total loss 0.665438294\n",
      "Trained batch 2591 batch loss 0.619472146 epoch total loss 0.665420532\n",
      "Trained batch 2592 batch loss 0.725797057 epoch total loss 0.665443838\n",
      "Trained batch 2593 batch loss 0.740724504 epoch total loss 0.665472865\n",
      "Trained batch 2594 batch loss 0.67289114 epoch total loss 0.665475726\n",
      "Trained batch 2595 batch loss 0.635339618 epoch total loss 0.665464103\n",
      "Trained batch 2596 batch loss 0.682494402 epoch total loss 0.66547066\n",
      "Trained batch 2597 batch loss 0.658418953 epoch total loss 0.665468\n",
      "Trained batch 2598 batch loss 0.704834282 epoch total loss 0.665483117\n",
      "Trained batch 2599 batch loss 0.734747887 epoch total loss 0.66550976\n",
      "Trained batch 2600 batch loss 0.611216664 epoch total loss 0.665488899\n",
      "Trained batch 2601 batch loss 0.671462953 epoch total loss 0.665491223\n",
      "Trained batch 2602 batch loss 0.565050125 epoch total loss 0.6654526\n",
      "Trained batch 2603 batch loss 0.601737082 epoch total loss 0.665428102\n",
      "Trained batch 2604 batch loss 0.672429442 epoch total loss 0.665430844\n",
      "Trained batch 2605 batch loss 0.633265436 epoch total loss 0.665418506\n",
      "Trained batch 2606 batch loss 0.652884841 epoch total loss 0.665413678\n",
      "Trained batch 2607 batch loss 0.648937583 epoch total loss 0.66540736\n",
      "Trained batch 2608 batch loss 0.669364274 epoch total loss 0.66540885\n",
      "Trained batch 2609 batch loss 0.612465858 epoch total loss 0.665388525\n",
      "Trained batch 2610 batch loss 0.609071255 epoch total loss 0.665366948\n",
      "Trained batch 2611 batch loss 0.715707064 epoch total loss 0.66538626\n",
      "Trained batch 2612 batch loss 0.752408 epoch total loss 0.665419579\n",
      "Trained batch 2613 batch loss 0.69449544 epoch total loss 0.665430665\n",
      "Trained batch 2614 batch loss 0.704509556 epoch total loss 0.665445626\n",
      "Trained batch 2615 batch loss 0.665418625 epoch total loss 0.665445626\n",
      "Trained batch 2616 batch loss 0.714141 epoch total loss 0.665464222\n",
      "Trained batch 2617 batch loss 0.664014935 epoch total loss 0.665463686\n",
      "Trained batch 2618 batch loss 0.62725383 epoch total loss 0.665449083\n",
      "Trained batch 2619 batch loss 0.639378726 epoch total loss 0.665439129\n",
      "Trained batch 2620 batch loss 0.635979414 epoch total loss 0.665427864\n",
      "Trained batch 2621 batch loss 0.636614919 epoch total loss 0.665416896\n",
      "Trained batch 2622 batch loss 0.660486162 epoch total loss 0.665415\n",
      "Trained batch 2623 batch loss 0.563484192 epoch total loss 0.665376127\n",
      "Trained batch 2624 batch loss 0.702010453 epoch total loss 0.665390134\n",
      "Trained batch 2625 batch loss 0.741663456 epoch total loss 0.665419161\n",
      "Trained batch 2626 batch loss 0.658920228 epoch total loss 0.665416718\n",
      "Trained batch 2627 batch loss 0.749728501 epoch total loss 0.665448785\n",
      "Trained batch 2628 batch loss 0.758858919 epoch total loss 0.665484369\n",
      "Trained batch 2629 batch loss 0.748959064 epoch total loss 0.665516078\n",
      "Trained batch 2630 batch loss 0.826763153 epoch total loss 0.665577412\n",
      "Trained batch 2631 batch loss 0.741839945 epoch total loss 0.66560638\n",
      "Trained batch 2632 batch loss 0.648115456 epoch total loss 0.665599763\n",
      "Trained batch 2633 batch loss 0.692567587 epoch total loss 0.66561\n",
      "Trained batch 2634 batch loss 0.65025121 epoch total loss 0.665604174\n",
      "Trained batch 2635 batch loss 0.685705781 epoch total loss 0.665611804\n",
      "Trained batch 2636 batch loss 0.678065896 epoch total loss 0.665616512\n",
      "Trained batch 2637 batch loss 0.671945035 epoch total loss 0.665618956\n",
      "Trained batch 2638 batch loss 0.697204649 epoch total loss 0.665630937\n",
      "Trained batch 2639 batch loss 0.748200536 epoch total loss 0.665662229\n",
      "Trained batch 2640 batch loss 0.718987644 epoch total loss 0.665682435\n",
      "Trained batch 2641 batch loss 0.735885561 epoch total loss 0.665709\n",
      "Trained batch 2642 batch loss 0.680601835 epoch total loss 0.665714622\n",
      "Trained batch 2643 batch loss 0.650499821 epoch total loss 0.66570884\n",
      "Trained batch 2644 batch loss 0.589702189 epoch total loss 0.66568011\n",
      "Trained batch 2645 batch loss 0.556537628 epoch total loss 0.665638864\n",
      "Trained batch 2646 batch loss 0.670699596 epoch total loss 0.665640712\n",
      "Trained batch 2647 batch loss 0.744562089 epoch total loss 0.665670514\n",
      "Trained batch 2648 batch loss 0.703580499 epoch total loss 0.665684879\n",
      "Trained batch 2649 batch loss 0.602434814 epoch total loss 0.665661\n",
      "Trained batch 2650 batch loss 0.582230508 epoch total loss 0.665629506\n",
      "Trained batch 2651 batch loss 0.66558826 epoch total loss 0.665629447\n",
      "Trained batch 2652 batch loss 0.708657086 epoch total loss 0.665645659\n",
      "Trained batch 2653 batch loss 0.686243594 epoch total loss 0.665653467\n",
      "Trained batch 2654 batch loss 0.667352557 epoch total loss 0.665654123\n",
      "Trained batch 2655 batch loss 0.609633625 epoch total loss 0.665633\n",
      "Trained batch 2656 batch loss 0.611691475 epoch total loss 0.665612698\n",
      "Trained batch 2657 batch loss 0.561222136 epoch total loss 0.665573418\n",
      "Trained batch 2658 batch loss 0.560990632 epoch total loss 0.665534079\n",
      "Trained batch 2659 batch loss 0.600748 epoch total loss 0.665509701\n",
      "Trained batch 2660 batch loss 0.648167193 epoch total loss 0.665503204\n",
      "Trained batch 2661 batch loss 0.627755046 epoch total loss 0.665489\n",
      "Trained batch 2662 batch loss 0.690907955 epoch total loss 0.665498614\n",
      "Trained batch 2663 batch loss 0.701228559 epoch total loss 0.665511966\n",
      "Trained batch 2664 batch loss 0.637311041 epoch total loss 0.665501416\n",
      "Trained batch 2665 batch loss 0.642251134 epoch total loss 0.665492654\n",
      "Trained batch 2666 batch loss 0.52606529 epoch total loss 0.665440381\n",
      "Trained batch 2667 batch loss 0.724245369 epoch total loss 0.665462434\n",
      "Trained batch 2668 batch loss 0.649573 epoch total loss 0.665456474\n",
      "Trained batch 2669 batch loss 0.66539669 epoch total loss 0.665456474\n",
      "Trained batch 2670 batch loss 0.656772792 epoch total loss 0.665453196\n",
      "Trained batch 2671 batch loss 0.6168347 epoch total loss 0.665434957\n",
      "Trained batch 2672 batch loss 0.618074894 epoch total loss 0.665417254\n",
      "Trained batch 2673 batch loss 0.613059461 epoch total loss 0.665397644\n",
      "Trained batch 2674 batch loss 0.650682449 epoch total loss 0.665392101\n",
      "Trained batch 2675 batch loss 0.604944229 epoch total loss 0.66536957\n",
      "Trained batch 2676 batch loss 0.720541239 epoch total loss 0.665390193\n",
      "Trained batch 2677 batch loss 0.684427857 epoch total loss 0.665397286\n",
      "Trained batch 2678 batch loss 0.615932 epoch total loss 0.665378809\n",
      "Trained batch 2679 batch loss 0.668595314 epoch total loss 0.66538\n",
      "Trained batch 2680 batch loss 0.647043645 epoch total loss 0.665373206\n",
      "Trained batch 2681 batch loss 0.620240152 epoch total loss 0.665356398\n",
      "Trained batch 2682 batch loss 0.665903747 epoch total loss 0.665356576\n",
      "Trained batch 2683 batch loss 0.642069697 epoch total loss 0.665347874\n",
      "Trained batch 2684 batch loss 0.564471126 epoch total loss 0.665310323\n",
      "Trained batch 2685 batch loss 0.516851723 epoch total loss 0.665255\n",
      "Trained batch 2686 batch loss 0.55996418 epoch total loss 0.66521579\n",
      "Trained batch 2687 batch loss 0.710490942 epoch total loss 0.665232658\n",
      "Trained batch 2688 batch loss 0.724358737 epoch total loss 0.665254653\n",
      "Trained batch 2689 batch loss 0.702891529 epoch total loss 0.66526866\n",
      "Trained batch 2690 batch loss 0.558857322 epoch total loss 0.665229082\n",
      "Trained batch 2691 batch loss 0.692420125 epoch total loss 0.665239155\n",
      "Trained batch 2692 batch loss 0.613856435 epoch total loss 0.665220082\n",
      "Trained batch 2693 batch loss 0.703493 epoch total loss 0.665234268\n",
      "Trained batch 2694 batch loss 0.712774456 epoch total loss 0.665251911\n",
      "Trained batch 2695 batch loss 0.70342195 epoch total loss 0.665266097\n",
      "Trained batch 2696 batch loss 0.594259858 epoch total loss 0.665239751\n",
      "Trained batch 2697 batch loss 0.634203494 epoch total loss 0.665228188\n",
      "Trained batch 2698 batch loss 0.698837638 epoch total loss 0.665240645\n",
      "Trained batch 2699 batch loss 0.66513741 epoch total loss 0.665240645\n",
      "Trained batch 2700 batch loss 0.678836584 epoch total loss 0.665245652\n",
      "Trained batch 2701 batch loss 0.65214318 epoch total loss 0.665240824\n",
      "Trained batch 2702 batch loss 0.646646082 epoch total loss 0.66523391\n",
      "Trained batch 2703 batch loss 0.682455122 epoch total loss 0.665240288\n",
      "Trained batch 2704 batch loss 0.667380452 epoch total loss 0.665241063\n",
      "Trained batch 2705 batch loss 0.63494122 epoch total loss 0.665229857\n",
      "Trained batch 2706 batch loss 0.683136761 epoch total loss 0.665236473\n",
      "Trained batch 2707 batch loss 0.726716161 epoch total loss 0.665259182\n",
      "Trained batch 2708 batch loss 0.755700469 epoch total loss 0.665292561\n",
      "Trained batch 2709 batch loss 0.709638953 epoch total loss 0.665308952\n",
      "Trained batch 2710 batch loss 0.695345461 epoch total loss 0.66532\n",
      "Trained batch 2711 batch loss 0.560304403 epoch total loss 0.665281296\n",
      "Trained batch 2712 batch loss 0.530904949 epoch total loss 0.665231705\n",
      "Trained batch 2713 batch loss 0.764987588 epoch total loss 0.665268481\n",
      "Trained batch 2714 batch loss 0.691451073 epoch total loss 0.665278137\n",
      "Trained batch 2715 batch loss 0.665129364 epoch total loss 0.665278077\n",
      "Trained batch 2716 batch loss 0.650340438 epoch total loss 0.665272593\n",
      "Trained batch 2717 batch loss 0.647361338 epoch total loss 0.665266\n",
      "Trained batch 2718 batch loss 0.651797652 epoch total loss 0.66526109\n",
      "Trained batch 2719 batch loss 0.692810714 epoch total loss 0.665271223\n",
      "Trained batch 2720 batch loss 0.651812732 epoch total loss 0.665266275\n",
      "Trained batch 2721 batch loss 0.647605419 epoch total loss 0.665259778\n",
      "Trained batch 2722 batch loss 0.666491747 epoch total loss 0.665260255\n",
      "Trained batch 2723 batch loss 0.641277611 epoch total loss 0.665251434\n",
      "Trained batch 2724 batch loss 0.632636905 epoch total loss 0.665239453\n",
      "Trained batch 2725 batch loss 0.66000092 epoch total loss 0.665237546\n",
      "Trained batch 2726 batch loss 0.618678391 epoch total loss 0.665220439\n",
      "Trained batch 2727 batch loss 0.580090165 epoch total loss 0.665189266\n",
      "Trained batch 2728 batch loss 0.744769096 epoch total loss 0.665218413\n",
      "Trained batch 2729 batch loss 0.706910253 epoch total loss 0.665233672\n",
      "Trained batch 2730 batch loss 0.626697779 epoch total loss 0.665219545\n",
      "Trained batch 2731 batch loss 0.676502 epoch total loss 0.665223718\n",
      "Trained batch 2732 batch loss 0.578922033 epoch total loss 0.665192127\n",
      "Trained batch 2733 batch loss 0.583295047 epoch total loss 0.665162146\n",
      "Trained batch 2734 batch loss 0.613163114 epoch total loss 0.665143132\n",
      "Trained batch 2735 batch loss 0.592386961 epoch total loss 0.665116549\n",
      "Trained batch 2736 batch loss 0.608231604 epoch total loss 0.665095747\n",
      "Trained batch 2737 batch loss 0.558446527 epoch total loss 0.665056825\n",
      "Trained batch 2738 batch loss 0.528319836 epoch total loss 0.665006876\n",
      "Trained batch 2739 batch loss 0.537701249 epoch total loss 0.664960384\n",
      "Trained batch 2740 batch loss 0.560158551 epoch total loss 0.664922178\n",
      "Trained batch 2741 batch loss 0.538140833 epoch total loss 0.664875865\n",
      "Trained batch 2742 batch loss 0.619405925 epoch total loss 0.664859295\n",
      "Trained batch 2743 batch loss 0.661176264 epoch total loss 0.664857924\n",
      "Trained batch 2744 batch loss 0.671055913 epoch total loss 0.664860189\n",
      "Trained batch 2745 batch loss 0.581566 epoch total loss 0.66482985\n",
      "Trained batch 2746 batch loss 0.564558148 epoch total loss 0.664793313\n",
      "Trained batch 2747 batch loss 0.683701694 epoch total loss 0.664800227\n",
      "Trained batch 2748 batch loss 0.659634709 epoch total loss 0.664798319\n",
      "Trained batch 2749 batch loss 0.681018829 epoch total loss 0.66480422\n",
      "Trained batch 2750 batch loss 0.716800511 epoch total loss 0.664823174\n",
      "Trained batch 2751 batch loss 0.699040353 epoch total loss 0.664835632\n",
      "Trained batch 2752 batch loss 0.595708728 epoch total loss 0.664810479\n",
      "Trained batch 2753 batch loss 0.679347 epoch total loss 0.664815784\n",
      "Trained batch 2754 batch loss 0.645856261 epoch total loss 0.664808869\n",
      "Trained batch 2755 batch loss 0.606383383 epoch total loss 0.66478765\n",
      "Trained batch 2756 batch loss 0.612006485 epoch total loss 0.664768517\n",
      "Trained batch 2757 batch loss 0.631020069 epoch total loss 0.664756298\n",
      "Trained batch 2758 batch loss 0.613594055 epoch total loss 0.664737761\n",
      "Trained batch 2759 batch loss 0.608548105 epoch total loss 0.664717376\n",
      "Trained batch 2760 batch loss 0.673681557 epoch total loss 0.664720595\n",
      "Trained batch 2761 batch loss 0.716133952 epoch total loss 0.664739251\n",
      "Trained batch 2762 batch loss 0.58393085 epoch total loss 0.664710045\n",
      "Trained batch 2763 batch loss 0.587204278 epoch total loss 0.664682\n",
      "Trained batch 2764 batch loss 0.442397237 epoch total loss 0.664601505\n",
      "Trained batch 2765 batch loss 0.476774126 epoch total loss 0.664533615\n",
      "Trained batch 2766 batch loss 0.487276524 epoch total loss 0.66446954\n",
      "Trained batch 2767 batch loss 0.54111582 epoch total loss 0.664424956\n",
      "Trained batch 2768 batch loss 0.688357234 epoch total loss 0.664433599\n",
      "Trained batch 2769 batch loss 0.697722077 epoch total loss 0.664445639\n",
      "Trained batch 2770 batch loss 0.724568963 epoch total loss 0.664467335\n",
      "Trained batch 2771 batch loss 0.751440167 epoch total loss 0.664498746\n",
      "Trained batch 2772 batch loss 0.73058331 epoch total loss 0.664522588\n",
      "Trained batch 2773 batch loss 0.711532414 epoch total loss 0.664539576\n",
      "Trained batch 2774 batch loss 0.718158484 epoch total loss 0.664558887\n",
      "Trained batch 2775 batch loss 0.716881 epoch total loss 0.664577723\n",
      "Trained batch 2776 batch loss 0.707194567 epoch total loss 0.664593101\n",
      "Epoch 2 train loss 0.6645931005477905\n",
      "Validated batch 1 batch loss 0.653750658\n",
      "Validated batch 2 batch loss 0.70597887\n",
      "Validated batch 3 batch loss 0.650446534\n",
      "Validated batch 4 batch loss 0.628109515\n",
      "Validated batch 5 batch loss 0.584294856\n",
      "Validated batch 6 batch loss 0.683484495\n",
      "Validated batch 7 batch loss 0.590277314\n",
      "Validated batch 8 batch loss 0.669609427\n",
      "Validated batch 9 batch loss 0.680619\n",
      "Validated batch 10 batch loss 0.61143589\n",
      "Validated batch 11 batch loss 0.712756813\n",
      "Validated batch 12 batch loss 0.650900483\n",
      "Validated batch 13 batch loss 0.693921089\n",
      "Validated batch 14 batch loss 0.604032278\n",
      "Validated batch 15 batch loss 0.635772\n",
      "Validated batch 16 batch loss 0.618848443\n",
      "Validated batch 17 batch loss 0.631859481\n",
      "Validated batch 18 batch loss 0.764823556\n",
      "Validated batch 19 batch loss 0.660558\n",
      "Validated batch 20 batch loss 0.654145956\n",
      "Validated batch 21 batch loss 0.606905818\n",
      "Validated batch 22 batch loss 0.644093156\n",
      "Validated batch 23 batch loss 0.586526096\n",
      "Validated batch 24 batch loss 0.621305168\n",
      "Validated batch 25 batch loss 0.636242926\n",
      "Validated batch 26 batch loss 0.713277\n",
      "Validated batch 27 batch loss 0.710552\n",
      "Validated batch 28 batch loss 0.64426291\n",
      "Validated batch 29 batch loss 0.722065389\n",
      "Validated batch 30 batch loss 0.74571085\n",
      "Validated batch 31 batch loss 0.742138684\n",
      "Validated batch 32 batch loss 0.673462629\n",
      "Validated batch 33 batch loss 0.58801496\n",
      "Validated batch 34 batch loss 0.700365603\n",
      "Validated batch 35 batch loss 0.771692336\n",
      "Validated batch 36 batch loss 0.664492428\n",
      "Validated batch 37 batch loss 0.638545513\n",
      "Validated batch 38 batch loss 0.690207839\n",
      "Validated batch 39 batch loss 0.700080276\n",
      "Validated batch 40 batch loss 0.625452459\n",
      "Validated batch 41 batch loss 0.725558758\n",
      "Validated batch 42 batch loss 0.64433676\n",
      "Validated batch 43 batch loss 0.530518472\n",
      "Validated batch 44 batch loss 0.596711338\n",
      "Validated batch 45 batch loss 0.678323448\n",
      "Validated batch 46 batch loss 0.699174643\n",
      "Validated batch 47 batch loss 0.644753337\n",
      "Validated batch 48 batch loss 0.606061041\n",
      "Validated batch 49 batch loss 0.596475542\n",
      "Validated batch 50 batch loss 0.631076455\n",
      "Validated batch 51 batch loss 0.668808579\n",
      "Validated batch 52 batch loss 0.613646626\n",
      "Validated batch 53 batch loss 0.666091204\n",
      "Validated batch 54 batch loss 0.608518898\n",
      "Validated batch 55 batch loss 0.660785258\n",
      "Validated batch 56 batch loss 0.654139936\n",
      "Validated batch 57 batch loss 0.617257714\n",
      "Validated batch 58 batch loss 0.690904617\n",
      "Validated batch 59 batch loss 0.636546731\n",
      "Validated batch 60 batch loss 0.721576452\n",
      "Validated batch 61 batch loss 0.615571678\n",
      "Validated batch 62 batch loss 0.666759253\n",
      "Validated batch 63 batch loss 0.7334764\n",
      "Validated batch 64 batch loss 0.607268929\n",
      "Validated batch 65 batch loss 0.621468782\n",
      "Validated batch 66 batch loss 0.721167862\n",
      "Validated batch 67 batch loss 0.688699067\n",
      "Validated batch 68 batch loss 0.638528049\n",
      "Validated batch 69 batch loss 0.707649171\n",
      "Validated batch 70 batch loss 0.636026859\n",
      "Validated batch 71 batch loss 0.588835597\n",
      "Validated batch 72 batch loss 0.679289043\n",
      "Validated batch 73 batch loss 0.62330997\n",
      "Validated batch 74 batch loss 0.667886734\n",
      "Validated batch 75 batch loss 0.647192\n",
      "Validated batch 76 batch loss 0.678661346\n",
      "Validated batch 77 batch loss 0.615754664\n",
      "Validated batch 78 batch loss 0.683110952\n",
      "Validated batch 79 batch loss 0.693014\n",
      "Validated batch 80 batch loss 0.725054085\n",
      "Validated batch 81 batch loss 0.618299544\n",
      "Validated batch 82 batch loss 0.746405184\n",
      "Validated batch 83 batch loss 0.646130323\n",
      "Validated batch 84 batch loss 0.59002471\n",
      "Validated batch 85 batch loss 0.689931393\n",
      "Validated batch 86 batch loss 0.729693353\n",
      "Validated batch 87 batch loss 0.653018773\n",
      "Validated batch 88 batch loss 0.643098593\n",
      "Validated batch 89 batch loss 0.724397242\n",
      "Validated batch 90 batch loss 0.555755734\n",
      "Validated batch 91 batch loss 0.684857368\n",
      "Validated batch 92 batch loss 0.688304126\n",
      "Validated batch 93 batch loss 0.671365142\n",
      "Validated batch 94 batch loss 0.629920483\n",
      "Validated batch 95 batch loss 0.64336735\n",
      "Validated batch 96 batch loss 0.647762656\n",
      "Validated batch 97 batch loss 0.641238093\n",
      "Validated batch 98 batch loss 0.684833109\n",
      "Validated batch 99 batch loss 0.623407364\n",
      "Validated batch 100 batch loss 0.591711879\n",
      "Validated batch 101 batch loss 0.6573174\n",
      "Validated batch 102 batch loss 0.649166226\n",
      "Validated batch 103 batch loss 0.670454741\n",
      "Validated batch 104 batch loss 0.719036341\n",
      "Validated batch 105 batch loss 0.650798\n",
      "Validated batch 106 batch loss 0.700274944\n",
      "Validated batch 107 batch loss 0.660374761\n",
      "Validated batch 108 batch loss 0.759853363\n",
      "Validated batch 109 batch loss 0.672539949\n",
      "Validated batch 110 batch loss 0.697490215\n",
      "Validated batch 111 batch loss 0.68358779\n",
      "Validated batch 112 batch loss 0.671880782\n",
      "Validated batch 113 batch loss 0.697687626\n",
      "Validated batch 114 batch loss 0.651227057\n",
      "Validated batch 115 batch loss 0.703461945\n",
      "Validated batch 116 batch loss 0.716250062\n",
      "Validated batch 117 batch loss 0.718417\n",
      "Validated batch 118 batch loss 0.67701\n",
      "Validated batch 119 batch loss 0.698753357\n",
      "Validated batch 120 batch loss 0.676582575\n",
      "Validated batch 121 batch loss 0.705839932\n",
      "Validated batch 122 batch loss 0.696103394\n",
      "Validated batch 123 batch loss 0.726899087\n",
      "Validated batch 124 batch loss 0.660600841\n",
      "Validated batch 125 batch loss 0.778018355\n",
      "Validated batch 126 batch loss 0.659240425\n",
      "Validated batch 127 batch loss 0.641909957\n",
      "Validated batch 128 batch loss 0.5815382\n",
      "Validated batch 129 batch loss 0.64505434\n",
      "Validated batch 130 batch loss 0.671093\n",
      "Validated batch 131 batch loss 0.694530427\n",
      "Validated batch 132 batch loss 0.705625653\n",
      "Validated batch 133 batch loss 0.713616371\n",
      "Validated batch 134 batch loss 0.637928426\n",
      "Validated batch 135 batch loss 0.638465285\n",
      "Validated batch 136 batch loss 0.676552951\n",
      "Validated batch 137 batch loss 0.681491315\n",
      "Validated batch 138 batch loss 0.619349658\n",
      "Validated batch 139 batch loss 0.628177583\n",
      "Validated batch 140 batch loss 0.637235105\n",
      "Validated batch 141 batch loss 0.710274339\n",
      "Validated batch 142 batch loss 0.627997041\n",
      "Validated batch 143 batch loss 0.679615915\n",
      "Validated batch 144 batch loss 0.627348542\n",
      "Validated batch 145 batch loss 0.519038916\n",
      "Validated batch 146 batch loss 0.641224444\n",
      "Validated batch 147 batch loss 0.645337105\n",
      "Validated batch 148 batch loss 0.575909257\n",
      "Validated batch 149 batch loss 0.710726738\n",
      "Validated batch 150 batch loss 0.683265388\n",
      "Validated batch 151 batch loss 0.629799664\n",
      "Validated batch 152 batch loss 0.628403127\n",
      "Validated batch 153 batch loss 0.654790282\n",
      "Validated batch 154 batch loss 0.546501517\n",
      "Validated batch 155 batch loss 0.608238697\n",
      "Validated batch 156 batch loss 0.645345509\n",
      "Validated batch 157 batch loss 0.657007813\n",
      "Validated batch 158 batch loss 0.640543938\n",
      "Validated batch 159 batch loss 0.614666343\n",
      "Validated batch 160 batch loss 0.591317\n",
      "Validated batch 161 batch loss 0.736947954\n",
      "Validated batch 162 batch loss 0.580928326\n",
      "Validated batch 163 batch loss 0.641212344\n",
      "Validated batch 164 batch loss 0.674626172\n",
      "Validated batch 165 batch loss 0.62413305\n",
      "Validated batch 166 batch loss 0.66485393\n",
      "Validated batch 167 batch loss 0.67425549\n",
      "Validated batch 168 batch loss 0.678968\n",
      "Validated batch 169 batch loss 0.694059372\n",
      "Validated batch 170 batch loss 0.709180415\n",
      "Validated batch 171 batch loss 0.642677665\n",
      "Validated batch 172 batch loss 0.634664416\n",
      "Validated batch 173 batch loss 0.757984102\n",
      "Validated batch 174 batch loss 0.632400513\n",
      "Validated batch 175 batch loss 0.606549859\n",
      "Validated batch 176 batch loss 0.55197227\n",
      "Validated batch 177 batch loss 0.577330887\n",
      "Validated batch 178 batch loss 0.667853594\n",
      "Validated batch 179 batch loss 0.66572392\n",
      "Validated batch 180 batch loss 0.617798328\n",
      "Validated batch 181 batch loss 0.628047526\n",
      "Validated batch 182 batch loss 0.690282\n",
      "Validated batch 183 batch loss 0.747419357\n",
      "Validated batch 184 batch loss 0.741647243\n",
      "Validated batch 185 batch loss 0.702100337\n",
      "Validated batch 186 batch loss 0.754059672\n",
      "Validated batch 187 batch loss 0.666136384\n",
      "Validated batch 188 batch loss 0.664593697\n",
      "Validated batch 189 batch loss 0.6900962\n",
      "Validated batch 190 batch loss 0.588836789\n",
      "Validated batch 191 batch loss 0.624156713\n",
      "Validated batch 192 batch loss 0.645927191\n",
      "Validated batch 193 batch loss 0.643061519\n",
      "Validated batch 194 batch loss 0.621312678\n",
      "Validated batch 195 batch loss 0.688165486\n",
      "Validated batch 196 batch loss 0.665647388\n",
      "Validated batch 197 batch loss 0.654271781\n",
      "Validated batch 198 batch loss 0.661235929\n",
      "Validated batch 199 batch loss 0.670219541\n",
      "Validated batch 200 batch loss 0.622227311\n",
      "Validated batch 201 batch loss 0.614375234\n",
      "Validated batch 202 batch loss 0.674668729\n",
      "Validated batch 203 batch loss 0.697928786\n",
      "Validated batch 204 batch loss 0.656059504\n",
      "Validated batch 205 batch loss 0.661861718\n",
      "Validated batch 206 batch loss 0.648197293\n",
      "Validated batch 207 batch loss 0.716105521\n",
      "Validated batch 208 batch loss 0.694313467\n",
      "Validated batch 209 batch loss 0.660026312\n",
      "Validated batch 210 batch loss 0.644262195\n",
      "Validated batch 211 batch loss 0.593942046\n",
      "Validated batch 212 batch loss 0.667685688\n",
      "Validated batch 213 batch loss 0.6986202\n",
      "Validated batch 214 batch loss 0.573991656\n",
      "Validated batch 215 batch loss 0.704580545\n",
      "Validated batch 216 batch loss 0.641603231\n",
      "Validated batch 217 batch loss 0.624390662\n",
      "Validated batch 218 batch loss 0.672239065\n",
      "Validated batch 219 batch loss 0.64433217\n",
      "Validated batch 220 batch loss 0.696909547\n",
      "Validated batch 221 batch loss 0.696998179\n",
      "Validated batch 222 batch loss 0.695520937\n",
      "Validated batch 223 batch loss 0.728698134\n",
      "Validated batch 224 batch loss 0.789713502\n",
      "Validated batch 225 batch loss 0.776882827\n",
      "Validated batch 226 batch loss 0.675643146\n",
      "Validated batch 227 batch loss 0.61814034\n",
      "Validated batch 228 batch loss 0.714584112\n",
      "Validated batch 229 batch loss 0.637374818\n",
      "Validated batch 230 batch loss 0.6094625\n",
      "Validated batch 231 batch loss 0.576839566\n",
      "Validated batch 232 batch loss 0.6646415\n",
      "Validated batch 233 batch loss 0.710620403\n",
      "Validated batch 234 batch loss 0.667105258\n",
      "Validated batch 235 batch loss 0.605537653\n",
      "Validated batch 236 batch loss 0.61983639\n",
      "Validated batch 237 batch loss 0.588151455\n",
      "Validated batch 238 batch loss 0.679946244\n",
      "Validated batch 239 batch loss 0.691801965\n",
      "Validated batch 240 batch loss 0.616310954\n",
      "Validated batch 241 batch loss 0.673356533\n",
      "Validated batch 242 batch loss 0.660562396\n",
      "Validated batch 243 batch loss 0.706811309\n",
      "Validated batch 244 batch loss 0.697802305\n",
      "Validated batch 245 batch loss 0.687044859\n",
      "Validated batch 246 batch loss 0.724306285\n",
      "Validated batch 247 batch loss 0.663705587\n",
      "Validated batch 248 batch loss 0.67855984\n",
      "Validated batch 249 batch loss 0.656417966\n",
      "Validated batch 250 batch loss 0.653841853\n",
      "Validated batch 251 batch loss 0.670024931\n",
      "Validated batch 252 batch loss 0.702742159\n",
      "Validated batch 253 batch loss 0.682969868\n",
      "Validated batch 254 batch loss 0.652383268\n",
      "Validated batch 255 batch loss 0.602825701\n",
      "Validated batch 256 batch loss 0.701366425\n",
      "Validated batch 257 batch loss 0.724659264\n",
      "Validated batch 258 batch loss 0.701264799\n",
      "Validated batch 259 batch loss 0.732737541\n",
      "Validated batch 260 batch loss 0.623529792\n",
      "Validated batch 261 batch loss 0.683978558\n",
      "Validated batch 262 batch loss 0.696460545\n",
      "Validated batch 263 batch loss 0.644202471\n",
      "Validated batch 264 batch loss 0.790410459\n",
      "Validated batch 265 batch loss 0.640004873\n",
      "Validated batch 266 batch loss 0.62554121\n",
      "Validated batch 267 batch loss 0.648286581\n",
      "Validated batch 268 batch loss 0.625410199\n",
      "Validated batch 269 batch loss 0.682753384\n",
      "Validated batch 270 batch loss 0.569442153\n",
      "Validated batch 271 batch loss 0.586899817\n",
      "Validated batch 272 batch loss 0.650604725\n",
      "Validated batch 273 batch loss 0.751619637\n",
      "Validated batch 274 batch loss 0.666821063\n",
      "Validated batch 275 batch loss 0.657522\n",
      "Validated batch 276 batch loss 0.626944542\n",
      "Validated batch 277 batch loss 0.664413333\n",
      "Validated batch 278 batch loss 0.614090443\n",
      "Validated batch 279 batch loss 0.64104867\n",
      "Validated batch 280 batch loss 0.656369269\n",
      "Validated batch 281 batch loss 0.71445334\n",
      "Validated batch 282 batch loss 0.696343541\n",
      "Validated batch 283 batch loss 0.669472277\n",
      "Validated batch 284 batch loss 0.6238994\n",
      "Validated batch 285 batch loss 0.667636633\n",
      "Validated batch 286 batch loss 0.719281435\n",
      "Validated batch 287 batch loss 0.739795\n",
      "Validated batch 288 batch loss 0.735872805\n",
      "Validated batch 289 batch loss 0.616903603\n",
      "Validated batch 290 batch loss 0.573259592\n",
      "Validated batch 291 batch loss 0.662020206\n",
      "Validated batch 292 batch loss 0.669357181\n",
      "Validated batch 293 batch loss 0.69874078\n",
      "Validated batch 294 batch loss 0.632551134\n",
      "Validated batch 295 batch loss 0.681039035\n",
      "Validated batch 296 batch loss 0.678659916\n",
      "Validated batch 297 batch loss 0.671361268\n",
      "Validated batch 298 batch loss 0.691928625\n",
      "Validated batch 299 batch loss 0.633581698\n",
      "Validated batch 300 batch loss 0.676441193\n",
      "Validated batch 301 batch loss 0.539988756\n",
      "Validated batch 302 batch loss 0.604167104\n",
      "Validated batch 303 batch loss 0.698657811\n",
      "Validated batch 304 batch loss 0.640726924\n",
      "Validated batch 305 batch loss 0.670127809\n",
      "Validated batch 306 batch loss 0.627456605\n",
      "Validated batch 307 batch loss 0.654512584\n",
      "Validated batch 308 batch loss 0.674334168\n",
      "Validated batch 309 batch loss 0.702447176\n",
      "Validated batch 310 batch loss 0.688964725\n",
      "Validated batch 311 batch loss 0.618753195\n",
      "Validated batch 312 batch loss 0.613411486\n",
      "Validated batch 313 batch loss 0.688195169\n",
      "Validated batch 314 batch loss 0.648594\n",
      "Validated batch 315 batch loss 0.695275366\n",
      "Validated batch 316 batch loss 0.699437737\n",
      "Validated batch 317 batch loss 0.663836956\n",
      "Validated batch 318 batch loss 0.686234\n",
      "Validated batch 319 batch loss 0.654662\n",
      "Validated batch 320 batch loss 0.639601588\n",
      "Validated batch 321 batch loss 0.677334428\n",
      "Validated batch 322 batch loss 0.597464263\n",
      "Validated batch 323 batch loss 0.67761761\n",
      "Validated batch 324 batch loss 0.652060509\n",
      "Validated batch 325 batch loss 0.70221\n",
      "Validated batch 326 batch loss 0.635592461\n",
      "Validated batch 327 batch loss 0.695760846\n",
      "Validated batch 328 batch loss 0.644096732\n",
      "Validated batch 329 batch loss 0.662839353\n",
      "Validated batch 330 batch loss 0.636665821\n",
      "Validated batch 331 batch loss 0.666313469\n",
      "Validated batch 332 batch loss 0.688816726\n",
      "Validated batch 333 batch loss 0.716598928\n",
      "Validated batch 334 batch loss 0.736620128\n",
      "Validated batch 335 batch loss 0.635962\n",
      "Validated batch 336 batch loss 0.62526381\n",
      "Validated batch 337 batch loss 0.638779938\n",
      "Validated batch 338 batch loss 0.699435592\n",
      "Validated batch 339 batch loss 0.647292137\n",
      "Validated batch 340 batch loss 0.624001861\n",
      "Validated batch 341 batch loss 0.7239272\n",
      "Validated batch 342 batch loss 0.658127904\n",
      "Validated batch 343 batch loss 0.653392196\n",
      "Validated batch 344 batch loss 0.668990314\n",
      "Validated batch 345 batch loss 0.693448126\n",
      "Validated batch 346 batch loss 0.618583918\n",
      "Validated batch 347 batch loss 0.619994\n",
      "Validated batch 348 batch loss 0.691100299\n",
      "Validated batch 349 batch loss 0.781617701\n",
      "Validated batch 350 batch loss 0.591808617\n",
      "Validated batch 351 batch loss 0.638729751\n",
      "Validated batch 352 batch loss 0.708828092\n",
      "Validated batch 353 batch loss 0.653364122\n",
      "Validated batch 354 batch loss 0.686984539\n",
      "Validated batch 355 batch loss 0.670294642\n",
      "Validated batch 356 batch loss 0.691251636\n",
      "Validated batch 357 batch loss 0.706936657\n",
      "Validated batch 358 batch loss 0.599423885\n",
      "Validated batch 359 batch loss 0.58797121\n",
      "Validated batch 360 batch loss 0.651848257\n",
      "Validated batch 361 batch loss 0.678940058\n",
      "Validated batch 362 batch loss 0.658622622\n",
      "Validated batch 363 batch loss 0.626755595\n",
      "Validated batch 364 batch loss 0.668541491\n",
      "Validated batch 365 batch loss 0.669692874\n",
      "Validated batch 366 batch loss 0.609560251\n",
      "Validated batch 367 batch loss 0.702576101\n",
      "Validated batch 368 batch loss 0.645902395\n",
      "Validated batch 369 batch loss 0.748557329\n",
      "Epoch 2 val loss 0.6621003746986389\n",
      "Model /aiffel/mpii/weights/shg_model-epoch-2-loss-0.6621.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 0.624721885 epoch total loss 0.624721885\n",
      "Trained batch 2 batch loss 0.630034864 epoch total loss 0.627378345\n",
      "Trained batch 3 batch loss 0.693778157 epoch total loss 0.649511635\n",
      "Trained batch 4 batch loss 0.656969666 epoch total loss 0.651376128\n",
      "Trained batch 5 batch loss 0.738625407 epoch total loss 0.668826\n",
      "Trained batch 6 batch loss 0.704630196 epoch total loss 0.674793422\n",
      "Trained batch 7 batch loss 0.640779138 epoch total loss 0.669934213\n",
      "Trained batch 8 batch loss 0.614954591 epoch total loss 0.663061738\n",
      "Trained batch 9 batch loss 0.637551665 epoch total loss 0.660227299\n",
      "Trained batch 10 batch loss 0.630943716 epoch total loss 0.657298923\n",
      "Trained batch 11 batch loss 0.676575124 epoch total loss 0.659051359\n",
      "Trained batch 12 batch loss 0.615427196 epoch total loss 0.655415952\n",
      "Trained batch 13 batch loss 0.587691367 epoch total loss 0.650206447\n",
      "Trained batch 14 batch loss 0.564001 epoch total loss 0.64404887\n",
      "Trained batch 15 batch loss 0.573586702 epoch total loss 0.639351428\n",
      "Trained batch 16 batch loss 0.637088299 epoch total loss 0.63921\n",
      "Trained batch 17 batch loss 0.619245887 epoch total loss 0.638035595\n",
      "Trained batch 18 batch loss 0.658784 epoch total loss 0.63918829\n",
      "Trained batch 19 batch loss 0.644645452 epoch total loss 0.639475524\n",
      "Trained batch 20 batch loss 0.667422593 epoch total loss 0.640872836\n",
      "Trained batch 21 batch loss 0.531178176 epoch total loss 0.635649323\n",
      "Trained batch 22 batch loss 0.642288 epoch total loss 0.635951102\n",
      "Trained batch 23 batch loss 0.625839949 epoch total loss 0.635511458\n",
      "Trained batch 24 batch loss 0.596863031 epoch total loss 0.633901119\n",
      "Trained batch 25 batch loss 0.624548614 epoch total loss 0.63352704\n",
      "Trained batch 26 batch loss 0.609839499 epoch total loss 0.632616\n",
      "Trained batch 27 batch loss 0.594533086 epoch total loss 0.631205499\n",
      "Trained batch 28 batch loss 0.658477724 epoch total loss 0.632179558\n",
      "Trained batch 29 batch loss 0.629544914 epoch total loss 0.632088721\n",
      "Trained batch 30 batch loss 0.600885868 epoch total loss 0.63104856\n",
      "Trained batch 31 batch loss 0.64326036 epoch total loss 0.631442547\n",
      "Trained batch 32 batch loss 0.614353776 epoch total loss 0.630908489\n",
      "Trained batch 33 batch loss 0.595497966 epoch total loss 0.629835427\n",
      "Trained batch 34 batch loss 0.575525522 epoch total loss 0.628238082\n",
      "Trained batch 35 batch loss 0.594245434 epoch total loss 0.627266884\n",
      "Trained batch 36 batch loss 0.640874743 epoch total loss 0.627644837\n",
      "Trained batch 37 batch loss 0.632861316 epoch total loss 0.627785861\n",
      "Trained batch 38 batch loss 0.597620308 epoch total loss 0.626992047\n",
      "Trained batch 39 batch loss 0.555130482 epoch total loss 0.625149429\n",
      "Trained batch 40 batch loss 0.645093739 epoch total loss 0.625648\n",
      "Trained batch 41 batch loss 0.636254132 epoch total loss 0.625906706\n",
      "Trained batch 42 batch loss 0.697612226 epoch total loss 0.627613962\n",
      "Trained batch 43 batch loss 0.64982307 epoch total loss 0.628130436\n",
      "Trained batch 44 batch loss 0.700113475 epoch total loss 0.629766405\n",
      "Trained batch 45 batch loss 0.636921048 epoch total loss 0.62992543\n",
      "Trained batch 46 batch loss 0.603708506 epoch total loss 0.62935549\n",
      "Trained batch 47 batch loss 0.708585501 epoch total loss 0.631041229\n",
      "Trained batch 48 batch loss 0.665861368 epoch total loss 0.631766617\n",
      "Trained batch 49 batch loss 0.73665303 epoch total loss 0.633907139\n",
      "Trained batch 50 batch loss 0.716011226 epoch total loss 0.635549247\n",
      "Trained batch 51 batch loss 0.721169472 epoch total loss 0.637228072\n",
      "Trained batch 52 batch loss 0.666318417 epoch total loss 0.637787461\n",
      "Trained batch 53 batch loss 0.626311958 epoch total loss 0.637570918\n",
      "Trained batch 54 batch loss 0.704006672 epoch total loss 0.638801217\n",
      "Trained batch 55 batch loss 0.71670717 epoch total loss 0.640217662\n",
      "Trained batch 56 batch loss 0.649035394 epoch total loss 0.640375137\n",
      "Trained batch 57 batch loss 0.628047 epoch total loss 0.640158892\n",
      "Trained batch 58 batch loss 0.623234928 epoch total loss 0.639867067\n",
      "Trained batch 59 batch loss 0.605521202 epoch total loss 0.639284968\n",
      "Trained batch 60 batch loss 0.585856676 epoch total loss 0.638394475\n",
      "Trained batch 61 batch loss 0.674542189 epoch total loss 0.638987064\n",
      "Trained batch 62 batch loss 0.587030828 epoch total loss 0.638149083\n",
      "Trained batch 63 batch loss 0.780267537 epoch total loss 0.64040488\n",
      "Trained batch 64 batch loss 0.746145844 epoch total loss 0.642057121\n",
      "Trained batch 65 batch loss 0.779705584 epoch total loss 0.644174755\n",
      "Trained batch 66 batch loss 0.679889798 epoch total loss 0.644715905\n",
      "Trained batch 67 batch loss 0.665727615 epoch total loss 0.645029545\n",
      "Trained batch 68 batch loss 0.64761281 epoch total loss 0.645067573\n",
      "Trained batch 69 batch loss 0.632150948 epoch total loss 0.644880354\n",
      "Trained batch 70 batch loss 0.600282252 epoch total loss 0.64424324\n",
      "Trained batch 71 batch loss 0.623956323 epoch total loss 0.643957496\n",
      "Trained batch 72 batch loss 0.676075459 epoch total loss 0.644403577\n",
      "Trained batch 73 batch loss 0.67954433 epoch total loss 0.644884944\n",
      "Trained batch 74 batch loss 0.611988902 epoch total loss 0.644440353\n",
      "Trained batch 75 batch loss 0.614937901 epoch total loss 0.644046962\n",
      "Trained batch 76 batch loss 0.659891486 epoch total loss 0.644255459\n",
      "Trained batch 77 batch loss 0.756524682 epoch total loss 0.645713508\n",
      "Trained batch 78 batch loss 0.700970531 epoch total loss 0.646421909\n",
      "Trained batch 79 batch loss 0.724350095 epoch total loss 0.647408366\n",
      "Trained batch 80 batch loss 0.748245776 epoch total loss 0.648668826\n",
      "Trained batch 81 batch loss 0.664972901 epoch total loss 0.648870111\n",
      "Trained batch 82 batch loss 0.622993529 epoch total loss 0.648554564\n",
      "Trained batch 83 batch loss 0.59642 epoch total loss 0.64792645\n",
      "Trained batch 84 batch loss 0.543511927 epoch total loss 0.646683395\n",
      "Trained batch 85 batch loss 0.59038192 epoch total loss 0.646021\n",
      "Trained batch 86 batch loss 0.573141813 epoch total loss 0.645173609\n",
      "Trained batch 87 batch loss 0.571990907 epoch total loss 0.644332409\n",
      "Trained batch 88 batch loss 0.697409928 epoch total loss 0.644935548\n",
      "Trained batch 89 batch loss 0.701126575 epoch total loss 0.64556694\n",
      "Trained batch 90 batch loss 0.683257103 epoch total loss 0.645985723\n",
      "Trained batch 91 batch loss 0.668099701 epoch total loss 0.646228731\n",
      "Trained batch 92 batch loss 0.737092078 epoch total loss 0.64721632\n",
      "Trained batch 93 batch loss 0.633297145 epoch total loss 0.647066653\n",
      "Trained batch 94 batch loss 0.57721 epoch total loss 0.646323502\n",
      "Trained batch 95 batch loss 0.526632488 epoch total loss 0.645063639\n",
      "Trained batch 96 batch loss 0.606719792 epoch total loss 0.644664228\n",
      "Trained batch 97 batch loss 0.604821682 epoch total loss 0.644253433\n",
      "Trained batch 98 batch loss 0.590331733 epoch total loss 0.643703222\n",
      "Trained batch 99 batch loss 0.704694867 epoch total loss 0.644319296\n",
      "Trained batch 100 batch loss 0.583628058 epoch total loss 0.643712401\n",
      "Trained batch 101 batch loss 0.594319284 epoch total loss 0.643223405\n",
      "Trained batch 102 batch loss 0.648396671 epoch total loss 0.643274128\n",
      "Trained batch 103 batch loss 0.668722332 epoch total loss 0.64352119\n",
      "Trained batch 104 batch loss 0.634082437 epoch total loss 0.643430412\n",
      "Trained batch 105 batch loss 0.655515969 epoch total loss 0.643545508\n",
      "Trained batch 106 batch loss 0.654033124 epoch total loss 0.643644452\n",
      "Trained batch 107 batch loss 0.655590296 epoch total loss 0.643756092\n",
      "Trained batch 108 batch loss 0.606465161 epoch total loss 0.643410861\n",
      "Trained batch 109 batch loss 0.635541558 epoch total loss 0.64333868\n",
      "Trained batch 110 batch loss 0.586455762 epoch total loss 0.64282155\n",
      "Trained batch 111 batch loss 0.554758966 epoch total loss 0.642028213\n",
      "Trained batch 112 batch loss 0.622086 epoch total loss 0.641850114\n",
      "Trained batch 113 batch loss 0.60287571 epoch total loss 0.641505241\n",
      "Trained batch 114 batch loss 0.607604265 epoch total loss 0.641207874\n",
      "Trained batch 115 batch loss 0.59799 epoch total loss 0.640832067\n",
      "Trained batch 116 batch loss 0.691443801 epoch total loss 0.641268373\n",
      "Trained batch 117 batch loss 0.510667622 epoch total loss 0.640152097\n",
      "Trained batch 118 batch loss 0.590790153 epoch total loss 0.639733791\n",
      "Trained batch 119 batch loss 0.606733322 epoch total loss 0.639456511\n",
      "Trained batch 120 batch loss 0.705924809 epoch total loss 0.640010417\n",
      "Trained batch 121 batch loss 0.593285799 epoch total loss 0.639624238\n",
      "Trained batch 122 batch loss 0.683497369 epoch total loss 0.639983833\n",
      "Trained batch 123 batch loss 0.65646863 epoch total loss 0.640117884\n",
      "Trained batch 124 batch loss 0.607262075 epoch total loss 0.639852881\n",
      "Trained batch 125 batch loss 0.600217104 epoch total loss 0.639535844\n",
      "Trained batch 126 batch loss 0.600675 epoch total loss 0.63922745\n",
      "Trained batch 127 batch loss 0.623494267 epoch total loss 0.639103591\n",
      "Trained batch 128 batch loss 0.559509277 epoch total loss 0.638481736\n",
      "Trained batch 129 batch loss 0.587567151 epoch total loss 0.638087094\n",
      "Trained batch 130 batch loss 0.566239595 epoch total loss 0.63753438\n",
      "Trained batch 131 batch loss 0.62370193 epoch total loss 0.63742882\n",
      "Trained batch 132 batch loss 0.644573689 epoch total loss 0.637482941\n",
      "Trained batch 133 batch loss 0.678066254 epoch total loss 0.637788117\n",
      "Trained batch 134 batch loss 0.612244308 epoch total loss 0.637597501\n",
      "Trained batch 135 batch loss 0.601695657 epoch total loss 0.637331545\n",
      "Trained batch 136 batch loss 0.705135643 epoch total loss 0.637830138\n",
      "Trained batch 137 batch loss 0.621072352 epoch total loss 0.63770777\n",
      "Trained batch 138 batch loss 0.610021412 epoch total loss 0.6375072\n",
      "Trained batch 139 batch loss 0.616441488 epoch total loss 0.637355626\n",
      "Trained batch 140 batch loss 0.765349865 epoch total loss 0.638269842\n",
      "Trained batch 141 batch loss 0.725769877 epoch total loss 0.638890445\n",
      "Trained batch 142 batch loss 0.746218324 epoch total loss 0.639646232\n",
      "Trained batch 143 batch loss 0.664455533 epoch total loss 0.639819741\n",
      "Trained batch 144 batch loss 0.529124618 epoch total loss 0.639051\n",
      "Trained batch 145 batch loss 0.564483881 epoch total loss 0.638536751\n",
      "Trained batch 146 batch loss 0.713886857 epoch total loss 0.639052868\n",
      "Trained batch 147 batch loss 0.839065254 epoch total loss 0.640413523\n",
      "Trained batch 148 batch loss 0.752171218 epoch total loss 0.641168654\n",
      "Trained batch 149 batch loss 0.685638905 epoch total loss 0.641467094\n",
      "Trained batch 150 batch loss 0.743999064 epoch total loss 0.64215064\n",
      "Trained batch 151 batch loss 0.632057369 epoch total loss 0.642083764\n",
      "Trained batch 152 batch loss 0.622073174 epoch total loss 0.641952097\n",
      "Trained batch 153 batch loss 0.682264447 epoch total loss 0.64221561\n",
      "Trained batch 154 batch loss 0.633236527 epoch total loss 0.642157316\n",
      "Trained batch 155 batch loss 0.624168396 epoch total loss 0.642041266\n",
      "Trained batch 156 batch loss 0.563471437 epoch total loss 0.641537607\n",
      "Trained batch 157 batch loss 0.56127578 epoch total loss 0.641026378\n",
      "Trained batch 158 batch loss 0.65264833 epoch total loss 0.64109993\n",
      "Trained batch 159 batch loss 0.678258777 epoch total loss 0.641333699\n",
      "Trained batch 160 batch loss 0.676414967 epoch total loss 0.641552925\n",
      "Trained batch 161 batch loss 0.645365059 epoch total loss 0.641576588\n",
      "Trained batch 162 batch loss 0.658399284 epoch total loss 0.641680419\n",
      "Trained batch 163 batch loss 0.59577024 epoch total loss 0.641398787\n",
      "Trained batch 164 batch loss 0.706028342 epoch total loss 0.641792893\n",
      "Trained batch 165 batch loss 0.581810772 epoch total loss 0.641429365\n",
      "Trained batch 166 batch loss 0.638708949 epoch total loss 0.641413\n",
      "Trained batch 167 batch loss 0.600037873 epoch total loss 0.641165197\n",
      "Trained batch 168 batch loss 0.638139307 epoch total loss 0.641147196\n",
      "Trained batch 169 batch loss 0.58096844 epoch total loss 0.640791118\n",
      "Trained batch 170 batch loss 0.616458 epoch total loss 0.640648\n",
      "Trained batch 171 batch loss 0.602958143 epoch total loss 0.640427589\n",
      "Trained batch 172 batch loss 0.615375 epoch total loss 0.640281916\n",
      "Trained batch 173 batch loss 0.597156763 epoch total loss 0.640032649\n",
      "Trained batch 174 batch loss 0.661818624 epoch total loss 0.640157878\n",
      "Trained batch 175 batch loss 0.640978694 epoch total loss 0.640162528\n",
      "Trained batch 176 batch loss 0.571807146 epoch total loss 0.639774144\n",
      "Trained batch 177 batch loss 0.535518467 epoch total loss 0.639185131\n",
      "Trained batch 178 batch loss 0.653345644 epoch total loss 0.639264643\n",
      "Trained batch 179 batch loss 0.663954496 epoch total loss 0.639402568\n",
      "Trained batch 180 batch loss 0.687036693 epoch total loss 0.639667213\n",
      "Trained batch 181 batch loss 0.648453534 epoch total loss 0.639715731\n",
      "Trained batch 182 batch loss 0.672809362 epoch total loss 0.639897585\n",
      "Trained batch 183 batch loss 0.707500815 epoch total loss 0.640267\n",
      "Trained batch 184 batch loss 0.629132628 epoch total loss 0.640206516\n",
      "Trained batch 185 batch loss 0.576427579 epoch total loss 0.639861763\n",
      "Trained batch 186 batch loss 0.580352485 epoch total loss 0.639541805\n",
      "Trained batch 187 batch loss 0.580067456 epoch total loss 0.639223814\n",
      "Trained batch 188 batch loss 0.540492415 epoch total loss 0.638698637\n",
      "Trained batch 189 batch loss 0.540125847 epoch total loss 0.638177037\n",
      "Trained batch 190 batch loss 0.562478483 epoch total loss 0.63777864\n",
      "Trained batch 191 batch loss 0.57553637 epoch total loss 0.637452781\n",
      "Trained batch 192 batch loss 0.599165857 epoch total loss 0.637253344\n",
      "Trained batch 193 batch loss 0.624019504 epoch total loss 0.637184799\n",
      "Trained batch 194 batch loss 0.731637597 epoch total loss 0.637671649\n",
      "Trained batch 195 batch loss 0.6359092 epoch total loss 0.63766259\n",
      "Trained batch 196 batch loss 0.666225731 epoch total loss 0.637808323\n",
      "Trained batch 197 batch loss 0.582753181 epoch total loss 0.637528896\n",
      "Trained batch 198 batch loss 0.561336458 epoch total loss 0.637144089\n",
      "Trained batch 199 batch loss 0.557093143 epoch total loss 0.636741817\n",
      "Trained batch 200 batch loss 0.573379934 epoch total loss 0.636424959\n",
      "Trained batch 201 batch loss 0.536947 epoch total loss 0.635930061\n",
      "Trained batch 202 batch loss 0.590681791 epoch total loss 0.635706067\n",
      "Trained batch 203 batch loss 0.62173903 epoch total loss 0.635637283\n",
      "Trained batch 204 batch loss 0.79329139 epoch total loss 0.636410058\n",
      "Trained batch 205 batch loss 0.715325177 epoch total loss 0.636795044\n",
      "Trained batch 206 batch loss 0.643208742 epoch total loss 0.636826158\n",
      "Trained batch 207 batch loss 0.649339557 epoch total loss 0.636886597\n",
      "Trained batch 208 batch loss 0.556195736 epoch total loss 0.63649869\n",
      "Trained batch 209 batch loss 0.627476811 epoch total loss 0.636455476\n",
      "Trained batch 210 batch loss 0.592385828 epoch total loss 0.636245668\n",
      "Trained batch 211 batch loss 0.515217423 epoch total loss 0.635672033\n",
      "Trained batch 212 batch loss 0.512611151 epoch total loss 0.635091543\n",
      "Trained batch 213 batch loss 0.443948507 epoch total loss 0.634194195\n",
      "Trained batch 214 batch loss 0.463827193 epoch total loss 0.633398056\n",
      "Trained batch 215 batch loss 0.419357657 epoch total loss 0.63240248\n",
      "Trained batch 216 batch loss 0.520925581 epoch total loss 0.631886363\n",
      "Trained batch 217 batch loss 0.558102131 epoch total loss 0.631546378\n",
      "Trained batch 218 batch loss 0.527851939 epoch total loss 0.631070673\n",
      "Trained batch 219 batch loss 0.605110168 epoch total loss 0.630952179\n",
      "Trained batch 220 batch loss 0.70590204 epoch total loss 0.63129288\n",
      "Trained batch 221 batch loss 0.650453448 epoch total loss 0.631379545\n",
      "Trained batch 222 batch loss 0.727267504 epoch total loss 0.63181144\n",
      "Trained batch 223 batch loss 0.736010849 epoch total loss 0.63227874\n",
      "Trained batch 224 batch loss 0.708375931 epoch total loss 0.632618427\n",
      "Trained batch 225 batch loss 0.84946543 epoch total loss 0.633582234\n",
      "Trained batch 226 batch loss 0.809690952 epoch total loss 0.634361446\n",
      "Trained batch 227 batch loss 0.76462245 epoch total loss 0.63493526\n",
      "Trained batch 228 batch loss 0.697362721 epoch total loss 0.635209084\n",
      "Trained batch 229 batch loss 0.63326025 epoch total loss 0.63520056\n",
      "Trained batch 230 batch loss 0.763704538 epoch total loss 0.635759234\n",
      "Trained batch 231 batch loss 0.600597262 epoch total loss 0.635607064\n",
      "Trained batch 232 batch loss 0.747581601 epoch total loss 0.636089742\n",
      "Trained batch 233 batch loss 0.657102 epoch total loss 0.636179924\n",
      "Trained batch 234 batch loss 0.688713 epoch total loss 0.636404395\n",
      "Trained batch 235 batch loss 0.563088119 epoch total loss 0.636092424\n",
      "Trained batch 236 batch loss 0.565633535 epoch total loss 0.635793865\n",
      "Trained batch 237 batch loss 0.628732264 epoch total loss 0.635764062\n",
      "Trained batch 238 batch loss 0.650634646 epoch total loss 0.635826588\n",
      "Trained batch 239 batch loss 0.676826656 epoch total loss 0.63599813\n",
      "Trained batch 240 batch loss 0.693252385 epoch total loss 0.636236727\n",
      "Trained batch 241 batch loss 0.725807309 epoch total loss 0.636608422\n",
      "Trained batch 242 batch loss 0.701727569 epoch total loss 0.636877477\n",
      "Trained batch 243 batch loss 0.711801708 epoch total loss 0.637185812\n",
      "Trained batch 244 batch loss 0.622377038 epoch total loss 0.637125134\n",
      "Trained batch 245 batch loss 0.638012528 epoch total loss 0.63712877\n",
      "Trained batch 246 batch loss 0.569632113 epoch total loss 0.636854351\n",
      "Trained batch 247 batch loss 0.573682904 epoch total loss 0.636598587\n",
      "Trained batch 248 batch loss 0.620512426 epoch total loss 0.636533737\n",
      "Trained batch 249 batch loss 0.565209508 epoch total loss 0.636247337\n",
      "Trained batch 250 batch loss 0.585297346 epoch total loss 0.636043489\n",
      "Trained batch 251 batch loss 0.577814698 epoch total loss 0.635811567\n",
      "Trained batch 252 batch loss 0.600309849 epoch total loss 0.635670662\n",
      "Trained batch 253 batch loss 0.568126738 epoch total loss 0.635403693\n",
      "Trained batch 254 batch loss 0.604510427 epoch total loss 0.635282099\n",
      "Trained batch 255 batch loss 0.601017535 epoch total loss 0.635147691\n",
      "Trained batch 256 batch loss 0.577344894 epoch total loss 0.634921908\n",
      "Trained batch 257 batch loss 0.546354771 epoch total loss 0.634577274\n",
      "Trained batch 258 batch loss 0.578103662 epoch total loss 0.634358406\n",
      "Trained batch 259 batch loss 0.642208934 epoch total loss 0.634388745\n",
      "Trained batch 260 batch loss 0.631439447 epoch total loss 0.63437742\n",
      "Trained batch 261 batch loss 0.626012325 epoch total loss 0.634345353\n",
      "Trained batch 262 batch loss 0.656525493 epoch total loss 0.63443\n",
      "Trained batch 263 batch loss 0.637921095 epoch total loss 0.634443283\n",
      "Trained batch 264 batch loss 0.622721791 epoch total loss 0.634398878\n",
      "Trained batch 265 batch loss 0.647832394 epoch total loss 0.634449542\n",
      "Trained batch 266 batch loss 0.578307569 epoch total loss 0.634238482\n",
      "Trained batch 267 batch loss 0.56145072 epoch total loss 0.633965909\n",
      "Trained batch 268 batch loss 0.630328715 epoch total loss 0.63395232\n",
      "Trained batch 269 batch loss 0.778783381 epoch total loss 0.634490669\n",
      "Trained batch 270 batch loss 0.727836072 epoch total loss 0.634836376\n",
      "Trained batch 271 batch loss 0.735558629 epoch total loss 0.63520807\n",
      "Trained batch 272 batch loss 0.691226 epoch total loss 0.635414\n",
      "Trained batch 273 batch loss 0.738102317 epoch total loss 0.635790169\n",
      "Trained batch 274 batch loss 0.658877 epoch total loss 0.635874391\n",
      "Trained batch 275 batch loss 0.664200604 epoch total loss 0.635977387\n",
      "Trained batch 276 batch loss 0.610332131 epoch total loss 0.635884464\n",
      "Trained batch 277 batch loss 0.619194388 epoch total loss 0.635824263\n",
      "Trained batch 278 batch loss 0.604556918 epoch total loss 0.635711789\n",
      "Trained batch 279 batch loss 0.636878073 epoch total loss 0.635715961\n",
      "Trained batch 280 batch loss 0.651130736 epoch total loss 0.635771036\n",
      "Trained batch 281 batch loss 0.590587616 epoch total loss 0.635610223\n",
      "Trained batch 282 batch loss 0.676124036 epoch total loss 0.63575387\n",
      "Trained batch 283 batch loss 0.679908037 epoch total loss 0.635909855\n",
      "Trained batch 284 batch loss 0.695680082 epoch total loss 0.636120319\n",
      "Trained batch 285 batch loss 0.667748094 epoch total loss 0.636231303\n",
      "Trained batch 286 batch loss 0.796685815 epoch total loss 0.636792362\n",
      "Trained batch 287 batch loss 0.725312293 epoch total loss 0.637100816\n",
      "Trained batch 288 batch loss 0.764371455 epoch total loss 0.637542725\n",
      "Trained batch 289 batch loss 0.613798618 epoch total loss 0.637460589\n",
      "Trained batch 290 batch loss 0.636848569 epoch total loss 0.637458503\n",
      "Trained batch 291 batch loss 0.615751863 epoch total loss 0.637383878\n",
      "Trained batch 292 batch loss 0.728607059 epoch total loss 0.637696326\n",
      "Trained batch 293 batch loss 0.66782 epoch total loss 0.637799084\n",
      "Trained batch 294 batch loss 0.726958513 epoch total loss 0.638102353\n",
      "Trained batch 295 batch loss 0.641225934 epoch total loss 0.638112962\n",
      "Trained batch 296 batch loss 0.652923405 epoch total loss 0.638163\n",
      "Trained batch 297 batch loss 0.699392378 epoch total loss 0.638369143\n",
      "Trained batch 298 batch loss 0.731502175 epoch total loss 0.63868165\n",
      "Trained batch 299 batch loss 0.704245 epoch total loss 0.638900936\n",
      "Trained batch 300 batch loss 0.682291806 epoch total loss 0.639045537\n",
      "Trained batch 301 batch loss 0.657245576 epoch total loss 0.639106035\n",
      "Trained batch 302 batch loss 0.507346451 epoch total loss 0.638669729\n",
      "Trained batch 303 batch loss 0.544289947 epoch total loss 0.638358235\n",
      "Trained batch 304 batch loss 0.573009431 epoch total loss 0.638143301\n",
      "Trained batch 305 batch loss 0.574359298 epoch total loss 0.637934148\n",
      "Trained batch 306 batch loss 0.626985908 epoch total loss 0.637898386\n",
      "Trained batch 307 batch loss 0.568591893 epoch total loss 0.637672603\n",
      "Trained batch 308 batch loss 0.621219277 epoch total loss 0.637619138\n",
      "Trained batch 309 batch loss 0.543277502 epoch total loss 0.637313843\n",
      "Trained batch 310 batch loss 0.5850842 epoch total loss 0.63714534\n",
      "Trained batch 311 batch loss 0.602849245 epoch total loss 0.637035072\n",
      "Trained batch 312 batch loss 0.644544899 epoch total loss 0.637059152\n",
      "Trained batch 313 batch loss 0.720781922 epoch total loss 0.637326598\n",
      "Trained batch 314 batch loss 0.665150642 epoch total loss 0.63741523\n",
      "Trained batch 315 batch loss 0.733227193 epoch total loss 0.637719393\n",
      "Trained batch 316 batch loss 0.676051915 epoch total loss 0.637840688\n",
      "Trained batch 317 batch loss 0.646238446 epoch total loss 0.637867212\n",
      "Trained batch 318 batch loss 0.620037556 epoch total loss 0.637811124\n",
      "Trained batch 319 batch loss 0.626096308 epoch total loss 0.637774408\n",
      "Trained batch 320 batch loss 0.57798934 epoch total loss 0.637587607\n",
      "Trained batch 321 batch loss 0.579920292 epoch total loss 0.637407959\n",
      "Trained batch 322 batch loss 0.568150818 epoch total loss 0.637192845\n",
      "Trained batch 323 batch loss 0.572880507 epoch total loss 0.636993766\n",
      "Trained batch 324 batch loss 0.649050772 epoch total loss 0.637030959\n",
      "Trained batch 325 batch loss 0.58318758 epoch total loss 0.636865258\n",
      "Trained batch 326 batch loss 0.623406291 epoch total loss 0.636824\n",
      "Trained batch 327 batch loss 0.650739193 epoch total loss 0.63686657\n",
      "Trained batch 328 batch loss 0.61146605 epoch total loss 0.636789143\n",
      "Trained batch 329 batch loss 0.699394226 epoch total loss 0.636979401\n",
      "Trained batch 330 batch loss 0.73946172 epoch total loss 0.637289941\n",
      "Trained batch 331 batch loss 0.727143884 epoch total loss 0.637561381\n",
      "Trained batch 332 batch loss 0.618781328 epoch total loss 0.637504816\n",
      "Trained batch 333 batch loss 0.657180727 epoch total loss 0.637563884\n",
      "Trained batch 334 batch loss 0.639465392 epoch total loss 0.637569606\n",
      "Trained batch 335 batch loss 0.649094105 epoch total loss 0.637604\n",
      "Trained batch 336 batch loss 0.601584494 epoch total loss 0.637496769\n",
      "Trained batch 337 batch loss 0.620494485 epoch total loss 0.637446344\n",
      "Trained batch 338 batch loss 0.680798709 epoch total loss 0.637574613\n",
      "Trained batch 339 batch loss 0.596547663 epoch total loss 0.637453556\n",
      "Trained batch 340 batch loss 0.623890042 epoch total loss 0.637413681\n",
      "Trained batch 341 batch loss 0.624464691 epoch total loss 0.637375712\n",
      "Trained batch 342 batch loss 0.62167871 epoch total loss 0.637329757\n",
      "Trained batch 343 batch loss 0.583281815 epoch total loss 0.637172222\n",
      "Trained batch 344 batch loss 0.669960558 epoch total loss 0.63726753\n",
      "Trained batch 345 batch loss 0.584903657 epoch total loss 0.637115717\n",
      "Trained batch 346 batch loss 0.757083714 epoch total loss 0.637462437\n",
      "Trained batch 347 batch loss 0.64233315 epoch total loss 0.637476504\n",
      "Trained batch 348 batch loss 0.593371272 epoch total loss 0.637349725\n",
      "Trained batch 349 batch loss 0.550107 epoch total loss 0.637099802\n",
      "Trained batch 350 batch loss 0.585028827 epoch total loss 0.63695097\n",
      "Trained batch 351 batch loss 0.54606986 epoch total loss 0.636692047\n",
      "Trained batch 352 batch loss 0.577072501 epoch total loss 0.63652271\n",
      "Trained batch 353 batch loss 0.596746743 epoch total loss 0.63641\n",
      "Trained batch 354 batch loss 0.577870727 epoch total loss 0.636244595\n",
      "Trained batch 355 batch loss 0.627993107 epoch total loss 0.636221349\n",
      "Trained batch 356 batch loss 0.695022345 epoch total loss 0.636386514\n",
      "Trained batch 357 batch loss 0.680876791 epoch total loss 0.636511147\n",
      "Trained batch 358 batch loss 0.626900196 epoch total loss 0.636484325\n",
      "Trained batch 359 batch loss 0.649912119 epoch total loss 0.636521757\n",
      "Trained batch 360 batch loss 0.67276293 epoch total loss 0.636622429\n",
      "Trained batch 361 batch loss 0.691005349 epoch total loss 0.63677305\n",
      "Trained batch 362 batch loss 0.678979635 epoch total loss 0.636889696\n",
      "Trained batch 363 batch loss 0.638130128 epoch total loss 0.636893094\n",
      "Trained batch 364 batch loss 0.719002485 epoch total loss 0.637118638\n",
      "Trained batch 365 batch loss 0.569232643 epoch total loss 0.636932671\n",
      "Trained batch 366 batch loss 0.622697055 epoch total loss 0.636893749\n",
      "Trained batch 367 batch loss 0.721171498 epoch total loss 0.637123406\n",
      "Trained batch 368 batch loss 0.667082965 epoch total loss 0.637204826\n",
      "Trained batch 369 batch loss 0.64143914 epoch total loss 0.63721627\n",
      "Trained batch 370 batch loss 0.649770856 epoch total loss 0.637250185\n",
      "Trained batch 371 batch loss 0.614324212 epoch total loss 0.637188375\n",
      "Trained batch 372 batch loss 0.64934361 epoch total loss 0.637221038\n",
      "Trained batch 373 batch loss 0.688854039 epoch total loss 0.6373595\n",
      "Trained batch 374 batch loss 0.643291354 epoch total loss 0.637375355\n",
      "Trained batch 375 batch loss 0.68343246 epoch total loss 0.63749814\n",
      "Trained batch 376 batch loss 0.689650238 epoch total loss 0.6376369\n",
      "Trained batch 377 batch loss 0.691093802 epoch total loss 0.637778699\n",
      "Trained batch 378 batch loss 0.646143258 epoch total loss 0.637800813\n",
      "Trained batch 379 batch loss 0.663051307 epoch total loss 0.637867451\n",
      "Trained batch 380 batch loss 0.617432654 epoch total loss 0.637813687\n",
      "Trained batch 381 batch loss 0.650904298 epoch total loss 0.637848079\n",
      "Trained batch 382 batch loss 0.666809916 epoch total loss 0.637923896\n",
      "Trained batch 383 batch loss 0.733629704 epoch total loss 0.638173759\n",
      "Trained batch 384 batch loss 0.681964517 epoch total loss 0.638287783\n",
      "Trained batch 385 batch loss 0.771481574 epoch total loss 0.638633728\n",
      "Trained batch 386 batch loss 0.738691509 epoch total loss 0.638892949\n",
      "Trained batch 387 batch loss 0.654959321 epoch total loss 0.638934493\n",
      "Trained batch 388 batch loss 0.615536928 epoch total loss 0.638874173\n",
      "Trained batch 389 batch loss 0.620156407 epoch total loss 0.638826072\n",
      "Trained batch 390 batch loss 0.609859645 epoch total loss 0.638751805\n",
      "Trained batch 391 batch loss 0.69508338 epoch total loss 0.638895869\n",
      "Trained batch 392 batch loss 0.679225802 epoch total loss 0.638998747\n",
      "Trained batch 393 batch loss 0.741488099 epoch total loss 0.639259577\n",
      "Trained batch 394 batch loss 0.625843167 epoch total loss 0.639225483\n",
      "Trained batch 395 batch loss 0.744997859 epoch total loss 0.639493287\n",
      "Trained batch 396 batch loss 0.650994956 epoch total loss 0.639522314\n",
      "Trained batch 397 batch loss 0.640845358 epoch total loss 0.639525652\n",
      "Trained batch 398 batch loss 0.699470162 epoch total loss 0.639676213\n",
      "Trained batch 399 batch loss 0.654125452 epoch total loss 0.639712453\n",
      "Trained batch 400 batch loss 0.656982124 epoch total loss 0.639755607\n",
      "Trained batch 401 batch loss 0.685129225 epoch total loss 0.639868736\n",
      "Trained batch 402 batch loss 0.668945253 epoch total loss 0.639941096\n",
      "Trained batch 403 batch loss 0.617319107 epoch total loss 0.639884949\n",
      "Trained batch 404 batch loss 0.555032849 epoch total loss 0.639674902\n",
      "Trained batch 405 batch loss 0.60100019 epoch total loss 0.639579415\n",
      "Trained batch 406 batch loss 0.591925 epoch total loss 0.639462054\n",
      "Trained batch 407 batch loss 0.6390118 epoch total loss 0.639460921\n",
      "Trained batch 408 batch loss 0.523769557 epoch total loss 0.639177382\n",
      "Trained batch 409 batch loss 0.488201976 epoch total loss 0.638808191\n",
      "Trained batch 410 batch loss 0.514254749 epoch total loss 0.638504386\n",
      "Trained batch 411 batch loss 0.546203494 epoch total loss 0.638279796\n",
      "Trained batch 412 batch loss 0.572492421 epoch total loss 0.638120115\n",
      "Trained batch 413 batch loss 0.576642036 epoch total loss 0.637971222\n",
      "Trained batch 414 batch loss 0.567510664 epoch total loss 0.637801\n",
      "Trained batch 415 batch loss 0.574987948 epoch total loss 0.637649655\n",
      "Trained batch 416 batch loss 0.542564571 epoch total loss 0.637421072\n",
      "Trained batch 417 batch loss 0.50173068 epoch total loss 0.63709569\n",
      "Trained batch 418 batch loss 0.463423938 epoch total loss 0.636680186\n",
      "Trained batch 419 batch loss 0.44769156 epoch total loss 0.636229157\n",
      "Trained batch 420 batch loss 0.458060801 epoch total loss 0.635804951\n",
      "Trained batch 421 batch loss 0.472130507 epoch total loss 0.63541621\n",
      "Trained batch 422 batch loss 0.417779207 epoch total loss 0.63490051\n",
      "Trained batch 423 batch loss 0.490239263 epoch total loss 0.634558499\n",
      "Trained batch 424 batch loss 0.628287375 epoch total loss 0.634543717\n",
      "Trained batch 425 batch loss 0.625343561 epoch total loss 0.63452208\n",
      "Trained batch 426 batch loss 0.673312664 epoch total loss 0.634613097\n",
      "Trained batch 427 batch loss 0.6223104 epoch total loss 0.634584308\n",
      "Trained batch 428 batch loss 0.590245247 epoch total loss 0.634480715\n",
      "Trained batch 429 batch loss 0.594275713 epoch total loss 0.634386957\n",
      "Trained batch 430 batch loss 0.630438864 epoch total loss 0.634377778\n",
      "Trained batch 431 batch loss 0.60974592 epoch total loss 0.634320617\n",
      "Trained batch 432 batch loss 0.567024589 epoch total loss 0.63416481\n",
      "Trained batch 433 batch loss 0.662116051 epoch total loss 0.634229362\n",
      "Trained batch 434 batch loss 0.702036202 epoch total loss 0.634385586\n",
      "Trained batch 435 batch loss 0.635731 epoch total loss 0.634388685\n",
      "Trained batch 436 batch loss 0.696696877 epoch total loss 0.634531558\n",
      "Trained batch 437 batch loss 0.68180871 epoch total loss 0.6346398\n",
      "Trained batch 438 batch loss 0.593637586 epoch total loss 0.634546161\n",
      "Trained batch 439 batch loss 0.595117867 epoch total loss 0.634456336\n",
      "Trained batch 440 batch loss 0.593043 epoch total loss 0.634362221\n",
      "Trained batch 441 batch loss 0.645251513 epoch total loss 0.634386957\n",
      "Trained batch 442 batch loss 0.620572925 epoch total loss 0.634355724\n",
      "Trained batch 443 batch loss 0.618257403 epoch total loss 0.634319365\n",
      "Trained batch 444 batch loss 0.594171 epoch total loss 0.634228945\n",
      "Trained batch 445 batch loss 0.637757182 epoch total loss 0.634236872\n",
      "Trained batch 446 batch loss 0.663798511 epoch total loss 0.634303153\n",
      "Trained batch 447 batch loss 0.650935709 epoch total loss 0.634340346\n",
      "Trained batch 448 batch loss 0.614492297 epoch total loss 0.63429606\n",
      "Trained batch 449 batch loss 0.691736579 epoch total loss 0.634424031\n",
      "Trained batch 450 batch loss 0.697433054 epoch total loss 0.634564\n",
      "Trained batch 451 batch loss 0.626323938 epoch total loss 0.634545684\n",
      "Trained batch 452 batch loss 0.66230011 epoch total loss 0.634607077\n",
      "Trained batch 453 batch loss 0.647385 epoch total loss 0.634635329\n",
      "Trained batch 454 batch loss 0.676355064 epoch total loss 0.63472724\n",
      "Trained batch 455 batch loss 0.69233 epoch total loss 0.63485384\n",
      "Trained batch 456 batch loss 0.655676723 epoch total loss 0.634899497\n",
      "Trained batch 457 batch loss 0.709961951 epoch total loss 0.635063708\n",
      "Trained batch 458 batch loss 0.590219796 epoch total loss 0.634965777\n",
      "Trained batch 459 batch loss 0.696083784 epoch total loss 0.635098934\n",
      "Trained batch 460 batch loss 0.752764046 epoch total loss 0.635354757\n",
      "Trained batch 461 batch loss 0.766073 epoch total loss 0.635638297\n",
      "Trained batch 462 batch loss 0.745685875 epoch total loss 0.635876536\n",
      "Trained batch 463 batch loss 0.660627842 epoch total loss 0.63593\n",
      "Trained batch 464 batch loss 0.697479844 epoch total loss 0.636062622\n",
      "Trained batch 465 batch loss 0.544230819 epoch total loss 0.635865092\n",
      "Trained batch 466 batch loss 0.60333 epoch total loss 0.635795295\n",
      "Trained batch 467 batch loss 0.615729451 epoch total loss 0.63575232\n",
      "Trained batch 468 batch loss 0.652610064 epoch total loss 0.635788381\n",
      "Trained batch 469 batch loss 0.61436975 epoch total loss 0.635742724\n",
      "Trained batch 470 batch loss 0.584005296 epoch total loss 0.635632634\n",
      "Trained batch 471 batch loss 0.590690553 epoch total loss 0.635537267\n",
      "Trained batch 472 batch loss 0.540998399 epoch total loss 0.635336936\n",
      "Trained batch 473 batch loss 0.560224354 epoch total loss 0.635178089\n",
      "Trained batch 474 batch loss 0.606473625 epoch total loss 0.635117531\n",
      "Trained batch 475 batch loss 0.65936029 epoch total loss 0.635168612\n",
      "Trained batch 476 batch loss 0.595679939 epoch total loss 0.635085583\n",
      "Trained batch 477 batch loss 0.580950797 epoch total loss 0.634972155\n",
      "Trained batch 478 batch loss 0.569492459 epoch total loss 0.634835124\n",
      "Trained batch 479 batch loss 0.501472116 epoch total loss 0.634556711\n",
      "Trained batch 480 batch loss 0.462005675 epoch total loss 0.634197235\n",
      "Trained batch 481 batch loss 0.530537307 epoch total loss 0.633981764\n",
      "Trained batch 482 batch loss 0.646224797 epoch total loss 0.634007096\n",
      "Trained batch 483 batch loss 0.703369439 epoch total loss 0.634150743\n",
      "Trained batch 484 batch loss 0.752527475 epoch total loss 0.634395301\n",
      "Trained batch 485 batch loss 0.725604892 epoch total loss 0.634583414\n",
      "Trained batch 486 batch loss 0.648444176 epoch total loss 0.634611905\n",
      "Trained batch 487 batch loss 0.718201876 epoch total loss 0.634783566\n",
      "Trained batch 488 batch loss 0.681866765 epoch total loss 0.63488\n",
      "Trained batch 489 batch loss 0.647845566 epoch total loss 0.63490653\n",
      "Trained batch 490 batch loss 0.655190349 epoch total loss 0.634947896\n",
      "Trained batch 491 batch loss 0.772663891 epoch total loss 0.635228395\n",
      "Trained batch 492 batch loss 0.725182772 epoch total loss 0.635411263\n",
      "Trained batch 493 batch loss 0.621960461 epoch total loss 0.635383964\n",
      "Trained batch 494 batch loss 0.634656668 epoch total loss 0.635382473\n",
      "Trained batch 495 batch loss 0.620453477 epoch total loss 0.635352314\n",
      "Trained batch 496 batch loss 0.700556874 epoch total loss 0.635483801\n",
      "Trained batch 497 batch loss 0.649488628 epoch total loss 0.635511935\n",
      "Trained batch 498 batch loss 0.732301891 epoch total loss 0.635706306\n",
      "Trained batch 499 batch loss 0.709071696 epoch total loss 0.635853291\n",
      "Trained batch 500 batch loss 0.657375038 epoch total loss 0.635896385\n",
      "Trained batch 501 batch loss 0.650271475 epoch total loss 0.635925055\n",
      "Trained batch 502 batch loss 0.612294555 epoch total loss 0.635878\n",
      "Trained batch 503 batch loss 0.560780704 epoch total loss 0.635728717\n",
      "Trained batch 504 batch loss 0.543095231 epoch total loss 0.635544896\n",
      "Trained batch 505 batch loss 0.633538127 epoch total loss 0.635540962\n",
      "Trained batch 506 batch loss 0.544756174 epoch total loss 0.635361552\n",
      "Trained batch 507 batch loss 0.526898384 epoch total loss 0.635147631\n",
      "Trained batch 508 batch loss 0.530588448 epoch total loss 0.634941757\n",
      "Trained batch 509 batch loss 0.565449536 epoch total loss 0.634805262\n",
      "Trained batch 510 batch loss 0.606248081 epoch total loss 0.634749293\n",
      "Trained batch 511 batch loss 0.63870126 epoch total loss 0.634757042\n",
      "Trained batch 512 batch loss 0.62898612 epoch total loss 0.634745777\n",
      "Trained batch 513 batch loss 0.655050099 epoch total loss 0.634785354\n",
      "Trained batch 514 batch loss 0.641017914 epoch total loss 0.634797513\n",
      "Trained batch 515 batch loss 0.703531265 epoch total loss 0.634930968\n",
      "Trained batch 516 batch loss 0.611398697 epoch total loss 0.634885311\n",
      "Trained batch 517 batch loss 0.757297873 epoch total loss 0.63512212\n",
      "Trained batch 518 batch loss 0.674576163 epoch total loss 0.635198295\n",
      "Trained batch 519 batch loss 0.711237967 epoch total loss 0.635344803\n",
      "Trained batch 520 batch loss 0.607925951 epoch total loss 0.635292113\n",
      "Trained batch 521 batch loss 0.606046438 epoch total loss 0.635235965\n",
      "Trained batch 522 batch loss 0.630018055 epoch total loss 0.635225952\n",
      "Trained batch 523 batch loss 0.699086726 epoch total loss 0.635348082\n",
      "Trained batch 524 batch loss 0.687355936 epoch total loss 0.635447323\n",
      "Trained batch 525 batch loss 0.497684 epoch total loss 0.635184884\n",
      "Trained batch 526 batch loss 0.572120786 epoch total loss 0.635065\n",
      "Trained batch 527 batch loss 0.635185719 epoch total loss 0.635065258\n",
      "Trained batch 528 batch loss 0.652679503 epoch total loss 0.635098577\n",
      "Trained batch 529 batch loss 0.670676291 epoch total loss 0.63516587\n",
      "Trained batch 530 batch loss 0.624257147 epoch total loss 0.635145307\n",
      "Trained batch 531 batch loss 0.568081379 epoch total loss 0.635019\n",
      "Trained batch 532 batch loss 0.489097357 epoch total loss 0.634744763\n",
      "Trained batch 533 batch loss 0.588925123 epoch total loss 0.634658813\n",
      "Trained batch 534 batch loss 0.750921607 epoch total loss 0.63487649\n",
      "Trained batch 535 batch loss 0.674952328 epoch total loss 0.634951413\n",
      "Trained batch 536 batch loss 0.718249679 epoch total loss 0.635106862\n",
      "Trained batch 537 batch loss 0.741268456 epoch total loss 0.63530457\n",
      "Trained batch 538 batch loss 0.683724403 epoch total loss 0.635394514\n",
      "Trained batch 539 batch loss 0.563595235 epoch total loss 0.635261297\n",
      "Trained batch 540 batch loss 0.529601812 epoch total loss 0.635065675\n",
      "Trained batch 541 batch loss 0.512381136 epoch total loss 0.634838879\n",
      "Trained batch 542 batch loss 0.569292665 epoch total loss 0.634718\n",
      "Trained batch 543 batch loss 0.534166634 epoch total loss 0.634532809\n",
      "Trained batch 544 batch loss 0.530773103 epoch total loss 0.634342074\n",
      "Trained batch 545 batch loss 0.51270777 epoch total loss 0.634118855\n",
      "Trained batch 546 batch loss 0.555584252 epoch total loss 0.633975\n",
      "Trained batch 547 batch loss 0.519249916 epoch total loss 0.63376528\n",
      "Trained batch 548 batch loss 0.540462494 epoch total loss 0.633595049\n",
      "Trained batch 549 batch loss 0.55614239 epoch total loss 0.633453965\n",
      "Trained batch 550 batch loss 0.522146642 epoch total loss 0.633251607\n",
      "Trained batch 551 batch loss 0.56656462 epoch total loss 0.63313061\n",
      "Trained batch 552 batch loss 0.55483216 epoch total loss 0.632988751\n",
      "Trained batch 553 batch loss 0.594704688 epoch total loss 0.63291949\n",
      "Trained batch 554 batch loss 0.582823 epoch total loss 0.63282907\n",
      "Trained batch 555 batch loss 0.581780255 epoch total loss 0.6327371\n",
      "Trained batch 556 batch loss 0.616145551 epoch total loss 0.632707298\n",
      "Trained batch 557 batch loss 0.585940361 epoch total loss 0.632623315\n",
      "Trained batch 558 batch loss 0.674246192 epoch total loss 0.63269794\n",
      "Trained batch 559 batch loss 0.644181728 epoch total loss 0.632718503\n",
      "Trained batch 560 batch loss 0.643175364 epoch total loss 0.63273716\n",
      "Trained batch 561 batch loss 0.688495934 epoch total loss 0.63283658\n",
      "Trained batch 562 batch loss 0.593254089 epoch total loss 0.632766187\n",
      "Trained batch 563 batch loss 0.620821834 epoch total loss 0.632744968\n",
      "Trained batch 564 batch loss 0.612868845 epoch total loss 0.632709682\n",
      "Trained batch 565 batch loss 0.565225482 epoch total loss 0.632590234\n",
      "Trained batch 566 batch loss 0.609568894 epoch total loss 0.632549524\n",
      "Trained batch 567 batch loss 0.556449234 epoch total loss 0.632415354\n",
      "Trained batch 568 batch loss 0.609830916 epoch total loss 0.632375598\n",
      "Trained batch 569 batch loss 0.659979284 epoch total loss 0.632424057\n",
      "Trained batch 570 batch loss 0.619143188 epoch total loss 0.632400751\n",
      "Trained batch 571 batch loss 0.562088847 epoch total loss 0.632277668\n",
      "Trained batch 572 batch loss 0.609634101 epoch total loss 0.63223803\n",
      "Trained batch 573 batch loss 0.684003949 epoch total loss 0.632328391\n",
      "Trained batch 574 batch loss 0.683106959 epoch total loss 0.632416844\n",
      "Trained batch 575 batch loss 0.651784658 epoch total loss 0.632450521\n",
      "Trained batch 576 batch loss 0.670215547 epoch total loss 0.632516146\n",
      "Trained batch 577 batch loss 0.675528944 epoch total loss 0.632590652\n",
      "Trained batch 578 batch loss 0.598430157 epoch total loss 0.632531583\n",
      "Trained batch 579 batch loss 0.623924673 epoch total loss 0.632516682\n",
      "Trained batch 580 batch loss 0.631335378 epoch total loss 0.632514715\n",
      "Trained batch 581 batch loss 0.688168287 epoch total loss 0.6326105\n",
      "Trained batch 582 batch loss 0.74921453 epoch total loss 0.632810831\n",
      "Trained batch 583 batch loss 0.669375181 epoch total loss 0.632873535\n",
      "Trained batch 584 batch loss 0.627144337 epoch total loss 0.6328637\n",
      "Trained batch 585 batch loss 0.678603709 epoch total loss 0.632941902\n",
      "Trained batch 586 batch loss 0.68412894 epoch total loss 0.633029222\n",
      "Trained batch 587 batch loss 0.645744145 epoch total loss 0.633050919\n",
      "Trained batch 588 batch loss 0.625424325 epoch total loss 0.633038\n",
      "Trained batch 589 batch loss 0.606176317 epoch total loss 0.632992327\n",
      "Trained batch 590 batch loss 0.536076546 epoch total loss 0.632828057\n",
      "Trained batch 591 batch loss 0.596564651 epoch total loss 0.632766724\n",
      "Trained batch 592 batch loss 0.607600689 epoch total loss 0.632724166\n",
      "Trained batch 593 batch loss 0.681116343 epoch total loss 0.632805824\n",
      "Trained batch 594 batch loss 0.715815783 epoch total loss 0.632945538\n",
      "Trained batch 595 batch loss 0.623978317 epoch total loss 0.632930517\n",
      "Trained batch 596 batch loss 0.6220752 epoch total loss 0.632912278\n",
      "Trained batch 597 batch loss 0.617437065 epoch total loss 0.63288635\n",
      "Trained batch 598 batch loss 0.767081916 epoch total loss 0.633110762\n",
      "Trained batch 599 batch loss 0.674521685 epoch total loss 0.633179963\n",
      "Trained batch 600 batch loss 0.665291309 epoch total loss 0.633233428\n",
      "Trained batch 601 batch loss 0.654778481 epoch total loss 0.63326931\n",
      "Trained batch 602 batch loss 0.692676544 epoch total loss 0.633368\n",
      "Trained batch 603 batch loss 0.609333098 epoch total loss 0.63332814\n",
      "Trained batch 604 batch loss 0.499934316 epoch total loss 0.633107305\n",
      "Trained batch 605 batch loss 0.584947 epoch total loss 0.633027732\n",
      "Trained batch 606 batch loss 0.672318578 epoch total loss 0.633092582\n",
      "Trained batch 607 batch loss 0.666780233 epoch total loss 0.633148074\n",
      "Trained batch 608 batch loss 0.630635619 epoch total loss 0.633143961\n",
      "Trained batch 609 batch loss 0.626500845 epoch total loss 0.633133054\n",
      "Trained batch 610 batch loss 0.627260685 epoch total loss 0.633123457\n",
      "Trained batch 611 batch loss 0.633202136 epoch total loss 0.633123577\n",
      "Trained batch 612 batch loss 0.599779308 epoch total loss 0.633069098\n",
      "Trained batch 613 batch loss 0.628106594 epoch total loss 0.633061051\n",
      "Trained batch 614 batch loss 0.542927921 epoch total loss 0.632914245\n",
      "Trained batch 615 batch loss 0.513103426 epoch total loss 0.632719398\n",
      "Trained batch 616 batch loss 0.54994607 epoch total loss 0.632585049\n",
      "Trained batch 617 batch loss 0.543932915 epoch total loss 0.632441401\n",
      "Trained batch 618 batch loss 0.58888 epoch total loss 0.632370889\n",
      "Trained batch 619 batch loss 0.685048878 epoch total loss 0.632456\n",
      "Trained batch 620 batch loss 0.583360314 epoch total loss 0.63237685\n",
      "Trained batch 621 batch loss 0.643833876 epoch total loss 0.632395267\n",
      "Trained batch 622 batch loss 0.659210742 epoch total loss 0.632438362\n",
      "Trained batch 623 batch loss 0.675523639 epoch total loss 0.632507563\n",
      "Trained batch 624 batch loss 0.584620953 epoch total loss 0.632430851\n",
      "Trained batch 625 batch loss 0.608561695 epoch total loss 0.632392645\n",
      "Trained batch 626 batch loss 0.66054678 epoch total loss 0.632437587\n",
      "Trained batch 627 batch loss 0.638642967 epoch total loss 0.632447481\n",
      "Trained batch 628 batch loss 0.551147342 epoch total loss 0.632318\n",
      "Trained batch 629 batch loss 0.538404882 epoch total loss 0.63216871\n",
      "Trained batch 630 batch loss 0.62590903 epoch total loss 0.632158816\n",
      "Trained batch 631 batch loss 0.677704334 epoch total loss 0.632231\n",
      "Trained batch 632 batch loss 0.648629963 epoch total loss 0.632256925\n",
      "Trained batch 633 batch loss 0.650885046 epoch total loss 0.63228631\n",
      "Trained batch 634 batch loss 0.718756914 epoch total loss 0.632422686\n",
      "Trained batch 635 batch loss 0.620003819 epoch total loss 0.632403135\n",
      "Trained batch 636 batch loss 0.574002802 epoch total loss 0.632311285\n",
      "Trained batch 637 batch loss 0.656243265 epoch total loss 0.632348895\n",
      "Trained batch 638 batch loss 0.642493308 epoch total loss 0.63236481\n",
      "Trained batch 639 batch loss 0.584151 epoch total loss 0.63228929\n",
      "Trained batch 640 batch loss 0.634557962 epoch total loss 0.632292867\n",
      "Trained batch 641 batch loss 0.608919 epoch total loss 0.632256389\n",
      "Trained batch 642 batch loss 0.525971234 epoch total loss 0.632090807\n",
      "Trained batch 643 batch loss 0.531045377 epoch total loss 0.631933689\n",
      "Trained batch 644 batch loss 0.627218544 epoch total loss 0.631926358\n",
      "Trained batch 645 batch loss 0.560901701 epoch total loss 0.631816268\n",
      "Trained batch 646 batch loss 0.662677765 epoch total loss 0.631864071\n",
      "Trained batch 647 batch loss 0.673768163 epoch total loss 0.631928802\n",
      "Trained batch 648 batch loss 0.598364711 epoch total loss 0.631877\n",
      "Trained batch 649 batch loss 0.606777251 epoch total loss 0.631838322\n",
      "Trained batch 650 batch loss 0.554346919 epoch total loss 0.631719112\n",
      "Trained batch 651 batch loss 0.599694669 epoch total loss 0.631669939\n",
      "Trained batch 652 batch loss 0.68132329 epoch total loss 0.631746113\n",
      "Trained batch 653 batch loss 0.684858859 epoch total loss 0.631827414\n",
      "Trained batch 654 batch loss 0.619621694 epoch total loss 0.631808758\n",
      "Trained batch 655 batch loss 0.620106161 epoch total loss 0.631790936\n",
      "Trained batch 656 batch loss 0.565239608 epoch total loss 0.631689489\n",
      "Trained batch 657 batch loss 0.645066559 epoch total loss 0.631709874\n",
      "Trained batch 658 batch loss 0.637445092 epoch total loss 0.631718576\n",
      "Trained batch 659 batch loss 0.642153442 epoch total loss 0.631734431\n",
      "Trained batch 660 batch loss 0.582464576 epoch total loss 0.631659746\n",
      "Trained batch 661 batch loss 0.67206651 epoch total loss 0.631720901\n",
      "Trained batch 662 batch loss 0.626414299 epoch total loss 0.631712854\n",
      "Trained batch 663 batch loss 0.683241 epoch total loss 0.631790578\n",
      "Trained batch 664 batch loss 0.616300702 epoch total loss 0.631767213\n",
      "Trained batch 665 batch loss 0.657538533 epoch total loss 0.631805956\n",
      "Trained batch 666 batch loss 0.631817698 epoch total loss 0.631805956\n",
      "Trained batch 667 batch loss 0.55321908 epoch total loss 0.631688178\n",
      "Trained batch 668 batch loss 0.60366118 epoch total loss 0.631646216\n",
      "Trained batch 669 batch loss 0.598253548 epoch total loss 0.631596327\n",
      "Trained batch 670 batch loss 0.562160075 epoch total loss 0.631492674\n",
      "Trained batch 671 batch loss 0.679580629 epoch total loss 0.631564319\n",
      "Trained batch 672 batch loss 0.631596386 epoch total loss 0.631564379\n",
      "Trained batch 673 batch loss 0.625584543 epoch total loss 0.631555498\n",
      "Trained batch 674 batch loss 0.681285 epoch total loss 0.631629229\n",
      "Trained batch 675 batch loss 0.72395587 epoch total loss 0.631766\n",
      "Trained batch 676 batch loss 0.577814698 epoch total loss 0.631686211\n",
      "Trained batch 677 batch loss 0.728553 epoch total loss 0.631829321\n",
      "Trained batch 678 batch loss 0.654382229 epoch total loss 0.631862581\n",
      "Trained batch 679 batch loss 0.634536505 epoch total loss 0.631866515\n",
      "Trained batch 680 batch loss 0.614360452 epoch total loss 0.631840765\n",
      "Trained batch 681 batch loss 0.678637207 epoch total loss 0.63190949\n",
      "Trained batch 682 batch loss 0.59659946 epoch total loss 0.631857693\n",
      "Trained batch 683 batch loss 0.643265128 epoch total loss 0.631874382\n",
      "Trained batch 684 batch loss 0.635295391 epoch total loss 0.631879389\n",
      "Trained batch 685 batch loss 0.588583052 epoch total loss 0.631816208\n",
      "Trained batch 686 batch loss 0.498655319 epoch total loss 0.631622076\n",
      "Trained batch 687 batch loss 0.578198671 epoch total loss 0.631544292\n",
      "Trained batch 688 batch loss 0.667455435 epoch total loss 0.631596506\n",
      "Trained batch 689 batch loss 0.666709185 epoch total loss 0.631647468\n",
      "Trained batch 690 batch loss 0.697394609 epoch total loss 0.631742775\n",
      "Trained batch 691 batch loss 0.677565217 epoch total loss 0.631809056\n",
      "Trained batch 692 batch loss 0.659274 epoch total loss 0.631848752\n",
      "Trained batch 693 batch loss 0.616536438 epoch total loss 0.631826639\n",
      "Trained batch 694 batch loss 0.620809674 epoch total loss 0.631810784\n",
      "Trained batch 695 batch loss 0.639595628 epoch total loss 0.631822\n",
      "Trained batch 696 batch loss 0.62010473 epoch total loss 0.631805182\n",
      "Trained batch 697 batch loss 0.550104558 epoch total loss 0.631687939\n",
      "Trained batch 698 batch loss 0.641032159 epoch total loss 0.631701291\n",
      "Trained batch 699 batch loss 0.671860576 epoch total loss 0.631758809\n",
      "Trained batch 700 batch loss 0.694367349 epoch total loss 0.631848216\n",
      "Trained batch 701 batch loss 0.741968632 epoch total loss 0.632005334\n",
      "Trained batch 702 batch loss 0.569376886 epoch total loss 0.631916106\n",
      "Trained batch 703 batch loss 0.546857893 epoch total loss 0.631795108\n",
      "Trained batch 704 batch loss 0.673468471 epoch total loss 0.631854296\n",
      "Trained batch 705 batch loss 0.66552496 epoch total loss 0.631902039\n",
      "Trained batch 706 batch loss 0.651735067 epoch total loss 0.631930113\n",
      "Trained batch 707 batch loss 0.583035707 epoch total loss 0.631861\n",
      "Trained batch 708 batch loss 0.605813563 epoch total loss 0.631824195\n",
      "Trained batch 709 batch loss 0.653561532 epoch total loss 0.631854832\n",
      "Trained batch 710 batch loss 0.709646225 epoch total loss 0.631964386\n",
      "Trained batch 711 batch loss 0.801059544 epoch total loss 0.632202208\n",
      "Trained batch 712 batch loss 0.595203757 epoch total loss 0.632150292\n",
      "Trained batch 713 batch loss 0.434634298 epoch total loss 0.63187325\n",
      "Trained batch 714 batch loss 0.591676772 epoch total loss 0.631817\n",
      "Trained batch 715 batch loss 0.526172876 epoch total loss 0.631669223\n",
      "Trained batch 716 batch loss 0.489975303 epoch total loss 0.631471336\n",
      "Trained batch 717 batch loss 0.568928599 epoch total loss 0.631384134\n",
      "Trained batch 718 batch loss 0.600319147 epoch total loss 0.631340861\n",
      "Trained batch 719 batch loss 0.642937779 epoch total loss 0.631357\n",
      "Trained batch 720 batch loss 0.629668713 epoch total loss 0.63135463\n",
      "Trained batch 721 batch loss 0.568792462 epoch total loss 0.631267846\n",
      "Trained batch 722 batch loss 0.567074895 epoch total loss 0.631179\n",
      "Trained batch 723 batch loss 0.566808105 epoch total loss 0.631089926\n",
      "Trained batch 724 batch loss 0.592600942 epoch total loss 0.631036758\n",
      "Trained batch 725 batch loss 0.687581062 epoch total loss 0.631114781\n",
      "Trained batch 726 batch loss 0.645198286 epoch total loss 0.631134152\n",
      "Trained batch 727 batch loss 0.688394248 epoch total loss 0.63121289\n",
      "Trained batch 728 batch loss 0.607196569 epoch total loss 0.631179929\n",
      "Trained batch 729 batch loss 0.67585516 epoch total loss 0.631241202\n",
      "Trained batch 730 batch loss 0.621965885 epoch total loss 0.631228507\n",
      "Trained batch 731 batch loss 0.64237386 epoch total loss 0.631243765\n",
      "Trained batch 732 batch loss 0.631736338 epoch total loss 0.631244421\n",
      "Trained batch 733 batch loss 0.712657 epoch total loss 0.631355464\n",
      "Trained batch 734 batch loss 0.792511344 epoch total loss 0.631575048\n",
      "Trained batch 735 batch loss 0.721536934 epoch total loss 0.631697416\n",
      "Trained batch 736 batch loss 0.69447279 epoch total loss 0.631782711\n",
      "Trained batch 737 batch loss 0.787203431 epoch total loss 0.631993592\n",
      "Trained batch 738 batch loss 0.681632459 epoch total loss 0.632060826\n",
      "Trained batch 739 batch loss 0.63899523 epoch total loss 0.632070243\n",
      "Trained batch 740 batch loss 0.707349062 epoch total loss 0.632172\n",
      "Trained batch 741 batch loss 0.602066517 epoch total loss 0.632131338\n",
      "Trained batch 742 batch loss 0.669071794 epoch total loss 0.632181108\n",
      "Trained batch 743 batch loss 0.635841548 epoch total loss 0.632186055\n",
      "Trained batch 744 batch loss 0.648482621 epoch total loss 0.63220793\n",
      "Trained batch 745 batch loss 0.653025091 epoch total loss 0.632235885\n",
      "Trained batch 746 batch loss 0.654035628 epoch total loss 0.632265091\n",
      "Trained batch 747 batch loss 0.72977972 epoch total loss 0.632395566\n",
      "Trained batch 748 batch loss 0.674486399 epoch total loss 0.632451892\n",
      "Trained batch 749 batch loss 0.688022852 epoch total loss 0.63252604\n",
      "Trained batch 750 batch loss 0.643349767 epoch total loss 0.632540464\n",
      "Trained batch 751 batch loss 0.542580247 epoch total loss 0.632420659\n",
      "Trained batch 752 batch loss 0.55254 epoch total loss 0.632314503\n",
      "Trained batch 753 batch loss 0.633015454 epoch total loss 0.632315397\n",
      "Trained batch 754 batch loss 0.61444515 epoch total loss 0.632291734\n",
      "Trained batch 755 batch loss 0.549615562 epoch total loss 0.63218224\n",
      "Trained batch 756 batch loss 0.590831339 epoch total loss 0.632127523\n",
      "Trained batch 757 batch loss 0.518384 epoch total loss 0.63197726\n",
      "Trained batch 758 batch loss 0.564296961 epoch total loss 0.631888\n",
      "Trained batch 759 batch loss 0.592128575 epoch total loss 0.63183558\n",
      "Trained batch 760 batch loss 0.604339957 epoch total loss 0.6317994\n",
      "Trained batch 761 batch loss 0.571661174 epoch total loss 0.631720364\n",
      "Trained batch 762 batch loss 0.572768569 epoch total loss 0.631643\n",
      "Trained batch 763 batch loss 0.620681703 epoch total loss 0.631628573\n",
      "Trained batch 764 batch loss 0.58865726 epoch total loss 0.631572366\n",
      "Trained batch 765 batch loss 0.600604296 epoch total loss 0.631531894\n",
      "Trained batch 766 batch loss 0.596104622 epoch total loss 0.631485641\n",
      "Trained batch 767 batch loss 0.563578 epoch total loss 0.631397069\n",
      "Trained batch 768 batch loss 0.603608 epoch total loss 0.631360888\n",
      "Trained batch 769 batch loss 0.702690065 epoch total loss 0.631453633\n",
      "Trained batch 770 batch loss 0.580195963 epoch total loss 0.631387115\n",
      "Trained batch 771 batch loss 0.555030704 epoch total loss 0.631288052\n",
      "Trained batch 772 batch loss 0.620679796 epoch total loss 0.631274283\n",
      "Trained batch 773 batch loss 0.640876532 epoch total loss 0.631286681\n",
      "Trained batch 774 batch loss 0.616315961 epoch total loss 0.631267369\n",
      "Trained batch 775 batch loss 0.68987757 epoch total loss 0.631342947\n",
      "Trained batch 776 batch loss 0.645393491 epoch total loss 0.631361067\n",
      "Trained batch 777 batch loss 0.643110394 epoch total loss 0.631376147\n",
      "Trained batch 778 batch loss 0.652107596 epoch total loss 0.631402791\n",
      "Trained batch 779 batch loss 0.586991489 epoch total loss 0.631345809\n",
      "Trained batch 780 batch loss 0.697843909 epoch total loss 0.631431043\n",
      "Trained batch 781 batch loss 0.540876448 epoch total loss 0.631315112\n",
      "Trained batch 782 batch loss 0.656239033 epoch total loss 0.631347\n",
      "Trained batch 783 batch loss 0.626063168 epoch total loss 0.631340265\n",
      "Trained batch 784 batch loss 0.563256 epoch total loss 0.631253421\n",
      "Trained batch 785 batch loss 0.594800711 epoch total loss 0.631207\n",
      "Trained batch 786 batch loss 0.532226086 epoch total loss 0.631081045\n",
      "Trained batch 787 batch loss 0.518109202 epoch total loss 0.630937457\n",
      "Trained batch 788 batch loss 0.52198559 epoch total loss 0.630799174\n",
      "Trained batch 789 batch loss 0.554539204 epoch total loss 0.630702555\n",
      "Trained batch 790 batch loss 0.547733188 epoch total loss 0.630597532\n",
      "Trained batch 791 batch loss 0.644938111 epoch total loss 0.630615592\n",
      "Trained batch 792 batch loss 0.692261815 epoch total loss 0.630693436\n",
      "Trained batch 793 batch loss 0.588616371 epoch total loss 0.630640388\n",
      "Trained batch 794 batch loss 0.580171227 epoch total loss 0.630576849\n",
      "Trained batch 795 batch loss 0.624293447 epoch total loss 0.630568922\n",
      "Trained batch 796 batch loss 0.637970567 epoch total loss 0.63057822\n",
      "Trained batch 797 batch loss 0.598699 epoch total loss 0.630538225\n",
      "Trained batch 798 batch loss 0.716262817 epoch total loss 0.630645692\n",
      "Trained batch 799 batch loss 0.693498254 epoch total loss 0.63072437\n",
      "Trained batch 800 batch loss 0.603839338 epoch total loss 0.630690753\n",
      "Trained batch 801 batch loss 0.630280375 epoch total loss 0.630690277\n",
      "Trained batch 802 batch loss 0.602561951 epoch total loss 0.630655169\n",
      "Trained batch 803 batch loss 0.661246896 epoch total loss 0.630693316\n",
      "Trained batch 804 batch loss 0.562818408 epoch total loss 0.630608857\n",
      "Trained batch 805 batch loss 0.586142778 epoch total loss 0.630553603\n",
      "Trained batch 806 batch loss 0.621705234 epoch total loss 0.630542636\n",
      "Trained batch 807 batch loss 0.602339327 epoch total loss 0.630507708\n",
      "Trained batch 808 batch loss 0.621457577 epoch total loss 0.630496502\n",
      "Trained batch 809 batch loss 0.660745382 epoch total loss 0.630533874\n",
      "Trained batch 810 batch loss 0.599264324 epoch total loss 0.63049525\n",
      "Trained batch 811 batch loss 0.539390802 epoch total loss 0.630382955\n",
      "Trained batch 812 batch loss 0.427594602 epoch total loss 0.630133212\n",
      "Trained batch 813 batch loss 0.476781547 epoch total loss 0.629944623\n",
      "Trained batch 814 batch loss 0.445371628 epoch total loss 0.629717827\n",
      "Trained batch 815 batch loss 0.514840961 epoch total loss 0.629576862\n",
      "Trained batch 816 batch loss 0.619039416 epoch total loss 0.629563928\n",
      "Trained batch 817 batch loss 0.637014508 epoch total loss 0.629573107\n",
      "Trained batch 818 batch loss 0.669891119 epoch total loss 0.62962234\n",
      "Trained batch 819 batch loss 0.73987186 epoch total loss 0.629756927\n",
      "Trained batch 820 batch loss 0.717046738 epoch total loss 0.629863381\n",
      "Trained batch 821 batch loss 0.752510309 epoch total loss 0.630012751\n",
      "Trained batch 822 batch loss 0.700931787 epoch total loss 0.630099058\n",
      "Trained batch 823 batch loss 0.688894153 epoch total loss 0.630170465\n",
      "Trained batch 824 batch loss 0.694521427 epoch total loss 0.630248606\n",
      "Trained batch 825 batch loss 0.635518789 epoch total loss 0.630254924\n",
      "Trained batch 826 batch loss 0.675693393 epoch total loss 0.63031\n",
      "Trained batch 827 batch loss 0.704504669 epoch total loss 0.630399704\n",
      "Trained batch 828 batch loss 0.676014 epoch total loss 0.630454838\n",
      "Trained batch 829 batch loss 0.573465 epoch total loss 0.630386114\n",
      "Trained batch 830 batch loss 0.707364082 epoch total loss 0.630478799\n",
      "Trained batch 831 batch loss 0.635887146 epoch total loss 0.630485296\n",
      "Trained batch 832 batch loss 0.687083781 epoch total loss 0.630553305\n",
      "Trained batch 833 batch loss 0.659906 epoch total loss 0.630588591\n",
      "Trained batch 834 batch loss 0.723230898 epoch total loss 0.630699635\n",
      "Trained batch 835 batch loss 0.649513304 epoch total loss 0.630722165\n",
      "Trained batch 836 batch loss 0.618746698 epoch total loss 0.63070786\n",
      "Trained batch 837 batch loss 0.652848482 epoch total loss 0.630734324\n",
      "Trained batch 838 batch loss 0.624963939 epoch total loss 0.63072741\n",
      "Trained batch 839 batch loss 0.652702391 epoch total loss 0.630753577\n",
      "Trained batch 840 batch loss 0.601146042 epoch total loss 0.63071835\n",
      "Trained batch 841 batch loss 0.530929089 epoch total loss 0.630599678\n",
      "Trained batch 842 batch loss 0.623107493 epoch total loss 0.630590796\n",
      "Trained batch 843 batch loss 0.678957224 epoch total loss 0.630648196\n",
      "Trained batch 844 batch loss 0.56793344 epoch total loss 0.630573869\n",
      "Trained batch 845 batch loss 0.57991 epoch total loss 0.630513906\n",
      "Trained batch 846 batch loss 0.555479228 epoch total loss 0.630425215\n",
      "Trained batch 847 batch loss 0.554559 epoch total loss 0.630335629\n",
      "Trained batch 848 batch loss 0.566924334 epoch total loss 0.630260825\n",
      "Trained batch 849 batch loss 0.665188968 epoch total loss 0.630301952\n",
      "Trained batch 850 batch loss 0.711810231 epoch total loss 0.630397797\n",
      "Trained batch 851 batch loss 0.73752892 epoch total loss 0.630523741\n",
      "Trained batch 852 batch loss 0.770619154 epoch total loss 0.63068819\n",
      "Trained batch 853 batch loss 0.742539644 epoch total loss 0.630819321\n",
      "Trained batch 854 batch loss 0.709202707 epoch total loss 0.630911112\n",
      "Trained batch 855 batch loss 0.700854063 epoch total loss 0.630992949\n",
      "Trained batch 856 batch loss 0.75032866 epoch total loss 0.631132305\n",
      "Trained batch 857 batch loss 0.656585515 epoch total loss 0.631162\n",
      "Trained batch 858 batch loss 0.659117043 epoch total loss 0.631194592\n",
      "Trained batch 859 batch loss 0.664178133 epoch total loss 0.631233\n",
      "Trained batch 860 batch loss 0.553764582 epoch total loss 0.631142914\n",
      "Trained batch 861 batch loss 0.550672 epoch total loss 0.631049454\n",
      "Trained batch 862 batch loss 0.538972378 epoch total loss 0.630942643\n",
      "Trained batch 863 batch loss 0.520901799 epoch total loss 0.630815089\n",
      "Trained batch 864 batch loss 0.473951548 epoch total loss 0.630633533\n",
      "Trained batch 865 batch loss 0.493348032 epoch total loss 0.630474806\n",
      "Trained batch 866 batch loss 0.543394864 epoch total loss 0.630374253\n",
      "Trained batch 867 batch loss 0.533206582 epoch total loss 0.630262196\n",
      "Trained batch 868 batch loss 0.597182333 epoch total loss 0.630224049\n",
      "Trained batch 869 batch loss 0.571496 epoch total loss 0.630156457\n",
      "Trained batch 870 batch loss 0.688844502 epoch total loss 0.63022393\n",
      "Trained batch 871 batch loss 0.608087838 epoch total loss 0.630198479\n",
      "Trained batch 872 batch loss 0.6918841 epoch total loss 0.630269229\n",
      "Trained batch 873 batch loss 0.674449742 epoch total loss 0.630319834\n",
      "Trained batch 874 batch loss 0.676162958 epoch total loss 0.630372286\n",
      "Trained batch 875 batch loss 0.595355034 epoch total loss 0.630332232\n",
      "Trained batch 876 batch loss 0.636077106 epoch total loss 0.630338788\n",
      "Trained batch 877 batch loss 0.823972 epoch total loss 0.630559564\n",
      "Trained batch 878 batch loss 0.801281393 epoch total loss 0.630754\n",
      "Trained batch 879 batch loss 0.680403411 epoch total loss 0.630810499\n",
      "Trained batch 880 batch loss 0.723892272 epoch total loss 0.630916238\n",
      "Trained batch 881 batch loss 0.653331697 epoch total loss 0.630941689\n",
      "Trained batch 882 batch loss 0.658386052 epoch total loss 0.630972803\n",
      "Trained batch 883 batch loss 0.600013256 epoch total loss 0.630937755\n",
      "Trained batch 884 batch loss 0.608244598 epoch total loss 0.630912066\n",
      "Trained batch 885 batch loss 0.580171645 epoch total loss 0.630854726\n",
      "Trained batch 886 batch loss 0.513904512 epoch total loss 0.630722761\n",
      "Trained batch 887 batch loss 0.488515317 epoch total loss 0.630562484\n",
      "Trained batch 888 batch loss 0.491048694 epoch total loss 0.630405307\n",
      "Trained batch 889 batch loss 0.557091892 epoch total loss 0.630322814\n",
      "Trained batch 890 batch loss 0.658624053 epoch total loss 0.630354643\n",
      "Trained batch 891 batch loss 0.654218197 epoch total loss 0.630381465\n",
      "Trained batch 892 batch loss 0.573703229 epoch total loss 0.630317926\n",
      "Trained batch 893 batch loss 0.486790478 epoch total loss 0.630157232\n",
      "Trained batch 894 batch loss 0.465304196 epoch total loss 0.629972875\n",
      "Trained batch 895 batch loss 0.49719137 epoch total loss 0.629824519\n",
      "Trained batch 896 batch loss 0.530046 epoch total loss 0.629713118\n",
      "Trained batch 897 batch loss 0.503635705 epoch total loss 0.62957263\n",
      "Trained batch 898 batch loss 0.539359748 epoch total loss 0.629472136\n",
      "Trained batch 899 batch loss 0.587451756 epoch total loss 0.629425406\n",
      "Trained batch 900 batch loss 0.544283926 epoch total loss 0.629330873\n",
      "Trained batch 901 batch loss 0.591571331 epoch total loss 0.629288912\n",
      "Trained batch 902 batch loss 0.645290613 epoch total loss 0.629306614\n",
      "Trained batch 903 batch loss 0.667859674 epoch total loss 0.629349291\n",
      "Trained batch 904 batch loss 0.60013634 epoch total loss 0.629317045\n",
      "Trained batch 905 batch loss 0.680825055 epoch total loss 0.629373968\n",
      "Trained batch 906 batch loss 0.66511023 epoch total loss 0.629413366\n",
      "Trained batch 907 batch loss 0.621123672 epoch total loss 0.629404247\n",
      "Trained batch 908 batch loss 0.682229221 epoch total loss 0.629462421\n",
      "Trained batch 909 batch loss 0.70009774 epoch total loss 0.629540086\n",
      "Trained batch 910 batch loss 0.669453621 epoch total loss 0.629583955\n",
      "Trained batch 911 batch loss 0.668950915 epoch total loss 0.629627168\n",
      "Trained batch 912 batch loss 0.710071802 epoch total loss 0.629715383\n",
      "Trained batch 913 batch loss 0.737804472 epoch total loss 0.629833758\n",
      "Trained batch 914 batch loss 0.707393825 epoch total loss 0.629918635\n",
      "Trained batch 915 batch loss 0.66358012 epoch total loss 0.629955411\n",
      "Trained batch 916 batch loss 0.543916702 epoch total loss 0.629861474\n",
      "Trained batch 917 batch loss 0.547725677 epoch total loss 0.629771948\n",
      "Trained batch 918 batch loss 0.555895329 epoch total loss 0.629691482\n",
      "Trained batch 919 batch loss 0.503410816 epoch total loss 0.629554033\n",
      "Trained batch 920 batch loss 0.611460924 epoch total loss 0.629534364\n",
      "Trained batch 921 batch loss 0.698166132 epoch total loss 0.629608929\n",
      "Trained batch 922 batch loss 0.732042789 epoch total loss 0.629720032\n",
      "Trained batch 923 batch loss 0.708745539 epoch total loss 0.629805624\n",
      "Trained batch 924 batch loss 0.724197626 epoch total loss 0.629907787\n",
      "Trained batch 925 batch loss 0.631111383 epoch total loss 0.629909098\n",
      "Trained batch 926 batch loss 0.689352334 epoch total loss 0.629973233\n",
      "Trained batch 927 batch loss 0.654506624 epoch total loss 0.629999697\n",
      "Trained batch 928 batch loss 0.670045733 epoch total loss 0.630042851\n",
      "Trained batch 929 batch loss 0.581518233 epoch total loss 0.629990637\n",
      "Trained batch 930 batch loss 0.692742944 epoch total loss 0.63005811\n",
      "Trained batch 931 batch loss 0.669852078 epoch total loss 0.630100846\n",
      "Trained batch 932 batch loss 0.663768291 epoch total loss 0.630136967\n",
      "Trained batch 933 batch loss 0.589656055 epoch total loss 0.630093575\n",
      "Trained batch 934 batch loss 0.637404919 epoch total loss 0.630101383\n",
      "Trained batch 935 batch loss 0.674441397 epoch total loss 0.630148828\n",
      "Trained batch 936 batch loss 0.638083041 epoch total loss 0.630157292\n",
      "Trained batch 937 batch loss 0.61471647 epoch total loss 0.630140841\n",
      "Trained batch 938 batch loss 0.660757899 epoch total loss 0.630173504\n",
      "Trained batch 939 batch loss 0.6221053 epoch total loss 0.630164921\n",
      "Trained batch 940 batch loss 0.595086455 epoch total loss 0.630127609\n",
      "Trained batch 941 batch loss 0.622045517 epoch total loss 0.630119\n",
      "Trained batch 942 batch loss 0.593390942 epoch total loss 0.630080044\n",
      "Trained batch 943 batch loss 0.62055403 epoch total loss 0.630069911\n",
      "Trained batch 944 batch loss 0.645306647 epoch total loss 0.630086064\n",
      "Trained batch 945 batch loss 0.620311141 epoch total loss 0.630075753\n",
      "Trained batch 946 batch loss 0.669209599 epoch total loss 0.630117059\n",
      "Trained batch 947 batch loss 0.635668933 epoch total loss 0.63012296\n",
      "Trained batch 948 batch loss 0.647625089 epoch total loss 0.630141437\n",
      "Trained batch 949 batch loss 0.651921034 epoch total loss 0.630164385\n",
      "Trained batch 950 batch loss 0.659161031 epoch total loss 0.630194902\n",
      "Trained batch 951 batch loss 0.666446269 epoch total loss 0.630233049\n",
      "Trained batch 952 batch loss 0.629249752 epoch total loss 0.630232036\n",
      "Trained batch 953 batch loss 0.628513455 epoch total loss 0.630230248\n",
      "Trained batch 954 batch loss 0.611808836 epoch total loss 0.630210936\n",
      "Trained batch 955 batch loss 0.697818518 epoch total loss 0.630281746\n",
      "Trained batch 956 batch loss 0.630842805 epoch total loss 0.630282342\n",
      "Trained batch 957 batch loss 0.713759 epoch total loss 0.630369544\n",
      "Trained batch 958 batch loss 0.590984344 epoch total loss 0.630328476\n",
      "Trained batch 959 batch loss 0.533519924 epoch total loss 0.630227506\n",
      "Trained batch 960 batch loss 0.532505691 epoch total loss 0.630125761\n",
      "Trained batch 961 batch loss 0.509130239 epoch total loss 0.629999876\n",
      "Trained batch 962 batch loss 0.506623447 epoch total loss 0.629871666\n",
      "Trained batch 963 batch loss 0.501921654 epoch total loss 0.629738748\n",
      "Trained batch 964 batch loss 0.498615801 epoch total loss 0.62960273\n",
      "Trained batch 965 batch loss 0.544264674 epoch total loss 0.629514277\n",
      "Trained batch 966 batch loss 0.611197829 epoch total loss 0.629495323\n",
      "Trained batch 967 batch loss 0.667244315 epoch total loss 0.629534364\n",
      "Trained batch 968 batch loss 0.572247446 epoch total loss 0.629475176\n",
      "Trained batch 969 batch loss 0.766110182 epoch total loss 0.629616201\n",
      "Trained batch 970 batch loss 0.723355651 epoch total loss 0.62971282\n",
      "Trained batch 971 batch loss 0.684714079 epoch total loss 0.629769444\n",
      "Trained batch 972 batch loss 0.771511674 epoch total loss 0.629915237\n",
      "Trained batch 973 batch loss 0.619514406 epoch total loss 0.629904509\n",
      "Trained batch 974 batch loss 0.58707726 epoch total loss 0.62986058\n",
      "Trained batch 975 batch loss 0.63253 epoch total loss 0.629863262\n",
      "Trained batch 976 batch loss 0.554664969 epoch total loss 0.629786253\n",
      "Trained batch 977 batch loss 0.564902663 epoch total loss 0.629719794\n",
      "Trained batch 978 batch loss 0.628601551 epoch total loss 0.629718661\n",
      "Trained batch 979 batch loss 0.702459872 epoch total loss 0.629793\n",
      "Trained batch 980 batch loss 0.642763674 epoch total loss 0.629806221\n",
      "Trained batch 981 batch loss 0.645137489 epoch total loss 0.629821837\n",
      "Trained batch 982 batch loss 0.641156375 epoch total loss 0.6298334\n",
      "Trained batch 983 batch loss 0.686893344 epoch total loss 0.629891455\n",
      "Trained batch 984 batch loss 0.63236624 epoch total loss 0.629893959\n",
      "Trained batch 985 batch loss 0.70297575 epoch total loss 0.629968226\n",
      "Trained batch 986 batch loss 0.688323 epoch total loss 0.630027354\n",
      "Trained batch 987 batch loss 0.732116699 epoch total loss 0.630130768\n",
      "Trained batch 988 batch loss 0.754093 epoch total loss 0.630256236\n",
      "Trained batch 989 batch loss 0.718153834 epoch total loss 0.630345106\n",
      "Trained batch 990 batch loss 0.602269948 epoch total loss 0.630316794\n",
      "Trained batch 991 batch loss 0.642135143 epoch total loss 0.630328715\n",
      "Trained batch 992 batch loss 0.624396741 epoch total loss 0.630322754\n",
      "Trained batch 993 batch loss 0.605315208 epoch total loss 0.630297542\n",
      "Trained batch 994 batch loss 0.620184541 epoch total loss 0.630287349\n",
      "Trained batch 995 batch loss 0.528838158 epoch total loss 0.630185366\n",
      "Trained batch 996 batch loss 0.566270173 epoch total loss 0.630121171\n",
      "Trained batch 997 batch loss 0.545449317 epoch total loss 0.630036294\n",
      "Trained batch 998 batch loss 0.657511652 epoch total loss 0.630063832\n",
      "Trained batch 999 batch loss 0.621414304 epoch total loss 0.630055189\n",
      "Trained batch 1000 batch loss 0.693431497 epoch total loss 0.630118549\n",
      "Trained batch 1001 batch loss 0.62611115 epoch total loss 0.630114496\n",
      "Trained batch 1002 batch loss 0.635429502 epoch total loss 0.630119801\n",
      "Trained batch 1003 batch loss 0.608606935 epoch total loss 0.630098343\n",
      "Trained batch 1004 batch loss 0.583634257 epoch total loss 0.63005203\n",
      "Trained batch 1005 batch loss 0.689687371 epoch total loss 0.630111396\n",
      "Trained batch 1006 batch loss 0.539798915 epoch total loss 0.630021632\n",
      "Trained batch 1007 batch loss 0.616120458 epoch total loss 0.630007863\n",
      "Trained batch 1008 batch loss 0.535475731 epoch total loss 0.629914045\n",
      "Trained batch 1009 batch loss 0.528176129 epoch total loss 0.629813254\n",
      "Trained batch 1010 batch loss 0.566324592 epoch total loss 0.629750431\n",
      "Trained batch 1011 batch loss 0.555041075 epoch total loss 0.629676521\n",
      "Trained batch 1012 batch loss 0.546893954 epoch total loss 0.629594684\n",
      "Trained batch 1013 batch loss 0.598134935 epoch total loss 0.62956363\n",
      "Trained batch 1014 batch loss 0.572043061 epoch total loss 0.629506886\n",
      "Trained batch 1015 batch loss 0.537815154 epoch total loss 0.629416585\n",
      "Trained batch 1016 batch loss 0.619649649 epoch total loss 0.629407\n",
      "Trained batch 1017 batch loss 0.580886 epoch total loss 0.629359245\n",
      "Trained batch 1018 batch loss 0.525020063 epoch total loss 0.629256725\n",
      "Trained batch 1019 batch loss 0.580951512 epoch total loss 0.62920934\n",
      "Trained batch 1020 batch loss 0.63199234 epoch total loss 0.629212081\n",
      "Trained batch 1021 batch loss 0.520574093 epoch total loss 0.629105687\n",
      "Trained batch 1022 batch loss 0.567711711 epoch total loss 0.629045606\n",
      "Trained batch 1023 batch loss 0.592377186 epoch total loss 0.629009783\n",
      "Trained batch 1024 batch loss 0.645258784 epoch total loss 0.629025638\n",
      "Trained batch 1025 batch loss 0.754709 epoch total loss 0.629148245\n",
      "Trained batch 1026 batch loss 0.682701 epoch total loss 0.629200399\n",
      "Trained batch 1027 batch loss 0.653310359 epoch total loss 0.629223883\n",
      "Trained batch 1028 batch loss 0.565505207 epoch total loss 0.629161894\n",
      "Trained batch 1029 batch loss 0.525854826 epoch total loss 0.62906152\n",
      "Trained batch 1030 batch loss 0.66856581 epoch total loss 0.629099905\n",
      "Trained batch 1031 batch loss 0.608080208 epoch total loss 0.629079521\n",
      "Trained batch 1032 batch loss 0.681036115 epoch total loss 0.629129887\n",
      "Trained batch 1033 batch loss 0.635135412 epoch total loss 0.629135668\n",
      "Trained batch 1034 batch loss 0.613369465 epoch total loss 0.629120409\n",
      "Trained batch 1035 batch loss 0.555647373 epoch total loss 0.62904942\n",
      "Trained batch 1036 batch loss 0.570619166 epoch total loss 0.628993034\n",
      "Trained batch 1037 batch loss 0.588459611 epoch total loss 0.628953934\n",
      "Trained batch 1038 batch loss 0.647975 epoch total loss 0.628972232\n",
      "Trained batch 1039 batch loss 0.730910897 epoch total loss 0.629070342\n",
      "Trained batch 1040 batch loss 0.661193967 epoch total loss 0.629101217\n",
      "Trained batch 1041 batch loss 0.624772 epoch total loss 0.629097044\n",
      "Trained batch 1042 batch loss 0.705253541 epoch total loss 0.62917012\n",
      "Trained batch 1043 batch loss 0.612951636 epoch total loss 0.629154623\n",
      "Trained batch 1044 batch loss 0.679383516 epoch total loss 0.629202724\n",
      "Trained batch 1045 batch loss 0.622920334 epoch total loss 0.629196703\n",
      "Trained batch 1046 batch loss 0.621220767 epoch total loss 0.629189074\n",
      "Trained batch 1047 batch loss 0.560659885 epoch total loss 0.629123628\n",
      "Trained batch 1048 batch loss 0.5883708 epoch total loss 0.629084766\n",
      "Trained batch 1049 batch loss 0.611388385 epoch total loss 0.629067898\n",
      "Trained batch 1050 batch loss 0.627468288 epoch total loss 0.629066348\n",
      "Trained batch 1051 batch loss 0.671577215 epoch total loss 0.62910676\n",
      "Trained batch 1052 batch loss 0.612482607 epoch total loss 0.629090965\n",
      "Trained batch 1053 batch loss 0.621459842 epoch total loss 0.629083753\n",
      "Trained batch 1054 batch loss 0.760906696 epoch total loss 0.629208803\n",
      "Trained batch 1055 batch loss 0.692961037 epoch total loss 0.629269242\n",
      "Trained batch 1056 batch loss 0.711205065 epoch total loss 0.629346788\n",
      "Trained batch 1057 batch loss 0.571047425 epoch total loss 0.629291654\n",
      "Trained batch 1058 batch loss 0.515081704 epoch total loss 0.62918365\n",
      "Trained batch 1059 batch loss 0.648507237 epoch total loss 0.629201889\n",
      "Trained batch 1060 batch loss 0.661034167 epoch total loss 0.62923193\n",
      "Trained batch 1061 batch loss 0.613150716 epoch total loss 0.62921679\n",
      "Trained batch 1062 batch loss 0.674805 epoch total loss 0.629259706\n",
      "Trained batch 1063 batch loss 0.638859034 epoch total loss 0.629268706\n",
      "Trained batch 1064 batch loss 0.5780617 epoch total loss 0.629220605\n",
      "Trained batch 1065 batch loss 0.641245127 epoch total loss 0.62923187\n",
      "Trained batch 1066 batch loss 0.63861835 epoch total loss 0.629240692\n",
      "Trained batch 1067 batch loss 0.627480447 epoch total loss 0.629239082\n",
      "Trained batch 1068 batch loss 0.582870841 epoch total loss 0.629195631\n",
      "Trained batch 1069 batch loss 0.561028957 epoch total loss 0.629131913\n",
      "Trained batch 1070 batch loss 0.619697809 epoch total loss 0.629123092\n",
      "Trained batch 1071 batch loss 0.695247054 epoch total loss 0.629184783\n",
      "Trained batch 1072 batch loss 0.663706243 epoch total loss 0.629217\n",
      "Trained batch 1073 batch loss 0.641754806 epoch total loss 0.629228711\n",
      "Trained batch 1074 batch loss 0.652090609 epoch total loss 0.62925\n",
      "Trained batch 1075 batch loss 0.64657855 epoch total loss 0.629266143\n",
      "Trained batch 1076 batch loss 0.652308762 epoch total loss 0.629287541\n",
      "Trained batch 1077 batch loss 0.675980747 epoch total loss 0.629330873\n",
      "Trained batch 1078 batch loss 0.681221783 epoch total loss 0.629379034\n",
      "Trained batch 1079 batch loss 0.706340969 epoch total loss 0.629450381\n",
      "Trained batch 1080 batch loss 0.645841718 epoch total loss 0.62946552\n",
      "Trained batch 1081 batch loss 0.7034868 epoch total loss 0.629534\n",
      "Trained batch 1082 batch loss 0.685080528 epoch total loss 0.629585326\n",
      "Trained batch 1083 batch loss 0.59701854 epoch total loss 0.629555285\n",
      "Trained batch 1084 batch loss 0.580605268 epoch total loss 0.629510105\n",
      "Trained batch 1085 batch loss 0.62042743 epoch total loss 0.62950176\n",
      "Trained batch 1086 batch loss 0.5397048 epoch total loss 0.629419088\n",
      "Trained batch 1087 batch loss 0.617982149 epoch total loss 0.629408598\n",
      "Trained batch 1088 batch loss 0.572287321 epoch total loss 0.629356\n",
      "Trained batch 1089 batch loss 0.580542624 epoch total loss 0.629311264\n",
      "Trained batch 1090 batch loss 0.612001598 epoch total loss 0.629295349\n",
      "Trained batch 1091 batch loss 0.579768836 epoch total loss 0.62925\n",
      "Trained batch 1092 batch loss 0.570810318 epoch total loss 0.629196465\n",
      "Trained batch 1093 batch loss 0.602094531 epoch total loss 0.629171669\n",
      "Trained batch 1094 batch loss 0.588521063 epoch total loss 0.629134476\n",
      "Trained batch 1095 batch loss 0.600163 epoch total loss 0.629108\n",
      "Trained batch 1096 batch loss 0.644395888 epoch total loss 0.629122\n",
      "Trained batch 1097 batch loss 0.577505469 epoch total loss 0.629074931\n",
      "Trained batch 1098 batch loss 0.662524402 epoch total loss 0.629105449\n",
      "Trained batch 1099 batch loss 0.657259703 epoch total loss 0.629131079\n",
      "Trained batch 1100 batch loss 0.672131658 epoch total loss 0.62917012\n",
      "Trained batch 1101 batch loss 0.716412663 epoch total loss 0.629249394\n",
      "Trained batch 1102 batch loss 0.658944607 epoch total loss 0.629276335\n",
      "Trained batch 1103 batch loss 0.633375645 epoch total loss 0.629280031\n",
      "Trained batch 1104 batch loss 0.626528 epoch total loss 0.629277527\n",
      "Trained batch 1105 batch loss 0.619178653 epoch total loss 0.629268408\n",
      "Trained batch 1106 batch loss 0.577645957 epoch total loss 0.629221737\n",
      "Trained batch 1107 batch loss 0.626184881 epoch total loss 0.629219\n",
      "Trained batch 1108 batch loss 0.539744735 epoch total loss 0.629138231\n",
      "Trained batch 1109 batch loss 0.484865904 epoch total loss 0.629008114\n",
      "Trained batch 1110 batch loss 0.557547212 epoch total loss 0.628943741\n",
      "Trained batch 1111 batch loss 0.538458526 epoch total loss 0.628862321\n",
      "Trained batch 1112 batch loss 0.604010761 epoch total loss 0.62883997\n",
      "Trained batch 1113 batch loss 0.666087568 epoch total loss 0.628873408\n",
      "Trained batch 1114 batch loss 0.592088282 epoch total loss 0.628840387\n",
      "Trained batch 1115 batch loss 0.716858149 epoch total loss 0.628919303\n",
      "Trained batch 1116 batch loss 0.703054368 epoch total loss 0.628985763\n",
      "Trained batch 1117 batch loss 0.633471489 epoch total loss 0.628989816\n",
      "Trained batch 1118 batch loss 0.618217945 epoch total loss 0.62898016\n",
      "Trained batch 1119 batch loss 0.669433594 epoch total loss 0.62901634\n",
      "Trained batch 1120 batch loss 0.56012404 epoch total loss 0.628954828\n",
      "Trained batch 1121 batch loss 0.646903038 epoch total loss 0.628970802\n",
      "Trained batch 1122 batch loss 0.567753434 epoch total loss 0.628916264\n",
      "Trained batch 1123 batch loss 0.578799486 epoch total loss 0.62887162\n",
      "Trained batch 1124 batch loss 0.687191248 epoch total loss 0.628923535\n",
      "Trained batch 1125 batch loss 0.660286903 epoch total loss 0.628951371\n",
      "Trained batch 1126 batch loss 0.5003016 epoch total loss 0.628837109\n",
      "Trained batch 1127 batch loss 0.521890819 epoch total loss 0.628742278\n",
      "Trained batch 1128 batch loss 0.498125851 epoch total loss 0.628626466\n",
      "Trained batch 1129 batch loss 0.502182245 epoch total loss 0.628514469\n",
      "Trained batch 1130 batch loss 0.558367431 epoch total loss 0.628452361\n",
      "Trained batch 1131 batch loss 0.5044294 epoch total loss 0.628342748\n",
      "Trained batch 1132 batch loss 0.614457607 epoch total loss 0.628330469\n",
      "Trained batch 1133 batch loss 0.524629593 epoch total loss 0.628239\n",
      "Trained batch 1134 batch loss 0.602060676 epoch total loss 0.628215849\n",
      "Trained batch 1135 batch loss 0.574024916 epoch total loss 0.628168106\n",
      "Trained batch 1136 batch loss 0.49038738 epoch total loss 0.62804687\n",
      "Trained batch 1137 batch loss 0.591283917 epoch total loss 0.628014565\n",
      "Trained batch 1138 batch loss 0.64220643 epoch total loss 0.628027\n",
      "Trained batch 1139 batch loss 0.534576 epoch total loss 0.627944946\n",
      "Trained batch 1140 batch loss 0.602688849 epoch total loss 0.627922773\n",
      "Trained batch 1141 batch loss 0.534223497 epoch total loss 0.627840698\n",
      "Trained batch 1142 batch loss 0.549982786 epoch total loss 0.62777251\n",
      "Trained batch 1143 batch loss 0.604958296 epoch total loss 0.627752542\n",
      "Trained batch 1144 batch loss 0.646324754 epoch total loss 0.627768755\n",
      "Trained batch 1145 batch loss 0.657360792 epoch total loss 0.627794623\n",
      "Trained batch 1146 batch loss 0.618350625 epoch total loss 0.627786338\n",
      "Trained batch 1147 batch loss 0.560329914 epoch total loss 0.627727509\n",
      "Trained batch 1148 batch loss 0.688794196 epoch total loss 0.627780735\n",
      "Trained batch 1149 batch loss 0.635399699 epoch total loss 0.627787352\n",
      "Trained batch 1150 batch loss 0.585038364 epoch total loss 0.627750158\n",
      "Trained batch 1151 batch loss 0.551552415 epoch total loss 0.627683938\n",
      "Trained batch 1152 batch loss 0.484662354 epoch total loss 0.627559841\n",
      "Trained batch 1153 batch loss 0.630290151 epoch total loss 0.627562225\n",
      "Trained batch 1154 batch loss 0.597036898 epoch total loss 0.62753576\n",
      "Trained batch 1155 batch loss 0.581546426 epoch total loss 0.627495944\n",
      "Trained batch 1156 batch loss 0.613682389 epoch total loss 0.627484\n",
      "Trained batch 1157 batch loss 0.56682986 epoch total loss 0.627431571\n",
      "Trained batch 1158 batch loss 0.660856128 epoch total loss 0.62746042\n",
      "Trained batch 1159 batch loss 0.635866761 epoch total loss 0.627467692\n",
      "Trained batch 1160 batch loss 0.672012925 epoch total loss 0.627506077\n",
      "Trained batch 1161 batch loss 0.780596 epoch total loss 0.627637923\n",
      "Trained batch 1162 batch loss 0.689914346 epoch total loss 0.627691507\n",
      "Trained batch 1163 batch loss 0.735372066 epoch total loss 0.627784133\n",
      "Trained batch 1164 batch loss 0.73601222 epoch total loss 0.627877116\n",
      "Trained batch 1165 batch loss 0.617462933 epoch total loss 0.627868176\n",
      "Trained batch 1166 batch loss 0.615946591 epoch total loss 0.627858\n",
      "Trained batch 1167 batch loss 0.566729248 epoch total loss 0.627805591\n",
      "Trained batch 1168 batch loss 0.622176647 epoch total loss 0.627800763\n",
      "Trained batch 1169 batch loss 0.579562604 epoch total loss 0.627759516\n",
      "Trained batch 1170 batch loss 0.607150555 epoch total loss 0.627741933\n",
      "Trained batch 1171 batch loss 0.637134492 epoch total loss 0.62775\n",
      "Trained batch 1172 batch loss 0.589742422 epoch total loss 0.627717495\n",
      "Trained batch 1173 batch loss 0.618861794 epoch total loss 0.627709925\n",
      "Trained batch 1174 batch loss 0.642680943 epoch total loss 0.62772274\n",
      "Trained batch 1175 batch loss 0.664936662 epoch total loss 0.62775439\n",
      "Trained batch 1176 batch loss 0.571462929 epoch total loss 0.627706528\n",
      "Trained batch 1177 batch loss 0.58257246 epoch total loss 0.627668202\n",
      "Trained batch 1178 batch loss 0.659241915 epoch total loss 0.627694964\n",
      "Trained batch 1179 batch loss 0.539196849 epoch total loss 0.627619922\n",
      "Trained batch 1180 batch loss 0.581973791 epoch total loss 0.627581239\n",
      "Trained batch 1181 batch loss 0.68341881 epoch total loss 0.627628505\n",
      "Trained batch 1182 batch loss 0.72312361 epoch total loss 0.627709329\n",
      "Trained batch 1183 batch loss 0.7288 epoch total loss 0.627794743\n",
      "Trained batch 1184 batch loss 0.660586715 epoch total loss 0.627822459\n",
      "Trained batch 1185 batch loss 0.783991814 epoch total loss 0.627954245\n",
      "Trained batch 1186 batch loss 0.710000396 epoch total loss 0.628023446\n",
      "Trained batch 1187 batch loss 0.666768789 epoch total loss 0.628056049\n",
      "Trained batch 1188 batch loss 0.702701926 epoch total loss 0.628118932\n",
      "Trained batch 1189 batch loss 0.703929 epoch total loss 0.62818265\n",
      "Trained batch 1190 batch loss 0.6288836 epoch total loss 0.628183246\n",
      "Trained batch 1191 batch loss 0.660281539 epoch total loss 0.628210187\n",
      "Trained batch 1192 batch loss 0.672748327 epoch total loss 0.628247559\n",
      "Trained batch 1193 batch loss 0.700893342 epoch total loss 0.628308415\n",
      "Trained batch 1194 batch loss 0.653985739 epoch total loss 0.628329933\n",
      "Trained batch 1195 batch loss 0.672134578 epoch total loss 0.62836659\n",
      "Trained batch 1196 batch loss 0.630220771 epoch total loss 0.628368139\n",
      "Trained batch 1197 batch loss 0.627261937 epoch total loss 0.628367245\n",
      "Trained batch 1198 batch loss 0.654431939 epoch total loss 0.628389\n",
      "Trained batch 1199 batch loss 0.634863555 epoch total loss 0.628394425\n",
      "Trained batch 1200 batch loss 0.656831443 epoch total loss 0.628418148\n",
      "Trained batch 1201 batch loss 0.612700701 epoch total loss 0.628405035\n",
      "Trained batch 1202 batch loss 0.579677701 epoch total loss 0.628364444\n",
      "Trained batch 1203 batch loss 0.650161743 epoch total loss 0.628382564\n",
      "Trained batch 1204 batch loss 0.741514206 epoch total loss 0.628476501\n",
      "Trained batch 1205 batch loss 0.616477907 epoch total loss 0.628466547\n",
      "Trained batch 1206 batch loss 0.611409783 epoch total loss 0.628452361\n",
      "Trained batch 1207 batch loss 0.605444968 epoch total loss 0.628433347\n",
      "Trained batch 1208 batch loss 0.656994045 epoch total loss 0.62845695\n",
      "Trained batch 1209 batch loss 0.629174709 epoch total loss 0.628457546\n",
      "Trained batch 1210 batch loss 0.559313178 epoch total loss 0.628400385\n",
      "Trained batch 1211 batch loss 0.540766656 epoch total loss 0.628328\n",
      "Trained batch 1212 batch loss 0.632011414 epoch total loss 0.628331125\n",
      "Trained batch 1213 batch loss 0.577652693 epoch total loss 0.628289282\n",
      "Trained batch 1214 batch loss 0.594400704 epoch total loss 0.628261387\n",
      "Trained batch 1215 batch loss 0.733807921 epoch total loss 0.628348291\n",
      "Trained batch 1216 batch loss 0.752077878 epoch total loss 0.628450036\n",
      "Trained batch 1217 batch loss 0.730840147 epoch total loss 0.628534198\n",
      "Trained batch 1218 batch loss 0.78181994 epoch total loss 0.62866\n",
      "Trained batch 1219 batch loss 0.786790609 epoch total loss 0.628789723\n",
      "Trained batch 1220 batch loss 0.620838 epoch total loss 0.628783226\n",
      "Trained batch 1221 batch loss 0.547484279 epoch total loss 0.628716648\n",
      "Trained batch 1222 batch loss 0.604295075 epoch total loss 0.62869668\n",
      "Trained batch 1223 batch loss 0.628839135 epoch total loss 0.628696799\n",
      "Trained batch 1224 batch loss 0.59976697 epoch total loss 0.628673196\n",
      "Trained batch 1225 batch loss 0.576263368 epoch total loss 0.6286304\n",
      "Trained batch 1226 batch loss 0.626452148 epoch total loss 0.628628612\n",
      "Trained batch 1227 batch loss 0.653452039 epoch total loss 0.628648818\n",
      "Trained batch 1228 batch loss 0.601973295 epoch total loss 0.628627121\n",
      "Trained batch 1229 batch loss 0.631153 epoch total loss 0.628629208\n",
      "Trained batch 1230 batch loss 0.60675329 epoch total loss 0.628611386\n",
      "Trained batch 1231 batch loss 0.659626067 epoch total loss 0.628636599\n",
      "Trained batch 1232 batch loss 0.672427714 epoch total loss 0.628672123\n",
      "Trained batch 1233 batch loss 0.635473847 epoch total loss 0.628677666\n",
      "Trained batch 1234 batch loss 0.639038444 epoch total loss 0.62868607\n",
      "Trained batch 1235 batch loss 0.732818544 epoch total loss 0.628770351\n",
      "Trained batch 1236 batch loss 0.699654877 epoch total loss 0.628827691\n",
      "Trained batch 1237 batch loss 0.684940934 epoch total loss 0.62887305\n",
      "Trained batch 1238 batch loss 0.753431618 epoch total loss 0.628973663\n",
      "Trained batch 1239 batch loss 0.702199459 epoch total loss 0.629032731\n",
      "Trained batch 1240 batch loss 0.632286191 epoch total loss 0.629035354\n",
      "Trained batch 1241 batch loss 0.658970475 epoch total loss 0.629059494\n",
      "Trained batch 1242 batch loss 0.619541764 epoch total loss 0.629051864\n",
      "Trained batch 1243 batch loss 0.577694297 epoch total loss 0.629010558\n",
      "Trained batch 1244 batch loss 0.57972753 epoch total loss 0.628970921\n",
      "Trained batch 1245 batch loss 0.632296801 epoch total loss 0.628973603\n",
      "Trained batch 1246 batch loss 0.607756257 epoch total loss 0.628956556\n",
      "Trained batch 1247 batch loss 0.653674662 epoch total loss 0.628976405\n",
      "Trained batch 1248 batch loss 0.653949857 epoch total loss 0.628996372\n",
      "Trained batch 1249 batch loss 0.63334924 epoch total loss 0.628999889\n",
      "Trained batch 1250 batch loss 0.613743901 epoch total loss 0.62898767\n",
      "Trained batch 1251 batch loss 0.57715416 epoch total loss 0.628946245\n",
      "Trained batch 1252 batch loss 0.620430052 epoch total loss 0.62893945\n",
      "Trained batch 1253 batch loss 0.547953844 epoch total loss 0.628874838\n",
      "Trained batch 1254 batch loss 0.598585963 epoch total loss 0.628850639\n",
      "Trained batch 1255 batch loss 0.604025722 epoch total loss 0.62883085\n",
      "Trained batch 1256 batch loss 0.555566 epoch total loss 0.628772497\n",
      "Trained batch 1257 batch loss 0.625619888 epoch total loss 0.62877\n",
      "Trained batch 1258 batch loss 0.64471674 epoch total loss 0.62878269\n",
      "Trained batch 1259 batch loss 0.522119522 epoch total loss 0.628697932\n",
      "Trained batch 1260 batch loss 0.664599538 epoch total loss 0.628726423\n",
      "Trained batch 1261 batch loss 0.602098048 epoch total loss 0.628705323\n",
      "Trained batch 1262 batch loss 0.585469902 epoch total loss 0.62867105\n",
      "Trained batch 1263 batch loss 0.541748405 epoch total loss 0.628602207\n",
      "Trained batch 1264 batch loss 0.55858016 epoch total loss 0.628546834\n",
      "Trained batch 1265 batch loss 0.667835355 epoch total loss 0.628577888\n",
      "Trained batch 1266 batch loss 0.677296758 epoch total loss 0.628616393\n",
      "Trained batch 1267 batch loss 0.600155354 epoch total loss 0.628593922\n",
      "Trained batch 1268 batch loss 0.62936759 epoch total loss 0.628594577\n",
      "Trained batch 1269 batch loss 0.551363647 epoch total loss 0.628533721\n",
      "Trained batch 1270 batch loss 0.595438838 epoch total loss 0.628507674\n",
      "Trained batch 1271 batch loss 0.550144136 epoch total loss 0.628446043\n",
      "Trained batch 1272 batch loss 0.519996405 epoch total loss 0.628360808\n",
      "Trained batch 1273 batch loss 0.569321096 epoch total loss 0.628314435\n",
      "Trained batch 1274 batch loss 0.646641076 epoch total loss 0.62832886\n",
      "Trained batch 1275 batch loss 0.608488262 epoch total loss 0.628313303\n",
      "Trained batch 1276 batch loss 0.59223336 epoch total loss 0.628285\n",
      "Trained batch 1277 batch loss 0.633903742 epoch total loss 0.628289402\n",
      "Trained batch 1278 batch loss 0.640863419 epoch total loss 0.628299236\n",
      "Trained batch 1279 batch loss 0.71975261 epoch total loss 0.628370702\n",
      "Trained batch 1280 batch loss 0.58585 epoch total loss 0.628337502\n",
      "Trained batch 1281 batch loss 0.582729042 epoch total loss 0.628301919\n",
      "Trained batch 1282 batch loss 0.54954195 epoch total loss 0.628240466\n",
      "Trained batch 1283 batch loss 0.521472037 epoch total loss 0.628157258\n",
      "Trained batch 1284 batch loss 0.62412107 epoch total loss 0.628154159\n",
      "Trained batch 1285 batch loss 0.605543911 epoch total loss 0.628136516\n",
      "Trained batch 1286 batch loss 0.658393621 epoch total loss 0.628160059\n",
      "Trained batch 1287 batch loss 0.626200616 epoch total loss 0.628158569\n",
      "Trained batch 1288 batch loss 0.636641383 epoch total loss 0.628165185\n",
      "Trained batch 1289 batch loss 0.614454448 epoch total loss 0.628154516\n",
      "Trained batch 1290 batch loss 0.636456072 epoch total loss 0.628160954\n",
      "Trained batch 1291 batch loss 0.639092505 epoch total loss 0.628169417\n",
      "Trained batch 1292 batch loss 0.674289584 epoch total loss 0.628205121\n",
      "Trained batch 1293 batch loss 0.626428962 epoch total loss 0.62820375\n",
      "Trained batch 1294 batch loss 0.583795547 epoch total loss 0.628169417\n",
      "Trained batch 1295 batch loss 0.514615417 epoch total loss 0.628081739\n",
      "Trained batch 1296 batch loss 0.563432 epoch total loss 0.62803185\n",
      "Trained batch 1297 batch loss 0.544837356 epoch total loss 0.627967715\n",
      "Trained batch 1298 batch loss 0.610651493 epoch total loss 0.627954364\n",
      "Trained batch 1299 batch loss 0.64601475 epoch total loss 0.627968252\n",
      "Trained batch 1300 batch loss 0.510531485 epoch total loss 0.627877951\n",
      "Trained batch 1301 batch loss 0.576495647 epoch total loss 0.627838433\n",
      "Trained batch 1302 batch loss 0.533336401 epoch total loss 0.627765834\n",
      "Trained batch 1303 batch loss 0.45684737 epoch total loss 0.627634704\n",
      "Trained batch 1304 batch loss 0.460473895 epoch total loss 0.627506495\n",
      "Trained batch 1305 batch loss 0.448979467 epoch total loss 0.627369642\n",
      "Trained batch 1306 batch loss 0.498772591 epoch total loss 0.627271175\n",
      "Trained batch 1307 batch loss 0.676577568 epoch total loss 0.627308905\n",
      "Trained batch 1308 batch loss 0.595061421 epoch total loss 0.627284229\n",
      "Trained batch 1309 batch loss 0.702715516 epoch total loss 0.627341866\n",
      "Trained batch 1310 batch loss 0.689486921 epoch total loss 0.627389312\n",
      "Trained batch 1311 batch loss 0.782488585 epoch total loss 0.627507627\n",
      "Trained batch 1312 batch loss 0.833764493 epoch total loss 0.627664804\n",
      "Trained batch 1313 batch loss 0.779058039 epoch total loss 0.62778008\n",
      "Trained batch 1314 batch loss 0.656882405 epoch total loss 0.627802253\n",
      "Trained batch 1315 batch loss 0.546844 epoch total loss 0.627740622\n",
      "Trained batch 1316 batch loss 0.705989361 epoch total loss 0.627800107\n",
      "Trained batch 1317 batch loss 0.506986618 epoch total loss 0.627708375\n",
      "Trained batch 1318 batch loss 0.639533758 epoch total loss 0.627717316\n",
      "Trained batch 1319 batch loss 0.666739583 epoch total loss 0.62774688\n",
      "Trained batch 1320 batch loss 0.683355808 epoch total loss 0.627789\n",
      "Trained batch 1321 batch loss 0.677768886 epoch total loss 0.627826869\n",
      "Trained batch 1322 batch loss 0.655883431 epoch total loss 0.627848089\n",
      "Trained batch 1323 batch loss 0.645566046 epoch total loss 0.6278615\n",
      "Trained batch 1324 batch loss 0.618812084 epoch total loss 0.627854705\n",
      "Trained batch 1325 batch loss 0.627013862 epoch total loss 0.627854049\n",
      "Trained batch 1326 batch loss 0.655344725 epoch total loss 0.627874792\n",
      "Trained batch 1327 batch loss 0.656990469 epoch total loss 0.627896726\n",
      "Trained batch 1328 batch loss 0.657157063 epoch total loss 0.62791872\n",
      "Trained batch 1329 batch loss 0.647936 epoch total loss 0.6279338\n",
      "Trained batch 1330 batch loss 0.640652239 epoch total loss 0.627943337\n",
      "Trained batch 1331 batch loss 0.56461978 epoch total loss 0.627895772\n",
      "Trained batch 1332 batch loss 0.647663713 epoch total loss 0.627910614\n",
      "Trained batch 1333 batch loss 0.657481 epoch total loss 0.627932787\n",
      "Trained batch 1334 batch loss 0.55025965 epoch total loss 0.627874553\n",
      "Trained batch 1335 batch loss 0.58923018 epoch total loss 0.627845585\n",
      "Trained batch 1336 batch loss 0.557711542 epoch total loss 0.627793133\n",
      "Trained batch 1337 batch loss 0.552822769 epoch total loss 0.627737045\n",
      "Trained batch 1338 batch loss 0.612550735 epoch total loss 0.627725661\n",
      "Trained batch 1339 batch loss 0.677657783 epoch total loss 0.627763\n",
      "Trained batch 1340 batch loss 0.69598943 epoch total loss 0.627813876\n",
      "Trained batch 1341 batch loss 0.603576601 epoch total loss 0.627795815\n",
      "Trained batch 1342 batch loss 0.606353283 epoch total loss 0.627779841\n",
      "Trained batch 1343 batch loss 0.541471 epoch total loss 0.627715528\n",
      "Trained batch 1344 batch loss 0.546115696 epoch total loss 0.62765485\n",
      "Trained batch 1345 batch loss 0.518065155 epoch total loss 0.627573371\n",
      "Trained batch 1346 batch loss 0.608891785 epoch total loss 0.627559483\n",
      "Trained batch 1347 batch loss 0.608693421 epoch total loss 0.627545476\n",
      "Trained batch 1348 batch loss 0.654807448 epoch total loss 0.627565682\n",
      "Trained batch 1349 batch loss 0.527408838 epoch total loss 0.627491474\n",
      "Trained batch 1350 batch loss 0.546816707 epoch total loss 0.627431691\n",
      "Trained batch 1351 batch loss 0.541457832 epoch total loss 0.627368033\n",
      "Trained batch 1352 batch loss 0.592725635 epoch total loss 0.627342403\n",
      "Trained batch 1353 batch loss 0.714163959 epoch total loss 0.627406597\n",
      "Trained batch 1354 batch loss 0.651441514 epoch total loss 0.6274243\n",
      "Trained batch 1355 batch loss 0.673060179 epoch total loss 0.627458\n",
      "Trained batch 1356 batch loss 0.635579228 epoch total loss 0.627463937\n",
      "Trained batch 1357 batch loss 0.662571847 epoch total loss 0.627489865\n",
      "Trained batch 1358 batch loss 0.605662823 epoch total loss 0.627473772\n",
      "Trained batch 1359 batch loss 0.611914098 epoch total loss 0.627462327\n",
      "Trained batch 1360 batch loss 0.646376848 epoch total loss 0.627476215\n",
      "Trained batch 1361 batch loss 0.586527348 epoch total loss 0.627446175\n",
      "Trained batch 1362 batch loss 0.630526185 epoch total loss 0.62744844\n",
      "Trained batch 1363 batch loss 0.618119538 epoch total loss 0.627441585\n",
      "Trained batch 1364 batch loss 0.683152616 epoch total loss 0.627482414\n",
      "Trained batch 1365 batch loss 0.711624205 epoch total loss 0.627544045\n",
      "Trained batch 1366 batch loss 0.697733462 epoch total loss 0.627595484\n",
      "Trained batch 1367 batch loss 0.621876299 epoch total loss 0.627591312\n",
      "Trained batch 1368 batch loss 0.593293 epoch total loss 0.627566218\n",
      "Trained batch 1369 batch loss 0.586257339 epoch total loss 0.627536058\n",
      "Trained batch 1370 batch loss 0.599936128 epoch total loss 0.627515912\n",
      "Trained batch 1371 batch loss 0.61169064 epoch total loss 0.627504349\n",
      "Trained batch 1372 batch loss 0.598722219 epoch total loss 0.627483368\n",
      "Trained batch 1373 batch loss 0.607582211 epoch total loss 0.627468884\n",
      "Trained batch 1374 batch loss 0.629227281 epoch total loss 0.627470136\n",
      "Trained batch 1375 batch loss 0.517357051 epoch total loss 0.62739\n",
      "Trained batch 1376 batch loss 0.556707203 epoch total loss 0.627338648\n",
      "Trained batch 1377 batch loss 0.53164947 epoch total loss 0.627269208\n",
      "Trained batch 1378 batch loss 0.61888 epoch total loss 0.627263129\n",
      "Trained batch 1379 batch loss 0.683329523 epoch total loss 0.627303779\n",
      "Trained batch 1380 batch loss 0.643527508 epoch total loss 0.627315581\n",
      "Trained batch 1381 batch loss 0.570036054 epoch total loss 0.627274096\n",
      "Trained batch 1382 batch loss 0.580621898 epoch total loss 0.6272403\n",
      "Trained batch 1383 batch loss 0.630619526 epoch total loss 0.627242744\n",
      "Trained batch 1384 batch loss 0.569718957 epoch total loss 0.6272012\n",
      "Trained batch 1385 batch loss 0.521398544 epoch total loss 0.627124846\n",
      "Trained batch 1386 batch loss 0.57500428 epoch total loss 0.627087235\n",
      "Trained batch 1387 batch loss 0.567590892 epoch total loss 0.62704432\n",
      "Trained batch 1388 batch loss 0.695849299 epoch total loss 0.627093911\n",
      "Trained batch 1389 batch loss 0.709274 epoch total loss 0.627153039\n",
      "Trained batch 1390 batch loss 0.645472646 epoch total loss 0.627166212\n",
      "Trained batch 1391 batch loss 0.739217281 epoch total loss 0.627246737\n",
      "Trained batch 1392 batch loss 0.731255531 epoch total loss 0.627321482\n",
      "Trained batch 1393 batch loss 0.695559144 epoch total loss 0.627370477\n",
      "Trained batch 1394 batch loss 0.627981246 epoch total loss 0.627370894\n",
      "Trained batch 1395 batch loss 0.665934205 epoch total loss 0.627398551\n",
      "Trained batch 1396 batch loss 0.607225 epoch total loss 0.627384126\n",
      "Trained batch 1397 batch loss 0.619880319 epoch total loss 0.627378762\n",
      "Trained batch 1398 batch loss 0.647549868 epoch total loss 0.627393186\n",
      "Trained batch 1399 batch loss 0.63514781 epoch total loss 0.62739867\n",
      "Trained batch 1400 batch loss 0.633748472 epoch total loss 0.6274032\n",
      "Trained batch 1401 batch loss 0.595862031 epoch total loss 0.627380729\n",
      "Trained batch 1402 batch loss 0.585481048 epoch total loss 0.627350867\n",
      "Trained batch 1403 batch loss 0.598719478 epoch total loss 0.627330422\n",
      "Trained batch 1404 batch loss 0.642381668 epoch total loss 0.627341151\n",
      "Trained batch 1405 batch loss 0.57074666 epoch total loss 0.627300858\n",
      "Trained batch 1406 batch loss 0.650619864 epoch total loss 0.627317488\n",
      "Trained batch 1407 batch loss 0.6289621 epoch total loss 0.627318621\n",
      "Trained batch 1408 batch loss 0.754951954 epoch total loss 0.627409279\n",
      "Trained batch 1409 batch loss 0.674176872 epoch total loss 0.627442479\n",
      "Trained batch 1410 batch loss 0.532031596 epoch total loss 0.627374828\n",
      "Trained batch 1411 batch loss 0.594496787 epoch total loss 0.627351522\n",
      "Trained batch 1412 batch loss 0.635968328 epoch total loss 0.627357662\n",
      "Trained batch 1413 batch loss 0.662715197 epoch total loss 0.627382636\n",
      "Trained batch 1414 batch loss 0.607698679 epoch total loss 0.627368748\n",
      "Trained batch 1415 batch loss 0.605326891 epoch total loss 0.627353191\n",
      "Trained batch 1416 batch loss 0.548220098 epoch total loss 0.627297282\n",
      "Trained batch 1417 batch loss 0.556478262 epoch total loss 0.627247334\n",
      "Trained batch 1418 batch loss 0.571731329 epoch total loss 0.627208173\n",
      "Trained batch 1419 batch loss 0.624559 epoch total loss 0.627206326\n",
      "Trained batch 1420 batch loss 0.624622166 epoch total loss 0.627204478\n",
      "Trained batch 1421 batch loss 0.657745838 epoch total loss 0.627226\n",
      "Trained batch 1422 batch loss 0.540298045 epoch total loss 0.627164841\n",
      "Trained batch 1423 batch loss 0.602378786 epoch total loss 0.627147436\n",
      "Trained batch 1424 batch loss 0.539845 epoch total loss 0.627086103\n",
      "Trained batch 1425 batch loss 0.56336242 epoch total loss 0.627041399\n",
      "Trained batch 1426 batch loss 0.527887106 epoch total loss 0.6269719\n",
      "Trained batch 1427 batch loss 0.618354321 epoch total loss 0.626965821\n",
      "Trained batch 1428 batch loss 0.596614659 epoch total loss 0.626944602\n",
      "Trained batch 1429 batch loss 0.568645179 epoch total loss 0.626903772\n",
      "Trained batch 1430 batch loss 0.672901869 epoch total loss 0.626935959\n",
      "Trained batch 1431 batch loss 0.673269033 epoch total loss 0.626968324\n",
      "Trained batch 1432 batch loss 0.638762891 epoch total loss 0.62697655\n",
      "Trained batch 1433 batch loss 0.68877393 epoch total loss 0.627019703\n",
      "Trained batch 1434 batch loss 0.660075366 epoch total loss 0.62704277\n",
      "Trained batch 1435 batch loss 0.574021637 epoch total loss 0.627005816\n",
      "Trained batch 1436 batch loss 0.616417766 epoch total loss 0.626998425\n",
      "Trained batch 1437 batch loss 0.64843446 epoch total loss 0.627013326\n",
      "Trained batch 1438 batch loss 0.684153318 epoch total loss 0.627053082\n",
      "Trained batch 1439 batch loss 0.596447051 epoch total loss 0.627031803\n",
      "Trained batch 1440 batch loss 0.57915175 epoch total loss 0.626998544\n",
      "Trained batch 1441 batch loss 0.628000498 epoch total loss 0.626999259\n",
      "Trained batch 1442 batch loss 0.622353 epoch total loss 0.62699604\n",
      "Trained batch 1443 batch loss 0.582412183 epoch total loss 0.626965165\n",
      "Trained batch 1444 batch loss 0.633788168 epoch total loss 0.626969874\n",
      "Trained batch 1445 batch loss 0.543262184 epoch total loss 0.626911938\n",
      "Trained batch 1446 batch loss 0.555524826 epoch total loss 0.626862586\n",
      "Trained batch 1447 batch loss 0.628352821 epoch total loss 0.626863599\n",
      "Trained batch 1448 batch loss 0.613630295 epoch total loss 0.626854479\n",
      "Trained batch 1449 batch loss 0.622540832 epoch total loss 0.626851499\n",
      "Trained batch 1450 batch loss 0.751944602 epoch total loss 0.626937807\n",
      "Trained batch 1451 batch loss 0.731417179 epoch total loss 0.627009809\n",
      "Trained batch 1452 batch loss 0.588333368 epoch total loss 0.626983166\n",
      "Trained batch 1453 batch loss 0.679971695 epoch total loss 0.627019644\n",
      "Trained batch 1454 batch loss 0.742728233 epoch total loss 0.627099276\n",
      "Trained batch 1455 batch loss 0.689027429 epoch total loss 0.627141833\n",
      "Trained batch 1456 batch loss 0.654899657 epoch total loss 0.627160907\n",
      "Trained batch 1457 batch loss 0.665493965 epoch total loss 0.627187192\n",
      "Trained batch 1458 batch loss 0.67145282 epoch total loss 0.627217531\n",
      "Trained batch 1459 batch loss 0.610825896 epoch total loss 0.627206326\n",
      "Trained batch 1460 batch loss 0.666583538 epoch total loss 0.627233267\n",
      "Trained batch 1461 batch loss 0.646251202 epoch total loss 0.627246261\n",
      "Trained batch 1462 batch loss 0.677499175 epoch total loss 0.627280653\n",
      "Trained batch 1463 batch loss 0.721747339 epoch total loss 0.627345204\n",
      "Trained batch 1464 batch loss 0.642093062 epoch total loss 0.627355278\n",
      "Trained batch 1465 batch loss 0.582312405 epoch total loss 0.627324522\n",
      "Trained batch 1466 batch loss 0.669995666 epoch total loss 0.627353668\n",
      "Trained batch 1467 batch loss 0.586903632 epoch total loss 0.627326071\n",
      "Trained batch 1468 batch loss 0.594071209 epoch total loss 0.627303421\n",
      "Trained batch 1469 batch loss 0.629809678 epoch total loss 0.62730515\n",
      "Trained batch 1470 batch loss 0.587106109 epoch total loss 0.627277792\n",
      "Trained batch 1471 batch loss 0.626476288 epoch total loss 0.627277195\n",
      "Trained batch 1472 batch loss 0.490803421 epoch total loss 0.62718451\n",
      "Trained batch 1473 batch loss 0.560468078 epoch total loss 0.627139211\n",
      "Trained batch 1474 batch loss 0.61282742 epoch total loss 0.627129495\n",
      "Trained batch 1475 batch loss 0.633044124 epoch total loss 0.627133548\n",
      "Trained batch 1476 batch loss 0.682706714 epoch total loss 0.627171159\n",
      "Trained batch 1477 batch loss 0.697376549 epoch total loss 0.627218723\n",
      "Trained batch 1478 batch loss 0.551706553 epoch total loss 0.627167642\n",
      "Trained batch 1479 batch loss 0.609005272 epoch total loss 0.627155364\n",
      "Trained batch 1480 batch loss 0.659617782 epoch total loss 0.627177238\n",
      "Trained batch 1481 batch loss 0.631151736 epoch total loss 0.62718\n",
      "Trained batch 1482 batch loss 0.605654359 epoch total loss 0.627165437\n",
      "Trained batch 1483 batch loss 0.662483454 epoch total loss 0.627189219\n",
      "Trained batch 1484 batch loss 0.597615302 epoch total loss 0.627169311\n",
      "Trained batch 1485 batch loss 0.510067701 epoch total loss 0.627090454\n",
      "Trained batch 1486 batch loss 0.568803906 epoch total loss 0.627051234\n",
      "Trained batch 1487 batch loss 0.606957495 epoch total loss 0.627037704\n",
      "Trained batch 1488 batch loss 0.688884139 epoch total loss 0.627079248\n",
      "Trained batch 1489 batch loss 0.625349462 epoch total loss 0.627078116\n",
      "Trained batch 1490 batch loss 0.589427114 epoch total loss 0.627052844\n",
      "Trained batch 1491 batch loss 0.62840414 epoch total loss 0.627053738\n",
      "Trained batch 1492 batch loss 0.657446 epoch total loss 0.627074122\n",
      "Trained batch 1493 batch loss 0.595774353 epoch total loss 0.627053142\n",
      "Trained batch 1494 batch loss 0.575636923 epoch total loss 0.62701875\n",
      "Trained batch 1495 batch loss 0.589718699 epoch total loss 0.626993775\n",
      "Trained batch 1496 batch loss 0.64889282 epoch total loss 0.627008379\n",
      "Trained batch 1497 batch loss 0.617395043 epoch total loss 0.627001941\n",
      "Trained batch 1498 batch loss 0.640906692 epoch total loss 0.62701124\n",
      "Trained batch 1499 batch loss 0.655984163 epoch total loss 0.627030611\n",
      "Trained batch 1500 batch loss 0.637661636 epoch total loss 0.627037704\n",
      "Trained batch 1501 batch loss 0.524600267 epoch total loss 0.626969457\n",
      "Trained batch 1502 batch loss 0.561184108 epoch total loss 0.626925588\n",
      "Trained batch 1503 batch loss 0.612039506 epoch total loss 0.626915753\n",
      "Trained batch 1504 batch loss 0.625046 epoch total loss 0.626914501\n",
      "Trained batch 1505 batch loss 0.684681892 epoch total loss 0.626952887\n",
      "Trained batch 1506 batch loss 0.637128711 epoch total loss 0.626959622\n",
      "Trained batch 1507 batch loss 0.561329484 epoch total loss 0.626916111\n",
      "Trained batch 1508 batch loss 0.615809679 epoch total loss 0.62690872\n",
      "Trained batch 1509 batch loss 0.695201695 epoch total loss 0.626953959\n",
      "Trained batch 1510 batch loss 0.625861466 epoch total loss 0.626953244\n",
      "Trained batch 1511 batch loss 0.624011755 epoch total loss 0.626951337\n",
      "Trained batch 1512 batch loss 0.576791167 epoch total loss 0.626918137\n",
      "Trained batch 1513 batch loss 0.553720236 epoch total loss 0.626869738\n",
      "Trained batch 1514 batch loss 0.493918478 epoch total loss 0.62678194\n",
      "Trained batch 1515 batch loss 0.563057303 epoch total loss 0.62673986\n",
      "Trained batch 1516 batch loss 0.577934861 epoch total loss 0.626707673\n",
      "Trained batch 1517 batch loss 0.750674605 epoch total loss 0.626789391\n",
      "Trained batch 1518 batch loss 0.657767951 epoch total loss 0.626809776\n",
      "Trained batch 1519 batch loss 0.581306 epoch total loss 0.626779795\n",
      "Trained batch 1520 batch loss 0.594316363 epoch total loss 0.626758456\n",
      "Trained batch 1521 batch loss 0.693972528 epoch total loss 0.626802623\n",
      "Trained batch 1522 batch loss 0.702352226 epoch total loss 0.626852274\n",
      "Trained batch 1523 batch loss 0.703566253 epoch total loss 0.62690264\n",
      "Trained batch 1524 batch loss 0.714412 epoch total loss 0.626960039\n",
      "Trained batch 1525 batch loss 0.650311768 epoch total loss 0.626975358\n",
      "Trained batch 1526 batch loss 0.693659604 epoch total loss 0.627019107\n",
      "Trained batch 1527 batch loss 0.695726573 epoch total loss 0.627064109\n",
      "Trained batch 1528 batch loss 0.657758057 epoch total loss 0.627084196\n",
      "Trained batch 1529 batch loss 0.659465253 epoch total loss 0.627105355\n",
      "Trained batch 1530 batch loss 0.645807266 epoch total loss 0.627117574\n",
      "Trained batch 1531 batch loss 0.620475888 epoch total loss 0.627113283\n",
      "Trained batch 1532 batch loss 0.619492233 epoch total loss 0.627108276\n",
      "Trained batch 1533 batch loss 0.656477392 epoch total loss 0.627127469\n",
      "Trained batch 1534 batch loss 0.583079457 epoch total loss 0.627098739\n",
      "Trained batch 1535 batch loss 0.673763 epoch total loss 0.627129138\n",
      "Trained batch 1536 batch loss 0.646111131 epoch total loss 0.627141535\n",
      "Trained batch 1537 batch loss 0.583786845 epoch total loss 0.627113342\n",
      "Trained batch 1538 batch loss 0.683637559 epoch total loss 0.627150059\n",
      "Trained batch 1539 batch loss 0.600640416 epoch total loss 0.627132833\n",
      "Trained batch 1540 batch loss 0.598214924 epoch total loss 0.627114058\n",
      "Trained batch 1541 batch loss 0.633291125 epoch total loss 0.627118111\n",
      "Trained batch 1542 batch loss 0.567795932 epoch total loss 0.627079606\n",
      "Trained batch 1543 batch loss 0.588032842 epoch total loss 0.627054334\n",
      "Trained batch 1544 batch loss 0.621034563 epoch total loss 0.6270504\n",
      "Trained batch 1545 batch loss 0.588860393 epoch total loss 0.627025723\n",
      "Trained batch 1546 batch loss 0.609751165 epoch total loss 0.627014518\n",
      "Trained batch 1547 batch loss 0.610429645 epoch total loss 0.627003789\n",
      "Trained batch 1548 batch loss 0.570871592 epoch total loss 0.62696749\n",
      "Trained batch 1549 batch loss 0.522161126 epoch total loss 0.626899838\n",
      "Trained batch 1550 batch loss 0.518489 epoch total loss 0.626829922\n",
      "Trained batch 1551 batch loss 0.504640698 epoch total loss 0.626751125\n",
      "Trained batch 1552 batch loss 0.486415744 epoch total loss 0.626660705\n",
      "Trained batch 1553 batch loss 0.549388051 epoch total loss 0.626610935\n",
      "Trained batch 1554 batch loss 0.591410637 epoch total loss 0.626588285\n",
      "Trained batch 1555 batch loss 0.770775557 epoch total loss 0.62668097\n",
      "Trained batch 1556 batch loss 0.679430127 epoch total loss 0.626714885\n",
      "Trained batch 1557 batch loss 0.647417486 epoch total loss 0.626728177\n",
      "Trained batch 1558 batch loss 0.734874904 epoch total loss 0.626797616\n",
      "Trained batch 1559 batch loss 0.687088728 epoch total loss 0.62683624\n",
      "Trained batch 1560 batch loss 0.638517499 epoch total loss 0.62684375\n",
      "Trained batch 1561 batch loss 0.705279112 epoch total loss 0.626893938\n",
      "Trained batch 1562 batch loss 0.621631801 epoch total loss 0.6268906\n",
      "Trained batch 1563 batch loss 0.592077374 epoch total loss 0.626868367\n",
      "Trained batch 1564 batch loss 0.612389266 epoch total loss 0.626859069\n",
      "Trained batch 1565 batch loss 0.57542485 epoch total loss 0.626826227\n",
      "Trained batch 1566 batch loss 0.587495089 epoch total loss 0.626801133\n",
      "Trained batch 1567 batch loss 0.673606873 epoch total loss 0.626831\n",
      "Trained batch 1568 batch loss 0.736308277 epoch total loss 0.626900792\n",
      "Trained batch 1569 batch loss 0.677508712 epoch total loss 0.626933038\n",
      "Trained batch 1570 batch loss 0.654594183 epoch total loss 0.626950681\n",
      "Trained batch 1571 batch loss 0.608047366 epoch total loss 0.626938641\n",
      "Trained batch 1572 batch loss 0.642046213 epoch total loss 0.626948237\n",
      "Trained batch 1573 batch loss 0.598331034 epoch total loss 0.626930058\n",
      "Trained batch 1574 batch loss 0.672330141 epoch total loss 0.626958847\n",
      "Trained batch 1575 batch loss 0.591192186 epoch total loss 0.626936138\n",
      "Trained batch 1576 batch loss 0.596042156 epoch total loss 0.626916587\n",
      "Trained batch 1577 batch loss 0.56834662 epoch total loss 0.626879454\n",
      "Trained batch 1578 batch loss 0.652742803 epoch total loss 0.626895845\n",
      "Trained batch 1579 batch loss 0.639601886 epoch total loss 0.626903892\n",
      "Trained batch 1580 batch loss 0.595874786 epoch total loss 0.626884222\n",
      "Trained batch 1581 batch loss 0.640489042 epoch total loss 0.626892865\n",
      "Trained batch 1582 batch loss 0.603902578 epoch total loss 0.626878321\n",
      "Trained batch 1583 batch loss 0.616017044 epoch total loss 0.626871467\n",
      "Trained batch 1584 batch loss 0.522626 epoch total loss 0.626805663\n",
      "Trained batch 1585 batch loss 0.559853613 epoch total loss 0.626763463\n",
      "Trained batch 1586 batch loss 0.549096525 epoch total loss 0.626714468\n",
      "Trained batch 1587 batch loss 0.578574061 epoch total loss 0.626684129\n",
      "Trained batch 1588 batch loss 0.63199 epoch total loss 0.626687467\n",
      "Trained batch 1589 batch loss 0.637573242 epoch total loss 0.626694322\n",
      "Trained batch 1590 batch loss 0.68406415 epoch total loss 0.626730382\n",
      "Trained batch 1591 batch loss 0.663068831 epoch total loss 0.626753271\n",
      "Trained batch 1592 batch loss 0.630830765 epoch total loss 0.626755834\n",
      "Trained batch 1593 batch loss 0.677528441 epoch total loss 0.626787722\n",
      "Trained batch 1594 batch loss 0.579963684 epoch total loss 0.626758337\n",
      "Trained batch 1595 batch loss 0.585259676 epoch total loss 0.626732349\n",
      "Trained batch 1596 batch loss 0.572080612 epoch total loss 0.626698077\n",
      "Trained batch 1597 batch loss 0.509327888 epoch total loss 0.626624584\n",
      "Trained batch 1598 batch loss 0.549205422 epoch total loss 0.626576126\n",
      "Trained batch 1599 batch loss 0.506434143 epoch total loss 0.626500964\n",
      "Trained batch 1600 batch loss 0.522199392 epoch total loss 0.626435816\n",
      "Trained batch 1601 batch loss 0.476965547 epoch total loss 0.626342475\n",
      "Trained batch 1602 batch loss 0.502761483 epoch total loss 0.626265347\n",
      "Trained batch 1603 batch loss 0.491855025 epoch total loss 0.626181483\n",
      "Trained batch 1604 batch loss 0.565734 epoch total loss 0.626143813\n",
      "Trained batch 1605 batch loss 0.601095617 epoch total loss 0.626128197\n",
      "Trained batch 1606 batch loss 0.557897747 epoch total loss 0.626085699\n",
      "Trained batch 1607 batch loss 0.549245656 epoch total loss 0.626037896\n",
      "Trained batch 1608 batch loss 0.556512117 epoch total loss 0.625994682\n",
      "Trained batch 1609 batch loss 0.528320312 epoch total loss 0.625933945\n",
      "Trained batch 1610 batch loss 0.568895161 epoch total loss 0.62589854\n",
      "Trained batch 1611 batch loss 0.650859594 epoch total loss 0.625914037\n",
      "Trained batch 1612 batch loss 0.669890881 epoch total loss 0.625941336\n",
      "Trained batch 1613 batch loss 0.62914443 epoch total loss 0.625943303\n",
      "Trained batch 1614 batch loss 0.633441627 epoch total loss 0.625947952\n",
      "Trained batch 1615 batch loss 0.640589535 epoch total loss 0.625957\n",
      "Trained batch 1616 batch loss 0.654714882 epoch total loss 0.625974774\n",
      "Trained batch 1617 batch loss 0.663036525 epoch total loss 0.625997722\n",
      "Trained batch 1618 batch loss 0.630306423 epoch total loss 0.626000345\n",
      "Trained batch 1619 batch loss 0.683653057 epoch total loss 0.626036\n",
      "Trained batch 1620 batch loss 0.703239679 epoch total loss 0.626083612\n",
      "Trained batch 1621 batch loss 0.686540604 epoch total loss 0.626120925\n",
      "Trained batch 1622 batch loss 0.725447655 epoch total loss 0.626182199\n",
      "Trained batch 1623 batch loss 0.713838041 epoch total loss 0.6262362\n",
      "Trained batch 1624 batch loss 0.662528753 epoch total loss 0.626258552\n",
      "Trained batch 1625 batch loss 0.734544516 epoch total loss 0.62632519\n",
      "Trained batch 1626 batch loss 0.711898327 epoch total loss 0.626377821\n",
      "Trained batch 1627 batch loss 0.62587887 epoch total loss 0.626377523\n",
      "Trained batch 1628 batch loss 0.607039511 epoch total loss 0.626365662\n",
      "Trained batch 1629 batch loss 0.54332006 epoch total loss 0.6263147\n",
      "Trained batch 1630 batch loss 0.611959577 epoch total loss 0.626305878\n",
      "Trained batch 1631 batch loss 0.681442 epoch total loss 0.626339674\n",
      "Trained batch 1632 batch loss 0.641285956 epoch total loss 0.626348853\n",
      "Trained batch 1633 batch loss 0.798743725 epoch total loss 0.626454413\n",
      "Trained batch 1634 batch loss 0.578796506 epoch total loss 0.626425266\n",
      "Trained batch 1635 batch loss 0.580978036 epoch total loss 0.626397431\n",
      "Trained batch 1636 batch loss 0.564747453 epoch total loss 0.626359701\n",
      "Trained batch 1637 batch loss 0.590560675 epoch total loss 0.626337826\n",
      "Trained batch 1638 batch loss 0.581703484 epoch total loss 0.626310587\n",
      "Trained batch 1639 batch loss 0.565918088 epoch total loss 0.626273751\n",
      "Trained batch 1640 batch loss 0.622236907 epoch total loss 0.626271248\n",
      "Trained batch 1641 batch loss 0.54310286 epoch total loss 0.626220584\n",
      "Trained batch 1642 batch loss 0.551126659 epoch total loss 0.626174808\n",
      "Trained batch 1643 batch loss 0.589510918 epoch total loss 0.626152515\n",
      "Trained batch 1644 batch loss 0.574449658 epoch total loss 0.626121044\n",
      "Trained batch 1645 batch loss 0.577599466 epoch total loss 0.626091599\n",
      "Trained batch 1646 batch loss 0.630625606 epoch total loss 0.626094341\n",
      "Trained batch 1647 batch loss 0.564705491 epoch total loss 0.626057\n",
      "Trained batch 1648 batch loss 0.631998837 epoch total loss 0.626060605\n",
      "Trained batch 1649 batch loss 0.689056754 epoch total loss 0.626098871\n",
      "Trained batch 1650 batch loss 0.628493667 epoch total loss 0.626100361\n",
      "Trained batch 1651 batch loss 0.531243443 epoch total loss 0.626042902\n",
      "Trained batch 1652 batch loss 0.622831464 epoch total loss 0.626040936\n",
      "Trained batch 1653 batch loss 0.634527504 epoch total loss 0.626046062\n",
      "Trained batch 1654 batch loss 0.700742662 epoch total loss 0.626091182\n",
      "Trained batch 1655 batch loss 0.581198573 epoch total loss 0.626064062\n",
      "Trained batch 1656 batch loss 0.70685035 epoch total loss 0.626112878\n",
      "Trained batch 1657 batch loss 0.707428813 epoch total loss 0.626161933\n",
      "Trained batch 1658 batch loss 0.705324 epoch total loss 0.626209676\n",
      "Trained batch 1659 batch loss 0.751821935 epoch total loss 0.626285374\n",
      "Trained batch 1660 batch loss 0.782335043 epoch total loss 0.626379371\n",
      "Trained batch 1661 batch loss 0.715321064 epoch total loss 0.626432955\n",
      "Trained batch 1662 batch loss 0.594384432 epoch total loss 0.626413643\n",
      "Trained batch 1663 batch loss 0.56823653 epoch total loss 0.626378655\n",
      "Trained batch 1664 batch loss 0.672544301 epoch total loss 0.626406372\n",
      "Trained batch 1665 batch loss 0.623345912 epoch total loss 0.626404524\n",
      "Trained batch 1666 batch loss 0.592677534 epoch total loss 0.626384258\n",
      "Trained batch 1667 batch loss 0.59955883 epoch total loss 0.626368165\n",
      "Trained batch 1668 batch loss 0.604286671 epoch total loss 0.626354933\n",
      "Trained batch 1669 batch loss 0.713604927 epoch total loss 0.626407206\n",
      "Trained batch 1670 batch loss 0.593876123 epoch total loss 0.626387715\n",
      "Trained batch 1671 batch loss 0.658594489 epoch total loss 0.626406968\n",
      "Trained batch 1672 batch loss 0.687559247 epoch total loss 0.626443505\n",
      "Trained batch 1673 batch loss 0.641953707 epoch total loss 0.626452804\n",
      "Trained batch 1674 batch loss 0.726125479 epoch total loss 0.626512289\n",
      "Trained batch 1675 batch loss 0.662993 epoch total loss 0.626534104\n",
      "Trained batch 1676 batch loss 0.677460194 epoch total loss 0.626564503\n",
      "Trained batch 1677 batch loss 0.707600832 epoch total loss 0.626612842\n",
      "Trained batch 1678 batch loss 0.696428299 epoch total loss 0.626654446\n",
      "Trained batch 1679 batch loss 0.697972894 epoch total loss 0.626696944\n",
      "Trained batch 1680 batch loss 0.738080919 epoch total loss 0.626763165\n",
      "Trained batch 1681 batch loss 0.716951549 epoch total loss 0.626816809\n",
      "Trained batch 1682 batch loss 0.710863 epoch total loss 0.626866758\n",
      "Trained batch 1683 batch loss 0.685971737 epoch total loss 0.626901865\n",
      "Trained batch 1684 batch loss 0.700865746 epoch total loss 0.626945734\n",
      "Trained batch 1685 batch loss 0.710863352 epoch total loss 0.626995504\n",
      "Trained batch 1686 batch loss 0.729872584 epoch total loss 0.627056539\n",
      "Trained batch 1687 batch loss 0.695183396 epoch total loss 0.627096891\n",
      "Trained batch 1688 batch loss 0.692632854 epoch total loss 0.627135754\n",
      "Trained batch 1689 batch loss 0.68936497 epoch total loss 0.62717253\n",
      "Trained batch 1690 batch loss 0.671596587 epoch total loss 0.627198875\n",
      "Trained batch 1691 batch loss 0.673853159 epoch total loss 0.627226412\n",
      "Trained batch 1692 batch loss 0.617451966 epoch total loss 0.627220631\n",
      "Trained batch 1693 batch loss 0.595630825 epoch total loss 0.627202\n",
      "Trained batch 1694 batch loss 0.625947714 epoch total loss 0.6272012\n",
      "Trained batch 1695 batch loss 0.59627521 epoch total loss 0.627183\n",
      "Trained batch 1696 batch loss 0.618881285 epoch total loss 0.627178133\n",
      "Trained batch 1697 batch loss 0.567640364 epoch total loss 0.627143\n",
      "Trained batch 1698 batch loss 0.589195669 epoch total loss 0.627120674\n",
      "Trained batch 1699 batch loss 0.565670907 epoch total loss 0.627084553\n",
      "Trained batch 1700 batch loss 0.553520381 epoch total loss 0.627041221\n",
      "Trained batch 1701 batch loss 0.58163166 epoch total loss 0.627014577\n",
      "Trained batch 1702 batch loss 0.586629272 epoch total loss 0.626990855\n",
      "Trained batch 1703 batch loss 0.535042107 epoch total loss 0.626936853\n",
      "Trained batch 1704 batch loss 0.526304185 epoch total loss 0.626877785\n",
      "Trained batch 1705 batch loss 0.468072712 epoch total loss 0.626784563\n",
      "Trained batch 1706 batch loss 0.593989253 epoch total loss 0.62676537\n",
      "Trained batch 1707 batch loss 0.54705 epoch total loss 0.62671864\n",
      "Trained batch 1708 batch loss 0.564876556 epoch total loss 0.626682401\n",
      "Trained batch 1709 batch loss 0.553843 epoch total loss 0.626639783\n",
      "Trained batch 1710 batch loss 0.592683256 epoch total loss 0.626619875\n",
      "Trained batch 1711 batch loss 0.525793195 epoch total loss 0.626560926\n",
      "Trained batch 1712 batch loss 0.586649597 epoch total loss 0.626537621\n",
      "Trained batch 1713 batch loss 0.521849036 epoch total loss 0.626476526\n",
      "Trained batch 1714 batch loss 0.540872633 epoch total loss 0.626426578\n",
      "Trained batch 1715 batch loss 0.66052 epoch total loss 0.626446486\n",
      "Trained batch 1716 batch loss 0.58083415 epoch total loss 0.626419902\n",
      "Trained batch 1717 batch loss 0.512025476 epoch total loss 0.626353323\n",
      "Trained batch 1718 batch loss 0.671699643 epoch total loss 0.626379728\n",
      "Trained batch 1719 batch loss 0.689142048 epoch total loss 0.626416206\n",
      "Trained batch 1720 batch loss 0.775039732 epoch total loss 0.626502573\n",
      "Trained batch 1721 batch loss 0.68865037 epoch total loss 0.626538694\n",
      "Trained batch 1722 batch loss 0.670180142 epoch total loss 0.626564\n",
      "Trained batch 1723 batch loss 0.599539638 epoch total loss 0.62654829\n",
      "Trained batch 1724 batch loss 0.673986 epoch total loss 0.626575768\n",
      "Trained batch 1725 batch loss 0.575900197 epoch total loss 0.626546443\n",
      "Trained batch 1726 batch loss 0.707016528 epoch total loss 0.626593053\n",
      "Trained batch 1727 batch loss 0.75580442 epoch total loss 0.626667917\n",
      "Trained batch 1728 batch loss 0.707486928 epoch total loss 0.626714706\n",
      "Trained batch 1729 batch loss 0.663010895 epoch total loss 0.626735687\n",
      "Trained batch 1730 batch loss 0.603358507 epoch total loss 0.626722157\n",
      "Trained batch 1731 batch loss 0.617636263 epoch total loss 0.626717\n",
      "Trained batch 1732 batch loss 0.672133207 epoch total loss 0.626743138\n",
      "Trained batch 1733 batch loss 0.612270296 epoch total loss 0.626734853\n",
      "Trained batch 1734 batch loss 0.556056798 epoch total loss 0.626694083\n",
      "Trained batch 1735 batch loss 0.570955634 epoch total loss 0.626661897\n",
      "Trained batch 1736 batch loss 0.626234233 epoch total loss 0.626661658\n",
      "Trained batch 1737 batch loss 0.667066216 epoch total loss 0.626684964\n",
      "Trained batch 1738 batch loss 0.640715361 epoch total loss 0.62669307\n",
      "Trained batch 1739 batch loss 0.615508735 epoch total loss 0.626686573\n",
      "Trained batch 1740 batch loss 0.563403189 epoch total loss 0.626650214\n",
      "Trained batch 1741 batch loss 0.569399476 epoch total loss 0.626617372\n",
      "Trained batch 1742 batch loss 0.45994696 epoch total loss 0.626521647\n",
      "Trained batch 1743 batch loss 0.494187742 epoch total loss 0.626445711\n",
      "Trained batch 1744 batch loss 0.475365758 epoch total loss 0.626359105\n",
      "Trained batch 1745 batch loss 0.568107903 epoch total loss 0.626325727\n",
      "Trained batch 1746 batch loss 0.564228296 epoch total loss 0.626290143\n",
      "Trained batch 1747 batch loss 0.604877055 epoch total loss 0.626277864\n",
      "Trained batch 1748 batch loss 0.585237384 epoch total loss 0.62625438\n",
      "Trained batch 1749 batch loss 0.575657845 epoch total loss 0.626225471\n",
      "Trained batch 1750 batch loss 0.581758082 epoch total loss 0.62620008\n",
      "Trained batch 1751 batch loss 0.539097786 epoch total loss 0.62615031\n",
      "Trained batch 1752 batch loss 0.61490494 epoch total loss 0.626143873\n",
      "Trained batch 1753 batch loss 0.592771471 epoch total loss 0.626124799\n",
      "Trained batch 1754 batch loss 0.658108652 epoch total loss 0.626143038\n",
      "Trained batch 1755 batch loss 0.701398253 epoch total loss 0.626185954\n",
      "Trained batch 1756 batch loss 0.752757311 epoch total loss 0.626258\n",
      "Trained batch 1757 batch loss 0.637374699 epoch total loss 0.626264334\n",
      "Trained batch 1758 batch loss 0.622138858 epoch total loss 0.626262\n",
      "Trained batch 1759 batch loss 0.679325044 epoch total loss 0.626292169\n",
      "Trained batch 1760 batch loss 0.655550838 epoch total loss 0.626308799\n",
      "Trained batch 1761 batch loss 0.618612587 epoch total loss 0.626304448\n",
      "Trained batch 1762 batch loss 0.595342219 epoch total loss 0.626286864\n",
      "Trained batch 1763 batch loss 0.521651864 epoch total loss 0.626227498\n",
      "Trained batch 1764 batch loss 0.523662627 epoch total loss 0.626169384\n",
      "Trained batch 1765 batch loss 0.585858 epoch total loss 0.626146495\n",
      "Trained batch 1766 batch loss 0.589544415 epoch total loss 0.626125813\n",
      "Trained batch 1767 batch loss 0.605670154 epoch total loss 0.626114249\n",
      "Trained batch 1768 batch loss 0.642246544 epoch total loss 0.626123369\n",
      "Trained batch 1769 batch loss 0.67983669 epoch total loss 0.626153708\n",
      "Trained batch 1770 batch loss 0.676668644 epoch total loss 0.626182199\n",
      "Trained batch 1771 batch loss 0.708306432 epoch total loss 0.626228571\n",
      "Trained batch 1772 batch loss 0.660343647 epoch total loss 0.626247823\n",
      "Trained batch 1773 batch loss 0.672453284 epoch total loss 0.62627393\n",
      "Trained batch 1774 batch loss 0.624469757 epoch total loss 0.626272917\n",
      "Trained batch 1775 batch loss 0.635130048 epoch total loss 0.626277924\n",
      "Trained batch 1776 batch loss 0.602592468 epoch total loss 0.626264572\n",
      "Trained batch 1777 batch loss 0.548161268 epoch total loss 0.626220644\n",
      "Trained batch 1778 batch loss 0.596376419 epoch total loss 0.626203895\n",
      "Trained batch 1779 batch loss 0.78826493 epoch total loss 0.626295\n",
      "Trained batch 1780 batch loss 0.780823529 epoch total loss 0.626381814\n",
      "Trained batch 1781 batch loss 0.71426475 epoch total loss 0.626431108\n",
      "Trained batch 1782 batch loss 0.715812802 epoch total loss 0.626481295\n",
      "Trained batch 1783 batch loss 0.673031569 epoch total loss 0.626507342\n",
      "Trained batch 1784 batch loss 0.665197313 epoch total loss 0.626529038\n",
      "Trained batch 1785 batch loss 0.603624105 epoch total loss 0.626516223\n",
      "Trained batch 1786 batch loss 0.719368935 epoch total loss 0.626568198\n",
      "Trained batch 1787 batch loss 0.741270244 epoch total loss 0.626632333\n",
      "Trained batch 1788 batch loss 0.632106245 epoch total loss 0.626635373\n",
      "Trained batch 1789 batch loss 0.681261301 epoch total loss 0.62666595\n",
      "Trained batch 1790 batch loss 0.615984797 epoch total loss 0.62665993\n",
      "Trained batch 1791 batch loss 0.588037 epoch total loss 0.626638353\n",
      "Trained batch 1792 batch loss 0.645537376 epoch total loss 0.626648903\n",
      "Trained batch 1793 batch loss 0.542041063 epoch total loss 0.626601696\n",
      "Trained batch 1794 batch loss 0.628044367 epoch total loss 0.626602471\n",
      "Trained batch 1795 batch loss 0.632154167 epoch total loss 0.62660563\n",
      "Trained batch 1796 batch loss 0.743733168 epoch total loss 0.626670837\n",
      "Trained batch 1797 batch loss 0.487624735 epoch total loss 0.62659353\n",
      "Trained batch 1798 batch loss 0.630841315 epoch total loss 0.626595855\n",
      "Trained batch 1799 batch loss 0.684242487 epoch total loss 0.626627922\n",
      "Trained batch 1800 batch loss 0.640551448 epoch total loss 0.626635611\n",
      "Trained batch 1801 batch loss 0.616844296 epoch total loss 0.626630127\n",
      "Trained batch 1802 batch loss 0.638442755 epoch total loss 0.626636684\n",
      "Trained batch 1803 batch loss 0.684111834 epoch total loss 0.626668572\n",
      "Trained batch 1804 batch loss 0.663920701 epoch total loss 0.626689255\n",
      "Trained batch 1805 batch loss 0.716739833 epoch total loss 0.626739144\n",
      "Trained batch 1806 batch loss 0.628968477 epoch total loss 0.626740396\n",
      "Trained batch 1807 batch loss 0.651090145 epoch total loss 0.626753926\n",
      "Trained batch 1808 batch loss 0.696370363 epoch total loss 0.626792431\n",
      "Trained batch 1809 batch loss 0.74772644 epoch total loss 0.626859248\n",
      "Trained batch 1810 batch loss 0.623819947 epoch total loss 0.626857579\n",
      "Trained batch 1811 batch loss 0.547178 epoch total loss 0.626813531\n",
      "Trained batch 1812 batch loss 0.565980315 epoch total loss 0.62678\n",
      "Trained batch 1813 batch loss 0.548211575 epoch total loss 0.626736641\n",
      "Trained batch 1814 batch loss 0.549691737 epoch total loss 0.626694202\n",
      "Trained batch 1815 batch loss 0.550377846 epoch total loss 0.626652181\n",
      "Trained batch 1816 batch loss 0.624437928 epoch total loss 0.626650929\n",
      "Trained batch 1817 batch loss 0.654148221 epoch total loss 0.626666069\n",
      "Trained batch 1818 batch loss 0.701212764 epoch total loss 0.626707\n",
      "Trained batch 1819 batch loss 0.599694908 epoch total loss 0.626692235\n",
      "Trained batch 1820 batch loss 0.592707872 epoch total loss 0.62667352\n",
      "Trained batch 1821 batch loss 0.625596404 epoch total loss 0.626672924\n",
      "Trained batch 1822 batch loss 0.578964889 epoch total loss 0.626646757\n",
      "Trained batch 1823 batch loss 0.577839136 epoch total loss 0.62662\n",
      "Trained batch 1824 batch loss 0.585059285 epoch total loss 0.626597226\n",
      "Trained batch 1825 batch loss 0.647820294 epoch total loss 0.626608849\n",
      "Trained batch 1826 batch loss 0.639834702 epoch total loss 0.62661612\n",
      "Trained batch 1827 batch loss 0.698410034 epoch total loss 0.6266554\n",
      "Trained batch 1828 batch loss 0.570582926 epoch total loss 0.626624703\n",
      "Trained batch 1829 batch loss 0.646229088 epoch total loss 0.626635432\n",
      "Trained batch 1830 batch loss 0.679793656 epoch total loss 0.626664519\n",
      "Trained batch 1831 batch loss 0.730940163 epoch total loss 0.626721442\n",
      "Trained batch 1832 batch loss 0.749264359 epoch total loss 0.626788318\n",
      "Trained batch 1833 batch loss 0.689779818 epoch total loss 0.62682271\n",
      "Trained batch 1834 batch loss 0.790191 epoch total loss 0.626911819\n",
      "Trained batch 1835 batch loss 0.810311556 epoch total loss 0.627011716\n",
      "Trained batch 1836 batch loss 0.688729227 epoch total loss 0.627045333\n",
      "Trained batch 1837 batch loss 0.667383134 epoch total loss 0.627067268\n",
      "Trained batch 1838 batch loss 0.597379446 epoch total loss 0.627051175\n",
      "Trained batch 1839 batch loss 0.551823 epoch total loss 0.627010286\n",
      "Trained batch 1840 batch loss 0.524441421 epoch total loss 0.626954496\n",
      "Trained batch 1841 batch loss 0.617252469 epoch total loss 0.626949251\n",
      "Trained batch 1842 batch loss 0.630535722 epoch total loss 0.626951218\n",
      "Trained batch 1843 batch loss 0.56219697 epoch total loss 0.626916111\n",
      "Trained batch 1844 batch loss 0.598350346 epoch total loss 0.626900613\n",
      "Trained batch 1845 batch loss 0.572832942 epoch total loss 0.626871347\n",
      "Trained batch 1846 batch loss 0.522453666 epoch total loss 0.626814783\n",
      "Trained batch 1847 batch loss 0.591949463 epoch total loss 0.626795888\n",
      "Trained batch 1848 batch loss 0.570252 epoch total loss 0.626765311\n",
      "Trained batch 1849 batch loss 0.569534779 epoch total loss 0.626734376\n",
      "Trained batch 1850 batch loss 0.588472903 epoch total loss 0.626713753\n",
      "Trained batch 1851 batch loss 0.54371357 epoch total loss 0.62666887\n",
      "Trained batch 1852 batch loss 0.600535691 epoch total loss 0.626654804\n",
      "Trained batch 1853 batch loss 0.519128799 epoch total loss 0.626596808\n",
      "Trained batch 1854 batch loss 0.495565087 epoch total loss 0.626526117\n",
      "Trained batch 1855 batch loss 0.5200544 epoch total loss 0.626468718\n",
      "Trained batch 1856 batch loss 0.474825919 epoch total loss 0.62638706\n",
      "Trained batch 1857 batch loss 0.593310714 epoch total loss 0.626369178\n",
      "Trained batch 1858 batch loss 0.560880721 epoch total loss 0.626333952\n",
      "Trained batch 1859 batch loss 0.587297618 epoch total loss 0.626313\n",
      "Trained batch 1860 batch loss 0.638510942 epoch total loss 0.626319528\n",
      "Trained batch 1861 batch loss 0.554649293 epoch total loss 0.626281\n",
      "Trained batch 1862 batch loss 0.490143299 epoch total loss 0.626207948\n",
      "Trained batch 1863 batch loss 0.524351537 epoch total loss 0.626153231\n",
      "Trained batch 1864 batch loss 0.518172622 epoch total loss 0.626095295\n",
      "Trained batch 1865 batch loss 0.551991761 epoch total loss 0.626055539\n",
      "Trained batch 1866 batch loss 0.682581067 epoch total loss 0.626085877\n",
      "Trained batch 1867 batch loss 0.619089603 epoch total loss 0.626082182\n",
      "Trained batch 1868 batch loss 0.685679317 epoch total loss 0.62611407\n",
      "Trained batch 1869 batch loss 0.717064559 epoch total loss 0.626162708\n",
      "Trained batch 1870 batch loss 0.629324615 epoch total loss 0.626164377\n",
      "Trained batch 1871 batch loss 0.61693269 epoch total loss 0.62615943\n",
      "Trained batch 1872 batch loss 0.665194035 epoch total loss 0.626180291\n",
      "Trained batch 1873 batch loss 0.670339286 epoch total loss 0.626203835\n",
      "Trained batch 1874 batch loss 0.59058547 epoch total loss 0.626184821\n",
      "Trained batch 1875 batch loss 0.682532847 epoch total loss 0.626214862\n",
      "Trained batch 1876 batch loss 0.642306924 epoch total loss 0.626223445\n",
      "Trained batch 1877 batch loss 0.625824 epoch total loss 0.626223266\n",
      "Trained batch 1878 batch loss 0.640901744 epoch total loss 0.626231\n",
      "Trained batch 1879 batch loss 0.614539623 epoch total loss 0.626224816\n",
      "Trained batch 1880 batch loss 0.664591908 epoch total loss 0.626245201\n",
      "Trained batch 1881 batch loss 0.637913048 epoch total loss 0.6262514\n",
      "Trained batch 1882 batch loss 0.66629231 epoch total loss 0.626272678\n",
      "Trained batch 1883 batch loss 0.594492 epoch total loss 0.626255751\n",
      "Trained batch 1884 batch loss 0.59866786 epoch total loss 0.626241088\n",
      "Trained batch 1885 batch loss 0.588629484 epoch total loss 0.62622118\n",
      "Trained batch 1886 batch loss 0.59013176 epoch total loss 0.626202\n",
      "Trained batch 1887 batch loss 0.575250566 epoch total loss 0.626175\n",
      "Trained batch 1888 batch loss 0.565892696 epoch total loss 0.626143038\n",
      "Trained batch 1889 batch loss 0.593833506 epoch total loss 0.626126\n",
      "Trained batch 1890 batch loss 0.628456295 epoch total loss 0.626127183\n",
      "Trained batch 1891 batch loss 0.600146532 epoch total loss 0.626113415\n",
      "Trained batch 1892 batch loss 0.595496058 epoch total loss 0.626097202\n",
      "Trained batch 1893 batch loss 0.543374538 epoch total loss 0.626053512\n",
      "Trained batch 1894 batch loss 0.578077257 epoch total loss 0.62602818\n",
      "Trained batch 1895 batch loss 0.515239477 epoch total loss 0.625969708\n",
      "Trained batch 1896 batch loss 0.561483741 epoch total loss 0.625935733\n",
      "Trained batch 1897 batch loss 0.513149917 epoch total loss 0.625876307\n",
      "Trained batch 1898 batch loss 0.545961201 epoch total loss 0.625834227\n",
      "Trained batch 1899 batch loss 0.605238 epoch total loss 0.625823379\n",
      "Trained batch 1900 batch loss 0.665949464 epoch total loss 0.625844479\n",
      "Trained batch 1901 batch loss 0.641822278 epoch total loss 0.625852883\n",
      "Trained batch 1902 batch loss 0.623062909 epoch total loss 0.625851393\n",
      "Trained batch 1903 batch loss 0.667588234 epoch total loss 0.625873327\n",
      "Trained batch 1904 batch loss 0.5516 epoch total loss 0.625834346\n",
      "Trained batch 1905 batch loss 0.687904119 epoch total loss 0.62586695\n",
      "Trained batch 1906 batch loss 0.569087863 epoch total loss 0.625837147\n",
      "Trained batch 1907 batch loss 0.677772045 epoch total loss 0.625864327\n",
      "Trained batch 1908 batch loss 0.623847246 epoch total loss 0.625863314\n",
      "Trained batch 1909 batch loss 0.504583538 epoch total loss 0.625799835\n",
      "Trained batch 1910 batch loss 0.513407 epoch total loss 0.625741\n",
      "Trained batch 1911 batch loss 0.511340737 epoch total loss 0.625681102\n",
      "Trained batch 1912 batch loss 0.560865521 epoch total loss 0.625647247\n",
      "Trained batch 1913 batch loss 0.680416286 epoch total loss 0.625675857\n",
      "Trained batch 1914 batch loss 0.719171882 epoch total loss 0.625724733\n",
      "Trained batch 1915 batch loss 0.666653514 epoch total loss 0.625746071\n",
      "Trained batch 1916 batch loss 0.588105857 epoch total loss 0.625726461\n",
      "Trained batch 1917 batch loss 0.68609184 epoch total loss 0.625757873\n",
      "Trained batch 1918 batch loss 0.599337 epoch total loss 0.625744104\n",
      "Trained batch 1919 batch loss 0.693049073 epoch total loss 0.625779152\n",
      "Trained batch 1920 batch loss 0.636869729 epoch total loss 0.625784934\n",
      "Trained batch 1921 batch loss 0.663134813 epoch total loss 0.625804365\n",
      "Trained batch 1922 batch loss 0.683804631 epoch total loss 0.625834525\n",
      "Trained batch 1923 batch loss 0.664425611 epoch total loss 0.625854611\n",
      "Trained batch 1924 batch loss 0.586149156 epoch total loss 0.625834\n",
      "Trained batch 1925 batch loss 0.554250896 epoch total loss 0.625796795\n",
      "Trained batch 1926 batch loss 0.564554 epoch total loss 0.625765\n",
      "Trained batch 1927 batch loss 0.544757128 epoch total loss 0.625723\n",
      "Trained batch 1928 batch loss 0.57060343 epoch total loss 0.625694394\n",
      "Trained batch 1929 batch loss 0.631741762 epoch total loss 0.625697494\n",
      "Trained batch 1930 batch loss 0.628704131 epoch total loss 0.625699043\n",
      "Trained batch 1931 batch loss 0.650669873 epoch total loss 0.625711918\n",
      "Trained batch 1932 batch loss 0.593707144 epoch total loss 0.625695407\n",
      "Trained batch 1933 batch loss 0.630999446 epoch total loss 0.625698149\n",
      "Trained batch 1934 batch loss 0.656695962 epoch total loss 0.625714183\n",
      "Trained batch 1935 batch loss 0.739336967 epoch total loss 0.625772893\n",
      "Trained batch 1936 batch loss 0.693825185 epoch total loss 0.62580806\n",
      "Trained batch 1937 batch loss 0.62273407 epoch total loss 0.625806451\n",
      "Trained batch 1938 batch loss 0.625236273 epoch total loss 0.625806153\n",
      "Trained batch 1939 batch loss 0.628495634 epoch total loss 0.625807583\n",
      "Trained batch 1940 batch loss 0.68158716 epoch total loss 0.625836372\n",
      "Trained batch 1941 batch loss 0.651444614 epoch total loss 0.625849605\n",
      "Trained batch 1942 batch loss 0.702199459 epoch total loss 0.625888884\n",
      "Trained batch 1943 batch loss 0.670941591 epoch total loss 0.62591207\n",
      "Trained batch 1944 batch loss 0.739495575 epoch total loss 0.625970483\n",
      "Trained batch 1945 batch loss 0.708704293 epoch total loss 0.626013041\n",
      "Trained batch 1946 batch loss 0.651458502 epoch total loss 0.626026094\n",
      "Trained batch 1947 batch loss 0.68948853 epoch total loss 0.626058698\n",
      "Trained batch 1948 batch loss 0.709995866 epoch total loss 0.626101792\n",
      "Trained batch 1949 batch loss 0.726618826 epoch total loss 0.62615329\n",
      "Trained batch 1950 batch loss 0.615410507 epoch total loss 0.626147747\n",
      "Trained batch 1951 batch loss 0.61697 epoch total loss 0.626143038\n",
      "Trained batch 1952 batch loss 0.641512275 epoch total loss 0.626150906\n",
      "Trained batch 1953 batch loss 0.583681285 epoch total loss 0.62612921\n",
      "Trained batch 1954 batch loss 0.572712362 epoch total loss 0.626101911\n",
      "Trained batch 1955 batch loss 0.575181 epoch total loss 0.626075864\n",
      "Trained batch 1956 batch loss 0.65313375 epoch total loss 0.626089633\n",
      "Trained batch 1957 batch loss 0.616309106 epoch total loss 0.626084685\n",
      "Trained batch 1958 batch loss 0.658804953 epoch total loss 0.626101375\n",
      "Trained batch 1959 batch loss 0.602331817 epoch total loss 0.626089215\n",
      "Trained batch 1960 batch loss 0.606495738 epoch total loss 0.626079202\n",
      "Trained batch 1961 batch loss 0.524138212 epoch total loss 0.626027226\n",
      "Trained batch 1962 batch loss 0.537127733 epoch total loss 0.625981927\n",
      "Trained batch 1963 batch loss 0.615698516 epoch total loss 0.625976682\n",
      "Trained batch 1964 batch loss 0.592035115 epoch total loss 0.625959396\n",
      "Trained batch 1965 batch loss 0.613314748 epoch total loss 0.625952959\n",
      "Trained batch 1966 batch loss 0.564584315 epoch total loss 0.625921726\n",
      "Trained batch 1967 batch loss 0.67283 epoch total loss 0.625945568\n",
      "Trained batch 1968 batch loss 0.571118057 epoch total loss 0.625917733\n",
      "Trained batch 1969 batch loss 0.592588 epoch total loss 0.625900805\n",
      "Trained batch 1970 batch loss 0.586018741 epoch total loss 0.625880599\n",
      "Trained batch 1971 batch loss 0.60353446 epoch total loss 0.625869215\n",
      "Trained batch 1972 batch loss 0.618766248 epoch total loss 0.625865638\n",
      "Trained batch 1973 batch loss 0.630021691 epoch total loss 0.625867724\n",
      "Trained batch 1974 batch loss 0.746802449 epoch total loss 0.625929\n",
      "Trained batch 1975 batch loss 0.607052326 epoch total loss 0.625919461\n",
      "Trained batch 1976 batch loss 0.497200638 epoch total loss 0.625854313\n",
      "Trained batch 1977 batch loss 0.635098815 epoch total loss 0.625859\n",
      "Trained batch 1978 batch loss 0.610659599 epoch total loss 0.625851333\n",
      "Trained batch 1979 batch loss 0.666313291 epoch total loss 0.625871778\n",
      "Trained batch 1980 batch loss 0.583101511 epoch total loss 0.625850201\n",
      "Trained batch 1981 batch loss 0.653912187 epoch total loss 0.625864327\n",
      "Trained batch 1982 batch loss 0.636686 epoch total loss 0.625869811\n",
      "Trained batch 1983 batch loss 0.577790618 epoch total loss 0.625845551\n",
      "Trained batch 1984 batch loss 0.662521839 epoch total loss 0.625864\n",
      "Trained batch 1985 batch loss 0.693200469 epoch total loss 0.625897944\n",
      "Trained batch 1986 batch loss 0.682107151 epoch total loss 0.625926256\n",
      "Trained batch 1987 batch loss 0.574810505 epoch total loss 0.625900567\n",
      "Trained batch 1988 batch loss 0.607520938 epoch total loss 0.625891328\n",
      "Trained batch 1989 batch loss 0.678672671 epoch total loss 0.625917912\n",
      "Trained batch 1990 batch loss 0.630405724 epoch total loss 0.625920117\n",
      "Trained batch 1991 batch loss 0.532129943 epoch total loss 0.625873\n",
      "Trained batch 1992 batch loss 0.613357425 epoch total loss 0.625866771\n",
      "Trained batch 1993 batch loss 0.538366914 epoch total loss 0.625822842\n",
      "Trained batch 1994 batch loss 0.632266045 epoch total loss 0.625826061\n",
      "Trained batch 1995 batch loss 0.638646185 epoch total loss 0.625832498\n",
      "Trained batch 1996 batch loss 0.596023858 epoch total loss 0.625817597\n",
      "Trained batch 1997 batch loss 0.622634768 epoch total loss 0.625816047\n",
      "Trained batch 1998 batch loss 0.6297189 epoch total loss 0.625818\n",
      "Trained batch 1999 batch loss 0.628144264 epoch total loss 0.625819206\n",
      "Trained batch 2000 batch loss 0.592414439 epoch total loss 0.625802517\n",
      "Trained batch 2001 batch loss 0.541432738 epoch total loss 0.625760317\n",
      "Trained batch 2002 batch loss 0.528289318 epoch total loss 0.62571162\n",
      "Trained batch 2003 batch loss 0.562821925 epoch total loss 0.625680268\n",
      "Trained batch 2004 batch loss 0.517443657 epoch total loss 0.625626266\n",
      "Trained batch 2005 batch loss 0.48874554 epoch total loss 0.625558\n",
      "Trained batch 2006 batch loss 0.551732421 epoch total loss 0.625521183\n",
      "Trained batch 2007 batch loss 0.496321708 epoch total loss 0.62545681\n",
      "Trained batch 2008 batch loss 0.61035651 epoch total loss 0.6254493\n",
      "Trained batch 2009 batch loss 0.669369 epoch total loss 0.625471175\n",
      "Trained batch 2010 batch loss 0.644901872 epoch total loss 0.625480831\n",
      "Trained batch 2011 batch loss 0.639156 epoch total loss 0.625487626\n",
      "Trained batch 2012 batch loss 0.648776233 epoch total loss 0.625499189\n",
      "Trained batch 2013 batch loss 0.708418369 epoch total loss 0.625540376\n",
      "Trained batch 2014 batch loss 0.824680686 epoch total loss 0.62563926\n",
      "Trained batch 2015 batch loss 0.68937242 epoch total loss 0.62567085\n",
      "Trained batch 2016 batch loss 0.699821889 epoch total loss 0.625707686\n",
      "Trained batch 2017 batch loss 0.641480327 epoch total loss 0.625715494\n",
      "Trained batch 2018 batch loss 0.646426201 epoch total loss 0.625725746\n",
      "Trained batch 2019 batch loss 0.675128281 epoch total loss 0.625750244\n",
      "Trained batch 2020 batch loss 0.635974884 epoch total loss 0.62575531\n",
      "Trained batch 2021 batch loss 0.655864537 epoch total loss 0.625770211\n",
      "Trained batch 2022 batch loss 0.582879245 epoch total loss 0.625749052\n",
      "Trained batch 2023 batch loss 0.540236235 epoch total loss 0.625706792\n",
      "Trained batch 2024 batch loss 0.50989449 epoch total loss 0.625649571\n",
      "Trained batch 2025 batch loss 0.64034164 epoch total loss 0.625656843\n",
      "Trained batch 2026 batch loss 0.604253 epoch total loss 0.625646234\n",
      "Trained batch 2027 batch loss 0.667056501 epoch total loss 0.625666738\n",
      "Trained batch 2028 batch loss 0.624612927 epoch total loss 0.625666201\n",
      "Trained batch 2029 batch loss 0.636300445 epoch total loss 0.625671506\n",
      "Trained batch 2030 batch loss 0.572712302 epoch total loss 0.625645399\n",
      "Trained batch 2031 batch loss 0.62109524 epoch total loss 0.625643194\n",
      "Trained batch 2032 batch loss 0.624895275 epoch total loss 0.625642776\n",
      "Trained batch 2033 batch loss 0.683155835 epoch total loss 0.625671\n",
      "Trained batch 2034 batch loss 0.622969091 epoch total loss 0.625669718\n",
      "Trained batch 2035 batch loss 0.605497837 epoch total loss 0.625659764\n",
      "Trained batch 2036 batch loss 0.629836619 epoch total loss 0.62566185\n",
      "Trained batch 2037 batch loss 0.612486243 epoch total loss 0.625655353\n",
      "Trained batch 2038 batch loss 0.602362573 epoch total loss 0.625643969\n",
      "Trained batch 2039 batch loss 0.686357141 epoch total loss 0.625673771\n",
      "Trained batch 2040 batch loss 0.760628402 epoch total loss 0.625739872\n",
      "Trained batch 2041 batch loss 0.709653199 epoch total loss 0.625781\n",
      "Trained batch 2042 batch loss 0.734115481 epoch total loss 0.625834048\n",
      "Trained batch 2043 batch loss 0.612421155 epoch total loss 0.625827491\n",
      "Trained batch 2044 batch loss 0.655223 epoch total loss 0.625841916\n",
      "Trained batch 2045 batch loss 0.644722819 epoch total loss 0.625851154\n",
      "Trained batch 2046 batch loss 0.613339722 epoch total loss 0.625845\n",
      "Trained batch 2047 batch loss 0.544221699 epoch total loss 0.62580514\n",
      "Trained batch 2048 batch loss 0.603761077 epoch total loss 0.625794351\n",
      "Trained batch 2049 batch loss 0.573850453 epoch total loss 0.625769\n",
      "Trained batch 2050 batch loss 0.653568685 epoch total loss 0.625782549\n",
      "Trained batch 2051 batch loss 0.681144297 epoch total loss 0.62580955\n",
      "Trained batch 2052 batch loss 0.574149072 epoch total loss 0.625784338\n",
      "Trained batch 2053 batch loss 0.565680921 epoch total loss 0.625755072\n",
      "Trained batch 2054 batch loss 0.546793818 epoch total loss 0.625716627\n",
      "Trained batch 2055 batch loss 0.582991779 epoch total loss 0.625695825\n",
      "Trained batch 2056 batch loss 0.492284983 epoch total loss 0.625631\n",
      "Trained batch 2057 batch loss 0.512677252 epoch total loss 0.625576079\n",
      "Trained batch 2058 batch loss 0.526511371 epoch total loss 0.625527918\n",
      "Trained batch 2059 batch loss 0.57353729 epoch total loss 0.625502646\n",
      "Trained batch 2060 batch loss 0.577270031 epoch total loss 0.625479221\n",
      "Trained batch 2061 batch loss 0.668201387 epoch total loss 0.625499964\n",
      "Trained batch 2062 batch loss 0.537032127 epoch total loss 0.625457048\n",
      "Trained batch 2063 batch loss 0.520360231 epoch total loss 0.625406086\n",
      "Trained batch 2064 batch loss 0.466688097 epoch total loss 0.625329196\n",
      "Trained batch 2065 batch loss 0.568922102 epoch total loss 0.625301898\n",
      "Trained batch 2066 batch loss 0.591129 epoch total loss 0.625285387\n",
      "Trained batch 2067 batch loss 0.61240083 epoch total loss 0.625279188\n",
      "Trained batch 2068 batch loss 0.606385171 epoch total loss 0.625270069\n",
      "Trained batch 2069 batch loss 0.600958824 epoch total loss 0.625258327\n",
      "Trained batch 2070 batch loss 0.61703974 epoch total loss 0.625254333\n",
      "Trained batch 2071 batch loss 0.616587877 epoch total loss 0.625250161\n",
      "Trained batch 2072 batch loss 0.539343297 epoch total loss 0.625208676\n",
      "Trained batch 2073 batch loss 0.614772 epoch total loss 0.625203609\n",
      "Trained batch 2074 batch loss 0.508824885 epoch total loss 0.625147521\n",
      "Trained batch 2075 batch loss 0.558834612 epoch total loss 0.625115514\n",
      "Trained batch 2076 batch loss 0.708206654 epoch total loss 0.625155568\n",
      "Trained batch 2077 batch loss 0.691804707 epoch total loss 0.625187635\n",
      "Trained batch 2078 batch loss 0.668311 epoch total loss 0.625208437\n",
      "Trained batch 2079 batch loss 0.589873075 epoch total loss 0.625191391\n",
      "Trained batch 2080 batch loss 0.671472192 epoch total loss 0.625213683\n",
      "Trained batch 2081 batch loss 0.640753567 epoch total loss 0.625221133\n",
      "Trained batch 2082 batch loss 0.625404477 epoch total loss 0.625221193\n",
      "Trained batch 2083 batch loss 0.608998299 epoch total loss 0.625213444\n",
      "Trained batch 2084 batch loss 0.600355864 epoch total loss 0.625201523\n",
      "Trained batch 2085 batch loss 0.600671351 epoch total loss 0.625189722\n",
      "Trained batch 2086 batch loss 0.545404732 epoch total loss 0.625151515\n",
      "Trained batch 2087 batch loss 0.59182471 epoch total loss 0.625135541\n",
      "Trained batch 2088 batch loss 0.605259538 epoch total loss 0.625126\n",
      "Trained batch 2089 batch loss 0.55624032 epoch total loss 0.625093043\n",
      "Trained batch 2090 batch loss 0.564888716 epoch total loss 0.625064254\n",
      "Trained batch 2091 batch loss 0.571417272 epoch total loss 0.625038564\n",
      "Trained batch 2092 batch loss 0.560982823 epoch total loss 0.625008\n",
      "Trained batch 2093 batch loss 0.578298807 epoch total loss 0.624985635\n",
      "Trained batch 2094 batch loss 0.620240927 epoch total loss 0.62498337\n",
      "Trained batch 2095 batch loss 0.678997278 epoch total loss 0.62500912\n",
      "Trained batch 2096 batch loss 0.643974185 epoch total loss 0.625018179\n",
      "Trained batch 2097 batch loss 0.576541781 epoch total loss 0.624995053\n",
      "Trained batch 2098 batch loss 0.537109792 epoch total loss 0.624953151\n",
      "Trained batch 2099 batch loss 0.546656728 epoch total loss 0.624915838\n",
      "Trained batch 2100 batch loss 0.572498739 epoch total loss 0.624890864\n",
      "Trained batch 2101 batch loss 0.491385907 epoch total loss 0.624827325\n",
      "Trained batch 2102 batch loss 0.391097933 epoch total loss 0.624716163\n",
      "Trained batch 2103 batch loss 0.528373837 epoch total loss 0.624670327\n",
      "Trained batch 2104 batch loss 0.536261439 epoch total loss 0.624628305\n",
      "Trained batch 2105 batch loss 0.680196881 epoch total loss 0.624654651\n",
      "Trained batch 2106 batch loss 0.643152475 epoch total loss 0.624663472\n",
      "Trained batch 2107 batch loss 0.70801425 epoch total loss 0.62470305\n",
      "Trained batch 2108 batch loss 0.629068196 epoch total loss 0.624705076\n",
      "Trained batch 2109 batch loss 0.646285653 epoch total loss 0.624715269\n",
      "Trained batch 2110 batch loss 0.654489398 epoch total loss 0.624729395\n",
      "Trained batch 2111 batch loss 0.624327362 epoch total loss 0.624729216\n",
      "Trained batch 2112 batch loss 0.615952253 epoch total loss 0.624725044\n",
      "Trained batch 2113 batch loss 0.680028617 epoch total loss 0.62475121\n",
      "Trained batch 2114 batch loss 0.601847827 epoch total loss 0.624740362\n",
      "Trained batch 2115 batch loss 0.565386832 epoch total loss 0.624712348\n",
      "Trained batch 2116 batch loss 0.502581716 epoch total loss 0.624654591\n",
      "Trained batch 2117 batch loss 0.552559495 epoch total loss 0.624620557\n",
      "Trained batch 2118 batch loss 0.547178566 epoch total loss 0.624584\n",
      "Trained batch 2119 batch loss 0.660591304 epoch total loss 0.624601\n",
      "Trained batch 2120 batch loss 0.540052533 epoch total loss 0.624561131\n",
      "Trained batch 2121 batch loss 0.593063593 epoch total loss 0.62454623\n",
      "Trained batch 2122 batch loss 0.692858279 epoch total loss 0.624578476\n",
      "Trained batch 2123 batch loss 0.601299 epoch total loss 0.624567509\n",
      "Trained batch 2124 batch loss 0.599100351 epoch total loss 0.624555528\n",
      "Trained batch 2125 batch loss 0.592786551 epoch total loss 0.624540567\n",
      "Trained batch 2126 batch loss 0.583293498 epoch total loss 0.624521136\n",
      "Trained batch 2127 batch loss 0.595897913 epoch total loss 0.624507725\n",
      "Trained batch 2128 batch loss 0.597690523 epoch total loss 0.624495089\n",
      "Trained batch 2129 batch loss 0.589805305 epoch total loss 0.624478817\n",
      "Trained batch 2130 batch loss 0.598549962 epoch total loss 0.624466598\n",
      "Trained batch 2131 batch loss 0.651689768 epoch total loss 0.624479413\n",
      "Trained batch 2132 batch loss 0.568318486 epoch total loss 0.624453068\n",
      "Trained batch 2133 batch loss 0.613316059 epoch total loss 0.624447823\n",
      "Trained batch 2134 batch loss 0.614521444 epoch total loss 0.624443173\n",
      "Trained batch 2135 batch loss 0.637737811 epoch total loss 0.624449372\n",
      "Trained batch 2136 batch loss 0.591403544 epoch total loss 0.624433935\n",
      "Trained batch 2137 batch loss 0.615466893 epoch total loss 0.624429762\n",
      "Trained batch 2138 batch loss 0.561686933 epoch total loss 0.624400377\n",
      "Trained batch 2139 batch loss 0.575983226 epoch total loss 0.624377728\n",
      "Trained batch 2140 batch loss 0.576523483 epoch total loss 0.624355376\n",
      "Trained batch 2141 batch loss 0.633728147 epoch total loss 0.624359787\n",
      "Trained batch 2142 batch loss 0.643484056 epoch total loss 0.624368668\n",
      "Trained batch 2143 batch loss 0.610280871 epoch total loss 0.624362051\n",
      "Trained batch 2144 batch loss 0.632336497 epoch total loss 0.624365807\n",
      "Trained batch 2145 batch loss 0.689430714 epoch total loss 0.624396145\n",
      "Trained batch 2146 batch loss 0.712352216 epoch total loss 0.624437153\n",
      "Trained batch 2147 batch loss 0.629203737 epoch total loss 0.624439359\n",
      "Trained batch 2148 batch loss 0.66739583 epoch total loss 0.624459326\n",
      "Trained batch 2149 batch loss 0.566901743 epoch total loss 0.624432564\n",
      "Trained batch 2150 batch loss 0.543238759 epoch total loss 0.624394774\n",
      "Trained batch 2151 batch loss 0.61421442 epoch total loss 0.624390066\n",
      "Trained batch 2152 batch loss 0.563397527 epoch total loss 0.624361694\n",
      "Trained batch 2153 batch loss 0.583679855 epoch total loss 0.624342799\n",
      "Trained batch 2154 batch loss 0.644392192 epoch total loss 0.624352157\n",
      "Trained batch 2155 batch loss 0.62963438 epoch total loss 0.624354601\n",
      "Trained batch 2156 batch loss 0.697411 epoch total loss 0.624388456\n",
      "Trained batch 2157 batch loss 0.676758826 epoch total loss 0.624412715\n",
      "Trained batch 2158 batch loss 0.659513354 epoch total loss 0.624429047\n",
      "Trained batch 2159 batch loss 0.670358539 epoch total loss 0.624450326\n",
      "Trained batch 2160 batch loss 0.717029452 epoch total loss 0.624493182\n",
      "Trained batch 2161 batch loss 0.722169757 epoch total loss 0.624538362\n",
      "Trained batch 2162 batch loss 0.686924398 epoch total loss 0.624567211\n",
      "Trained batch 2163 batch loss 0.659394264 epoch total loss 0.624583364\n",
      "Trained batch 2164 batch loss 0.64353013 epoch total loss 0.624592125\n",
      "Trained batch 2165 batch loss 0.664362252 epoch total loss 0.624610424\n",
      "Trained batch 2166 batch loss 0.634900749 epoch total loss 0.624615192\n",
      "Trained batch 2167 batch loss 0.583621264 epoch total loss 0.624596298\n",
      "Trained batch 2168 batch loss 0.591075182 epoch total loss 0.624580801\n",
      "Trained batch 2169 batch loss 0.565072775 epoch total loss 0.624553382\n",
      "Trained batch 2170 batch loss 0.613181 epoch total loss 0.624548137\n",
      "Trained batch 2171 batch loss 0.653179109 epoch total loss 0.62456131\n",
      "Trained batch 2172 batch loss 0.628620923 epoch total loss 0.624563217\n",
      "Trained batch 2173 batch loss 0.631851196 epoch total loss 0.624566555\n",
      "Trained batch 2174 batch loss 0.597601175 epoch total loss 0.624554157\n",
      "Trained batch 2175 batch loss 0.527716 epoch total loss 0.624509633\n",
      "Trained batch 2176 batch loss 0.581486821 epoch total loss 0.624489903\n",
      "Trained batch 2177 batch loss 0.500802 epoch total loss 0.6244331\n",
      "Trained batch 2178 batch loss 0.595668614 epoch total loss 0.624419928\n",
      "Trained batch 2179 batch loss 0.544987142 epoch total loss 0.624383509\n",
      "Trained batch 2180 batch loss 0.566593289 epoch total loss 0.624357\n",
      "Trained batch 2181 batch loss 0.579113841 epoch total loss 0.624336243\n",
      "Trained batch 2182 batch loss 0.591841936 epoch total loss 0.624321342\n",
      "Trained batch 2183 batch loss 0.52744472 epoch total loss 0.624277\n",
      "Trained batch 2184 batch loss 0.535574794 epoch total loss 0.624236345\n",
      "Trained batch 2185 batch loss 0.550912 epoch total loss 0.624202788\n",
      "Trained batch 2186 batch loss 0.55508554 epoch total loss 0.624171138\n",
      "Trained batch 2187 batch loss 0.558201075 epoch total loss 0.624141\n",
      "Trained batch 2188 batch loss 0.587187052 epoch total loss 0.62412411\n",
      "Trained batch 2189 batch loss 0.620772302 epoch total loss 0.62412256\n",
      "Trained batch 2190 batch loss 0.662433445 epoch total loss 0.62414\n",
      "Trained batch 2191 batch loss 0.520445347 epoch total loss 0.624092698\n",
      "Trained batch 2192 batch loss 0.536599755 epoch total loss 0.624052763\n",
      "Trained batch 2193 batch loss 0.489891469 epoch total loss 0.623991609\n",
      "Trained batch 2194 batch loss 0.590667546 epoch total loss 0.623976409\n",
      "Trained batch 2195 batch loss 0.619275212 epoch total loss 0.623974264\n",
      "Trained batch 2196 batch loss 0.609848797 epoch total loss 0.623967826\n",
      "Trained batch 2197 batch loss 0.643970788 epoch total loss 0.623976946\n",
      "Trained batch 2198 batch loss 0.656411231 epoch total loss 0.623991668\n",
      "Trained batch 2199 batch loss 0.604572833 epoch total loss 0.623982847\n",
      "Trained batch 2200 batch loss 0.706442297 epoch total loss 0.624020338\n",
      "Trained batch 2201 batch loss 0.656664848 epoch total loss 0.62403512\n",
      "Trained batch 2202 batch loss 0.671807945 epoch total loss 0.624056816\n",
      "Trained batch 2203 batch loss 0.653790593 epoch total loss 0.624070287\n",
      "Trained batch 2204 batch loss 0.561135173 epoch total loss 0.624041796\n",
      "Trained batch 2205 batch loss 0.609906614 epoch total loss 0.624035358\n",
      "Trained batch 2206 batch loss 0.701828599 epoch total loss 0.624070585\n",
      "Trained batch 2207 batch loss 0.596823692 epoch total loss 0.624058247\n",
      "Trained batch 2208 batch loss 0.60707 epoch total loss 0.624050498\n",
      "Trained batch 2209 batch loss 0.624791145 epoch total loss 0.624050856\n",
      "Trained batch 2210 batch loss 0.522637129 epoch total loss 0.62400496\n",
      "Trained batch 2211 batch loss 0.625926137 epoch total loss 0.624005854\n",
      "Trained batch 2212 batch loss 0.549473047 epoch total loss 0.623972118\n",
      "Trained batch 2213 batch loss 0.662048936 epoch total loss 0.623989344\n",
      "Trained batch 2214 batch loss 0.598918259 epoch total loss 0.623978\n",
      "Trained batch 2215 batch loss 0.683187187 epoch total loss 0.624004781\n",
      "Trained batch 2216 batch loss 0.602238774 epoch total loss 0.623994946\n",
      "Trained batch 2217 batch loss 0.590178132 epoch total loss 0.623979747\n",
      "Trained batch 2218 batch loss 0.505405843 epoch total loss 0.623926222\n",
      "Trained batch 2219 batch loss 0.506984353 epoch total loss 0.623873532\n",
      "Trained batch 2220 batch loss 0.438513875 epoch total loss 0.62379\n",
      "Trained batch 2221 batch loss 0.574526489 epoch total loss 0.623767853\n",
      "Trained batch 2222 batch loss 0.61753422 epoch total loss 0.623765051\n",
      "Trained batch 2223 batch loss 0.540756106 epoch total loss 0.623727739\n",
      "Trained batch 2224 batch loss 0.555792749 epoch total loss 0.623697162\n",
      "Trained batch 2225 batch loss 0.530408561 epoch total loss 0.62365526\n",
      "Trained batch 2226 batch loss 0.578878939 epoch total loss 0.623635113\n",
      "Trained batch 2227 batch loss 0.543469548 epoch total loss 0.623599112\n",
      "Trained batch 2228 batch loss 0.593374789 epoch total loss 0.623585582\n",
      "Trained batch 2229 batch loss 0.647465348 epoch total loss 0.623596251\n",
      "Trained batch 2230 batch loss 0.710861325 epoch total loss 0.623635411\n",
      "Trained batch 2231 batch loss 0.572886884 epoch total loss 0.623612642\n",
      "Trained batch 2232 batch loss 0.64227289 epoch total loss 0.623621\n",
      "Trained batch 2233 batch loss 0.494979262 epoch total loss 0.623563349\n",
      "Trained batch 2234 batch loss 0.556130588 epoch total loss 0.623533189\n",
      "Trained batch 2235 batch loss 0.655772865 epoch total loss 0.623547614\n",
      "Trained batch 2236 batch loss 0.635219097 epoch total loss 0.623552859\n",
      "Trained batch 2237 batch loss 0.611350656 epoch total loss 0.623547375\n",
      "Trained batch 2238 batch loss 0.676784337 epoch total loss 0.623571157\n",
      "Trained batch 2239 batch loss 0.718826771 epoch total loss 0.623613715\n",
      "Trained batch 2240 batch loss 0.776256382 epoch total loss 0.623681843\n",
      "Trained batch 2241 batch loss 0.779684901 epoch total loss 0.623751462\n",
      "Trained batch 2242 batch loss 0.795457959 epoch total loss 0.623828053\n",
      "Trained batch 2243 batch loss 0.703089535 epoch total loss 0.623863399\n",
      "Trained batch 2244 batch loss 0.632534921 epoch total loss 0.623867273\n",
      "Trained batch 2245 batch loss 0.696011305 epoch total loss 0.6238994\n",
      "Trained batch 2246 batch loss 0.545295 epoch total loss 0.623864412\n",
      "Trained batch 2247 batch loss 0.544366717 epoch total loss 0.623829\n",
      "Trained batch 2248 batch loss 0.479755312 epoch total loss 0.623764932\n",
      "Trained batch 2249 batch loss 0.62579155 epoch total loss 0.623765767\n",
      "Trained batch 2250 batch loss 0.724357784 epoch total loss 0.62381047\n",
      "Trained batch 2251 batch loss 0.738851666 epoch total loss 0.623861611\n",
      "Trained batch 2252 batch loss 0.782992482 epoch total loss 0.623932242\n",
      "Trained batch 2253 batch loss 0.684760749 epoch total loss 0.623959303\n",
      "Trained batch 2254 batch loss 0.720411062 epoch total loss 0.624002099\n",
      "Trained batch 2255 batch loss 0.643477678 epoch total loss 0.624010742\n",
      "Trained batch 2256 batch loss 0.677176654 epoch total loss 0.624034286\n",
      "Trained batch 2257 batch loss 0.713852406 epoch total loss 0.624074042\n",
      "Trained batch 2258 batch loss 0.663831949 epoch total loss 0.624091685\n",
      "Trained batch 2259 batch loss 0.653924704 epoch total loss 0.624104857\n",
      "Trained batch 2260 batch loss 0.585597277 epoch total loss 0.624087811\n",
      "Trained batch 2261 batch loss 0.650050461 epoch total loss 0.624099314\n",
      "Trained batch 2262 batch loss 0.664537311 epoch total loss 0.624117196\n",
      "Trained batch 2263 batch loss 0.620808959 epoch total loss 0.624115705\n",
      "Trained batch 2264 batch loss 0.613958597 epoch total loss 0.624111295\n",
      "Trained batch 2265 batch loss 0.619379401 epoch total loss 0.624109209\n",
      "Trained batch 2266 batch loss 0.648466229 epoch total loss 0.624119937\n",
      "Trained batch 2267 batch loss 0.605703175 epoch total loss 0.624111772\n",
      "Trained batch 2268 batch loss 0.642064512 epoch total loss 0.624119699\n",
      "Trained batch 2269 batch loss 0.634461522 epoch total loss 0.624124289\n",
      "Trained batch 2270 batch loss 0.634092689 epoch total loss 0.624128699\n",
      "Trained batch 2271 batch loss 0.590731204 epoch total loss 0.624114\n",
      "Trained batch 2272 batch loss 0.625766039 epoch total loss 0.624114692\n",
      "Trained batch 2273 batch loss 0.688063741 epoch total loss 0.624142826\n",
      "Trained batch 2274 batch loss 0.705331266 epoch total loss 0.624178529\n",
      "Trained batch 2275 batch loss 0.65486449 epoch total loss 0.624192\n",
      "Trained batch 2276 batch loss 0.729231358 epoch total loss 0.624238193\n",
      "Trained batch 2277 batch loss 0.679227948 epoch total loss 0.624262333\n",
      "Trained batch 2278 batch loss 0.664974 epoch total loss 0.624280155\n",
      "Trained batch 2279 batch loss 0.642389536 epoch total loss 0.624288082\n",
      "Trained batch 2280 batch loss 0.654888809 epoch total loss 0.624301493\n",
      "Trained batch 2281 batch loss 0.616008282 epoch total loss 0.624297857\n",
      "Trained batch 2282 batch loss 0.586667836 epoch total loss 0.624281406\n",
      "Trained batch 2283 batch loss 0.554425895 epoch total loss 0.62425077\n",
      "Trained batch 2284 batch loss 0.619793832 epoch total loss 0.624248803\n",
      "Trained batch 2285 batch loss 0.638015568 epoch total loss 0.624254882\n",
      "Trained batch 2286 batch loss 0.602433801 epoch total loss 0.624245286\n",
      "Trained batch 2287 batch loss 0.581210673 epoch total loss 0.624226451\n",
      "Trained batch 2288 batch loss 0.631221652 epoch total loss 0.62422955\n",
      "Trained batch 2289 batch loss 0.646533847 epoch total loss 0.624239266\n",
      "Trained batch 2290 batch loss 0.638834774 epoch total loss 0.624245644\n",
      "Trained batch 2291 batch loss 0.575616241 epoch total loss 0.624224365\n",
      "Trained batch 2292 batch loss 0.631677389 epoch total loss 0.624227643\n",
      "Trained batch 2293 batch loss 0.689128399 epoch total loss 0.624255896\n",
      "Trained batch 2294 batch loss 0.627050817 epoch total loss 0.624257147\n",
      "Trained batch 2295 batch loss 0.624011636 epoch total loss 0.624257\n",
      "Trained batch 2296 batch loss 0.647585273 epoch total loss 0.62426722\n",
      "Trained batch 2297 batch loss 0.566247642 epoch total loss 0.624241948\n",
      "Trained batch 2298 batch loss 0.586777389 epoch total loss 0.624225676\n",
      "Trained batch 2299 batch loss 0.717348337 epoch total loss 0.624266207\n",
      "Trained batch 2300 batch loss 0.675579309 epoch total loss 0.624288499\n",
      "Trained batch 2301 batch loss 0.622908592 epoch total loss 0.624287903\n",
      "Trained batch 2302 batch loss 0.560408354 epoch total loss 0.624260128\n",
      "Trained batch 2303 batch loss 0.623459637 epoch total loss 0.62425977\n",
      "Trained batch 2304 batch loss 0.599277914 epoch total loss 0.624248922\n",
      "Trained batch 2305 batch loss 0.66911912 epoch total loss 0.624268353\n",
      "Trained batch 2306 batch loss 0.645944178 epoch total loss 0.624277771\n",
      "Trained batch 2307 batch loss 0.685102224 epoch total loss 0.624304116\n",
      "Trained batch 2308 batch loss 0.568007529 epoch total loss 0.624279737\n",
      "Trained batch 2309 batch loss 0.541656613 epoch total loss 0.624244\n",
      "Trained batch 2310 batch loss 0.559255421 epoch total loss 0.624215782\n",
      "Trained batch 2311 batch loss 0.599777222 epoch total loss 0.624205172\n",
      "Trained batch 2312 batch loss 0.592700839 epoch total loss 0.624191523\n",
      "Trained batch 2313 batch loss 0.650555313 epoch total loss 0.624202907\n",
      "Trained batch 2314 batch loss 0.553796768 epoch total loss 0.624172509\n",
      "Trained batch 2315 batch loss 0.590089917 epoch total loss 0.624157786\n",
      "Trained batch 2316 batch loss 0.515099645 epoch total loss 0.624110699\n",
      "Trained batch 2317 batch loss 0.555696845 epoch total loss 0.624081194\n",
      "Trained batch 2318 batch loss 0.49436155 epoch total loss 0.624025226\n",
      "Trained batch 2319 batch loss 0.468765557 epoch total loss 0.62395829\n",
      "Trained batch 2320 batch loss 0.520608664 epoch total loss 0.623913705\n",
      "Trained batch 2321 batch loss 0.538927317 epoch total loss 0.623877108\n",
      "Trained batch 2322 batch loss 0.548401177 epoch total loss 0.623844624\n",
      "Trained batch 2323 batch loss 0.619781733 epoch total loss 0.623842895\n",
      "Trained batch 2324 batch loss 0.621011734 epoch total loss 0.623841643\n",
      "Trained batch 2325 batch loss 0.545056462 epoch total loss 0.623807728\n",
      "Trained batch 2326 batch loss 0.549282968 epoch total loss 0.623775721\n",
      "Trained batch 2327 batch loss 0.549032807 epoch total loss 0.623743653\n",
      "Trained batch 2328 batch loss 0.606939614 epoch total loss 0.623736382\n",
      "Trained batch 2329 batch loss 0.567963541 epoch total loss 0.62371248\n",
      "Trained batch 2330 batch loss 0.611656308 epoch total loss 0.623707294\n",
      "Trained batch 2331 batch loss 0.524131835 epoch total loss 0.623664618\n",
      "Trained batch 2332 batch loss 0.591934502 epoch total loss 0.623650968\n",
      "Trained batch 2333 batch loss 0.680415809 epoch total loss 0.623675346\n",
      "Trained batch 2334 batch loss 0.642268062 epoch total loss 0.623683274\n",
      "Trained batch 2335 batch loss 0.649836719 epoch total loss 0.62369442\n",
      "Trained batch 2336 batch loss 0.591408134 epoch total loss 0.623680651\n",
      "Trained batch 2337 batch loss 0.727877259 epoch total loss 0.623725235\n",
      "Trained batch 2338 batch loss 0.647285402 epoch total loss 0.623735309\n",
      "Trained batch 2339 batch loss 0.614560902 epoch total loss 0.623731375\n",
      "Trained batch 2340 batch loss 0.708928645 epoch total loss 0.623767793\n",
      "Trained batch 2341 batch loss 0.678002238 epoch total loss 0.623791\n",
      "Trained batch 2342 batch loss 0.669702828 epoch total loss 0.623810589\n",
      "Trained batch 2343 batch loss 0.599262 epoch total loss 0.623800099\n",
      "Trained batch 2344 batch loss 0.648533225 epoch total loss 0.623810649\n",
      "Trained batch 2345 batch loss 0.576968 epoch total loss 0.623790681\n",
      "Trained batch 2346 batch loss 0.554683268 epoch total loss 0.623761237\n",
      "Trained batch 2347 batch loss 0.620128334 epoch total loss 0.623759687\n",
      "Trained batch 2348 batch loss 0.582392812 epoch total loss 0.623742044\n",
      "Trained batch 2349 batch loss 0.63501966 epoch total loss 0.623746872\n",
      "Trained batch 2350 batch loss 0.714722574 epoch total loss 0.623785555\n",
      "Trained batch 2351 batch loss 0.653518796 epoch total loss 0.623798251\n",
      "Trained batch 2352 batch loss 0.635401905 epoch total loss 0.623803198\n",
      "Trained batch 2353 batch loss 0.593100846 epoch total loss 0.623790145\n",
      "Trained batch 2354 batch loss 0.669652641 epoch total loss 0.623809636\n",
      "Trained batch 2355 batch loss 0.628153443 epoch total loss 0.623811483\n",
      "Trained batch 2356 batch loss 0.598741174 epoch total loss 0.623800874\n",
      "Trained batch 2357 batch loss 0.572541535 epoch total loss 0.623779118\n",
      "Trained batch 2358 batch loss 0.574354947 epoch total loss 0.623758137\n",
      "Trained batch 2359 batch loss 0.547670722 epoch total loss 0.623725891\n",
      "Trained batch 2360 batch loss 0.504934132 epoch total loss 0.623675525\n",
      "Trained batch 2361 batch loss 0.538818061 epoch total loss 0.623639584\n",
      "Trained batch 2362 batch loss 0.605225265 epoch total loss 0.623631775\n",
      "Trained batch 2363 batch loss 0.597618341 epoch total loss 0.623620808\n",
      "Trained batch 2364 batch loss 0.677582145 epoch total loss 0.623643637\n",
      "Trained batch 2365 batch loss 0.663342059 epoch total loss 0.623660445\n",
      "Trained batch 2366 batch loss 0.676476717 epoch total loss 0.623682737\n",
      "Trained batch 2367 batch loss 0.655843616 epoch total loss 0.623696387\n",
      "Trained batch 2368 batch loss 0.613351882 epoch total loss 0.623692036\n",
      "Trained batch 2369 batch loss 0.623652697 epoch total loss 0.623692036\n",
      "Trained batch 2370 batch loss 0.650148749 epoch total loss 0.623703182\n",
      "Trained batch 2371 batch loss 0.640564799 epoch total loss 0.623710334\n",
      "Trained batch 2372 batch loss 0.604405701 epoch total loss 0.623702168\n",
      "Trained batch 2373 batch loss 0.615070462 epoch total loss 0.623698533\n",
      "Trained batch 2374 batch loss 0.626503646 epoch total loss 0.623699725\n",
      "Trained batch 2375 batch loss 0.631967 epoch total loss 0.623703182\n",
      "Trained batch 2376 batch loss 0.621853888 epoch total loss 0.623702407\n",
      "Trained batch 2377 batch loss 0.572859406 epoch total loss 0.623681\n",
      "Trained batch 2378 batch loss 0.612414 epoch total loss 0.6236763\n",
      "Trained batch 2379 batch loss 0.554690361 epoch total loss 0.623647273\n",
      "Trained batch 2380 batch loss 0.6078825 epoch total loss 0.623640656\n",
      "Trained batch 2381 batch loss 0.583907485 epoch total loss 0.623623967\n",
      "Trained batch 2382 batch loss 0.628261805 epoch total loss 0.623625934\n",
      "Trained batch 2383 batch loss 0.535544634 epoch total loss 0.62358892\n",
      "Trained batch 2384 batch loss 0.545658231 epoch total loss 0.623556256\n",
      "Trained batch 2385 batch loss 0.692311823 epoch total loss 0.623585045\n",
      "Trained batch 2386 batch loss 0.579289734 epoch total loss 0.623566508\n",
      "Trained batch 2387 batch loss 0.560254216 epoch total loss 0.623540044\n",
      "Trained batch 2388 batch loss 0.675419807 epoch total loss 0.62356174\n",
      "Trained batch 2389 batch loss 0.641451597 epoch total loss 0.62356925\n",
      "Trained batch 2390 batch loss 0.713604569 epoch total loss 0.62360692\n",
      "Trained batch 2391 batch loss 0.614355147 epoch total loss 0.623603046\n",
      "Trained batch 2392 batch loss 0.64739114 epoch total loss 0.623613\n",
      "Trained batch 2393 batch loss 0.589559 epoch total loss 0.623598754\n",
      "Trained batch 2394 batch loss 0.656430125 epoch total loss 0.623612463\n",
      "Trained batch 2395 batch loss 0.65141809 epoch total loss 0.623624\n",
      "Trained batch 2396 batch loss 0.613476694 epoch total loss 0.623619854\n",
      "Trained batch 2397 batch loss 0.661311746 epoch total loss 0.62363553\n",
      "Trained batch 2398 batch loss 0.637962937 epoch total loss 0.623641491\n",
      "Trained batch 2399 batch loss 0.685973 epoch total loss 0.623667479\n",
      "Trained batch 2400 batch loss 0.562650144 epoch total loss 0.623642\n",
      "Trained batch 2401 batch loss 0.674143076 epoch total loss 0.623663068\n",
      "Trained batch 2402 batch loss 0.647036552 epoch total loss 0.623672843\n",
      "Trained batch 2403 batch loss 0.70517993 epoch total loss 0.623706758\n",
      "Trained batch 2404 batch loss 0.733980656 epoch total loss 0.623752654\n",
      "Trained batch 2405 batch loss 0.678661168 epoch total loss 0.623775482\n",
      "Trained batch 2406 batch loss 0.710498452 epoch total loss 0.623811543\n",
      "Trained batch 2407 batch loss 0.578764558 epoch total loss 0.623792768\n",
      "Trained batch 2408 batch loss 0.536771894 epoch total loss 0.623756647\n",
      "Trained batch 2409 batch loss 0.62001121 epoch total loss 0.623755097\n",
      "Trained batch 2410 batch loss 0.677586138 epoch total loss 0.623777449\n",
      "Trained batch 2411 batch loss 0.65624547 epoch total loss 0.62379092\n",
      "Trained batch 2412 batch loss 0.64964509 epoch total loss 0.623801589\n",
      "Trained batch 2413 batch loss 0.561197 epoch total loss 0.623775661\n",
      "Trained batch 2414 batch loss 0.653053224 epoch total loss 0.62378782\n",
      "Trained batch 2415 batch loss 0.635732651 epoch total loss 0.623792768\n",
      "Trained batch 2416 batch loss 0.661147773 epoch total loss 0.623808205\n",
      "Trained batch 2417 batch loss 0.643517375 epoch total loss 0.623816371\n",
      "Trained batch 2418 batch loss 0.602233231 epoch total loss 0.62380743\n",
      "Trained batch 2419 batch loss 0.611733735 epoch total loss 0.623802423\n",
      "Trained batch 2420 batch loss 0.614637911 epoch total loss 0.623798609\n",
      "Trained batch 2421 batch loss 0.655154884 epoch total loss 0.623811543\n",
      "Trained batch 2422 batch loss 0.592314541 epoch total loss 0.623798549\n",
      "Trained batch 2423 batch loss 0.556858838 epoch total loss 0.623770952\n",
      "Trained batch 2424 batch loss 0.680710614 epoch total loss 0.623794377\n",
      "Trained batch 2425 batch loss 0.662613213 epoch total loss 0.62381041\n",
      "Trained batch 2426 batch loss 0.571593642 epoch total loss 0.623788834\n",
      "Trained batch 2427 batch loss 0.617373 epoch total loss 0.623786211\n",
      "Trained batch 2428 batch loss 0.615412056 epoch total loss 0.623782754\n",
      "Trained batch 2429 batch loss 0.632505476 epoch total loss 0.62378633\n",
      "Trained batch 2430 batch loss 0.701266 epoch total loss 0.623818219\n",
      "Trained batch 2431 batch loss 0.577198327 epoch total loss 0.623799\n",
      "Trained batch 2432 batch loss 0.581788838 epoch total loss 0.623781741\n",
      "Trained batch 2433 batch loss 0.615607619 epoch total loss 0.623778403\n",
      "Trained batch 2434 batch loss 0.594125867 epoch total loss 0.623766184\n",
      "Trained batch 2435 batch loss 0.556652784 epoch total loss 0.623738647\n",
      "Trained batch 2436 batch loss 0.534460187 epoch total loss 0.623702\n",
      "Trained batch 2437 batch loss 0.496323824 epoch total loss 0.623649716\n",
      "Trained batch 2438 batch loss 0.471216887 epoch total loss 0.623587191\n",
      "Trained batch 2439 batch loss 0.56535095 epoch total loss 0.62356329\n",
      "Trained batch 2440 batch loss 0.472902954 epoch total loss 0.623501539\n",
      "Trained batch 2441 batch loss 0.734465897 epoch total loss 0.623547\n",
      "Trained batch 2442 batch loss 0.846150219 epoch total loss 0.623638153\n",
      "Trained batch 2443 batch loss 0.778193116 epoch total loss 0.623701453\n",
      "Trained batch 2444 batch loss 0.639400601 epoch total loss 0.623707891\n",
      "Trained batch 2445 batch loss 0.677570105 epoch total loss 0.623729944\n",
      "Trained batch 2446 batch loss 0.54441607 epoch total loss 0.623697519\n",
      "Trained batch 2447 batch loss 0.556799233 epoch total loss 0.623670161\n",
      "Trained batch 2448 batch loss 0.579518855 epoch total loss 0.623652101\n",
      "Trained batch 2449 batch loss 0.574122548 epoch total loss 0.623631835\n",
      "Trained batch 2450 batch loss 0.584351242 epoch total loss 0.623615801\n",
      "Trained batch 2451 batch loss 0.647082448 epoch total loss 0.623625398\n",
      "Trained batch 2452 batch loss 0.552252591 epoch total loss 0.623596311\n",
      "Trained batch 2453 batch loss 0.573302209 epoch total loss 0.623575747\n",
      "Trained batch 2454 batch loss 0.54490608 epoch total loss 0.623543739\n",
      "Trained batch 2455 batch loss 0.516938031 epoch total loss 0.623500288\n",
      "Trained batch 2456 batch loss 0.630742192 epoch total loss 0.623503268\n",
      "Trained batch 2457 batch loss 0.647134304 epoch total loss 0.623512864\n",
      "Trained batch 2458 batch loss 0.62291652 epoch total loss 0.623512626\n",
      "Trained batch 2459 batch loss 0.63765 epoch total loss 0.623518348\n",
      "Trained batch 2460 batch loss 0.541142046 epoch total loss 0.62348491\n",
      "Trained batch 2461 batch loss 0.64985317 epoch total loss 0.623495638\n",
      "Trained batch 2462 batch loss 0.633876681 epoch total loss 0.62349987\n",
      "Trained batch 2463 batch loss 0.626407146 epoch total loss 0.623501062\n",
      "Trained batch 2464 batch loss 0.65797317 epoch total loss 0.623515069\n",
      "Trained batch 2465 batch loss 0.658114552 epoch total loss 0.623529077\n",
      "Trained batch 2466 batch loss 0.613067091 epoch total loss 0.623524785\n",
      "Trained batch 2467 batch loss 0.505977631 epoch total loss 0.623477161\n",
      "Trained batch 2468 batch loss 0.563085556 epoch total loss 0.623452723\n",
      "Trained batch 2469 batch loss 0.622631192 epoch total loss 0.623452365\n",
      "Trained batch 2470 batch loss 0.576313496 epoch total loss 0.623433292\n",
      "Trained batch 2471 batch loss 0.543523 epoch total loss 0.623401\n",
      "Trained batch 2472 batch loss 0.573573291 epoch total loss 0.62338084\n",
      "Trained batch 2473 batch loss 0.642249942 epoch total loss 0.623388469\n",
      "Trained batch 2474 batch loss 0.632982671 epoch total loss 0.623392284\n",
      "Trained batch 2475 batch loss 0.552701712 epoch total loss 0.623363793\n",
      "Trained batch 2476 batch loss 0.659775376 epoch total loss 0.623378456\n",
      "Trained batch 2477 batch loss 0.63671577 epoch total loss 0.62338388\n",
      "Trained batch 2478 batch loss 0.610860109 epoch total loss 0.623378813\n",
      "Trained batch 2479 batch loss 0.685096204 epoch total loss 0.623403668\n",
      "Trained batch 2480 batch loss 0.653981 epoch total loss 0.623416\n",
      "Trained batch 2481 batch loss 0.600685954 epoch total loss 0.623406827\n",
      "Trained batch 2482 batch loss 0.640203238 epoch total loss 0.623413622\n",
      "Trained batch 2483 batch loss 0.646115303 epoch total loss 0.623422742\n",
      "Trained batch 2484 batch loss 0.607396662 epoch total loss 0.623416305\n",
      "Trained batch 2485 batch loss 0.679907 epoch total loss 0.623439074\n",
      "Trained batch 2486 batch loss 0.608708739 epoch total loss 0.623433173\n",
      "Trained batch 2487 batch loss 0.667880177 epoch total loss 0.623451054\n",
      "Trained batch 2488 batch loss 0.658699512 epoch total loss 0.62346518\n",
      "Trained batch 2489 batch loss 0.635328948 epoch total loss 0.623469949\n",
      "Trained batch 2490 batch loss 0.676687121 epoch total loss 0.623491347\n",
      "Trained batch 2491 batch loss 0.597736955 epoch total loss 0.623481035\n",
      "Trained batch 2492 batch loss 0.634368181 epoch total loss 0.623485386\n",
      "Trained batch 2493 batch loss 0.553473592 epoch total loss 0.623457313\n",
      "Trained batch 2494 batch loss 0.630178511 epoch total loss 0.62346\n",
      "Trained batch 2495 batch loss 0.575796902 epoch total loss 0.623440862\n",
      "Trained batch 2496 batch loss 0.706076324 epoch total loss 0.623474\n",
      "Trained batch 2497 batch loss 0.73165977 epoch total loss 0.623517334\n",
      "Trained batch 2498 batch loss 0.640945911 epoch total loss 0.623524308\n",
      "Trained batch 2499 batch loss 0.690528631 epoch total loss 0.62355113\n",
      "Trained batch 2500 batch loss 0.613846481 epoch total loss 0.623547256\n",
      "Trained batch 2501 batch loss 0.595414877 epoch total loss 0.62353605\n",
      "Trained batch 2502 batch loss 0.653826177 epoch total loss 0.62354815\n",
      "Trained batch 2503 batch loss 0.580545306 epoch total loss 0.623531\n",
      "Trained batch 2504 batch loss 0.56684953 epoch total loss 0.623508334\n",
      "Trained batch 2505 batch loss 0.47385487 epoch total loss 0.62344861\n",
      "Trained batch 2506 batch loss 0.544521928 epoch total loss 0.623417139\n",
      "Trained batch 2507 batch loss 0.551246881 epoch total loss 0.62338835\n",
      "Trained batch 2508 batch loss 0.592057 epoch total loss 0.623375833\n",
      "Trained batch 2509 batch loss 0.572602391 epoch total loss 0.623355627\n",
      "Trained batch 2510 batch loss 0.619488895 epoch total loss 0.623354077\n",
      "Trained batch 2511 batch loss 0.556623757 epoch total loss 0.623327553\n",
      "Trained batch 2512 batch loss 0.61822927 epoch total loss 0.623325527\n",
      "Trained batch 2513 batch loss 0.756059527 epoch total loss 0.623378336\n",
      "Trained batch 2514 batch loss 0.679616749 epoch total loss 0.623400688\n",
      "Trained batch 2515 batch loss 0.612716317 epoch total loss 0.623396456\n",
      "Trained batch 2516 batch loss 0.690530181 epoch total loss 0.6234231\n",
      "Trained batch 2517 batch loss 0.652151763 epoch total loss 0.623434544\n",
      "Trained batch 2518 batch loss 0.647106707 epoch total loss 0.623443902\n",
      "Trained batch 2519 batch loss 0.568647921 epoch total loss 0.623422146\n",
      "Trained batch 2520 batch loss 0.541956842 epoch total loss 0.62338984\n",
      "Trained batch 2521 batch loss 0.661137223 epoch total loss 0.623404801\n",
      "Trained batch 2522 batch loss 0.599771202 epoch total loss 0.623395443\n",
      "Trained batch 2523 batch loss 0.558376729 epoch total loss 0.623369634\n",
      "Trained batch 2524 batch loss 0.527059853 epoch total loss 0.623331487\n",
      "Trained batch 2525 batch loss 0.636313796 epoch total loss 0.623336673\n",
      "Trained batch 2526 batch loss 0.537284851 epoch total loss 0.623302579\n",
      "Trained batch 2527 batch loss 0.644554615 epoch total loss 0.623311\n",
      "Trained batch 2528 batch loss 0.635654449 epoch total loss 0.623315811\n",
      "Trained batch 2529 batch loss 0.552743137 epoch total loss 0.623287916\n",
      "Trained batch 2530 batch loss 0.597527921 epoch total loss 0.623277724\n",
      "Trained batch 2531 batch loss 0.544819832 epoch total loss 0.623246729\n",
      "Trained batch 2532 batch loss 0.554440558 epoch total loss 0.62321955\n",
      "Trained batch 2533 batch loss 0.59284234 epoch total loss 0.623207569\n",
      "Trained batch 2534 batch loss 0.617167711 epoch total loss 0.623205245\n",
      "Trained batch 2535 batch loss 0.581384182 epoch total loss 0.623188734\n",
      "Trained batch 2536 batch loss 0.557635069 epoch total loss 0.623162866\n",
      "Trained batch 2537 batch loss 0.567871332 epoch total loss 0.62314111\n",
      "Trained batch 2538 batch loss 0.515768647 epoch total loss 0.623098791\n",
      "Trained batch 2539 batch loss 0.557611108 epoch total loss 0.623073\n",
      "Trained batch 2540 batch loss 0.544424534 epoch total loss 0.623042047\n",
      "Trained batch 2541 batch loss 0.517647445 epoch total loss 0.623000562\n",
      "Trained batch 2542 batch loss 0.600487888 epoch total loss 0.622991681\n",
      "Trained batch 2543 batch loss 0.553261042 epoch total loss 0.622964263\n",
      "Trained batch 2544 batch loss 0.624359548 epoch total loss 0.622964799\n",
      "Trained batch 2545 batch loss 0.618708134 epoch total loss 0.62296313\n",
      "Trained batch 2546 batch loss 0.612162352 epoch total loss 0.622958899\n",
      "Trained batch 2547 batch loss 0.611112416 epoch total loss 0.622954249\n",
      "Trained batch 2548 batch loss 0.602882743 epoch total loss 0.622946382\n",
      "Trained batch 2549 batch loss 0.577876508 epoch total loss 0.622928679\n",
      "Trained batch 2550 batch loss 0.596938074 epoch total loss 0.622918487\n",
      "Trained batch 2551 batch loss 0.694432259 epoch total loss 0.622946501\n",
      "Trained batch 2552 batch loss 0.64558965 epoch total loss 0.622955441\n",
      "Trained batch 2553 batch loss 0.647304058 epoch total loss 0.622965\n",
      "Trained batch 2554 batch loss 0.632946849 epoch total loss 0.622968853\n",
      "Trained batch 2555 batch loss 0.634045482 epoch total loss 0.622973204\n",
      "Trained batch 2556 batch loss 0.672205865 epoch total loss 0.622992456\n",
      "Trained batch 2557 batch loss 0.643646836 epoch total loss 0.623000562\n",
      "Trained batch 2558 batch loss 0.626038074 epoch total loss 0.623001754\n",
      "Trained batch 2559 batch loss 0.646585345 epoch total loss 0.623011\n",
      "Trained batch 2560 batch loss 0.593814433 epoch total loss 0.622999609\n",
      "Trained batch 2561 batch loss 0.601221621 epoch total loss 0.622991085\n",
      "Trained batch 2562 batch loss 0.602335 epoch total loss 0.622983038\n",
      "Trained batch 2563 batch loss 0.553291917 epoch total loss 0.622955859\n",
      "Trained batch 2564 batch loss 0.659389377 epoch total loss 0.622970104\n",
      "Trained batch 2565 batch loss 0.61598593 epoch total loss 0.622967362\n",
      "Trained batch 2566 batch loss 0.599203825 epoch total loss 0.622958124\n",
      "Trained batch 2567 batch loss 0.66827023 epoch total loss 0.622975707\n",
      "Trained batch 2568 batch loss 0.64021951 epoch total loss 0.622982442\n",
      "Trained batch 2569 batch loss 0.742278695 epoch total loss 0.623028934\n",
      "Trained batch 2570 batch loss 0.634390235 epoch total loss 0.623033345\n",
      "Trained batch 2571 batch loss 0.619214654 epoch total loss 0.623031855\n",
      "Trained batch 2572 batch loss 0.591902256 epoch total loss 0.623019755\n",
      "Trained batch 2573 batch loss 0.631072819 epoch total loss 0.623022914\n",
      "Trained batch 2574 batch loss 0.659822345 epoch total loss 0.623037219\n",
      "Trained batch 2575 batch loss 0.686695755 epoch total loss 0.623061895\n",
      "Trained batch 2576 batch loss 0.632631183 epoch total loss 0.62306565\n",
      "Trained batch 2577 batch loss 0.617355585 epoch total loss 0.623063385\n",
      "Trained batch 2578 batch loss 0.582462847 epoch total loss 0.623047709\n",
      "Trained batch 2579 batch loss 0.714172244 epoch total loss 0.623083\n",
      "Trained batch 2580 batch loss 0.638393104 epoch total loss 0.623088956\n",
      "Trained batch 2581 batch loss 0.662735701 epoch total loss 0.623104274\n",
      "Trained batch 2582 batch loss 0.659465 epoch total loss 0.623118341\n",
      "Trained batch 2583 batch loss 0.563114643 epoch total loss 0.623095155\n",
      "Trained batch 2584 batch loss 0.6565938 epoch total loss 0.623108089\n",
      "Trained batch 2585 batch loss 0.612046063 epoch total loss 0.623103797\n",
      "Trained batch 2586 batch loss 0.74242 epoch total loss 0.62315\n",
      "Trained batch 2587 batch loss 0.656084061 epoch total loss 0.623162746\n",
      "Trained batch 2588 batch loss 0.606287122 epoch total loss 0.62315619\n",
      "Trained batch 2589 batch loss 0.610028923 epoch total loss 0.623151124\n",
      "Trained batch 2590 batch loss 0.594575882 epoch total loss 0.623140097\n",
      "Trained batch 2591 batch loss 0.633892953 epoch total loss 0.623144269\n",
      "Trained batch 2592 batch loss 0.590668678 epoch total loss 0.623131752\n",
      "Trained batch 2593 batch loss 0.691117525 epoch total loss 0.623158\n",
      "Trained batch 2594 batch loss 0.649000287 epoch total loss 0.623167932\n",
      "Trained batch 2595 batch loss 0.623966277 epoch total loss 0.62316829\n",
      "Trained batch 2596 batch loss 0.625176489 epoch total loss 0.623169065\n",
      "Trained batch 2597 batch loss 0.568983495 epoch total loss 0.623148143\n",
      "Trained batch 2598 batch loss 0.555528 epoch total loss 0.623122156\n",
      "Trained batch 2599 batch loss 0.547371566 epoch total loss 0.623093\n",
      "Trained batch 2600 batch loss 0.638145506 epoch total loss 0.623098791\n",
      "Trained batch 2601 batch loss 0.49842909 epoch total loss 0.623050869\n",
      "Trained batch 2602 batch loss 0.563217 epoch total loss 0.623027861\n",
      "Trained batch 2603 batch loss 0.661836207 epoch total loss 0.623042822\n",
      "Trained batch 2604 batch loss 0.662148654 epoch total loss 0.623057783\n",
      "Trained batch 2605 batch loss 0.654301763 epoch total loss 0.623069763\n",
      "Trained batch 2606 batch loss 0.625452161 epoch total loss 0.623070717\n",
      "Trained batch 2607 batch loss 0.579212785 epoch total loss 0.623053908\n",
      "Trained batch 2608 batch loss 0.626742065 epoch total loss 0.623055279\n",
      "Trained batch 2609 batch loss 0.611776829 epoch total loss 0.623051\n",
      "Trained batch 2610 batch loss 0.651539564 epoch total loss 0.623061895\n",
      "Trained batch 2611 batch loss 0.650045872 epoch total loss 0.623072207\n",
      "Trained batch 2612 batch loss 0.661004543 epoch total loss 0.623086751\n",
      "Trained batch 2613 batch loss 0.645849 epoch total loss 0.623095453\n",
      "Trained batch 2614 batch loss 0.624939322 epoch total loss 0.623096168\n",
      "Trained batch 2615 batch loss 0.665849686 epoch total loss 0.623112559\n",
      "Trained batch 2616 batch loss 0.604342937 epoch total loss 0.623105407\n",
      "Trained batch 2617 batch loss 0.59346652 epoch total loss 0.623094082\n",
      "Trained batch 2618 batch loss 0.639295101 epoch total loss 0.623100281\n",
      "Trained batch 2619 batch loss 0.634353459 epoch total loss 0.623104572\n",
      "Trained batch 2620 batch loss 0.694853365 epoch total loss 0.623131931\n",
      "Trained batch 2621 batch loss 0.592510521 epoch total loss 0.623120248\n",
      "Trained batch 2622 batch loss 0.599860489 epoch total loss 0.623111427\n",
      "Trained batch 2623 batch loss 0.593553424 epoch total loss 0.623100102\n",
      "Trained batch 2624 batch loss 0.608378887 epoch total loss 0.623094499\n",
      "Trained batch 2625 batch loss 0.695086062 epoch total loss 0.623121917\n",
      "Trained batch 2626 batch loss 0.629651666 epoch total loss 0.623124421\n",
      "Trained batch 2627 batch loss 0.60597086 epoch total loss 0.623117864\n",
      "Trained batch 2628 batch loss 0.718191326 epoch total loss 0.623154044\n",
      "Trained batch 2629 batch loss 0.661192298 epoch total loss 0.623168468\n",
      "Trained batch 2630 batch loss 0.52467227 epoch total loss 0.623131037\n",
      "Trained batch 2631 batch loss 0.593757272 epoch total loss 0.623119831\n",
      "Trained batch 2632 batch loss 0.537954867 epoch total loss 0.623087525\n",
      "Trained batch 2633 batch loss 0.551563442 epoch total loss 0.623060346\n",
      "Trained batch 2634 batch loss 0.538307548 epoch total loss 0.623028159\n",
      "Trained batch 2635 batch loss 0.490758479 epoch total loss 0.622977912\n",
      "Trained batch 2636 batch loss 0.514881 epoch total loss 0.622936964\n",
      "Trained batch 2637 batch loss 0.547630787 epoch total loss 0.622908354\n",
      "Trained batch 2638 batch loss 0.59496516 epoch total loss 0.622897804\n",
      "Trained batch 2639 batch loss 0.54834336 epoch total loss 0.622869551\n",
      "Trained batch 2640 batch loss 0.581082225 epoch total loss 0.622853696\n",
      "Trained batch 2641 batch loss 0.587796 epoch total loss 0.622840405\n",
      "Trained batch 2642 batch loss 0.562761486 epoch total loss 0.622817636\n",
      "Trained batch 2643 batch loss 0.571253538 epoch total loss 0.622798145\n",
      "Trained batch 2644 batch loss 0.522336602 epoch total loss 0.622760177\n",
      "Trained batch 2645 batch loss 0.564543843 epoch total loss 0.622738183\n",
      "Trained batch 2646 batch loss 0.5317 epoch total loss 0.622703791\n",
      "Trained batch 2647 batch loss 0.61238718 epoch total loss 0.622699916\n",
      "Trained batch 2648 batch loss 0.547062099 epoch total loss 0.622671366\n",
      "Trained batch 2649 batch loss 0.634741068 epoch total loss 0.622675896\n",
      "Trained batch 2650 batch loss 0.605264544 epoch total loss 0.622669339\n",
      "Trained batch 2651 batch loss 0.793262243 epoch total loss 0.622733653\n",
      "Trained batch 2652 batch loss 0.769165 epoch total loss 0.622788906\n",
      "Trained batch 2653 batch loss 0.742901921 epoch total loss 0.622834146\n",
      "Trained batch 2654 batch loss 0.648571968 epoch total loss 0.622843862\n",
      "Trained batch 2655 batch loss 0.653672695 epoch total loss 0.622855484\n",
      "Trained batch 2656 batch loss 0.635959327 epoch total loss 0.622860432\n",
      "Trained batch 2657 batch loss 0.628688037 epoch total loss 0.622862577\n",
      "Trained batch 2658 batch loss 0.637773335 epoch total loss 0.62286824\n",
      "Trained batch 2659 batch loss 0.577848911 epoch total loss 0.622851312\n",
      "Trained batch 2660 batch loss 0.625136554 epoch total loss 0.622852147\n",
      "Trained batch 2661 batch loss 0.629062116 epoch total loss 0.622854471\n",
      "Trained batch 2662 batch loss 0.544464111 epoch total loss 0.622825\n",
      "Trained batch 2663 batch loss 0.554144263 epoch total loss 0.622799277\n",
      "Trained batch 2664 batch loss 0.614943206 epoch total loss 0.622796297\n",
      "Trained batch 2665 batch loss 0.631321788 epoch total loss 0.622799516\n",
      "Trained batch 2666 batch loss 0.572284698 epoch total loss 0.622780561\n",
      "Trained batch 2667 batch loss 0.642903149 epoch total loss 0.622788131\n",
      "Trained batch 2668 batch loss 0.622058809 epoch total loss 0.622787893\n",
      "Trained batch 2669 batch loss 0.52203846 epoch total loss 0.622750163\n",
      "Trained batch 2670 batch loss 0.543792188 epoch total loss 0.622720599\n",
      "Trained batch 2671 batch loss 0.580117702 epoch total loss 0.622704625\n",
      "Trained batch 2672 batch loss 0.508593142 epoch total loss 0.622661889\n",
      "Trained batch 2673 batch loss 0.54907757 epoch total loss 0.622634351\n",
      "Trained batch 2674 batch loss 0.481209487 epoch total loss 0.622581482\n",
      "Trained batch 2675 batch loss 0.465873301 epoch total loss 0.622522891\n",
      "Trained batch 2676 batch loss 0.557388365 epoch total loss 0.622498512\n",
      "Trained batch 2677 batch loss 0.694340348 epoch total loss 0.622525334\n",
      "Trained batch 2678 batch loss 0.694770575 epoch total loss 0.622552335\n",
      "Trained batch 2679 batch loss 0.743454456 epoch total loss 0.622597456\n",
      "Trained batch 2680 batch loss 0.837334514 epoch total loss 0.622677565\n",
      "Trained batch 2681 batch loss 0.717540324 epoch total loss 0.62271297\n",
      "Trained batch 2682 batch loss 0.813143253 epoch total loss 0.622783959\n",
      "Trained batch 2683 batch loss 0.691235125 epoch total loss 0.62280947\n",
      "Trained batch 2684 batch loss 0.628882706 epoch total loss 0.622811735\n",
      "Trained batch 2685 batch loss 0.669892073 epoch total loss 0.622829258\n",
      "Trained batch 2686 batch loss 0.719093084 epoch total loss 0.62286514\n",
      "Trained batch 2687 batch loss 0.714746296 epoch total loss 0.622899294\n",
      "Trained batch 2688 batch loss 0.692780435 epoch total loss 0.622925282\n",
      "Trained batch 2689 batch loss 0.627273679 epoch total loss 0.62292695\n",
      "Trained batch 2690 batch loss 0.637153447 epoch total loss 0.622932255\n",
      "Trained batch 2691 batch loss 0.568422139 epoch total loss 0.622912\n",
      "Trained batch 2692 batch loss 0.686394334 epoch total loss 0.622935593\n",
      "Trained batch 2693 batch loss 0.640548766 epoch total loss 0.62294215\n",
      "Trained batch 2694 batch loss 0.738139927 epoch total loss 0.622984886\n",
      "Trained batch 2695 batch loss 0.755277216 epoch total loss 0.623033941\n",
      "Trained batch 2696 batch loss 0.674491405 epoch total loss 0.623053\n",
      "Trained batch 2697 batch loss 0.600560665 epoch total loss 0.623044729\n",
      "Trained batch 2698 batch loss 0.608993053 epoch total loss 0.623039484\n",
      "Trained batch 2699 batch loss 0.556176782 epoch total loss 0.623014688\n",
      "Trained batch 2700 batch loss 0.510728836 epoch total loss 0.622973144\n",
      "Trained batch 2701 batch loss 0.467612445 epoch total loss 0.622915626\n",
      "Trained batch 2702 batch loss 0.553862154 epoch total loss 0.622890055\n",
      "Trained batch 2703 batch loss 0.615204036 epoch total loss 0.622887254\n",
      "Trained batch 2704 batch loss 0.594437599 epoch total loss 0.622876704\n",
      "Trained batch 2705 batch loss 0.574183285 epoch total loss 0.622858763\n",
      "Trained batch 2706 batch loss 0.713508248 epoch total loss 0.622892261\n",
      "Trained batch 2707 batch loss 0.685650706 epoch total loss 0.622915447\n",
      "Trained batch 2708 batch loss 0.611030221 epoch total loss 0.622911036\n",
      "Trained batch 2709 batch loss 0.659228683 epoch total loss 0.622924447\n",
      "Trained batch 2710 batch loss 0.602709174 epoch total loss 0.622917\n",
      "Trained batch 2711 batch loss 0.636101961 epoch total loss 0.622921824\n",
      "Trained batch 2712 batch loss 0.668192565 epoch total loss 0.622938514\n",
      "Trained batch 2713 batch loss 0.612318099 epoch total loss 0.622934639\n",
      "Trained batch 2714 batch loss 0.606559277 epoch total loss 0.62292856\n",
      "Trained batch 2715 batch loss 0.589517653 epoch total loss 0.622916281\n",
      "Trained batch 2716 batch loss 0.592417419 epoch total loss 0.622905\n",
      "Trained batch 2717 batch loss 0.623206437 epoch total loss 0.622905135\n",
      "Trained batch 2718 batch loss 0.621174455 epoch total loss 0.62290448\n",
      "Trained batch 2719 batch loss 0.614627659 epoch total loss 0.62290144\n",
      "Trained batch 2720 batch loss 0.668422639 epoch total loss 0.622918189\n",
      "Trained batch 2721 batch loss 0.647789 epoch total loss 0.622927368\n",
      "Trained batch 2722 batch loss 0.722878039 epoch total loss 0.622964084\n",
      "Trained batch 2723 batch loss 0.704283714 epoch total loss 0.622993946\n",
      "Trained batch 2724 batch loss 0.66229 epoch total loss 0.623008311\n",
      "Trained batch 2725 batch loss 0.568539917 epoch total loss 0.622988343\n",
      "Trained batch 2726 batch loss 0.516571105 epoch total loss 0.622949302\n",
      "Trained batch 2727 batch loss 0.59739095 epoch total loss 0.622939944\n",
      "Trained batch 2728 batch loss 0.603792131 epoch total loss 0.622932911\n",
      "Trained batch 2729 batch loss 0.538672328 epoch total loss 0.622902036\n",
      "Trained batch 2730 batch loss 0.520847142 epoch total loss 0.622864664\n",
      "Trained batch 2731 batch loss 0.569947481 epoch total loss 0.622845292\n",
      "Trained batch 2732 batch loss 0.618416 epoch total loss 0.622843683\n",
      "Trained batch 2733 batch loss 0.611650705 epoch total loss 0.62283957\n",
      "Trained batch 2734 batch loss 0.674877346 epoch total loss 0.622858644\n",
      "Trained batch 2735 batch loss 0.623137832 epoch total loss 0.622858763\n",
      "Trained batch 2736 batch loss 0.653521359 epoch total loss 0.622869968\n",
      "Trained batch 2737 batch loss 0.617255092 epoch total loss 0.622867942\n",
      "Trained batch 2738 batch loss 0.659888 epoch total loss 0.622881472\n",
      "Trained batch 2739 batch loss 0.633045316 epoch total loss 0.622885168\n",
      "Trained batch 2740 batch loss 0.677455366 epoch total loss 0.622905135\n",
      "Trained batch 2741 batch loss 0.671493411 epoch total loss 0.622922838\n",
      "Trained batch 2742 batch loss 0.576704681 epoch total loss 0.62290597\n",
      "Trained batch 2743 batch loss 0.589666486 epoch total loss 0.62289387\n",
      "Trained batch 2744 batch loss 0.551994324 epoch total loss 0.622868061\n",
      "Trained batch 2745 batch loss 0.508749247 epoch total loss 0.622826457\n",
      "Trained batch 2746 batch loss 0.505736 epoch total loss 0.62278384\n",
      "Trained batch 2747 batch loss 0.536229253 epoch total loss 0.622752368\n",
      "Trained batch 2748 batch loss 0.659418166 epoch total loss 0.62276566\n",
      "Trained batch 2749 batch loss 0.681751728 epoch total loss 0.622787178\n",
      "Trained batch 2750 batch loss 0.677417696 epoch total loss 0.622806966\n",
      "Trained batch 2751 batch loss 0.619745314 epoch total loss 0.622805893\n",
      "Trained batch 2752 batch loss 0.709788442 epoch total loss 0.622837484\n",
      "Trained batch 2753 batch loss 0.64114207 epoch total loss 0.62284416\n",
      "Trained batch 2754 batch loss 0.612412274 epoch total loss 0.622840345\n",
      "Trained batch 2755 batch loss 0.74146831 epoch total loss 0.622883439\n",
      "Trained batch 2756 batch loss 0.717385888 epoch total loss 0.622917712\n",
      "Trained batch 2757 batch loss 0.639844894 epoch total loss 0.622923851\n",
      "Trained batch 2758 batch loss 0.588173389 epoch total loss 0.622911274\n",
      "Trained batch 2759 batch loss 0.647368968 epoch total loss 0.622920096\n",
      "Trained batch 2760 batch loss 0.655120909 epoch total loss 0.622931778\n",
      "Trained batch 2761 batch loss 0.735049 epoch total loss 0.622972429\n",
      "Trained batch 2762 batch loss 0.668753445 epoch total loss 0.622989\n",
      "Trained batch 2763 batch loss 0.586153567 epoch total loss 0.622975647\n",
      "Trained batch 2764 batch loss 0.652148068 epoch total loss 0.622986197\n",
      "Trained batch 2765 batch loss 0.634087563 epoch total loss 0.622990191\n",
      "Trained batch 2766 batch loss 0.657123566 epoch total loss 0.623002529\n",
      "Trained batch 2767 batch loss 0.615991414 epoch total loss 0.622999966\n",
      "Trained batch 2768 batch loss 0.590724885 epoch total loss 0.622988284\n",
      "Trained batch 2769 batch loss 0.724143207 epoch total loss 0.623024821\n",
      "Trained batch 2770 batch loss 0.639968276 epoch total loss 0.623030961\n",
      "Trained batch 2771 batch loss 0.684432507 epoch total loss 0.623053133\n",
      "Trained batch 2772 batch loss 0.661249757 epoch total loss 0.623066902\n",
      "Trained batch 2773 batch loss 0.66195941 epoch total loss 0.623080969\n",
      "Trained batch 2774 batch loss 0.616408646 epoch total loss 0.623078585\n",
      "Trained batch 2775 batch loss 0.660617411 epoch total loss 0.623092115\n",
      "Trained batch 2776 batch loss 0.59157443 epoch total loss 0.62308073\n",
      "Epoch 3 train loss 0.6230807304382324\n",
      "Validated batch 1 batch loss 0.690051258\n",
      "Validated batch 2 batch loss 0.634503\n",
      "Validated batch 3 batch loss 0.571183562\n",
      "Validated batch 4 batch loss 0.6969\n",
      "Validated batch 5 batch loss 0.561160684\n",
      "Validated batch 6 batch loss 0.582367361\n",
      "Validated batch 7 batch loss 0.570166349\n",
      "Validated batch 8 batch loss 0.605746806\n",
      "Validated batch 9 batch loss 0.506233275\n",
      "Validated batch 10 batch loss 0.616112232\n",
      "Validated batch 11 batch loss 0.652324378\n",
      "Validated batch 12 batch loss 0.581808865\n",
      "Validated batch 13 batch loss 0.634439409\n",
      "Validated batch 14 batch loss 0.634767711\n",
      "Validated batch 15 batch loss 0.546757519\n",
      "Validated batch 16 batch loss 0.540462852\n",
      "Validated batch 17 batch loss 0.562673807\n",
      "Validated batch 18 batch loss 0.684655666\n",
      "Validated batch 19 batch loss 0.585886419\n",
      "Validated batch 20 batch loss 0.603749037\n",
      "Validated batch 21 batch loss 0.599166214\n",
      "Validated batch 22 batch loss 0.673306108\n",
      "Validated batch 23 batch loss 0.623464286\n",
      "Validated batch 24 batch loss 0.589708\n",
      "Validated batch 25 batch loss 0.67406559\n",
      "Validated batch 26 batch loss 0.564206\n",
      "Validated batch 27 batch loss 0.543983519\n",
      "Validated batch 28 batch loss 0.649382472\n",
      "Validated batch 29 batch loss 0.603291452\n",
      "Validated batch 30 batch loss 0.561952293\n",
      "Validated batch 31 batch loss 0.627831042\n",
      "Validated batch 32 batch loss 0.598188937\n",
      "Validated batch 33 batch loss 0.581375599\n",
      "Validated batch 34 batch loss 0.591558039\n",
      "Validated batch 35 batch loss 0.659138739\n",
      "Validated batch 36 batch loss 0.641332924\n",
      "Validated batch 37 batch loss 0.626117945\n",
      "Validated batch 38 batch loss 0.678931177\n",
      "Validated batch 39 batch loss 0.69303894\n",
      "Validated batch 40 batch loss 0.771528602\n",
      "Validated batch 41 batch loss 0.653550744\n",
      "Validated batch 42 batch loss 0.600166142\n",
      "Validated batch 43 batch loss 0.604086399\n",
      "Validated batch 44 batch loss 0.637072086\n",
      "Validated batch 45 batch loss 0.537099242\n",
      "Validated batch 46 batch loss 0.549600303\n",
      "Validated batch 47 batch loss 0.578466237\n",
      "Validated batch 48 batch loss 0.60739547\n",
      "Validated batch 49 batch loss 0.525971\n",
      "Validated batch 50 batch loss 0.580653429\n",
      "Validated batch 51 batch loss 0.597191155\n",
      "Validated batch 52 batch loss 0.620765269\n",
      "Validated batch 53 batch loss 0.546847939\n",
      "Validated batch 54 batch loss 0.569713414\n",
      "Validated batch 55 batch loss 0.637654\n",
      "Validated batch 56 batch loss 0.590325773\n",
      "Validated batch 57 batch loss 0.612636745\n",
      "Validated batch 58 batch loss 0.63908416\n",
      "Validated batch 59 batch loss 0.575698495\n",
      "Validated batch 60 batch loss 0.632540584\n",
      "Validated batch 61 batch loss 0.637847543\n",
      "Validated batch 62 batch loss 0.743735\n",
      "Validated batch 63 batch loss 0.599598229\n",
      "Validated batch 64 batch loss 0.679500639\n",
      "Validated batch 65 batch loss 0.614938\n",
      "Validated batch 66 batch loss 0.593367577\n",
      "Validated batch 67 batch loss 0.623991787\n",
      "Validated batch 68 batch loss 0.597078204\n",
      "Validated batch 69 batch loss 0.683805346\n",
      "Validated batch 70 batch loss 0.61642921\n",
      "Validated batch 71 batch loss 0.661781192\n",
      "Validated batch 72 batch loss 0.623282373\n",
      "Validated batch 73 batch loss 0.651353061\n",
      "Validated batch 74 batch loss 0.646549225\n",
      "Validated batch 75 batch loss 0.619351685\n",
      "Validated batch 76 batch loss 0.620177031\n",
      "Validated batch 77 batch loss 0.707230389\n",
      "Validated batch 78 batch loss 0.629604816\n",
      "Validated batch 79 batch loss 0.696918964\n",
      "Validated batch 80 batch loss 0.605459213\n",
      "Validated batch 81 batch loss 0.580016613\n",
      "Validated batch 82 batch loss 0.545201719\n",
      "Validated batch 83 batch loss 0.58880192\n",
      "Validated batch 84 batch loss 0.643669546\n",
      "Validated batch 85 batch loss 0.640993237\n",
      "Validated batch 86 batch loss 0.667995691\n",
      "Validated batch 87 batch loss 0.645825207\n",
      "Validated batch 88 batch loss 0.576753139\n",
      "Validated batch 89 batch loss 0.590970218\n",
      "Validated batch 90 batch loss 0.620674789\n",
      "Validated batch 91 batch loss 0.638386726\n",
      "Validated batch 92 batch loss 0.551001\n",
      "Validated batch 93 batch loss 0.5873698\n",
      "Validated batch 94 batch loss 0.611593366\n",
      "Validated batch 95 batch loss 0.639864504\n",
      "Validated batch 96 batch loss 0.616271913\n",
      "Validated batch 97 batch loss 0.593266845\n",
      "Validated batch 98 batch loss 0.52373147\n",
      "Validated batch 99 batch loss 0.579465389\n",
      "Validated batch 100 batch loss 0.636756\n",
      "Validated batch 101 batch loss 0.555624902\n",
      "Validated batch 102 batch loss 0.627119899\n",
      "Validated batch 103 batch loss 0.56638962\n",
      "Validated batch 104 batch loss 0.661209226\n",
      "Validated batch 105 batch loss 0.612793803\n",
      "Validated batch 106 batch loss 0.656682134\n",
      "Validated batch 107 batch loss 0.643476725\n",
      "Validated batch 108 batch loss 0.649589777\n",
      "Validated batch 109 batch loss 0.67559433\n",
      "Validated batch 110 batch loss 0.593391776\n",
      "Validated batch 111 batch loss 0.589235187\n",
      "Validated batch 112 batch loss 0.605632842\n",
      "Validated batch 113 batch loss 0.623456836\n",
      "Validated batch 114 batch loss 0.642879546\n",
      "Validated batch 115 batch loss 0.636349857\n",
      "Validated batch 116 batch loss 0.586417794\n",
      "Validated batch 117 batch loss 0.578077197\n",
      "Validated batch 118 batch loss 0.636140406\n",
      "Validated batch 119 batch loss 0.65492332\n",
      "Validated batch 120 batch loss 0.666002691\n",
      "Validated batch 121 batch loss 0.66247797\n",
      "Validated batch 122 batch loss 0.606288195\n",
      "Validated batch 123 batch loss 0.624418616\n",
      "Validated batch 124 batch loss 0.609652102\n",
      "Validated batch 125 batch loss 0.640907764\n",
      "Validated batch 126 batch loss 0.731693745\n",
      "Validated batch 127 batch loss 0.515983403\n",
      "Validated batch 128 batch loss 0.560097277\n",
      "Validated batch 129 batch loss 0.611231387\n",
      "Validated batch 130 batch loss 0.688939095\n",
      "Validated batch 131 batch loss 0.517976344\n",
      "Validated batch 132 batch loss 0.498905361\n",
      "Validated batch 133 batch loss 0.548085392\n",
      "Validated batch 134 batch loss 0.650578558\n",
      "Validated batch 135 batch loss 0.617771566\n",
      "Validated batch 136 batch loss 0.676933706\n",
      "Validated batch 137 batch loss 0.544329464\n",
      "Validated batch 138 batch loss 0.610397\n",
      "Validated batch 139 batch loss 0.588279665\n",
      "Validated batch 140 batch loss 0.667092681\n",
      "Validated batch 141 batch loss 0.602277339\n",
      "Validated batch 142 batch loss 0.588894665\n",
      "Validated batch 143 batch loss 0.612282157\n",
      "Validated batch 144 batch loss 0.611474216\n",
      "Validated batch 145 batch loss 0.560719371\n",
      "Validated batch 146 batch loss 0.627258956\n",
      "Validated batch 147 batch loss 0.681873202\n",
      "Validated batch 148 batch loss 0.601955533\n",
      "Validated batch 149 batch loss 0.764172852\n",
      "Validated batch 150 batch loss 0.622768283\n",
      "Validated batch 151 batch loss 0.556684613\n",
      "Validated batch 152 batch loss 0.594765\n",
      "Validated batch 153 batch loss 0.598798\n",
      "Validated batch 154 batch loss 0.669129908\n",
      "Validated batch 155 batch loss 0.576788843\n",
      "Validated batch 156 batch loss 0.623066783\n",
      "Validated batch 157 batch loss 0.646876216\n",
      "Validated batch 158 batch loss 0.631492198\n",
      "Validated batch 159 batch loss 0.609324157\n",
      "Validated batch 160 batch loss 0.634556711\n",
      "Validated batch 161 batch loss 0.607375681\n",
      "Validated batch 162 batch loss 0.519846916\n",
      "Validated batch 163 batch loss 0.629235744\n",
      "Validated batch 164 batch loss 0.731039\n",
      "Validated batch 165 batch loss 0.656132162\n",
      "Validated batch 166 batch loss 0.538657188\n",
      "Validated batch 167 batch loss 0.611763835\n",
      "Validated batch 168 batch loss 0.655310273\n",
      "Validated batch 169 batch loss 0.578302383\n",
      "Validated batch 170 batch loss 0.642963707\n",
      "Validated batch 171 batch loss 0.627283514\n",
      "Validated batch 172 batch loss 0.657982111\n",
      "Validated batch 173 batch loss 0.642232239\n",
      "Validated batch 174 batch loss 0.495568752\n",
      "Validated batch 175 batch loss 0.594772816\n",
      "Validated batch 176 batch loss 0.628364742\n",
      "Validated batch 177 batch loss 0.608888\n",
      "Validated batch 178 batch loss 0.560053468\n",
      "Validated batch 179 batch loss 0.589486897\n",
      "Validated batch 180 batch loss 0.594089\n",
      "Validated batch 181 batch loss 0.636189103\n",
      "Validated batch 182 batch loss 0.596479416\n",
      "Validated batch 183 batch loss 0.57915324\n",
      "Validated batch 184 batch loss 0.593816042\n",
      "Validated batch 185 batch loss 0.66917932\n",
      "Validated batch 186 batch loss 0.617980838\n",
      "Validated batch 187 batch loss 0.687315702\n",
      "Validated batch 188 batch loss 0.588984072\n",
      "Validated batch 189 batch loss 0.492407739\n",
      "Validated batch 190 batch loss 0.64035\n",
      "Validated batch 191 batch loss 0.624997258\n",
      "Validated batch 192 batch loss 0.573268712\n",
      "Validated batch 193 batch loss 0.55431658\n",
      "Validated batch 194 batch loss 0.639505148\n",
      "Validated batch 195 batch loss 0.547876954\n",
      "Validated batch 196 batch loss 0.635329127\n",
      "Validated batch 197 batch loss 0.627322555\n",
      "Validated batch 198 batch loss 0.598308623\n",
      "Validated batch 199 batch loss 0.544243574\n",
      "Validated batch 200 batch loss 0.649058104\n",
      "Validated batch 201 batch loss 0.436702132\n",
      "Validated batch 202 batch loss 0.714211047\n",
      "Validated batch 203 batch loss 0.700110495\n",
      "Validated batch 204 batch loss 0.597073436\n",
      "Validated batch 205 batch loss 0.580397\n",
      "Validated batch 206 batch loss 0.567348242\n",
      "Validated batch 207 batch loss 0.541364968\n",
      "Validated batch 208 batch loss 0.576362193\n",
      "Validated batch 209 batch loss 0.609883785\n",
      "Validated batch 210 batch loss 0.590289354\n",
      "Validated batch 211 batch loss 0.643622041\n",
      "Validated batch 212 batch loss 0.67091912\n",
      "Validated batch 213 batch loss 0.631796479\n",
      "Validated batch 214 batch loss 0.684949517\n",
      "Validated batch 215 batch loss 0.667291462\n",
      "Validated batch 216 batch loss 0.680795908\n",
      "Validated batch 217 batch loss 0.568451762\n",
      "Validated batch 218 batch loss 0.615249574\n",
      "Validated batch 219 batch loss 0.699261069\n",
      "Validated batch 220 batch loss 0.629178286\n",
      "Validated batch 221 batch loss 0.606190801\n",
      "Validated batch 222 batch loss 0.593665898\n",
      "Validated batch 223 batch loss 0.746886492\n",
      "Validated batch 224 batch loss 0.536776543\n",
      "Validated batch 225 batch loss 0.607730448\n",
      "Validated batch 226 batch loss 0.663356781\n",
      "Validated batch 227 batch loss 0.498993933\n",
      "Validated batch 228 batch loss 0.495880932\n",
      "Validated batch 229 batch loss 0.571253181\n",
      "Validated batch 230 batch loss 0.663965106\n",
      "Validated batch 231 batch loss 0.641233683\n",
      "Validated batch 232 batch loss 0.5385409\n",
      "Validated batch 233 batch loss 0.578585863\n",
      "Validated batch 234 batch loss 0.612460792\n",
      "Validated batch 235 batch loss 0.666436315\n",
      "Validated batch 236 batch loss 0.613010168\n",
      "Validated batch 237 batch loss 0.574921608\n",
      "Validated batch 238 batch loss 0.558422327\n",
      "Validated batch 239 batch loss 0.617571473\n",
      "Validated batch 240 batch loss 0.648009539\n",
      "Validated batch 241 batch loss 0.720631599\n",
      "Validated batch 242 batch loss 0.677557647\n",
      "Validated batch 243 batch loss 0.565226197\n",
      "Validated batch 244 batch loss 0.52268362\n",
      "Validated batch 245 batch loss 0.598394632\n",
      "Validated batch 246 batch loss 0.583365321\n",
      "Validated batch 247 batch loss 0.640037417\n",
      "Validated batch 248 batch loss 0.595649302\n",
      "Validated batch 249 batch loss 0.610263\n",
      "Validated batch 250 batch loss 0.637419283\n",
      "Validated batch 251 batch loss 0.639609098\n",
      "Validated batch 252 batch loss 0.620364785\n",
      "Validated batch 253 batch loss 0.586809099\n",
      "Validated batch 254 batch loss 0.596344352\n",
      "Validated batch 255 batch loss 0.450545\n",
      "Validated batch 256 batch loss 0.572540164\n",
      "Validated batch 257 batch loss 0.631655037\n",
      "Validated batch 258 batch loss 0.586199403\n",
      "Validated batch 259 batch loss 0.59528482\n",
      "Validated batch 260 batch loss 0.55486995\n",
      "Validated batch 261 batch loss 0.583923\n",
      "Validated batch 262 batch loss 0.595686674\n",
      "Validated batch 263 batch loss 0.662818551\n",
      "Validated batch 264 batch loss 0.612777233\n",
      "Validated batch 265 batch loss 0.589348257\n",
      "Validated batch 266 batch loss 0.523969352\n",
      "Validated batch 267 batch loss 0.612320423\n",
      "Validated batch 268 batch loss 0.600845397\n",
      "Validated batch 269 batch loss 0.646723747\n",
      "Validated batch 270 batch loss 0.635410488\n",
      "Validated batch 271 batch loss 0.588493347\n",
      "Validated batch 272 batch loss 0.614637315\n",
      "Validated batch 273 batch loss 0.623369277\n",
      "Validated batch 274 batch loss 0.612869918\n",
      "Validated batch 275 batch loss 0.548641443\n",
      "Validated batch 276 batch loss 0.544781685\n",
      "Validated batch 277 batch loss 0.578958154\n",
      "Validated batch 278 batch loss 0.594115138\n",
      "Validated batch 279 batch loss 0.657889903\n",
      "Validated batch 280 batch loss 0.605344892\n",
      "Validated batch 281 batch loss 0.601679087\n",
      "Validated batch 282 batch loss 0.604764223\n",
      "Validated batch 283 batch loss 0.512442708\n",
      "Validated batch 284 batch loss 0.589233756\n",
      "Validated batch 285 batch loss 0.573006749\n",
      "Validated batch 286 batch loss 0.584337234\n",
      "Validated batch 287 batch loss 0.590437651\n",
      "Validated batch 288 batch loss 0.708839118\n",
      "Validated batch 289 batch loss 0.547430396\n",
      "Validated batch 290 batch loss 0.571080506\n",
      "Validated batch 291 batch loss 0.626718223\n",
      "Validated batch 292 batch loss 0.451506019\n",
      "Validated batch 293 batch loss 0.551715434\n",
      "Validated batch 294 batch loss 0.611190677\n",
      "Validated batch 295 batch loss 0.604277\n",
      "Validated batch 296 batch loss 0.637352526\n",
      "Validated batch 297 batch loss 0.538537\n",
      "Validated batch 298 batch loss 0.542206764\n",
      "Validated batch 299 batch loss 0.611270905\n",
      "Validated batch 300 batch loss 0.621437311\n",
      "Validated batch 301 batch loss 0.533935428\n",
      "Validated batch 302 batch loss 0.56159693\n",
      "Validated batch 303 batch loss 0.666189134\n",
      "Validated batch 304 batch loss 0.534241378\n",
      "Validated batch 305 batch loss 0.640641868\n",
      "Validated batch 306 batch loss 0.645718455\n",
      "Validated batch 307 batch loss 0.659932792\n",
      "Validated batch 308 batch loss 0.647313416\n",
      "Validated batch 309 batch loss 0.558785439\n",
      "Validated batch 310 batch loss 0.566143036\n",
      "Validated batch 311 batch loss 0.661084414\n",
      "Validated batch 312 batch loss 0.658991098\n",
      "Validated batch 313 batch loss 0.537252665\n",
      "Validated batch 314 batch loss 0.498171151\n",
      "Validated batch 315 batch loss 0.5526793\n",
      "Validated batch 316 batch loss 0.514804\n",
      "Validated batch 317 batch loss 0.662012875\n",
      "Validated batch 318 batch loss 0.518439174\n",
      "Validated batch 319 batch loss 0.534809\n",
      "Validated batch 320 batch loss 0.631559551\n",
      "Validated batch 321 batch loss 0.660824597\n",
      "Validated batch 322 batch loss 0.674021304\n",
      "Validated batch 323 batch loss 0.664684832\n",
      "Validated batch 324 batch loss 0.561928749\n",
      "Validated batch 325 batch loss 0.581244588\n",
      "Validated batch 326 batch loss 0.543614209\n",
      "Validated batch 327 batch loss 0.594781339\n",
      "Validated batch 328 batch loss 0.594716\n",
      "Validated batch 329 batch loss 0.559678793\n",
      "Validated batch 330 batch loss 0.603296518\n",
      "Validated batch 331 batch loss 0.561714113\n",
      "Validated batch 332 batch loss 0.579466641\n",
      "Validated batch 333 batch loss 0.598613322\n",
      "Validated batch 334 batch loss 0.648811\n",
      "Validated batch 335 batch loss 0.588283777\n",
      "Validated batch 336 batch loss 0.555082083\n",
      "Validated batch 337 batch loss 0.650985\n",
      "Validated batch 338 batch loss 0.592761219\n",
      "Validated batch 339 batch loss 0.663628578\n",
      "Validated batch 340 batch loss 0.653593481\n",
      "Validated batch 341 batch loss 0.538852811\n",
      "Validated batch 342 batch loss 0.535795\n",
      "Validated batch 343 batch loss 0.679266214\n",
      "Validated batch 344 batch loss 0.59610486\n",
      "Validated batch 345 batch loss 0.585335314\n",
      "Validated batch 346 batch loss 0.633540869\n",
      "Validated batch 347 batch loss 0.537312031\n",
      "Validated batch 348 batch loss 0.559407771\n",
      "Validated batch 349 batch loss 0.665304899\n",
      "Validated batch 350 batch loss 0.551774\n",
      "Validated batch 351 batch loss 0.637911677\n",
      "Validated batch 352 batch loss 0.608290076\n",
      "Validated batch 353 batch loss 0.611150861\n",
      "Validated batch 354 batch loss 0.562262475\n",
      "Validated batch 355 batch loss 0.622217178\n",
      "Validated batch 356 batch loss 0.676758826\n",
      "Validated batch 357 batch loss 0.637980938\n",
      "Validated batch 358 batch loss 0.58742404\n",
      "Validated batch 359 batch loss 0.693229377\n",
      "Validated batch 360 batch loss 0.580785\n",
      "Validated batch 361 batch loss 0.523868263\n",
      "Validated batch 362 batch loss 0.625374854\n",
      "Validated batch 363 batch loss 0.645856857\n",
      "Validated batch 364 batch loss 0.594811082\n",
      "Validated batch 365 batch loss 0.592657208\n",
      "Validated batch 366 batch loss 0.61894232\n",
      "Validated batch 367 batch loss 0.49668473\n",
      "Validated batch 368 batch loss 0.626022041\n",
      "Validated batch 369 batch loss 0.63199687\n",
      "Epoch 3 val loss 0.6077619791030884\n",
      "Model /aiffel/mpii/weights/shg_model-epoch-3-loss-0.6078.h5 saved.\n",
      "Start epoch 4 with learning rate 0.0007\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 0.724416494 epoch total loss 0.724416494\n",
      "Trained batch 2 batch loss 0.698308229 epoch total loss 0.711362362\n",
      "Trained batch 3 batch loss 0.670503378 epoch total loss 0.697742701\n",
      "Trained batch 4 batch loss 0.590219319 epoch total loss 0.67086184\n",
      "Trained batch 5 batch loss 0.63700074 epoch total loss 0.66408962\n",
      "Trained batch 6 batch loss 0.576148093 epoch total loss 0.649432719\n",
      "Trained batch 7 batch loss 0.644397438 epoch total loss 0.64871341\n",
      "Trained batch 8 batch loss 0.536475062 epoch total loss 0.634683609\n",
      "Trained batch 9 batch loss 0.543251395 epoch total loss 0.624524474\n",
      "Trained batch 10 batch loss 0.554768 epoch total loss 0.617548823\n",
      "Trained batch 11 batch loss 0.632273853 epoch total loss 0.618887484\n",
      "Trained batch 12 batch loss 0.689832211 epoch total loss 0.62479955\n",
      "Trained batch 13 batch loss 0.643983126 epoch total loss 0.626275182\n",
      "Trained batch 14 batch loss 0.547186255 epoch total loss 0.620626\n",
      "Trained batch 15 batch loss 0.512678862 epoch total loss 0.613429487\n",
      "Trained batch 16 batch loss 0.503826499 epoch total loss 0.606579304\n",
      "Trained batch 17 batch loss 0.509919107 epoch total loss 0.600893438\n",
      "Trained batch 18 batch loss 0.492860258 epoch total loss 0.594891548\n",
      "Trained batch 19 batch loss 0.485778451 epoch total loss 0.58914876\n",
      "Trained batch 20 batch loss 0.575867474 epoch total loss 0.588484704\n",
      "Trained batch 21 batch loss 0.578839421 epoch total loss 0.588025391\n",
      "Trained batch 22 batch loss 0.541009247 epoch total loss 0.585888326\n",
      "Trained batch 23 batch loss 0.55791086 epoch total loss 0.584671915\n",
      "Trained batch 24 batch loss 0.492777467 epoch total loss 0.580843\n",
      "Trained batch 25 batch loss 0.539996564 epoch total loss 0.579209089\n",
      "Trained batch 26 batch loss 0.543069661 epoch total loss 0.577819109\n",
      "Trained batch 27 batch loss 0.570947 epoch total loss 0.577564597\n",
      "Trained batch 28 batch loss 0.56855315 epoch total loss 0.577242792\n",
      "Trained batch 29 batch loss 0.632341504 epoch total loss 0.579142749\n",
      "Trained batch 30 batch loss 0.573690772 epoch total loss 0.578961\n",
      "Trained batch 31 batch loss 0.618962 epoch total loss 0.580251336\n",
      "Trained batch 32 batch loss 0.666032612 epoch total loss 0.582932\n",
      "Trained batch 33 batch loss 0.54284066 epoch total loss 0.581717134\n",
      "Trained batch 34 batch loss 0.539963484 epoch total loss 0.580489039\n",
      "Trained batch 35 batch loss 0.518330693 epoch total loss 0.578713119\n",
      "Trained batch 36 batch loss 0.558253229 epoch total loss 0.578144789\n",
      "Trained batch 37 batch loss 0.60115093 epoch total loss 0.578766525\n",
      "Trained batch 38 batch loss 0.642602 epoch total loss 0.580446422\n",
      "Trained batch 39 batch loss 0.617696345 epoch total loss 0.581401587\n",
      "Trained batch 40 batch loss 0.675414801 epoch total loss 0.583751917\n",
      "Trained batch 41 batch loss 0.64598614 epoch total loss 0.585269809\n",
      "Trained batch 42 batch loss 0.583952069 epoch total loss 0.585238457\n",
      "Trained batch 43 batch loss 0.648652554 epoch total loss 0.586713195\n",
      "Trained batch 44 batch loss 0.708131909 epoch total loss 0.589472711\n",
      "Trained batch 45 batch loss 0.577059567 epoch total loss 0.589196861\n",
      "Trained batch 46 batch loss 0.592150807 epoch total loss 0.589261115\n",
      "Trained batch 47 batch loss 0.660796642 epoch total loss 0.590783119\n",
      "Trained batch 48 batch loss 0.527931392 epoch total loss 0.589473724\n",
      "Trained batch 49 batch loss 0.588018298 epoch total loss 0.589444041\n",
      "Trained batch 50 batch loss 0.573139489 epoch total loss 0.589117944\n",
      "Trained batch 51 batch loss 0.611825585 epoch total loss 0.589563191\n",
      "Trained batch 52 batch loss 0.683890641 epoch total loss 0.591377199\n",
      "Trained batch 53 batch loss 0.641988695 epoch total loss 0.592332125\n",
      "Trained batch 54 batch loss 0.639751494 epoch total loss 0.59321028\n",
      "Trained batch 55 batch loss 0.564171731 epoch total loss 0.592682302\n",
      "Trained batch 56 batch loss 0.643228352 epoch total loss 0.593584895\n",
      "Trained batch 57 batch loss 0.587048173 epoch total loss 0.593470216\n",
      "Trained batch 58 batch loss 0.559712291 epoch total loss 0.592888117\n",
      "Trained batch 59 batch loss 0.604091823 epoch total loss 0.593078\n",
      "Trained batch 60 batch loss 0.662548482 epoch total loss 0.594235837\n",
      "Trained batch 61 batch loss 0.650586605 epoch total loss 0.59515965\n",
      "Trained batch 62 batch loss 0.58831 epoch total loss 0.595049143\n",
      "Trained batch 63 batch loss 0.596363425 epoch total loss 0.59507\n",
      "Trained batch 64 batch loss 0.566180229 epoch total loss 0.594618618\n",
      "Trained batch 65 batch loss 0.591361523 epoch total loss 0.594568491\n",
      "Trained batch 66 batch loss 0.665486693 epoch total loss 0.595643044\n",
      "Trained batch 67 batch loss 0.569670618 epoch total loss 0.595255375\n",
      "Trained batch 68 batch loss 0.604705 epoch total loss 0.595394373\n",
      "Trained batch 69 batch loss 0.64463532 epoch total loss 0.59610796\n",
      "Trained batch 70 batch loss 0.625882089 epoch total loss 0.596533298\n",
      "Trained batch 71 batch loss 0.568533182 epoch total loss 0.596138954\n",
      "Trained batch 72 batch loss 0.564987659 epoch total loss 0.595706284\n",
      "Trained batch 73 batch loss 0.544506 epoch total loss 0.595004916\n",
      "Trained batch 74 batch loss 0.499464512 epoch total loss 0.59371388\n",
      "Trained batch 75 batch loss 0.51573658 epoch total loss 0.592674136\n",
      "Trained batch 76 batch loss 0.447407067 epoch total loss 0.590762734\n",
      "Trained batch 77 batch loss 0.527320683 epoch total loss 0.589938819\n",
      "Trained batch 78 batch loss 0.531905293 epoch total loss 0.589194834\n",
      "Trained batch 79 batch loss 0.560878694 epoch total loss 0.588836372\n",
      "Trained batch 80 batch loss 0.593534112 epoch total loss 0.588895082\n",
      "Trained batch 81 batch loss 0.612032056 epoch total loss 0.589180768\n",
      "Trained batch 82 batch loss 0.565377 epoch total loss 0.588890433\n",
      "Trained batch 83 batch loss 0.488777339 epoch total loss 0.587684274\n",
      "Trained batch 84 batch loss 0.537693202 epoch total loss 0.587089121\n",
      "Trained batch 85 batch loss 0.519305527 epoch total loss 0.586291671\n",
      "Trained batch 86 batch loss 0.521004677 epoch total loss 0.585532546\n",
      "Trained batch 87 batch loss 0.577306092 epoch total loss 0.585437953\n",
      "Trained batch 88 batch loss 0.594234824 epoch total loss 0.58553791\n",
      "Trained batch 89 batch loss 0.571274519 epoch total loss 0.585377634\n",
      "Trained batch 90 batch loss 0.56969732 epoch total loss 0.585203409\n",
      "Trained batch 91 batch loss 0.497736931 epoch total loss 0.584242284\n",
      "Trained batch 92 batch loss 0.546070635 epoch total loss 0.583827376\n",
      "Trained batch 93 batch loss 0.546427846 epoch total loss 0.583425224\n",
      "Trained batch 94 batch loss 0.57530731 epoch total loss 0.583338857\n",
      "Trained batch 95 batch loss 0.553716838 epoch total loss 0.583027065\n",
      "Trained batch 96 batch loss 0.561871529 epoch total loss 0.582806647\n",
      "Trained batch 97 batch loss 0.514547348 epoch total loss 0.582102954\n",
      "Trained batch 98 batch loss 0.587117851 epoch total loss 0.582154155\n",
      "Trained batch 99 batch loss 0.585147321 epoch total loss 0.582184374\n",
      "Trained batch 100 batch loss 0.638617635 epoch total loss 0.582748711\n",
      "Trained batch 101 batch loss 0.550420821 epoch total loss 0.582428634\n",
      "Trained batch 102 batch loss 0.548606157 epoch total loss 0.582097054\n",
      "Trained batch 103 batch loss 0.568746865 epoch total loss 0.581967473\n",
      "Trained batch 104 batch loss 0.553403914 epoch total loss 0.581692815\n",
      "Trained batch 105 batch loss 0.498851359 epoch total loss 0.580903888\n",
      "Trained batch 106 batch loss 0.475989312 epoch total loss 0.579914153\n",
      "Trained batch 107 batch loss 0.437646508 epoch total loss 0.578584492\n",
      "Trained batch 108 batch loss 0.496781528 epoch total loss 0.577827036\n",
      "Trained batch 109 batch loss 0.611540377 epoch total loss 0.578136384\n",
      "Trained batch 110 batch loss 0.623567462 epoch total loss 0.578549385\n",
      "Trained batch 111 batch loss 0.707767665 epoch total loss 0.579713523\n",
      "Trained batch 112 batch loss 0.609492242 epoch total loss 0.57997936\n",
      "Trained batch 113 batch loss 0.594998777 epoch total loss 0.580112278\n",
      "Trained batch 114 batch loss 0.610123098 epoch total loss 0.580375552\n",
      "Trained batch 115 batch loss 0.603653789 epoch total loss 0.58057797\n",
      "Trained batch 116 batch loss 0.653532803 epoch total loss 0.581206858\n",
      "Trained batch 117 batch loss 0.621480525 epoch total loss 0.581551135\n",
      "Trained batch 118 batch loss 0.586075783 epoch total loss 0.58158946\n",
      "Trained batch 119 batch loss 0.565250278 epoch total loss 0.581452131\n",
      "Trained batch 120 batch loss 0.518271744 epoch total loss 0.580925643\n",
      "Trained batch 121 batch loss 0.49184 epoch total loss 0.580189347\n",
      "Trained batch 122 batch loss 0.556998 epoch total loss 0.579999268\n",
      "Trained batch 123 batch loss 0.534620702 epoch total loss 0.579630375\n",
      "Trained batch 124 batch loss 0.554033399 epoch total loss 0.579423904\n",
      "Trained batch 125 batch loss 0.564867198 epoch total loss 0.579307437\n",
      "Trained batch 126 batch loss 0.587220669 epoch total loss 0.579370201\n",
      "Trained batch 127 batch loss 0.687819898 epoch total loss 0.580224156\n",
      "Trained batch 128 batch loss 0.625769556 epoch total loss 0.58058\n",
      "Trained batch 129 batch loss 0.66664958 epoch total loss 0.581247211\n",
      "Trained batch 130 batch loss 0.528538346 epoch total loss 0.58084178\n",
      "Trained batch 131 batch loss 0.588722646 epoch total loss 0.580901921\n",
      "Trained batch 132 batch loss 0.603350639 epoch total loss 0.581072\n",
      "Trained batch 133 batch loss 0.614092708 epoch total loss 0.581320226\n",
      "Trained batch 134 batch loss 0.587260127 epoch total loss 0.581364512\n",
      "Trained batch 135 batch loss 0.590937436 epoch total loss 0.581435442\n",
      "Trained batch 136 batch loss 0.527005494 epoch total loss 0.581035197\n",
      "Trained batch 137 batch loss 0.571465433 epoch total loss 0.58096534\n",
      "Trained batch 138 batch loss 0.51027447 epoch total loss 0.580453098\n",
      "Trained batch 139 batch loss 0.512734175 epoch total loss 0.579965949\n",
      "Trained batch 140 batch loss 0.504410565 epoch total loss 0.579426229\n",
      "Trained batch 141 batch loss 0.486774057 epoch total loss 0.578769088\n",
      "Trained batch 142 batch loss 0.513896406 epoch total loss 0.578312218\n",
      "Trained batch 143 batch loss 0.61673224 epoch total loss 0.578580916\n",
      "Trained batch 144 batch loss 0.646839857 epoch total loss 0.579054952\n",
      "Trained batch 145 batch loss 0.642259419 epoch total loss 0.57949084\n",
      "Trained batch 146 batch loss 0.643639207 epoch total loss 0.579930186\n",
      "Trained batch 147 batch loss 0.665868938 epoch total loss 0.580514789\n",
      "Trained batch 148 batch loss 0.694298625 epoch total loss 0.581283629\n",
      "Trained batch 149 batch loss 0.800976276 epoch total loss 0.582758069\n",
      "Trained batch 150 batch loss 0.632288873 epoch total loss 0.583088279\n",
      "Trained batch 151 batch loss 0.731994808 epoch total loss 0.584074438\n",
      "Trained batch 152 batch loss 0.623445868 epoch total loss 0.58433342\n",
      "Trained batch 153 batch loss 0.609951496 epoch total loss 0.584500909\n",
      "Trained batch 154 batch loss 0.643773317 epoch total loss 0.584885776\n",
      "Trained batch 155 batch loss 0.630065799 epoch total loss 0.585177243\n",
      "Trained batch 156 batch loss 0.64054811 epoch total loss 0.585532188\n",
      "Trained batch 157 batch loss 0.580769062 epoch total loss 0.585501909\n",
      "Trained batch 158 batch loss 0.524210811 epoch total loss 0.585113943\n",
      "Trained batch 159 batch loss 0.525037766 epoch total loss 0.584736109\n",
      "Trained batch 160 batch loss 0.580256283 epoch total loss 0.584708095\n",
      "Trained batch 161 batch loss 0.590064406 epoch total loss 0.584741414\n",
      "Trained batch 162 batch loss 0.650671184 epoch total loss 0.585148394\n",
      "Trained batch 163 batch loss 0.646666169 epoch total loss 0.585525811\n",
      "Trained batch 164 batch loss 0.623782635 epoch total loss 0.585759044\n",
      "Trained batch 165 batch loss 0.599384904 epoch total loss 0.585841656\n",
      "Trained batch 166 batch loss 0.578935623 epoch total loss 0.585800052\n",
      "Trained batch 167 batch loss 0.603295863 epoch total loss 0.585904777\n",
      "Trained batch 168 batch loss 0.655249119 epoch total loss 0.586317539\n",
      "Trained batch 169 batch loss 0.570982158 epoch total loss 0.586226821\n",
      "Trained batch 170 batch loss 0.621300101 epoch total loss 0.586433113\n",
      "Trained batch 171 batch loss 0.579766214 epoch total loss 0.586394131\n",
      "Trained batch 172 batch loss 0.59051609 epoch total loss 0.586418092\n",
      "Trained batch 173 batch loss 0.637381136 epoch total loss 0.586712718\n",
      "Trained batch 174 batch loss 0.606878877 epoch total loss 0.586828589\n",
      "Trained batch 175 batch loss 0.556163311 epoch total loss 0.586653352\n",
      "Trained batch 176 batch loss 0.613527596 epoch total loss 0.586806059\n",
      "Trained batch 177 batch loss 0.638768196 epoch total loss 0.587099612\n",
      "Trained batch 178 batch loss 0.699468076 epoch total loss 0.587730944\n",
      "Trained batch 179 batch loss 0.731325388 epoch total loss 0.588533103\n",
      "Trained batch 180 batch loss 0.647741079 epoch total loss 0.588862062\n",
      "Trained batch 181 batch loss 0.537915111 epoch total loss 0.588580608\n",
      "Trained batch 182 batch loss 0.658912838 epoch total loss 0.588967\n",
      "Trained batch 183 batch loss 0.516417623 epoch total loss 0.588570595\n",
      "Trained batch 184 batch loss 0.449533969 epoch total loss 0.587814927\n",
      "Trained batch 185 batch loss 0.492571086 epoch total loss 0.587300122\n",
      "Trained batch 186 batch loss 0.550508857 epoch total loss 0.587102294\n",
      "Trained batch 187 batch loss 0.515512347 epoch total loss 0.586719453\n",
      "Trained batch 188 batch loss 0.593642652 epoch total loss 0.586756289\n",
      "Trained batch 189 batch loss 0.535961568 epoch total loss 0.586487532\n",
      "Trained batch 190 batch loss 0.620026767 epoch total loss 0.586664081\n",
      "Trained batch 191 batch loss 0.738430858 epoch total loss 0.58745867\n",
      "Trained batch 192 batch loss 0.623522282 epoch total loss 0.587646484\n",
      "Trained batch 193 batch loss 0.623563766 epoch total loss 0.58783257\n",
      "Trained batch 194 batch loss 0.562627852 epoch total loss 0.587702692\n",
      "Trained batch 195 batch loss 0.599135399 epoch total loss 0.587761343\n",
      "Trained batch 196 batch loss 0.609712 epoch total loss 0.58787328\n",
      "Trained batch 197 batch loss 0.56609422 epoch total loss 0.587762773\n",
      "Trained batch 198 batch loss 0.609179795 epoch total loss 0.587870896\n",
      "Trained batch 199 batch loss 0.506618738 epoch total loss 0.587462604\n",
      "Trained batch 200 batch loss 0.575788379 epoch total loss 0.587404251\n",
      "Trained batch 201 batch loss 0.587992191 epoch total loss 0.587407172\n",
      "Trained batch 202 batch loss 0.56983 epoch total loss 0.587320149\n",
      "Trained batch 203 batch loss 0.604161918 epoch total loss 0.587403119\n",
      "Trained batch 204 batch loss 0.601503193 epoch total loss 0.58747226\n",
      "Trained batch 205 batch loss 0.634853542 epoch total loss 0.587703407\n",
      "Trained batch 206 batch loss 0.690916 epoch total loss 0.588204443\n",
      "Trained batch 207 batch loss 0.650311947 epoch total loss 0.588504493\n",
      "Trained batch 208 batch loss 0.619194388 epoch total loss 0.588652\n",
      "Trained batch 209 batch loss 0.596732736 epoch total loss 0.588690698\n",
      "Trained batch 210 batch loss 0.495889634 epoch total loss 0.588248789\n",
      "Trained batch 211 batch loss 0.53495872 epoch total loss 0.587996185\n",
      "Trained batch 212 batch loss 0.568007886 epoch total loss 0.58790195\n",
      "Trained batch 213 batch loss 0.551843166 epoch total loss 0.587732613\n",
      "Trained batch 214 batch loss 0.491615 epoch total loss 0.587283492\n",
      "Trained batch 215 batch loss 0.532794178 epoch total loss 0.587030053\n",
      "Trained batch 216 batch loss 0.620325446 epoch total loss 0.587184191\n",
      "Trained batch 217 batch loss 0.651153 epoch total loss 0.587478936\n",
      "Trained batch 218 batch loss 0.609828234 epoch total loss 0.587581456\n",
      "Trained batch 219 batch loss 0.593948245 epoch total loss 0.587610543\n",
      "Trained batch 220 batch loss 0.648315191 epoch total loss 0.587886453\n",
      "Trained batch 221 batch loss 0.551573396 epoch total loss 0.587722182\n",
      "Trained batch 222 batch loss 0.490750641 epoch total loss 0.58728534\n",
      "Trained batch 223 batch loss 0.509962678 epoch total loss 0.58693862\n",
      "Trained batch 224 batch loss 0.517290831 epoch total loss 0.586627662\n",
      "Trained batch 225 batch loss 0.458304971 epoch total loss 0.586057305\n",
      "Trained batch 226 batch loss 0.544470251 epoch total loss 0.585873306\n",
      "Trained batch 227 batch loss 0.59416467 epoch total loss 0.585909784\n",
      "Trained batch 228 batch loss 0.61468643 epoch total loss 0.586036\n",
      "Trained batch 229 batch loss 0.620087504 epoch total loss 0.58618468\n",
      "Trained batch 230 batch loss 0.598490238 epoch total loss 0.586238205\n",
      "Trained batch 231 batch loss 0.565853477 epoch total loss 0.58615\n",
      "Trained batch 232 batch loss 0.592176199 epoch total loss 0.586176\n",
      "Trained batch 233 batch loss 0.62112242 epoch total loss 0.586326\n",
      "Trained batch 234 batch loss 0.670714319 epoch total loss 0.586686611\n",
      "Trained batch 235 batch loss 0.673152924 epoch total loss 0.587054551\n",
      "Trained batch 236 batch loss 0.682828367 epoch total loss 0.587460399\n",
      "Trained batch 237 batch loss 0.660355151 epoch total loss 0.587767959\n",
      "Trained batch 238 batch loss 0.545129895 epoch total loss 0.587588847\n",
      "Trained batch 239 batch loss 0.638555586 epoch total loss 0.587802052\n",
      "Trained batch 240 batch loss 0.558400214 epoch total loss 0.587679565\n",
      "Trained batch 241 batch loss 0.591977894 epoch total loss 0.587697387\n",
      "Trained batch 242 batch loss 0.59354341 epoch total loss 0.587721527\n",
      "Trained batch 243 batch loss 0.585922241 epoch total loss 0.587714136\n",
      "Trained batch 244 batch loss 0.661373675 epoch total loss 0.588016033\n",
      "Trained batch 245 batch loss 0.583106935 epoch total loss 0.587995946\n",
      "Trained batch 246 batch loss 0.61250335 epoch total loss 0.588095546\n",
      "Trained batch 247 batch loss 0.438394159 epoch total loss 0.587489486\n",
      "Trained batch 248 batch loss 0.474986553 epoch total loss 0.587035894\n",
      "Trained batch 249 batch loss 0.46118173 epoch total loss 0.586530447\n",
      "Trained batch 250 batch loss 0.411156833 epoch total loss 0.58582896\n",
      "Trained batch 251 batch loss 0.500582457 epoch total loss 0.585489333\n",
      "Trained batch 252 batch loss 0.627075732 epoch total loss 0.585654378\n",
      "Trained batch 253 batch loss 0.629898667 epoch total loss 0.585829258\n",
      "Trained batch 254 batch loss 0.627972662 epoch total loss 0.585995197\n",
      "Trained batch 255 batch loss 0.701686263 epoch total loss 0.586448908\n",
      "Trained batch 256 batch loss 0.718859613 epoch total loss 0.586966097\n",
      "Trained batch 257 batch loss 0.732118249 epoch total loss 0.587530911\n",
      "Trained batch 258 batch loss 0.644419909 epoch total loss 0.587751389\n",
      "Trained batch 259 batch loss 0.687978923 epoch total loss 0.588138342\n",
      "Trained batch 260 batch loss 0.632770658 epoch total loss 0.58831\n",
      "Trained batch 261 batch loss 0.627807796 epoch total loss 0.588461339\n",
      "Trained batch 262 batch loss 0.589979589 epoch total loss 0.588467121\n",
      "Trained batch 263 batch loss 0.556039393 epoch total loss 0.588343859\n",
      "Trained batch 264 batch loss 0.558193326 epoch total loss 0.588229656\n",
      "Trained batch 265 batch loss 0.560552835 epoch total loss 0.588125229\n",
      "Trained batch 266 batch loss 0.558491826 epoch total loss 0.588013768\n",
      "Trained batch 267 batch loss 0.644945741 epoch total loss 0.588227034\n",
      "Trained batch 268 batch loss 0.615354657 epoch total loss 0.588328242\n",
      "Trained batch 269 batch loss 0.606623 epoch total loss 0.588396251\n",
      "Trained batch 270 batch loss 0.651395917 epoch total loss 0.588629603\n",
      "Trained batch 271 batch loss 0.653085768 epoch total loss 0.588867486\n",
      "Trained batch 272 batch loss 0.631286204 epoch total loss 0.589023411\n",
      "Trained batch 273 batch loss 0.686360478 epoch total loss 0.589379966\n",
      "Trained batch 274 batch loss 0.584979951 epoch total loss 0.589363873\n",
      "Trained batch 275 batch loss 0.676921129 epoch total loss 0.589682281\n",
      "Trained batch 276 batch loss 0.681520522 epoch total loss 0.590015054\n",
      "Trained batch 277 batch loss 0.738098323 epoch total loss 0.590549648\n",
      "Trained batch 278 batch loss 0.660174847 epoch total loss 0.590800047\n",
      "Trained batch 279 batch loss 0.673984766 epoch total loss 0.591098189\n",
      "Trained batch 280 batch loss 0.701675951 epoch total loss 0.59149313\n",
      "Trained batch 281 batch loss 0.677491903 epoch total loss 0.59179914\n",
      "Trained batch 282 batch loss 0.584066749 epoch total loss 0.591771722\n",
      "Trained batch 283 batch loss 0.598181665 epoch total loss 0.591794312\n",
      "Trained batch 284 batch loss 0.542316437 epoch total loss 0.591620088\n",
      "Trained batch 285 batch loss 0.58137 epoch total loss 0.591584146\n",
      "Trained batch 286 batch loss 0.669080853 epoch total loss 0.591855109\n",
      "Trained batch 287 batch loss 0.582462072 epoch total loss 0.591822386\n",
      "Trained batch 288 batch loss 0.751682818 epoch total loss 0.592377424\n",
      "Trained batch 289 batch loss 0.568951249 epoch total loss 0.592296422\n",
      "Trained batch 290 batch loss 0.566339612 epoch total loss 0.592206895\n",
      "Trained batch 291 batch loss 0.597085059 epoch total loss 0.592223704\n",
      "Trained batch 292 batch loss 0.557569861 epoch total loss 0.592105031\n",
      "Trained batch 293 batch loss 0.565822124 epoch total loss 0.592015326\n",
      "Trained batch 294 batch loss 0.581984103 epoch total loss 0.591981232\n",
      "Trained batch 295 batch loss 0.540986896 epoch total loss 0.591808379\n",
      "Trained batch 296 batch loss 0.550897479 epoch total loss 0.591670156\n",
      "Trained batch 297 batch loss 0.558610678 epoch total loss 0.591558874\n",
      "Trained batch 298 batch loss 0.531338632 epoch total loss 0.591356754\n",
      "Trained batch 299 batch loss 0.521918833 epoch total loss 0.591124535\n",
      "Trained batch 300 batch loss 0.546623886 epoch total loss 0.590976179\n",
      "Trained batch 301 batch loss 0.580685377 epoch total loss 0.590942\n",
      "Trained batch 302 batch loss 0.626124144 epoch total loss 0.591058552\n",
      "Trained batch 303 batch loss 0.612205088 epoch total loss 0.59112829\n",
      "Trained batch 304 batch loss 0.585461199 epoch total loss 0.591109693\n",
      "Trained batch 305 batch loss 0.561094761 epoch total loss 0.591011286\n",
      "Trained batch 306 batch loss 0.523044229 epoch total loss 0.590789139\n",
      "Trained batch 307 batch loss 0.503125787 epoch total loss 0.590503633\n",
      "Trained batch 308 batch loss 0.500686407 epoch total loss 0.590212\n",
      "Trained batch 309 batch loss 0.5422104 epoch total loss 0.590056658\n",
      "Trained batch 310 batch loss 0.556450546 epoch total loss 0.589948237\n",
      "Trained batch 311 batch loss 0.592097282 epoch total loss 0.589955151\n",
      "Trained batch 312 batch loss 0.679319203 epoch total loss 0.590241611\n",
      "Trained batch 313 batch loss 0.610879898 epoch total loss 0.590307534\n",
      "Trained batch 314 batch loss 0.614596665 epoch total loss 0.590384901\n",
      "Trained batch 315 batch loss 0.602988243 epoch total loss 0.590424895\n",
      "Trained batch 316 batch loss 0.71265322 epoch total loss 0.59081167\n",
      "Trained batch 317 batch loss 0.613046169 epoch total loss 0.590881824\n",
      "Trained batch 318 batch loss 0.624971569 epoch total loss 0.590989\n",
      "Trained batch 319 batch loss 0.58806479 epoch total loss 0.590979815\n",
      "Trained batch 320 batch loss 0.572014272 epoch total loss 0.590920568\n",
      "Trained batch 321 batch loss 0.514497 epoch total loss 0.590682507\n",
      "Trained batch 322 batch loss 0.489808619 epoch total loss 0.590369225\n",
      "Trained batch 323 batch loss 0.615282297 epoch total loss 0.590446353\n",
      "Trained batch 324 batch loss 0.590815663 epoch total loss 0.590447485\n",
      "Trained batch 325 batch loss 0.517266631 epoch total loss 0.590222359\n",
      "Trained batch 326 batch loss 0.559985578 epoch total loss 0.590129614\n",
      "Trained batch 327 batch loss 0.592142701 epoch total loss 0.590135753\n",
      "Trained batch 328 batch loss 0.634276152 epoch total loss 0.59027034\n",
      "Trained batch 329 batch loss 0.670461357 epoch total loss 0.590514064\n",
      "Trained batch 330 batch loss 0.637849867 epoch total loss 0.590657532\n",
      "Trained batch 331 batch loss 0.662548363 epoch total loss 0.590874732\n",
      "Trained batch 332 batch loss 0.63445282 epoch total loss 0.591006\n",
      "Trained batch 333 batch loss 0.628266 epoch total loss 0.591117859\n",
      "Trained batch 334 batch loss 0.626935244 epoch total loss 0.591225147\n",
      "Trained batch 335 batch loss 0.559712827 epoch total loss 0.591131032\n",
      "Trained batch 336 batch loss 0.571411252 epoch total loss 0.591072381\n",
      "Trained batch 337 batch loss 0.596372247 epoch total loss 0.591088116\n",
      "Trained batch 338 batch loss 0.591180921 epoch total loss 0.591088355\n",
      "Trained batch 339 batch loss 0.605364561 epoch total loss 0.591130495\n",
      "Trained batch 340 batch loss 0.555264115 epoch total loss 0.591025\n",
      "Trained batch 341 batch loss 0.594888389 epoch total loss 0.591036379\n",
      "Trained batch 342 batch loss 0.664596856 epoch total loss 0.591251433\n",
      "Trained batch 343 batch loss 0.583968401 epoch total loss 0.591230214\n",
      "Trained batch 344 batch loss 0.598144352 epoch total loss 0.5912503\n",
      "Trained batch 345 batch loss 0.624184549 epoch total loss 0.591345787\n",
      "Trained batch 346 batch loss 0.659886658 epoch total loss 0.591543853\n",
      "Trained batch 347 batch loss 0.655856907 epoch total loss 0.591729224\n",
      "Trained batch 348 batch loss 0.546926677 epoch total loss 0.591600418\n",
      "Trained batch 349 batch loss 0.603375435 epoch total loss 0.591634214\n",
      "Trained batch 350 batch loss 0.62075603 epoch total loss 0.591717422\n",
      "Trained batch 351 batch loss 0.65146929 epoch total loss 0.591887653\n",
      "Trained batch 352 batch loss 0.69354713 epoch total loss 0.592176437\n",
      "Trained batch 353 batch loss 0.610992432 epoch total loss 0.592229724\n",
      "Trained batch 354 batch loss 0.576118886 epoch total loss 0.592184246\n",
      "Trained batch 355 batch loss 0.628043532 epoch total loss 0.592285216\n",
      "Trained batch 356 batch loss 0.712120771 epoch total loss 0.592621863\n",
      "Trained batch 357 batch loss 0.687504888 epoch total loss 0.59288764\n",
      "Trained batch 358 batch loss 0.710104942 epoch total loss 0.593215048\n",
      "Trained batch 359 batch loss 0.692261 epoch total loss 0.593490958\n",
      "Trained batch 360 batch loss 0.822664618 epoch total loss 0.594127536\n",
      "Trained batch 361 batch loss 0.722615242 epoch total loss 0.594483435\n",
      "Trained batch 362 batch loss 0.618085802 epoch total loss 0.594548643\n",
      "Trained batch 363 batch loss 0.633519948 epoch total loss 0.594656\n",
      "Trained batch 364 batch loss 0.50437206 epoch total loss 0.594408\n",
      "Trained batch 365 batch loss 0.472841591 epoch total loss 0.594074905\n",
      "Trained batch 366 batch loss 0.567449749 epoch total loss 0.594002128\n",
      "Trained batch 367 batch loss 0.642014146 epoch total loss 0.59413296\n",
      "Trained batch 368 batch loss 0.651701331 epoch total loss 0.594289422\n",
      "Trained batch 369 batch loss 0.540977538 epoch total loss 0.59414494\n",
      "Trained batch 370 batch loss 0.550426543 epoch total loss 0.594026804\n",
      "Trained batch 371 batch loss 0.486793458 epoch total loss 0.593737721\n",
      "Trained batch 372 batch loss 0.533930838 epoch total loss 0.593576968\n",
      "Trained batch 373 batch loss 0.522592783 epoch total loss 0.59338671\n",
      "Trained batch 374 batch loss 0.527169287 epoch total loss 0.593209684\n",
      "Trained batch 375 batch loss 0.588756 epoch total loss 0.593197823\n",
      "Trained batch 376 batch loss 0.509928346 epoch total loss 0.592976332\n",
      "Trained batch 377 batch loss 0.535240173 epoch total loss 0.592823207\n",
      "Trained batch 378 batch loss 0.529099882 epoch total loss 0.592654645\n",
      "Trained batch 379 batch loss 0.463101715 epoch total loss 0.592312813\n",
      "Trained batch 380 batch loss 0.480222642 epoch total loss 0.592017829\n",
      "Trained batch 381 batch loss 0.483920515 epoch total loss 0.591734111\n",
      "Trained batch 382 batch loss 0.527888238 epoch total loss 0.591567\n",
      "Trained batch 383 batch loss 0.597380042 epoch total loss 0.591582179\n",
      "Trained batch 384 batch loss 0.597155213 epoch total loss 0.591596663\n",
      "Trained batch 385 batch loss 0.609742641 epoch total loss 0.59164381\n",
      "Trained batch 386 batch loss 0.538132071 epoch total loss 0.59150517\n",
      "Trained batch 387 batch loss 0.433195561 epoch total loss 0.591096103\n",
      "Trained batch 388 batch loss 0.572161257 epoch total loss 0.591047287\n",
      "Trained batch 389 batch loss 0.539135933 epoch total loss 0.590913892\n",
      "Trained batch 390 batch loss 0.56767863 epoch total loss 0.590854287\n",
      "Trained batch 391 batch loss 0.545866609 epoch total loss 0.590739191\n",
      "Trained batch 392 batch loss 0.593588769 epoch total loss 0.590746462\n",
      "Trained batch 393 batch loss 0.570897043 epoch total loss 0.590696\n",
      "Trained batch 394 batch loss 0.641778469 epoch total loss 0.590825617\n",
      "Trained batch 395 batch loss 0.607764781 epoch total loss 0.590868473\n",
      "Trained batch 396 batch loss 0.557328522 epoch total loss 0.590783775\n",
      "Trained batch 397 batch loss 0.737567306 epoch total loss 0.591153502\n",
      "Trained batch 398 batch loss 0.677146852 epoch total loss 0.591369569\n",
      "Trained batch 399 batch loss 0.673891664 epoch total loss 0.591576397\n",
      "Trained batch 400 batch loss 0.587221086 epoch total loss 0.59156549\n",
      "Trained batch 401 batch loss 0.676765919 epoch total loss 0.591778\n",
      "Trained batch 402 batch loss 0.565584779 epoch total loss 0.591712773\n",
      "Trained batch 403 batch loss 0.597957075 epoch total loss 0.59172833\n",
      "Trained batch 404 batch loss 0.525192142 epoch total loss 0.591563582\n",
      "Trained batch 405 batch loss 0.555982232 epoch total loss 0.591475785\n",
      "Trained batch 406 batch loss 0.597784758 epoch total loss 0.591491282\n",
      "Trained batch 407 batch loss 0.480308294 epoch total loss 0.591218114\n",
      "Trained batch 408 batch loss 0.546887636 epoch total loss 0.591109455\n",
      "Trained batch 409 batch loss 0.620132625 epoch total loss 0.591180384\n",
      "Trained batch 410 batch loss 0.60652411 epoch total loss 0.591217816\n",
      "Trained batch 411 batch loss 0.619474 epoch total loss 0.5912866\n",
      "Trained batch 412 batch loss 0.636222839 epoch total loss 0.591395617\n",
      "Trained batch 413 batch loss 0.674286604 epoch total loss 0.591596305\n",
      "Trained batch 414 batch loss 0.549788058 epoch total loss 0.591495335\n",
      "Trained batch 415 batch loss 0.55877918 epoch total loss 0.591416538\n",
      "Trained batch 416 batch loss 0.568429232 epoch total loss 0.591361284\n",
      "Trained batch 417 batch loss 0.565800786 epoch total loss 0.591299951\n",
      "Trained batch 418 batch loss 0.549963951 epoch total loss 0.591201067\n",
      "Trained batch 419 batch loss 0.613096058 epoch total loss 0.59125334\n",
      "Trained batch 420 batch loss 0.60021174 epoch total loss 0.591274619\n",
      "Trained batch 421 batch loss 0.735628128 epoch total loss 0.591617525\n",
      "Trained batch 422 batch loss 0.706248641 epoch total loss 0.591889143\n",
      "Trained batch 423 batch loss 0.575084686 epoch total loss 0.591849446\n",
      "Trained batch 424 batch loss 0.618750095 epoch total loss 0.591912866\n",
      "Trained batch 425 batch loss 0.575519 epoch total loss 0.591874301\n",
      "Trained batch 426 batch loss 0.588166475 epoch total loss 0.591865599\n",
      "Trained batch 427 batch loss 0.532339 epoch total loss 0.591726184\n",
      "Trained batch 428 batch loss 0.454111338 epoch total loss 0.591404617\n",
      "Trained batch 429 batch loss 0.459540606 epoch total loss 0.591097236\n",
      "Trained batch 430 batch loss 0.400416285 epoch total loss 0.590653837\n",
      "Trained batch 431 batch loss 0.383927464 epoch total loss 0.590174139\n",
      "Trained batch 432 batch loss 0.400307626 epoch total loss 0.589734674\n",
      "Trained batch 433 batch loss 0.488216221 epoch total loss 0.589500248\n",
      "Trained batch 434 batch loss 0.47417897 epoch total loss 0.589234531\n",
      "Trained batch 435 batch loss 0.684163392 epoch total loss 0.589452803\n",
      "Trained batch 436 batch loss 0.594240606 epoch total loss 0.58946377\n",
      "Trained batch 437 batch loss 0.598574221 epoch total loss 0.589484572\n",
      "Trained batch 438 batch loss 0.59249866 epoch total loss 0.589491487\n",
      "Trained batch 439 batch loss 0.633345068 epoch total loss 0.589591324\n",
      "Trained batch 440 batch loss 0.631805658 epoch total loss 0.589687288\n",
      "Trained batch 441 batch loss 0.613777161 epoch total loss 0.589741886\n",
      "Trained batch 442 batch loss 0.652129173 epoch total loss 0.589883\n",
      "Trained batch 443 batch loss 0.552624166 epoch total loss 0.589798927\n",
      "Trained batch 444 batch loss 0.539131 epoch total loss 0.589684784\n",
      "Trained batch 445 batch loss 0.547802925 epoch total loss 0.589590609\n",
      "Trained batch 446 batch loss 0.578564584 epoch total loss 0.589565873\n",
      "Trained batch 447 batch loss 0.588703156 epoch total loss 0.589563966\n",
      "Trained batch 448 batch loss 0.603372633 epoch total loss 0.589594781\n",
      "Trained batch 449 batch loss 0.586521268 epoch total loss 0.589587927\n",
      "Trained batch 450 batch loss 0.586536646 epoch total loss 0.589581192\n",
      "Trained batch 451 batch loss 0.516885757 epoch total loss 0.589419961\n",
      "Trained batch 452 batch loss 0.508461237 epoch total loss 0.589240849\n",
      "Trained batch 453 batch loss 0.463543117 epoch total loss 0.58896333\n",
      "Trained batch 454 batch loss 0.464940846 epoch total loss 0.588690102\n",
      "Trained batch 455 batch loss 0.486137152 epoch total loss 0.588464737\n",
      "Trained batch 456 batch loss 0.605293036 epoch total loss 0.588501632\n",
      "Trained batch 457 batch loss 0.583247781 epoch total loss 0.588490188\n",
      "Trained batch 458 batch loss 0.584917307 epoch total loss 0.58848238\n",
      "Trained batch 459 batch loss 0.618911266 epoch total loss 0.58854866\n",
      "Trained batch 460 batch loss 0.521489918 epoch total loss 0.588402867\n",
      "Trained batch 461 batch loss 0.524314582 epoch total loss 0.588263869\n",
      "Trained batch 462 batch loss 0.509940565 epoch total loss 0.588094354\n",
      "Trained batch 463 batch loss 0.573818922 epoch total loss 0.588063538\n",
      "Trained batch 464 batch loss 0.593630433 epoch total loss 0.588075519\n",
      "Trained batch 465 batch loss 0.560570121 epoch total loss 0.588016391\n",
      "Trained batch 466 batch loss 0.487761587 epoch total loss 0.587801218\n",
      "Trained batch 467 batch loss 0.5383026 epoch total loss 0.587695241\n",
      "Trained batch 468 batch loss 0.642964423 epoch total loss 0.587813377\n",
      "Trained batch 469 batch loss 0.761768281 epoch total loss 0.588184297\n",
      "Trained batch 470 batch loss 0.583907664 epoch total loss 0.588175178\n",
      "Trained batch 471 batch loss 0.670409501 epoch total loss 0.58834976\n",
      "Trained batch 472 batch loss 0.707645535 epoch total loss 0.588602483\n",
      "Trained batch 473 batch loss 0.62967962 epoch total loss 0.588689327\n",
      "Trained batch 474 batch loss 0.614394426 epoch total loss 0.588743508\n",
      "Trained batch 475 batch loss 0.621467412 epoch total loss 0.588812411\n",
      "Trained batch 476 batch loss 0.58465296 epoch total loss 0.588803649\n",
      "Trained batch 477 batch loss 0.647539198 epoch total loss 0.588926792\n",
      "Trained batch 478 batch loss 0.597204 epoch total loss 0.588944137\n",
      "Trained batch 479 batch loss 0.631040096 epoch total loss 0.589032\n",
      "Trained batch 480 batch loss 0.631149054 epoch total loss 0.589119732\n",
      "Trained batch 481 batch loss 0.687152803 epoch total loss 0.58932358\n",
      "Trained batch 482 batch loss 0.596907377 epoch total loss 0.589339256\n",
      "Trained batch 483 batch loss 0.58476913 epoch total loss 0.589329839\n",
      "Trained batch 484 batch loss 0.572021484 epoch total loss 0.589294076\n",
      "Trained batch 485 batch loss 0.686873 epoch total loss 0.589495242\n",
      "Trained batch 486 batch loss 0.618699491 epoch total loss 0.589555323\n",
      "Trained batch 487 batch loss 0.673300266 epoch total loss 0.589727342\n",
      "Trained batch 488 batch loss 0.620840311 epoch total loss 0.589791119\n",
      "Trained batch 489 batch loss 0.670533478 epoch total loss 0.589956224\n",
      "Trained batch 490 batch loss 0.568050563 epoch total loss 0.58991152\n",
      "Trained batch 491 batch loss 0.556574345 epoch total loss 0.589843631\n",
      "Trained batch 492 batch loss 0.584950626 epoch total loss 0.589833677\n",
      "Trained batch 493 batch loss 0.573354781 epoch total loss 0.589800298\n",
      "Trained batch 494 batch loss 0.521918237 epoch total loss 0.58966285\n",
      "Trained batch 495 batch loss 0.627218187 epoch total loss 0.589738786\n",
      "Trained batch 496 batch loss 0.559113383 epoch total loss 0.589677036\n",
      "Trained batch 497 batch loss 0.611331224 epoch total loss 0.589720547\n",
      "Trained batch 498 batch loss 0.586999297 epoch total loss 0.589715123\n",
      "Trained batch 499 batch loss 0.600552917 epoch total loss 0.589736819\n",
      "Trained batch 500 batch loss 0.557041645 epoch total loss 0.589671433\n",
      "Trained batch 501 batch loss 0.559671283 epoch total loss 0.58961153\n",
      "Trained batch 502 batch loss 0.640126407 epoch total loss 0.589712203\n",
      "Trained batch 503 batch loss 0.687262535 epoch total loss 0.589906096\n",
      "Trained batch 504 batch loss 0.69102478 epoch total loss 0.590106785\n",
      "Trained batch 505 batch loss 0.661855876 epoch total loss 0.590248883\n",
      "Trained batch 506 batch loss 0.790489 epoch total loss 0.590644598\n",
      "Trained batch 507 batch loss 0.657329857 epoch total loss 0.590776145\n",
      "Trained batch 508 batch loss 0.668815494 epoch total loss 0.590929747\n",
      "Trained batch 509 batch loss 0.673755646 epoch total loss 0.591092527\n",
      "Trained batch 510 batch loss 0.729880273 epoch total loss 0.591364682\n",
      "Trained batch 511 batch loss 0.598215163 epoch total loss 0.591378033\n",
      "Trained batch 512 batch loss 0.638439953 epoch total loss 0.591469944\n",
      "Trained batch 513 batch loss 0.551490784 epoch total loss 0.591392\n",
      "Trained batch 514 batch loss 0.505308628 epoch total loss 0.591224551\n",
      "Trained batch 515 batch loss 0.539929271 epoch total loss 0.591124892\n",
      "Trained batch 516 batch loss 0.480054 epoch total loss 0.5909096\n",
      "Trained batch 517 batch loss 0.460602641 epoch total loss 0.590657592\n",
      "Trained batch 518 batch loss 0.45267415 epoch total loss 0.590391159\n",
      "Trained batch 519 batch loss 0.480207086 epoch total loss 0.590178847\n",
      "Trained batch 520 batch loss 0.536524117 epoch total loss 0.590075672\n",
      "Trained batch 521 batch loss 0.483681083 epoch total loss 0.589871466\n",
      "Trained batch 522 batch loss 0.54293 epoch total loss 0.589781523\n",
      "Trained batch 523 batch loss 0.563451052 epoch total loss 0.589731216\n",
      "Trained batch 524 batch loss 0.622287095 epoch total loss 0.589793324\n",
      "Trained batch 525 batch loss 0.576083899 epoch total loss 0.589767218\n",
      "Trained batch 526 batch loss 0.559075534 epoch total loss 0.589708865\n",
      "Trained batch 527 batch loss 0.659612 epoch total loss 0.589841485\n",
      "Trained batch 528 batch loss 0.59959352 epoch total loss 0.589859962\n",
      "Trained batch 529 batch loss 0.554025412 epoch total loss 0.589792192\n",
      "Trained batch 530 batch loss 0.492567569 epoch total loss 0.589608729\n",
      "Trained batch 531 batch loss 0.642507136 epoch total loss 0.589708328\n",
      "Trained batch 532 batch loss 0.584679961 epoch total loss 0.589698911\n",
      "Trained batch 533 batch loss 0.566632926 epoch total loss 0.589655578\n",
      "Trained batch 534 batch loss 0.584314466 epoch total loss 0.589645624\n",
      "Trained batch 535 batch loss 0.530636668 epoch total loss 0.589535296\n",
      "Trained batch 536 batch loss 0.622928262 epoch total loss 0.589597642\n",
      "Trained batch 537 batch loss 0.520909667 epoch total loss 0.589469671\n",
      "Trained batch 538 batch loss 0.59884876 epoch total loss 0.589487135\n",
      "Trained batch 539 batch loss 0.601286411 epoch total loss 0.589509\n",
      "Trained batch 540 batch loss 0.637863338 epoch total loss 0.589598596\n",
      "Trained batch 541 batch loss 0.567754567 epoch total loss 0.589558184\n",
      "Trained batch 542 batch loss 0.621626258 epoch total loss 0.589617372\n",
      "Trained batch 543 batch loss 0.661106706 epoch total loss 0.589749\n",
      "Trained batch 544 batch loss 0.512383 epoch total loss 0.589606762\n",
      "Trained batch 545 batch loss 0.512422383 epoch total loss 0.589465141\n",
      "Trained batch 546 batch loss 0.692865431 epoch total loss 0.589654565\n",
      "Trained batch 547 batch loss 0.656418264 epoch total loss 0.589776635\n",
      "Trained batch 548 batch loss 0.563542962 epoch total loss 0.589728773\n",
      "Trained batch 549 batch loss 0.567814827 epoch total loss 0.589688838\n",
      "Trained batch 550 batch loss 0.662007749 epoch total loss 0.589820325\n",
      "Trained batch 551 batch loss 0.587754548 epoch total loss 0.58981663\n",
      "Trained batch 552 batch loss 0.544917941 epoch total loss 0.58973527\n",
      "Trained batch 553 batch loss 0.754641116 epoch total loss 0.590033472\n",
      "Trained batch 554 batch loss 0.642065644 epoch total loss 0.590127409\n",
      "Trained batch 555 batch loss 0.608919382 epoch total loss 0.590161264\n",
      "Trained batch 556 batch loss 0.56849283 epoch total loss 0.590122223\n",
      "Trained batch 557 batch loss 0.626159489 epoch total loss 0.590186954\n",
      "Trained batch 558 batch loss 0.631789744 epoch total loss 0.590261459\n",
      "Trained batch 559 batch loss 0.610260308 epoch total loss 0.590297282\n",
      "Trained batch 560 batch loss 0.564113259 epoch total loss 0.590250492\n",
      "Trained batch 561 batch loss 0.518402815 epoch total loss 0.590122461\n",
      "Trained batch 562 batch loss 0.58244586 epoch total loss 0.590108812\n",
      "Trained batch 563 batch loss 0.615486324 epoch total loss 0.590153873\n",
      "Trained batch 564 batch loss 0.591175318 epoch total loss 0.590155661\n",
      "Trained batch 565 batch loss 0.611099243 epoch total loss 0.590192795\n",
      "Trained batch 566 batch loss 0.673790336 epoch total loss 0.590340495\n",
      "Trained batch 567 batch loss 0.629901826 epoch total loss 0.590410292\n",
      "Trained batch 568 batch loss 0.515930355 epoch total loss 0.590279162\n",
      "Trained batch 569 batch loss 0.616823137 epoch total loss 0.590325832\n",
      "Trained batch 570 batch loss 0.570392311 epoch total loss 0.590290844\n",
      "Trained batch 571 batch loss 0.584603846 epoch total loss 0.59028089\n",
      "Trained batch 572 batch loss 0.638713121 epoch total loss 0.590365529\n",
      "Trained batch 573 batch loss 0.726921082 epoch total loss 0.590603888\n",
      "Trained batch 574 batch loss 0.488509536 epoch total loss 0.590425968\n",
      "Trained batch 575 batch loss 0.486825407 epoch total loss 0.590245783\n",
      "Trained batch 576 batch loss 0.529506385 epoch total loss 0.590140343\n",
      "Trained batch 577 batch loss 0.671368182 epoch total loss 0.590281069\n",
      "Trained batch 578 batch loss 0.725665689 epoch total loss 0.590515375\n",
      "Trained batch 579 batch loss 0.660601258 epoch total loss 0.590636432\n",
      "Trained batch 580 batch loss 0.706056356 epoch total loss 0.590835392\n",
      "Trained batch 581 batch loss 0.681574941 epoch total loss 0.590991616\n",
      "Trained batch 582 batch loss 0.52347815 epoch total loss 0.590875566\n",
      "Trained batch 583 batch loss 0.531091928 epoch total loss 0.590773046\n",
      "Trained batch 584 batch loss 0.45192802 epoch total loss 0.590535283\n",
      "Trained batch 585 batch loss 0.491216302 epoch total loss 0.590365529\n",
      "Trained batch 586 batch loss 0.531603217 epoch total loss 0.590265274\n",
      "Trained batch 587 batch loss 0.514641762 epoch total loss 0.590136468\n",
      "Trained batch 588 batch loss 0.53681314 epoch total loss 0.59004575\n",
      "Trained batch 589 batch loss 0.534319 epoch total loss 0.589951158\n",
      "Trained batch 590 batch loss 0.486501545 epoch total loss 0.58977586\n",
      "Trained batch 591 batch loss 0.466503412 epoch total loss 0.589567244\n",
      "Trained batch 592 batch loss 0.522748709 epoch total loss 0.589454353\n",
      "Trained batch 593 batch loss 0.528196216 epoch total loss 0.589351058\n",
      "Trained batch 594 batch loss 0.516657948 epoch total loss 0.58922869\n",
      "Trained batch 595 batch loss 0.525469065 epoch total loss 0.589121521\n",
      "Trained batch 596 batch loss 0.533711433 epoch total loss 0.589028597\n",
      "Trained batch 597 batch loss 0.584040701 epoch total loss 0.589020252\n",
      "Trained batch 598 batch loss 0.573692739 epoch total loss 0.588994622\n",
      "Trained batch 599 batch loss 0.540552 epoch total loss 0.588913739\n",
      "Trained batch 600 batch loss 0.582405031 epoch total loss 0.588902891\n",
      "Trained batch 601 batch loss 0.540616274 epoch total loss 0.588822544\n",
      "Trained batch 602 batch loss 0.646439552 epoch total loss 0.588918269\n",
      "Trained batch 603 batch loss 0.642234087 epoch total loss 0.589006722\n",
      "Trained batch 604 batch loss 0.655572176 epoch total loss 0.589116931\n",
      "Trained batch 605 batch loss 0.626688898 epoch total loss 0.589179039\n",
      "Trained batch 606 batch loss 0.609435916 epoch total loss 0.589212477\n",
      "Trained batch 607 batch loss 0.627821386 epoch total loss 0.589276\n",
      "Trained batch 608 batch loss 0.739624441 epoch total loss 0.589523315\n",
      "Trained batch 609 batch loss 0.832271099 epoch total loss 0.589921951\n",
      "Trained batch 610 batch loss 0.731028199 epoch total loss 0.590153217\n",
      "Trained batch 611 batch loss 0.741350412 epoch total loss 0.590400696\n",
      "Trained batch 612 batch loss 0.660160244 epoch total loss 0.59051466\n",
      "Trained batch 613 batch loss 0.620217741 epoch total loss 0.590563118\n",
      "Trained batch 614 batch loss 0.502354622 epoch total loss 0.590419471\n",
      "Trained batch 615 batch loss 0.667498946 epoch total loss 0.59054482\n",
      "Trained batch 616 batch loss 0.574920058 epoch total loss 0.590519428\n",
      "Trained batch 617 batch loss 0.505788445 epoch total loss 0.590382159\n",
      "Trained batch 618 batch loss 0.558526337 epoch total loss 0.590330601\n",
      "Trained batch 619 batch loss 0.638900459 epoch total loss 0.59040904\n",
      "Trained batch 620 batch loss 0.665107548 epoch total loss 0.590529501\n",
      "Trained batch 621 batch loss 0.574472606 epoch total loss 0.590503633\n",
      "Trained batch 622 batch loss 0.607009768 epoch total loss 0.590530157\n",
      "Trained batch 623 batch loss 0.616627336 epoch total loss 0.590572059\n",
      "Trained batch 624 batch loss 0.592130423 epoch total loss 0.590574563\n",
      "Trained batch 625 batch loss 0.695519388 epoch total loss 0.590742469\n",
      "Trained batch 626 batch loss 0.662017763 epoch total loss 0.590856314\n",
      "Trained batch 627 batch loss 0.65699476 epoch total loss 0.590961814\n",
      "Trained batch 628 batch loss 0.672070444 epoch total loss 0.591090918\n",
      "Trained batch 629 batch loss 0.617160857 epoch total loss 0.591132402\n",
      "Trained batch 630 batch loss 0.695125282 epoch total loss 0.591297448\n",
      "Trained batch 631 batch loss 0.748725653 epoch total loss 0.591546953\n",
      "Trained batch 632 batch loss 0.628329456 epoch total loss 0.591605127\n",
      "Trained batch 633 batch loss 0.622887552 epoch total loss 0.591654539\n",
      "Trained batch 634 batch loss 0.647041798 epoch total loss 0.59174192\n",
      "Trained batch 635 batch loss 0.589238405 epoch total loss 0.591737926\n",
      "Trained batch 636 batch loss 0.583085895 epoch total loss 0.591724396\n",
      "Trained batch 637 batch loss 0.592373371 epoch total loss 0.591725409\n",
      "Trained batch 638 batch loss 0.591100633 epoch total loss 0.591724396\n",
      "Trained batch 639 batch loss 0.649407744 epoch total loss 0.591814697\n",
      "Trained batch 640 batch loss 0.614479363 epoch total loss 0.591850102\n",
      "Trained batch 641 batch loss 0.677979231 epoch total loss 0.591984451\n",
      "Trained batch 642 batch loss 0.578793466 epoch total loss 0.591963887\n",
      "Trained batch 643 batch loss 0.599581897 epoch total loss 0.591975749\n",
      "Trained batch 644 batch loss 0.577054679 epoch total loss 0.591952562\n",
      "Trained batch 645 batch loss 0.546029866 epoch total loss 0.591881394\n",
      "Trained batch 646 batch loss 0.603200257 epoch total loss 0.591898918\n",
      "Trained batch 647 batch loss 0.555390537 epoch total loss 0.591842473\n",
      "Trained batch 648 batch loss 0.576741338 epoch total loss 0.591819227\n",
      "Trained batch 649 batch loss 0.575206041 epoch total loss 0.591793597\n",
      "Trained batch 650 batch loss 0.591968775 epoch total loss 0.591793895\n",
      "Trained batch 651 batch loss 0.565028727 epoch total loss 0.591752768\n",
      "Trained batch 652 batch loss 0.541777611 epoch total loss 0.591676116\n",
      "Trained batch 653 batch loss 0.522222102 epoch total loss 0.591569722\n",
      "Trained batch 654 batch loss 0.46702078 epoch total loss 0.591379285\n",
      "Trained batch 655 batch loss 0.536576927 epoch total loss 0.59129566\n",
      "Trained batch 656 batch loss 0.543532193 epoch total loss 0.591222823\n",
      "Trained batch 657 batch loss 0.553527117 epoch total loss 0.591165423\n",
      "Trained batch 658 batch loss 0.592014134 epoch total loss 0.591166735\n",
      "Trained batch 659 batch loss 0.610307157 epoch total loss 0.591195762\n",
      "Trained batch 660 batch loss 0.646378 epoch total loss 0.591279387\n",
      "Trained batch 661 batch loss 0.627737463 epoch total loss 0.591334581\n",
      "Trained batch 662 batch loss 0.604700148 epoch total loss 0.591354787\n",
      "Trained batch 663 batch loss 0.613735139 epoch total loss 0.591388524\n",
      "Trained batch 664 batch loss 0.684357822 epoch total loss 0.591528535\n",
      "Trained batch 665 batch loss 0.645813704 epoch total loss 0.591610193\n",
      "Trained batch 666 batch loss 0.588801324 epoch total loss 0.591605961\n",
      "Trained batch 667 batch loss 0.693015575 epoch total loss 0.591758\n",
      "Trained batch 668 batch loss 0.644933283 epoch total loss 0.591837645\n",
      "Trained batch 669 batch loss 0.493021399 epoch total loss 0.591689885\n",
      "Trained batch 670 batch loss 0.636874318 epoch total loss 0.591757357\n",
      "Trained batch 671 batch loss 0.69793 epoch total loss 0.591915607\n",
      "Trained batch 672 batch loss 0.562097 epoch total loss 0.591871202\n",
      "Trained batch 673 batch loss 0.573094487 epoch total loss 0.591843307\n",
      "Trained batch 674 batch loss 0.569180667 epoch total loss 0.59180969\n",
      "Trained batch 675 batch loss 0.587927043 epoch total loss 0.591803908\n",
      "Trained batch 676 batch loss 0.575899 epoch total loss 0.591780424\n",
      "Trained batch 677 batch loss 0.66014576 epoch total loss 0.591881394\n",
      "Trained batch 678 batch loss 0.652974546 epoch total loss 0.591971517\n",
      "Trained batch 679 batch loss 0.679406822 epoch total loss 0.592100322\n",
      "Trained batch 680 batch loss 0.647916257 epoch total loss 0.592182398\n",
      "Trained batch 681 batch loss 0.652694345 epoch total loss 0.592271209\n",
      "Trained batch 682 batch loss 0.607596576 epoch total loss 0.59229368\n",
      "Trained batch 683 batch loss 0.624613166 epoch total loss 0.592341\n",
      "Trained batch 684 batch loss 0.588852644 epoch total loss 0.592335939\n",
      "Trained batch 685 batch loss 0.64375627 epoch total loss 0.592411041\n",
      "Trained batch 686 batch loss 0.671420515 epoch total loss 0.592526197\n",
      "Trained batch 687 batch loss 0.666868627 epoch total loss 0.59263438\n",
      "Trained batch 688 batch loss 0.64705354 epoch total loss 0.592713535\n",
      "Trained batch 689 batch loss 0.760166168 epoch total loss 0.592956543\n",
      "Trained batch 690 batch loss 0.626589 epoch total loss 0.5930053\n",
      "Trained batch 691 batch loss 0.640443206 epoch total loss 0.593073905\n",
      "Trained batch 692 batch loss 0.513175368 epoch total loss 0.59295851\n",
      "Trained batch 693 batch loss 0.605374813 epoch total loss 0.592976391\n",
      "Trained batch 694 batch loss 0.561182737 epoch total loss 0.592930615\n",
      "Trained batch 695 batch loss 0.704779 epoch total loss 0.593091547\n",
      "Trained batch 696 batch loss 0.635964155 epoch total loss 0.593153119\n",
      "Trained batch 697 batch loss 0.61403209 epoch total loss 0.5931831\n",
      "Trained batch 698 batch loss 0.607871711 epoch total loss 0.593204141\n",
      "Trained batch 699 batch loss 0.668983281 epoch total loss 0.593312562\n",
      "Trained batch 700 batch loss 0.736243725 epoch total loss 0.593516707\n",
      "Trained batch 701 batch loss 0.619675756 epoch total loss 0.59355408\n",
      "Trained batch 702 batch loss 0.654266834 epoch total loss 0.593640566\n",
      "Trained batch 703 batch loss 0.644922495 epoch total loss 0.593713462\n",
      "Trained batch 704 batch loss 0.574688852 epoch total loss 0.593686461\n",
      "Trained batch 705 batch loss 0.707460165 epoch total loss 0.593847811\n",
      "Trained batch 706 batch loss 0.621612132 epoch total loss 0.59388715\n",
      "Trained batch 707 batch loss 0.576802611 epoch total loss 0.593863\n",
      "Trained batch 708 batch loss 0.614063323 epoch total loss 0.593891561\n",
      "Trained batch 709 batch loss 0.560966551 epoch total loss 0.593845129\n",
      "Trained batch 710 batch loss 0.574948251 epoch total loss 0.593818486\n",
      "Trained batch 711 batch loss 0.581702232 epoch total loss 0.593801439\n",
      "Trained batch 712 batch loss 0.537693441 epoch total loss 0.593722641\n",
      "Trained batch 713 batch loss 0.676770747 epoch total loss 0.593839109\n",
      "Trained batch 714 batch loss 0.690518498 epoch total loss 0.593974531\n",
      "Trained batch 715 batch loss 0.736443877 epoch total loss 0.594173789\n",
      "Trained batch 716 batch loss 0.700678825 epoch total loss 0.594322562\n",
      "Trained batch 717 batch loss 0.57108444 epoch total loss 0.594290137\n",
      "Trained batch 718 batch loss 0.603341579 epoch total loss 0.594302714\n",
      "Trained batch 719 batch loss 0.587921381 epoch total loss 0.594293833\n",
      "Trained batch 720 batch loss 0.596254408 epoch total loss 0.594296575\n",
      "Trained batch 721 batch loss 0.585891843 epoch total loss 0.594284952\n",
      "Trained batch 722 batch loss 0.612479627 epoch total loss 0.594310164\n",
      "Trained batch 723 batch loss 0.685628533 epoch total loss 0.594436467\n",
      "Trained batch 724 batch loss 0.5897 epoch total loss 0.59442991\n",
      "Trained batch 725 batch loss 0.547908366 epoch total loss 0.594365716\n",
      "Trained batch 726 batch loss 0.632735431 epoch total loss 0.594418585\n",
      "Trained batch 727 batch loss 0.659107864 epoch total loss 0.594507575\n",
      "Trained batch 728 batch loss 0.735610902 epoch total loss 0.59470135\n",
      "Trained batch 729 batch loss 0.671853065 epoch total loss 0.594807208\n",
      "Trained batch 730 batch loss 0.667502344 epoch total loss 0.594906807\n",
      "Trained batch 731 batch loss 0.732595742 epoch total loss 0.595095158\n",
      "Trained batch 732 batch loss 0.619232297 epoch total loss 0.595128119\n",
      "Trained batch 733 batch loss 0.605388045 epoch total loss 0.595142126\n",
      "Trained batch 734 batch loss 0.532017827 epoch total loss 0.595056117\n",
      "Trained batch 735 batch loss 0.54131031 epoch total loss 0.594983\n",
      "Trained batch 736 batch loss 0.556505859 epoch total loss 0.594930708\n",
      "Trained batch 737 batch loss 0.524692595 epoch total loss 0.594835401\n",
      "Trained batch 738 batch loss 0.622615635 epoch total loss 0.594873071\n",
      "Trained batch 739 batch loss 0.59953022 epoch total loss 0.594879329\n",
      "Trained batch 740 batch loss 0.621243477 epoch total loss 0.594915\n",
      "Trained batch 741 batch loss 0.583451211 epoch total loss 0.594899535\n",
      "Trained batch 742 batch loss 0.651796401 epoch total loss 0.594976187\n",
      "Trained batch 743 batch loss 0.619503 epoch total loss 0.595009208\n",
      "Trained batch 744 batch loss 0.569235921 epoch total loss 0.594974577\n",
      "Trained batch 745 batch loss 0.503832936 epoch total loss 0.594852269\n",
      "Trained batch 746 batch loss 0.479965925 epoch total loss 0.59469831\n",
      "Trained batch 747 batch loss 0.5916152 epoch total loss 0.594694138\n",
      "Trained batch 748 batch loss 0.575401187 epoch total loss 0.594668388\n",
      "Trained batch 749 batch loss 0.606086612 epoch total loss 0.594683647\n",
      "Trained batch 750 batch loss 0.584581912 epoch total loss 0.594670177\n",
      "Trained batch 751 batch loss 0.569438279 epoch total loss 0.594636559\n",
      "Trained batch 752 batch loss 0.659882247 epoch total loss 0.594723344\n",
      "Trained batch 753 batch loss 0.70638907 epoch total loss 0.59487164\n",
      "Trained batch 754 batch loss 0.544680238 epoch total loss 0.594805062\n",
      "Trained batch 755 batch loss 0.589140475 epoch total loss 0.594797552\n",
      "Trained batch 756 batch loss 0.644073 epoch total loss 0.5948627\n",
      "Trained batch 757 batch loss 0.6055 epoch total loss 0.594876766\n",
      "Trained batch 758 batch loss 0.550269 epoch total loss 0.594817936\n",
      "Trained batch 759 batch loss 0.528169751 epoch total loss 0.594730079\n",
      "Trained batch 760 batch loss 0.628273726 epoch total loss 0.594774246\n",
      "Trained batch 761 batch loss 0.694873512 epoch total loss 0.594905794\n",
      "Trained batch 762 batch loss 0.650850177 epoch total loss 0.594979167\n",
      "Trained batch 763 batch loss 0.614750206 epoch total loss 0.595005095\n",
      "Trained batch 764 batch loss 0.674257159 epoch total loss 0.595108807\n",
      "Trained batch 765 batch loss 0.572483599 epoch total loss 0.595079243\n",
      "Trained batch 766 batch loss 0.635588646 epoch total loss 0.595132113\n",
      "Trained batch 767 batch loss 0.630168736 epoch total loss 0.595177829\n",
      "Trained batch 768 batch loss 0.575443506 epoch total loss 0.59515208\n",
      "Trained batch 769 batch loss 0.631509125 epoch total loss 0.595199347\n",
      "Trained batch 770 batch loss 0.557685375 epoch total loss 0.59515065\n",
      "Trained batch 771 batch loss 0.616936326 epoch total loss 0.595178902\n",
      "Trained batch 772 batch loss 0.601941168 epoch total loss 0.595187664\n",
      "Trained batch 773 batch loss 0.493738413 epoch total loss 0.595056415\n",
      "Trained batch 774 batch loss 0.565285861 epoch total loss 0.59501791\n",
      "Trained batch 775 batch loss 0.582710505 epoch total loss 0.595002055\n",
      "Trained batch 776 batch loss 0.656395733 epoch total loss 0.595081151\n",
      "Trained batch 777 batch loss 0.625290811 epoch total loss 0.595120072\n",
      "Trained batch 778 batch loss 0.619933963 epoch total loss 0.595151961\n",
      "Trained batch 779 batch loss 0.575816512 epoch total loss 0.595127106\n",
      "Trained batch 780 batch loss 0.648729 epoch total loss 0.595195889\n",
      "Trained batch 781 batch loss 0.573856354 epoch total loss 0.595168531\n",
      "Trained batch 782 batch loss 0.593419969 epoch total loss 0.595166326\n",
      "Trained batch 783 batch loss 0.539882243 epoch total loss 0.595095694\n",
      "Trained batch 784 batch loss 0.583064258 epoch total loss 0.595080376\n",
      "Trained batch 785 batch loss 0.528478146 epoch total loss 0.594995499\n",
      "Trained batch 786 batch loss 0.650176048 epoch total loss 0.595065713\n",
      "Trained batch 787 batch loss 0.59610641 epoch total loss 0.595067\n",
      "Trained batch 788 batch loss 0.486545265 epoch total loss 0.594929278\n",
      "Trained batch 789 batch loss 0.523331583 epoch total loss 0.59483856\n",
      "Trained batch 790 batch loss 0.584589481 epoch total loss 0.594825625\n",
      "Trained batch 791 batch loss 0.521876216 epoch total loss 0.594733417\n",
      "Trained batch 792 batch loss 0.594629765 epoch total loss 0.594733238\n",
      "Trained batch 793 batch loss 0.68207258 epoch total loss 0.594843388\n",
      "Trained batch 794 batch loss 0.541525424 epoch total loss 0.594776273\n",
      "Trained batch 795 batch loss 0.53777051 epoch total loss 0.594704568\n",
      "Trained batch 796 batch loss 0.434225619 epoch total loss 0.594503\n",
      "Trained batch 797 batch loss 0.578982234 epoch total loss 0.594483495\n",
      "Trained batch 798 batch loss 0.585366428 epoch total loss 0.594472051\n",
      "Trained batch 799 batch loss 0.575802088 epoch total loss 0.594448686\n",
      "Trained batch 800 batch loss 0.52029866 epoch total loss 0.594356\n",
      "Trained batch 801 batch loss 0.56037879 epoch total loss 0.594313562\n",
      "Trained batch 802 batch loss 0.589307427 epoch total loss 0.594307303\n",
      "Trained batch 803 batch loss 0.544935107 epoch total loss 0.594245791\n",
      "Trained batch 804 batch loss 0.542533934 epoch total loss 0.594181478\n",
      "Trained batch 805 batch loss 0.512886882 epoch total loss 0.594080508\n",
      "Trained batch 806 batch loss 0.50203979 epoch total loss 0.593966305\n",
      "Trained batch 807 batch loss 0.579803407 epoch total loss 0.593948781\n",
      "Trained batch 808 batch loss 0.575474322 epoch total loss 0.593925893\n",
      "Trained batch 809 batch loss 0.576261878 epoch total loss 0.593904078\n",
      "Trained batch 810 batch loss 0.642105222 epoch total loss 0.593963563\n",
      "Trained batch 811 batch loss 0.584524751 epoch total loss 0.593951941\n",
      "Trained batch 812 batch loss 0.577617466 epoch total loss 0.593931854\n",
      "Trained batch 813 batch loss 0.550661445 epoch total loss 0.593878627\n",
      "Trained batch 814 batch loss 0.573026 epoch total loss 0.593853\n",
      "Trained batch 815 batch loss 0.614081621 epoch total loss 0.593877792\n",
      "Trained batch 816 batch loss 0.652179122 epoch total loss 0.593949258\n",
      "Trained batch 817 batch loss 0.65474 epoch total loss 0.594023705\n",
      "Trained batch 818 batch loss 0.632617354 epoch total loss 0.594070911\n",
      "Trained batch 819 batch loss 0.63860631 epoch total loss 0.594125271\n",
      "Trained batch 820 batch loss 0.64418596 epoch total loss 0.594186306\n",
      "Trained batch 821 batch loss 0.614686728 epoch total loss 0.59421128\n",
      "Trained batch 822 batch loss 0.627641141 epoch total loss 0.594252\n",
      "Trained batch 823 batch loss 0.598201811 epoch total loss 0.594256759\n",
      "Trained batch 824 batch loss 0.651332259 epoch total loss 0.594326079\n",
      "Trained batch 825 batch loss 0.573626 epoch total loss 0.594301\n",
      "Trained batch 826 batch loss 0.495989442 epoch total loss 0.594181955\n",
      "Trained batch 827 batch loss 0.443967342 epoch total loss 0.59400034\n",
      "Trained batch 828 batch loss 0.60029465 epoch total loss 0.594007909\n",
      "Trained batch 829 batch loss 0.541541338 epoch total loss 0.593944609\n",
      "Trained batch 830 batch loss 0.63174361 epoch total loss 0.593990147\n",
      "Trained batch 831 batch loss 0.480260462 epoch total loss 0.593853295\n",
      "Trained batch 832 batch loss 0.523366332 epoch total loss 0.593768597\n",
      "Trained batch 833 batch loss 0.664867043 epoch total loss 0.593853951\n",
      "Trained batch 834 batch loss 0.61540556 epoch total loss 0.593879819\n",
      "Trained batch 835 batch loss 0.614635468 epoch total loss 0.593904614\n",
      "Trained batch 836 batch loss 0.63601321 epoch total loss 0.593955\n",
      "Trained batch 837 batch loss 0.6389184 epoch total loss 0.594008744\n",
      "Trained batch 838 batch loss 0.585495353 epoch total loss 0.593998611\n",
      "Trained batch 839 batch loss 0.577762604 epoch total loss 0.593979239\n",
      "Trained batch 840 batch loss 0.582174361 epoch total loss 0.593965173\n",
      "Trained batch 841 batch loss 0.656058192 epoch total loss 0.594039\n",
      "Trained batch 842 batch loss 0.675760865 epoch total loss 0.594136059\n",
      "Trained batch 843 batch loss 0.620551348 epoch total loss 0.594167411\n",
      "Trained batch 844 batch loss 0.604345739 epoch total loss 0.594179451\n",
      "Trained batch 845 batch loss 0.630334 epoch total loss 0.594222248\n",
      "Trained batch 846 batch loss 0.684848368 epoch total loss 0.594329357\n",
      "Trained batch 847 batch loss 0.642644763 epoch total loss 0.594386399\n",
      "Trained batch 848 batch loss 0.581536174 epoch total loss 0.594371259\n",
      "Trained batch 849 batch loss 0.569702 epoch total loss 0.594342172\n",
      "Trained batch 850 batch loss 0.552667797 epoch total loss 0.594293177\n",
      "Trained batch 851 batch loss 0.566549063 epoch total loss 0.594260573\n",
      "Trained batch 852 batch loss 0.531270444 epoch total loss 0.594186664\n",
      "Trained batch 853 batch loss 0.548758507 epoch total loss 0.594133437\n",
      "Trained batch 854 batch loss 0.642859459 epoch total loss 0.594190478\n",
      "Trained batch 855 batch loss 0.634016752 epoch total loss 0.594237\n",
      "Trained batch 856 batch loss 0.586954355 epoch total loss 0.594228506\n",
      "Trained batch 857 batch loss 0.660550237 epoch total loss 0.594305933\n",
      "Trained batch 858 batch loss 0.673069298 epoch total loss 0.594397724\n",
      "Trained batch 859 batch loss 0.650477707 epoch total loss 0.594463\n",
      "Trained batch 860 batch loss 0.708368778 epoch total loss 0.594595432\n",
      "Trained batch 861 batch loss 0.65609324 epoch total loss 0.594666898\n",
      "Trained batch 862 batch loss 0.653643429 epoch total loss 0.594735265\n",
      "Trained batch 863 batch loss 0.603820443 epoch total loss 0.594745815\n",
      "Trained batch 864 batch loss 0.549849451 epoch total loss 0.59469384\n",
      "Trained batch 865 batch loss 0.50211221 epoch total loss 0.594586849\n",
      "Trained batch 866 batch loss 0.609815 epoch total loss 0.594604433\n",
      "Trained batch 867 batch loss 0.656603634 epoch total loss 0.594675958\n",
      "Trained batch 868 batch loss 0.543586671 epoch total loss 0.594617069\n",
      "Trained batch 869 batch loss 0.47053811 epoch total loss 0.594474256\n",
      "Trained batch 870 batch loss 0.453679353 epoch total loss 0.594312429\n",
      "Trained batch 871 batch loss 0.501952171 epoch total loss 0.594206393\n",
      "Trained batch 872 batch loss 0.549415648 epoch total loss 0.594155073\n",
      "Trained batch 873 batch loss 0.574910522 epoch total loss 0.594133\n",
      "Trained batch 874 batch loss 0.568651736 epoch total loss 0.594103873\n",
      "Trained batch 875 batch loss 0.535859704 epoch total loss 0.594037294\n",
      "Trained batch 876 batch loss 0.620095491 epoch total loss 0.594067097\n",
      "Trained batch 877 batch loss 0.564948142 epoch total loss 0.594033897\n",
      "Trained batch 878 batch loss 0.586471736 epoch total loss 0.594025254\n",
      "Trained batch 879 batch loss 0.534735203 epoch total loss 0.593957841\n",
      "Trained batch 880 batch loss 0.547405064 epoch total loss 0.593905\n",
      "Trained batch 881 batch loss 0.55458951 epoch total loss 0.593860269\n",
      "Trained batch 882 batch loss 0.518556356 epoch total loss 0.593774915\n",
      "Trained batch 883 batch loss 0.61234 epoch total loss 0.593795955\n",
      "Trained batch 884 batch loss 0.582403183 epoch total loss 0.593783081\n",
      "Trained batch 885 batch loss 0.622729897 epoch total loss 0.593815804\n",
      "Trained batch 886 batch loss 0.564607382 epoch total loss 0.593782842\n",
      "Trained batch 887 batch loss 0.517700672 epoch total loss 0.593697071\n",
      "Trained batch 888 batch loss 0.746597409 epoch total loss 0.593869269\n",
      "Trained batch 889 batch loss 0.612673104 epoch total loss 0.593890429\n",
      "Trained batch 890 batch loss 0.492115945 epoch total loss 0.593776047\n",
      "Trained batch 891 batch loss 0.570175648 epoch total loss 0.593749583\n",
      "Trained batch 892 batch loss 0.482528985 epoch total loss 0.59362489\n",
      "Trained batch 893 batch loss 0.455456376 epoch total loss 0.593470156\n",
      "Trained batch 894 batch loss 0.584018886 epoch total loss 0.593459666\n",
      "Trained batch 895 batch loss 0.548807323 epoch total loss 0.593409777\n",
      "Trained batch 896 batch loss 0.597712874 epoch total loss 0.593414605\n",
      "Trained batch 897 batch loss 0.585215151 epoch total loss 0.593405426\n",
      "Trained batch 898 batch loss 0.653197885 epoch total loss 0.593472\n",
      "Trained batch 899 batch loss 0.636306047 epoch total loss 0.593519628\n",
      "Trained batch 900 batch loss 0.594359815 epoch total loss 0.593520582\n",
      "Trained batch 901 batch loss 0.684479952 epoch total loss 0.593621552\n",
      "Trained batch 902 batch loss 0.746526361 epoch total loss 0.593791068\n",
      "Trained batch 903 batch loss 0.695310354 epoch total loss 0.593903482\n",
      "Trained batch 904 batch loss 0.640383124 epoch total loss 0.593954921\n",
      "Trained batch 905 batch loss 0.616563678 epoch total loss 0.593979895\n",
      "Trained batch 906 batch loss 0.58019048 epoch total loss 0.593964696\n",
      "Trained batch 907 batch loss 0.565330327 epoch total loss 0.593933105\n",
      "Trained batch 908 batch loss 0.625914931 epoch total loss 0.593968332\n",
      "Trained batch 909 batch loss 0.523284376 epoch total loss 0.593890548\n",
      "Trained batch 910 batch loss 0.56461364 epoch total loss 0.593858361\n",
      "Trained batch 911 batch loss 0.678498507 epoch total loss 0.593951344\n",
      "Trained batch 912 batch loss 0.588717759 epoch total loss 0.593945622\n",
      "Trained batch 913 batch loss 0.577001929 epoch total loss 0.593927085\n",
      "Trained batch 914 batch loss 0.580753505 epoch total loss 0.593912661\n",
      "Trained batch 915 batch loss 0.630959392 epoch total loss 0.593953192\n",
      "Trained batch 916 batch loss 0.625732183 epoch total loss 0.593987882\n",
      "Trained batch 917 batch loss 0.609944224 epoch total loss 0.594005227\n",
      "Trained batch 918 batch loss 0.572968066 epoch total loss 0.593982399\n",
      "Trained batch 919 batch loss 0.609486759 epoch total loss 0.593999267\n",
      "Trained batch 920 batch loss 0.575499654 epoch total loss 0.59397912\n",
      "Trained batch 921 batch loss 0.598833501 epoch total loss 0.593984425\n",
      "Trained batch 922 batch loss 0.607408941 epoch total loss 0.593998969\n",
      "Trained batch 923 batch loss 0.608818173 epoch total loss 0.594015062\n",
      "Trained batch 924 batch loss 0.537915111 epoch total loss 0.593954325\n",
      "Trained batch 925 batch loss 0.5396806 epoch total loss 0.593895614\n",
      "Trained batch 926 batch loss 0.632276416 epoch total loss 0.593937039\n",
      "Trained batch 927 batch loss 0.598015189 epoch total loss 0.59394145\n",
      "Trained batch 928 batch loss 0.670783639 epoch total loss 0.594024241\n",
      "Trained batch 929 batch loss 0.608628213 epoch total loss 0.59404\n",
      "Trained batch 930 batch loss 0.606056809 epoch total loss 0.594053\n",
      "Trained batch 931 batch loss 0.623368859 epoch total loss 0.594084442\n",
      "Trained batch 932 batch loss 0.638342679 epoch total loss 0.594131947\n",
      "Trained batch 933 batch loss 0.63899374 epoch total loss 0.59418\n",
      "Trained batch 934 batch loss 0.594715357 epoch total loss 0.594180584\n",
      "Trained batch 935 batch loss 0.567879 epoch total loss 0.594152451\n",
      "Trained batch 936 batch loss 0.532951534 epoch total loss 0.594087064\n",
      "Trained batch 937 batch loss 0.549861 epoch total loss 0.594039857\n",
      "Trained batch 938 batch loss 0.59596777 epoch total loss 0.594041884\n",
      "Trained batch 939 batch loss 0.620963573 epoch total loss 0.594070554\n",
      "Trained batch 940 batch loss 0.610772192 epoch total loss 0.594088376\n",
      "Trained batch 941 batch loss 0.629325032 epoch total loss 0.594125807\n",
      "Trained batch 942 batch loss 0.64053762 epoch total loss 0.5941751\n",
      "Trained batch 943 batch loss 0.662033 epoch total loss 0.594247103\n",
      "Trained batch 944 batch loss 0.614866078 epoch total loss 0.594268918\n",
      "Trained batch 945 batch loss 0.679168224 epoch total loss 0.594358742\n",
      "Trained batch 946 batch loss 0.683410764 epoch total loss 0.594452858\n",
      "Trained batch 947 batch loss 0.672755361 epoch total loss 0.59453553\n",
      "Trained batch 948 batch loss 0.673938751 epoch total loss 0.594619274\n",
      "Trained batch 949 batch loss 0.657385886 epoch total loss 0.594685435\n",
      "Trained batch 950 batch loss 0.622774482 epoch total loss 0.594715059\n",
      "Trained batch 951 batch loss 0.710397959 epoch total loss 0.594836712\n",
      "Trained batch 952 batch loss 0.590546966 epoch total loss 0.594832242\n",
      "Trained batch 953 batch loss 0.554910481 epoch total loss 0.594790339\n",
      "Trained batch 954 batch loss 0.527593851 epoch total loss 0.594719887\n",
      "Trained batch 955 batch loss 0.609116435 epoch total loss 0.594735\n",
      "Trained batch 956 batch loss 0.646550179 epoch total loss 0.594789207\n",
      "Trained batch 957 batch loss 0.510632873 epoch total loss 0.594701231\n",
      "Trained batch 958 batch loss 0.604164541 epoch total loss 0.594711125\n",
      "Trained batch 959 batch loss 0.62098372 epoch total loss 0.594738543\n",
      "Trained batch 960 batch loss 0.650524676 epoch total loss 0.594796598\n",
      "Trained batch 961 batch loss 0.720906734 epoch total loss 0.594927847\n",
      "Trained batch 962 batch loss 0.603033125 epoch total loss 0.594936252\n",
      "Trained batch 963 batch loss 0.571014285 epoch total loss 0.594911397\n",
      "Trained batch 964 batch loss 0.602046847 epoch total loss 0.594918787\n",
      "Trained batch 965 batch loss 0.557860076 epoch total loss 0.594880402\n",
      "Trained batch 966 batch loss 0.604757786 epoch total loss 0.594890594\n",
      "Trained batch 967 batch loss 0.608803689 epoch total loss 0.594905\n",
      "Trained batch 968 batch loss 0.656915963 epoch total loss 0.594969034\n",
      "Trained batch 969 batch loss 0.626979113 epoch total loss 0.595002055\n",
      "Trained batch 970 batch loss 0.569170415 epoch total loss 0.594975412\n",
      "Trained batch 971 batch loss 0.730431139 epoch total loss 0.595114887\n",
      "Trained batch 972 batch loss 0.6644032 epoch total loss 0.595186234\n",
      "Trained batch 973 batch loss 0.551978528 epoch total loss 0.595141828\n",
      "Trained batch 974 batch loss 0.466552645 epoch total loss 0.595009804\n",
      "Trained batch 975 batch loss 0.537314832 epoch total loss 0.594950616\n",
      "Trained batch 976 batch loss 0.482705563 epoch total loss 0.594835639\n",
      "Trained batch 977 batch loss 0.568561316 epoch total loss 0.594808698\n",
      "Trained batch 978 batch loss 0.548899889 epoch total loss 0.594761789\n",
      "Trained batch 979 batch loss 0.664109409 epoch total loss 0.594832599\n",
      "Trained batch 980 batch loss 0.546977758 epoch total loss 0.594783783\n",
      "Trained batch 981 batch loss 0.561814785 epoch total loss 0.594750226\n",
      "Trained batch 982 batch loss 0.59764564 epoch total loss 0.594753146\n",
      "Trained batch 983 batch loss 0.617552638 epoch total loss 0.594776332\n",
      "Trained batch 984 batch loss 0.599441409 epoch total loss 0.594781101\n",
      "Trained batch 985 batch loss 0.56735456 epoch total loss 0.594753265\n",
      "Trained batch 986 batch loss 0.643700838 epoch total loss 0.594802916\n",
      "Trained batch 987 batch loss 0.646224737 epoch total loss 0.594855\n",
      "Trained batch 988 batch loss 0.611206889 epoch total loss 0.594871581\n",
      "Trained batch 989 batch loss 0.669455111 epoch total loss 0.594946921\n",
      "Trained batch 990 batch loss 0.689848542 epoch total loss 0.595042765\n",
      "Trained batch 991 batch loss 0.650503814 epoch total loss 0.595098734\n",
      "Trained batch 992 batch loss 0.577815235 epoch total loss 0.595081329\n",
      "Trained batch 993 batch loss 0.647799 epoch total loss 0.595134437\n",
      "Trained batch 994 batch loss 0.722250462 epoch total loss 0.595262289\n",
      "Trained batch 995 batch loss 0.720361352 epoch total loss 0.595388\n",
      "Trained batch 996 batch loss 0.690627217 epoch total loss 0.595483601\n",
      "Trained batch 997 batch loss 0.739790678 epoch total loss 0.595628381\n",
      "Trained batch 998 batch loss 0.631334066 epoch total loss 0.595664144\n",
      "Trained batch 999 batch loss 0.589348733 epoch total loss 0.595657825\n",
      "Trained batch 1000 batch loss 0.573821902 epoch total loss 0.595636\n",
      "Trained batch 1001 batch loss 0.55553174 epoch total loss 0.595595956\n",
      "Trained batch 1002 batch loss 0.5498873 epoch total loss 0.595550299\n",
      "Trained batch 1003 batch loss 0.53913635 epoch total loss 0.595494032\n",
      "Trained batch 1004 batch loss 0.558329225 epoch total loss 0.595457\n",
      "Trained batch 1005 batch loss 0.571232319 epoch total loss 0.595432937\n",
      "Trained batch 1006 batch loss 0.572657347 epoch total loss 0.595410287\n",
      "Trained batch 1007 batch loss 0.568148732 epoch total loss 0.595383227\n",
      "Trained batch 1008 batch loss 0.583748519 epoch total loss 0.595371664\n",
      "Trained batch 1009 batch loss 0.619860351 epoch total loss 0.595396\n",
      "Trained batch 1010 batch loss 0.608923554 epoch total loss 0.595409393\n",
      "Trained batch 1011 batch loss 0.607655883 epoch total loss 0.595421493\n",
      "Trained batch 1012 batch loss 0.705607 epoch total loss 0.595530391\n",
      "Trained batch 1013 batch loss 0.634500861 epoch total loss 0.595568895\n",
      "Trained batch 1014 batch loss 0.649726808 epoch total loss 0.595622301\n",
      "Trained batch 1015 batch loss 0.674011648 epoch total loss 0.595699489\n",
      "Trained batch 1016 batch loss 0.639548182 epoch total loss 0.595742643\n",
      "Trained batch 1017 batch loss 0.563693404 epoch total loss 0.595711172\n",
      "Trained batch 1018 batch loss 0.596059382 epoch total loss 0.595711529\n",
      "Trained batch 1019 batch loss 0.571763098 epoch total loss 0.595688045\n",
      "Trained batch 1020 batch loss 0.582184255 epoch total loss 0.595674813\n",
      "Trained batch 1021 batch loss 0.622580528 epoch total loss 0.595701158\n",
      "Trained batch 1022 batch loss 0.461594671 epoch total loss 0.595569968\n",
      "Trained batch 1023 batch loss 0.502577484 epoch total loss 0.595479\n",
      "Trained batch 1024 batch loss 0.544158101 epoch total loss 0.595428884\n",
      "Trained batch 1025 batch loss 0.613402784 epoch total loss 0.595446408\n",
      "Trained batch 1026 batch loss 0.568932772 epoch total loss 0.595420539\n",
      "Trained batch 1027 batch loss 0.561882615 epoch total loss 0.595387876\n",
      "Trained batch 1028 batch loss 0.67516005 epoch total loss 0.595465541\n",
      "Trained batch 1029 batch loss 0.695971072 epoch total loss 0.595563173\n",
      "Trained batch 1030 batch loss 0.676472843 epoch total loss 0.595641732\n",
      "Trained batch 1031 batch loss 0.584308088 epoch total loss 0.595630705\n",
      "Trained batch 1032 batch loss 0.635570645 epoch total loss 0.595669389\n",
      "Trained batch 1033 batch loss 0.567140937 epoch total loss 0.595641792\n",
      "Trained batch 1034 batch loss 0.546176136 epoch total loss 0.595594\n",
      "Trained batch 1035 batch loss 0.605353355 epoch total loss 0.595603406\n",
      "Trained batch 1036 batch loss 0.58131671 epoch total loss 0.595589578\n",
      "Trained batch 1037 batch loss 0.633056581 epoch total loss 0.595625699\n",
      "Trained batch 1038 batch loss 0.595184803 epoch total loss 0.595625341\n",
      "Trained batch 1039 batch loss 0.529705882 epoch total loss 0.595561922\n",
      "Trained batch 1040 batch loss 0.538028955 epoch total loss 0.595506608\n",
      "Trained batch 1041 batch loss 0.495013654 epoch total loss 0.595410049\n",
      "Trained batch 1042 batch loss 0.569662094 epoch total loss 0.595385313\n",
      "Trained batch 1043 batch loss 0.663518727 epoch total loss 0.59545064\n",
      "Trained batch 1044 batch loss 0.59282577 epoch total loss 0.595448136\n",
      "Trained batch 1045 batch loss 0.581409097 epoch total loss 0.595434666\n",
      "Trained batch 1046 batch loss 0.531725287 epoch total loss 0.595373809\n",
      "Trained batch 1047 batch loss 0.480159968 epoch total loss 0.595263779\n",
      "Trained batch 1048 batch loss 0.482519507 epoch total loss 0.595156193\n",
      "Trained batch 1049 batch loss 0.516274571 epoch total loss 0.595081031\n",
      "Trained batch 1050 batch loss 0.453074247 epoch total loss 0.594945788\n",
      "Trained batch 1051 batch loss 0.482760936 epoch total loss 0.594839036\n",
      "Trained batch 1052 batch loss 0.533761382 epoch total loss 0.594781\n",
      "Trained batch 1053 batch loss 0.673580825 epoch total loss 0.594855845\n",
      "Trained batch 1054 batch loss 0.592971683 epoch total loss 0.594854\n",
      "Trained batch 1055 batch loss 0.65833354 epoch total loss 0.594914198\n",
      "Trained batch 1056 batch loss 0.736765742 epoch total loss 0.595048487\n",
      "Trained batch 1057 batch loss 0.683785617 epoch total loss 0.59513247\n",
      "Trained batch 1058 batch loss 0.686344862 epoch total loss 0.595218658\n",
      "Trained batch 1059 batch loss 0.695246935 epoch total loss 0.595313132\n",
      "Trained batch 1060 batch loss 0.511287868 epoch total loss 0.595233858\n",
      "Trained batch 1061 batch loss 0.551339805 epoch total loss 0.595192492\n",
      "Trained batch 1062 batch loss 0.535775661 epoch total loss 0.595136523\n",
      "Trained batch 1063 batch loss 0.498016655 epoch total loss 0.595045149\n",
      "Trained batch 1064 batch loss 0.543340385 epoch total loss 0.594996572\n",
      "Trained batch 1065 batch loss 0.640159667 epoch total loss 0.59503895\n",
      "Trained batch 1066 batch loss 0.674728274 epoch total loss 0.595113754\n",
      "Trained batch 1067 batch loss 0.585181355 epoch total loss 0.595104456\n",
      "Trained batch 1068 batch loss 0.569494 epoch total loss 0.595080495\n",
      "Trained batch 1069 batch loss 0.691274405 epoch total loss 0.595170498\n",
      "Trained batch 1070 batch loss 0.646429181 epoch total loss 0.59521836\n",
      "Trained batch 1071 batch loss 0.570500553 epoch total loss 0.595195293\n",
      "Trained batch 1072 batch loss 0.649431288 epoch total loss 0.595245898\n",
      "Trained batch 1073 batch loss 0.647228956 epoch total loss 0.595294297\n",
      "Trained batch 1074 batch loss 0.719218731 epoch total loss 0.595409691\n",
      "Trained batch 1075 batch loss 0.732532203 epoch total loss 0.595537305\n",
      "Trained batch 1076 batch loss 0.69180119 epoch total loss 0.595626712\n",
      "Trained batch 1077 batch loss 0.51234138 epoch total loss 0.595549405\n",
      "Trained batch 1078 batch loss 0.632559657 epoch total loss 0.595583737\n",
      "Trained batch 1079 batch loss 0.620718 epoch total loss 0.595607042\n",
      "Trained batch 1080 batch loss 0.641682 epoch total loss 0.59564966\n",
      "Trained batch 1081 batch loss 0.545942605 epoch total loss 0.595603704\n",
      "Trained batch 1082 batch loss 0.567153335 epoch total loss 0.595577359\n",
      "Trained batch 1083 batch loss 0.54480952 epoch total loss 0.59553051\n",
      "Trained batch 1084 batch loss 0.509664476 epoch total loss 0.595451295\n",
      "Trained batch 1085 batch loss 0.716719329 epoch total loss 0.595563054\n",
      "Trained batch 1086 batch loss 0.569775283 epoch total loss 0.595539272\n",
      "Trained batch 1087 batch loss 0.640428 epoch total loss 0.595580578\n",
      "Trained batch 1088 batch loss 0.640754342 epoch total loss 0.595622122\n",
      "Trained batch 1089 batch loss 0.533800244 epoch total loss 0.595565379\n",
      "Trained batch 1090 batch loss 0.612276196 epoch total loss 0.595580697\n",
      "Trained batch 1091 batch loss 0.646765471 epoch total loss 0.595627666\n",
      "Trained batch 1092 batch loss 0.654881477 epoch total loss 0.595681965\n",
      "Trained batch 1093 batch loss 0.640197039 epoch total loss 0.595722675\n",
      "Trained batch 1094 batch loss 0.598403513 epoch total loss 0.595725119\n",
      "Trained batch 1095 batch loss 0.689125061 epoch total loss 0.595810413\n",
      "Trained batch 1096 batch loss 0.621317 epoch total loss 0.595833719\n",
      "Trained batch 1097 batch loss 0.605270803 epoch total loss 0.595842361\n",
      "Trained batch 1098 batch loss 0.600457788 epoch total loss 0.595846534\n",
      "Trained batch 1099 batch loss 0.608963549 epoch total loss 0.595858455\n",
      "Trained batch 1100 batch loss 0.635778308 epoch total loss 0.595894754\n",
      "Trained batch 1101 batch loss 0.603160441 epoch total loss 0.59590137\n",
      "Trained batch 1102 batch loss 0.564348161 epoch total loss 0.5958727\n",
      "Trained batch 1103 batch loss 0.563617647 epoch total loss 0.595843434\n",
      "Trained batch 1104 batch loss 0.594755054 epoch total loss 0.595842421\n",
      "Trained batch 1105 batch loss 0.555689 epoch total loss 0.595806062\n",
      "Trained batch 1106 batch loss 0.572741508 epoch total loss 0.59578526\n",
      "Trained batch 1107 batch loss 0.614903867 epoch total loss 0.595802546\n",
      "Trained batch 1108 batch loss 0.560068309 epoch total loss 0.595770299\n",
      "Trained batch 1109 batch loss 0.589956045 epoch total loss 0.595765054\n",
      "Trained batch 1110 batch loss 0.588624597 epoch total loss 0.595758617\n",
      "Trained batch 1111 batch loss 0.558692694 epoch total loss 0.595725238\n",
      "Trained batch 1112 batch loss 0.50475651 epoch total loss 0.595643461\n",
      "Trained batch 1113 batch loss 0.568196774 epoch total loss 0.595618784\n",
      "Trained batch 1114 batch loss 0.551595807 epoch total loss 0.595579267\n",
      "Trained batch 1115 batch loss 0.514812708 epoch total loss 0.595506847\n",
      "Trained batch 1116 batch loss 0.485766709 epoch total loss 0.595408499\n",
      "Trained batch 1117 batch loss 0.513376474 epoch total loss 0.595335066\n",
      "Trained batch 1118 batch loss 0.561795 epoch total loss 0.595305\n",
      "Trained batch 1119 batch loss 0.626663327 epoch total loss 0.59533304\n",
      "Trained batch 1120 batch loss 0.599874496 epoch total loss 0.595337093\n",
      "Trained batch 1121 batch loss 0.585781157 epoch total loss 0.59532851\n",
      "Trained batch 1122 batch loss 0.616088629 epoch total loss 0.595347047\n",
      "Trained batch 1123 batch loss 0.563749671 epoch total loss 0.595318854\n",
      "Trained batch 1124 batch loss 0.572580397 epoch total loss 0.595298648\n",
      "Trained batch 1125 batch loss 0.627403259 epoch total loss 0.595327139\n",
      "Trained batch 1126 batch loss 0.624040127 epoch total loss 0.59535265\n",
      "Trained batch 1127 batch loss 0.625324965 epoch total loss 0.595379233\n",
      "Trained batch 1128 batch loss 0.713011265 epoch total loss 0.595483482\n",
      "Trained batch 1129 batch loss 0.597287893 epoch total loss 0.595485091\n",
      "Trained batch 1130 batch loss 0.684721 epoch total loss 0.595564067\n",
      "Trained batch 1131 batch loss 0.562517583 epoch total loss 0.595534801\n",
      "Trained batch 1132 batch loss 0.605732739 epoch total loss 0.595543802\n",
      "Trained batch 1133 batch loss 0.549299955 epoch total loss 0.595503\n",
      "Trained batch 1134 batch loss 0.56201148 epoch total loss 0.595473468\n",
      "Trained batch 1135 batch loss 0.430278957 epoch total loss 0.595327914\n",
      "Trained batch 1136 batch loss 0.47983557 epoch total loss 0.595226288\n",
      "Trained batch 1137 batch loss 0.516560316 epoch total loss 0.595157087\n",
      "Trained batch 1138 batch loss 0.534901321 epoch total loss 0.595104158\n",
      "Trained batch 1139 batch loss 0.524075747 epoch total loss 0.595041752\n",
      "Trained batch 1140 batch loss 0.774405 epoch total loss 0.595199108\n",
      "Trained batch 1141 batch loss 0.748123884 epoch total loss 0.595333099\n",
      "Trained batch 1142 batch loss 0.598379374 epoch total loss 0.595335782\n",
      "Trained batch 1143 batch loss 0.697725654 epoch total loss 0.595425367\n",
      "Trained batch 1144 batch loss 0.605644584 epoch total loss 0.595434308\n",
      "Trained batch 1145 batch loss 0.572182357 epoch total loss 0.595414042\n",
      "Trained batch 1146 batch loss 0.513936341 epoch total loss 0.595342934\n",
      "Trained batch 1147 batch loss 0.584348559 epoch total loss 0.595333338\n",
      "Trained batch 1148 batch loss 0.576394498 epoch total loss 0.595316887\n",
      "Trained batch 1149 batch loss 0.606324196 epoch total loss 0.595326424\n",
      "Trained batch 1150 batch loss 0.626414955 epoch total loss 0.595353484\n",
      "Trained batch 1151 batch loss 0.555910528 epoch total loss 0.595319211\n",
      "Trained batch 1152 batch loss 0.548565507 epoch total loss 0.595278621\n",
      "Trained batch 1153 batch loss 0.545684695 epoch total loss 0.595235586\n",
      "Trained batch 1154 batch loss 0.557683289 epoch total loss 0.595203042\n",
      "Trained batch 1155 batch loss 0.57562089 epoch total loss 0.595186114\n",
      "Trained batch 1156 batch loss 0.619643271 epoch total loss 0.595207214\n",
      "Trained batch 1157 batch loss 0.592403114 epoch total loss 0.59520483\n",
      "Trained batch 1158 batch loss 0.615627348 epoch total loss 0.595222414\n",
      "Trained batch 1159 batch loss 0.62358737 epoch total loss 0.595246911\n",
      "Trained batch 1160 batch loss 0.545600057 epoch total loss 0.595204115\n",
      "Trained batch 1161 batch loss 0.624305665 epoch total loss 0.595229208\n",
      "Trained batch 1162 batch loss 0.587907195 epoch total loss 0.59522289\n",
      "Trained batch 1163 batch loss 0.68787086 epoch total loss 0.595302522\n",
      "Trained batch 1164 batch loss 0.594449401 epoch total loss 0.595301807\n",
      "Trained batch 1165 batch loss 0.559364855 epoch total loss 0.595270932\n",
      "Trained batch 1166 batch loss 0.516073108 epoch total loss 0.595203042\n",
      "Trained batch 1167 batch loss 0.507765174 epoch total loss 0.595128059\n",
      "Trained batch 1168 batch loss 0.587380767 epoch total loss 0.595121443\n",
      "Trained batch 1169 batch loss 0.550766766 epoch total loss 0.595083535\n",
      "Trained batch 1170 batch loss 0.573154569 epoch total loss 0.595064819\n",
      "Trained batch 1171 batch loss 0.56048286 epoch total loss 0.595035255\n",
      "Trained batch 1172 batch loss 0.613589644 epoch total loss 0.59505111\n",
      "Trained batch 1173 batch loss 0.574922442 epoch total loss 0.595034\n",
      "Trained batch 1174 batch loss 0.696116805 epoch total loss 0.595120072\n",
      "Trained batch 1175 batch loss 0.754593253 epoch total loss 0.595255792\n",
      "Trained batch 1176 batch loss 0.603975832 epoch total loss 0.595263243\n",
      "Trained batch 1177 batch loss 0.716031194 epoch total loss 0.595365822\n",
      "Trained batch 1178 batch loss 0.652837276 epoch total loss 0.595414579\n",
      "Trained batch 1179 batch loss 0.691073835 epoch total loss 0.59549576\n",
      "Trained batch 1180 batch loss 0.666383386 epoch total loss 0.595555842\n",
      "Trained batch 1181 batch loss 0.722037077 epoch total loss 0.595662892\n",
      "Trained batch 1182 batch loss 0.616226137 epoch total loss 0.595680296\n",
      "Trained batch 1183 batch loss 0.618653476 epoch total loss 0.595699728\n",
      "Trained batch 1184 batch loss 0.629019439 epoch total loss 0.595727861\n",
      "Trained batch 1185 batch loss 0.611259699 epoch total loss 0.595741\n",
      "Trained batch 1186 batch loss 0.598127663 epoch total loss 0.595743\n",
      "Trained batch 1187 batch loss 0.547326326 epoch total loss 0.595702171\n",
      "Trained batch 1188 batch loss 0.66961062 epoch total loss 0.595764399\n",
      "Trained batch 1189 batch loss 0.581055701 epoch total loss 0.59575206\n",
      "Trained batch 1190 batch loss 0.670307398 epoch total loss 0.595814705\n",
      "Trained batch 1191 batch loss 0.626172066 epoch total loss 0.595840156\n",
      "Trained batch 1192 batch loss 0.567707896 epoch total loss 0.595816553\n",
      "Trained batch 1193 batch loss 0.61486274 epoch total loss 0.595832527\n",
      "Trained batch 1194 batch loss 0.642919064 epoch total loss 0.595872\n",
      "Trained batch 1195 batch loss 0.613354623 epoch total loss 0.595886588\n",
      "Trained batch 1196 batch loss 0.588541865 epoch total loss 0.595880449\n",
      "Trained batch 1197 batch loss 0.567686081 epoch total loss 0.595856905\n",
      "Trained batch 1198 batch loss 0.570726931 epoch total loss 0.595835924\n",
      "Trained batch 1199 batch loss 0.586108267 epoch total loss 0.595827818\n",
      "Trained batch 1200 batch loss 0.547031045 epoch total loss 0.595787227\n",
      "Trained batch 1201 batch loss 0.589485705 epoch total loss 0.595781922\n",
      "Trained batch 1202 batch loss 0.527466238 epoch total loss 0.595725119\n",
      "Trained batch 1203 batch loss 0.54303652 epoch total loss 0.59568131\n",
      "Trained batch 1204 batch loss 0.479132622 epoch total loss 0.595584512\n",
      "Trained batch 1205 batch loss 0.51160109 epoch total loss 0.595514774\n",
      "Trained batch 1206 batch loss 0.541388631 epoch total loss 0.595469892\n",
      "Trained batch 1207 batch loss 0.598361313 epoch total loss 0.595472336\n",
      "Trained batch 1208 batch loss 0.631453693 epoch total loss 0.595502138\n",
      "Trained batch 1209 batch loss 0.659796596 epoch total loss 0.595555305\n",
      "Trained batch 1210 batch loss 0.629707336 epoch total loss 0.595583498\n",
      "Trained batch 1211 batch loss 0.652727842 epoch total loss 0.595630705\n",
      "Trained batch 1212 batch loss 0.695122898 epoch total loss 0.595712781\n",
      "Trained batch 1213 batch loss 0.709801674 epoch total loss 0.595806837\n",
      "Trained batch 1214 batch loss 0.612874091 epoch total loss 0.595820844\n",
      "Trained batch 1215 batch loss 0.630545676 epoch total loss 0.595849454\n",
      "Trained batch 1216 batch loss 0.557911277 epoch total loss 0.595818281\n",
      "Trained batch 1217 batch loss 0.646841645 epoch total loss 0.595860183\n",
      "Trained batch 1218 batch loss 0.619425416 epoch total loss 0.595879555\n",
      "Trained batch 1219 batch loss 0.564436316 epoch total loss 0.595853806\n",
      "Trained batch 1220 batch loss 0.559936523 epoch total loss 0.595824361\n",
      "Trained batch 1221 batch loss 0.608296037 epoch total loss 0.595834553\n",
      "Trained batch 1222 batch loss 0.529533386 epoch total loss 0.595780313\n",
      "Trained batch 1223 batch loss 0.639770746 epoch total loss 0.595816255\n",
      "Trained batch 1224 batch loss 0.609217525 epoch total loss 0.595827162\n",
      "Trained batch 1225 batch loss 0.611994147 epoch total loss 0.595840394\n",
      "Trained batch 1226 batch loss 0.626293659 epoch total loss 0.59586525\n",
      "Trained batch 1227 batch loss 0.674369216 epoch total loss 0.595929205\n",
      "Trained batch 1228 batch loss 0.73996985 epoch total loss 0.596046507\n",
      "Trained batch 1229 batch loss 0.652836561 epoch total loss 0.596092701\n",
      "Trained batch 1230 batch loss 0.673363924 epoch total loss 0.596155524\n",
      "Trained batch 1231 batch loss 0.653963625 epoch total loss 0.596202493\n",
      "Trained batch 1232 batch loss 0.592644513 epoch total loss 0.596199632\n",
      "Trained batch 1233 batch loss 0.598026931 epoch total loss 0.596201122\n",
      "Trained batch 1234 batch loss 0.641950428 epoch total loss 0.596238196\n",
      "Trained batch 1235 batch loss 0.616277277 epoch total loss 0.596254408\n",
      "Trained batch 1236 batch loss 0.693645 epoch total loss 0.596333206\n",
      "Trained batch 1237 batch loss 0.666932 epoch total loss 0.596390307\n",
      "Trained batch 1238 batch loss 0.594591141 epoch total loss 0.596388876\n",
      "Trained batch 1239 batch loss 0.630073488 epoch total loss 0.596416056\n",
      "Trained batch 1240 batch loss 0.523875415 epoch total loss 0.596357524\n",
      "Trained batch 1241 batch loss 0.61169821 epoch total loss 0.596369863\n",
      "Trained batch 1242 batch loss 0.622910321 epoch total loss 0.596391261\n",
      "Trained batch 1243 batch loss 0.620114803 epoch total loss 0.596410334\n",
      "Trained batch 1244 batch loss 0.544557869 epoch total loss 0.59636867\n",
      "Trained batch 1245 batch loss 0.583256543 epoch total loss 0.59635812\n",
      "Trained batch 1246 batch loss 0.58834815 epoch total loss 0.596351683\n",
      "Trained batch 1247 batch loss 0.669751525 epoch total loss 0.596410513\n",
      "Trained batch 1248 batch loss 0.597221673 epoch total loss 0.596411169\n",
      "Trained batch 1249 batch loss 0.62593478 epoch total loss 0.596434832\n",
      "Trained batch 1250 batch loss 0.616366625 epoch total loss 0.596450806\n",
      "Trained batch 1251 batch loss 0.585771799 epoch total loss 0.596442223\n",
      "Trained batch 1252 batch loss 0.55295217 epoch total loss 0.596407533\n",
      "Trained batch 1253 batch loss 0.575186551 epoch total loss 0.596390605\n",
      "Trained batch 1254 batch loss 0.55338192 epoch total loss 0.596356332\n",
      "Trained batch 1255 batch loss 0.559086204 epoch total loss 0.59632659\n",
      "Trained batch 1256 batch loss 0.593066454 epoch total loss 0.596324\n",
      "Trained batch 1257 batch loss 0.558463097 epoch total loss 0.596293926\n",
      "Trained batch 1258 batch loss 0.565466285 epoch total loss 0.596269429\n",
      "Trained batch 1259 batch loss 0.718392789 epoch total loss 0.596366405\n",
      "Trained batch 1260 batch loss 0.702819526 epoch total loss 0.596450925\n",
      "Trained batch 1261 batch loss 0.594796538 epoch total loss 0.596449554\n",
      "Trained batch 1262 batch loss 0.558423221 epoch total loss 0.596419454\n",
      "Trained batch 1263 batch loss 0.734151304 epoch total loss 0.596528471\n",
      "Trained batch 1264 batch loss 0.697023928 epoch total loss 0.596608\n",
      "Trained batch 1265 batch loss 0.507077575 epoch total loss 0.596537232\n",
      "Trained batch 1266 batch loss 0.521804869 epoch total loss 0.596478164\n",
      "Trained batch 1267 batch loss 0.526724 epoch total loss 0.59642309\n",
      "Trained batch 1268 batch loss 0.515400529 epoch total loss 0.596359193\n",
      "Trained batch 1269 batch loss 0.545854509 epoch total loss 0.596319377\n",
      "Trained batch 1270 batch loss 0.588225484 epoch total loss 0.596313\n",
      "Trained batch 1271 batch loss 0.591709614 epoch total loss 0.596309423\n",
      "Trained batch 1272 batch loss 0.569305122 epoch total loss 0.596288145\n",
      "Trained batch 1273 batch loss 0.614356 epoch total loss 0.59630233\n",
      "Trained batch 1274 batch loss 0.625743628 epoch total loss 0.596325457\n",
      "Trained batch 1275 batch loss 0.651481748 epoch total loss 0.59636873\n",
      "Trained batch 1276 batch loss 0.578153908 epoch total loss 0.596354425\n",
      "Trained batch 1277 batch loss 0.584679127 epoch total loss 0.596345246\n",
      "Trained batch 1278 batch loss 0.646895766 epoch total loss 0.596384823\n",
      "Trained batch 1279 batch loss 0.580240071 epoch total loss 0.596372247\n",
      "Trained batch 1280 batch loss 0.572147 epoch total loss 0.596353292\n",
      "Trained batch 1281 batch loss 0.570008397 epoch total loss 0.596332729\n",
      "Trained batch 1282 batch loss 0.577322364 epoch total loss 0.596317887\n",
      "Trained batch 1283 batch loss 0.548692286 epoch total loss 0.596280813\n",
      "Trained batch 1284 batch loss 0.550092757 epoch total loss 0.596244812\n",
      "Trained batch 1285 batch loss 0.60664773 epoch total loss 0.596252918\n",
      "Trained batch 1286 batch loss 0.58437562 epoch total loss 0.59624368\n",
      "Trained batch 1287 batch loss 0.607259929 epoch total loss 0.596252203\n",
      "Trained batch 1288 batch loss 0.530627429 epoch total loss 0.596201241\n",
      "Trained batch 1289 batch loss 0.517492771 epoch total loss 0.596140206\n",
      "Trained batch 1290 batch loss 0.564878106 epoch total loss 0.596116\n",
      "Trained batch 1291 batch loss 0.673488915 epoch total loss 0.596175909\n",
      "Trained batch 1292 batch loss 0.555474281 epoch total loss 0.596144378\n",
      "Trained batch 1293 batch loss 0.59217 epoch total loss 0.596141338\n",
      "Trained batch 1294 batch loss 0.566859245 epoch total loss 0.596118689\n",
      "Trained batch 1295 batch loss 0.629635751 epoch total loss 0.596144557\n",
      "Trained batch 1296 batch loss 0.557469785 epoch total loss 0.596114755\n",
      "Trained batch 1297 batch loss 0.555616 epoch total loss 0.596083522\n",
      "Trained batch 1298 batch loss 0.569098949 epoch total loss 0.59606272\n",
      "Trained batch 1299 batch loss 0.556891918 epoch total loss 0.59603256\n",
      "Trained batch 1300 batch loss 0.573620915 epoch total loss 0.596015275\n",
      "Trained batch 1301 batch loss 0.557980835 epoch total loss 0.595986068\n",
      "Trained batch 1302 batch loss 0.629735947 epoch total loss 0.596012\n",
      "Trained batch 1303 batch loss 0.595654 epoch total loss 0.596011698\n",
      "Trained batch 1304 batch loss 0.715625405 epoch total loss 0.59610343\n",
      "Trained batch 1305 batch loss 0.72439748 epoch total loss 0.596201777\n",
      "Trained batch 1306 batch loss 0.637284219 epoch total loss 0.596233249\n",
      "Trained batch 1307 batch loss 0.673370957 epoch total loss 0.596292257\n",
      "Trained batch 1308 batch loss 0.666579068 epoch total loss 0.596346\n",
      "Trained batch 1309 batch loss 0.604461193 epoch total loss 0.59635216\n",
      "Trained batch 1310 batch loss 0.552230954 epoch total loss 0.596318483\n",
      "Trained batch 1311 batch loss 0.617477655 epoch total loss 0.596334636\n",
      "Trained batch 1312 batch loss 0.609112144 epoch total loss 0.596344411\n",
      "Trained batch 1313 batch loss 0.593606293 epoch total loss 0.596342325\n",
      "Trained batch 1314 batch loss 0.589955211 epoch total loss 0.596337497\n",
      "Trained batch 1315 batch loss 0.610535324 epoch total loss 0.596348286\n",
      "Trained batch 1316 batch loss 0.569619536 epoch total loss 0.596328\n",
      "Trained batch 1317 batch loss 0.556068897 epoch total loss 0.596297443\n",
      "Trained batch 1318 batch loss 0.54616195 epoch total loss 0.596259415\n",
      "Trained batch 1319 batch loss 0.614497304 epoch total loss 0.596273243\n",
      "Trained batch 1320 batch loss 0.680690587 epoch total loss 0.59633714\n",
      "Trained batch 1321 batch loss 0.633980334 epoch total loss 0.596365631\n",
      "Trained batch 1322 batch loss 0.575631678 epoch total loss 0.596349955\n",
      "Trained batch 1323 batch loss 0.649070919 epoch total loss 0.596389771\n",
      "Trained batch 1324 batch loss 0.559969604 epoch total loss 0.596362293\n",
      "Trained batch 1325 batch loss 0.647482395 epoch total loss 0.596400857\n",
      "Trained batch 1326 batch loss 0.518129289 epoch total loss 0.596341848\n",
      "Trained batch 1327 batch loss 0.684499264 epoch total loss 0.596408248\n",
      "Trained batch 1328 batch loss 0.621223688 epoch total loss 0.596426964\n",
      "Trained batch 1329 batch loss 0.596954882 epoch total loss 0.596427381\n",
      "Trained batch 1330 batch loss 0.45990783 epoch total loss 0.596324742\n",
      "Trained batch 1331 batch loss 0.529874682 epoch total loss 0.596274793\n",
      "Trained batch 1332 batch loss 0.574757814 epoch total loss 0.59625864\n",
      "Trained batch 1333 batch loss 0.634186387 epoch total loss 0.596287131\n",
      "Trained batch 1334 batch loss 0.588202417 epoch total loss 0.596281052\n",
      "Trained batch 1335 batch loss 0.510725856 epoch total loss 0.596217\n",
      "Trained batch 1336 batch loss 0.529304087 epoch total loss 0.596166849\n",
      "Trained batch 1337 batch loss 0.510945439 epoch total loss 0.596103132\n",
      "Trained batch 1338 batch loss 0.511998534 epoch total loss 0.596040249\n",
      "Trained batch 1339 batch loss 0.572484076 epoch total loss 0.596022725\n",
      "Trained batch 1340 batch loss 0.531534553 epoch total loss 0.595974624\n",
      "Trained batch 1341 batch loss 0.564701736 epoch total loss 0.595951259\n",
      "Trained batch 1342 batch loss 0.564033628 epoch total loss 0.595927477\n",
      "Trained batch 1343 batch loss 0.584574 epoch total loss 0.595919073\n",
      "Trained batch 1344 batch loss 0.696283 epoch total loss 0.595993698\n",
      "Trained batch 1345 batch loss 0.599289894 epoch total loss 0.595996201\n",
      "Trained batch 1346 batch loss 0.411621153 epoch total loss 0.59585923\n",
      "Trained batch 1347 batch loss 0.457722366 epoch total loss 0.59575665\n",
      "Trained batch 1348 batch loss 0.510130167 epoch total loss 0.595693111\n",
      "Trained batch 1349 batch loss 0.652717173 epoch total loss 0.595735371\n",
      "Trained batch 1350 batch loss 0.703288078 epoch total loss 0.595815063\n",
      "Trained batch 1351 batch loss 0.58369714 epoch total loss 0.595806062\n",
      "Trained batch 1352 batch loss 0.634983897 epoch total loss 0.59583509\n",
      "Trained batch 1353 batch loss 0.621773362 epoch total loss 0.595854223\n",
      "Trained batch 1354 batch loss 0.607572377 epoch total loss 0.595862865\n",
      "Trained batch 1355 batch loss 0.58509171 epoch total loss 0.595854938\n",
      "Trained batch 1356 batch loss 0.660449386 epoch total loss 0.595902562\n",
      "Trained batch 1357 batch loss 0.655561805 epoch total loss 0.59594655\n",
      "Trained batch 1358 batch loss 0.655606031 epoch total loss 0.595990479\n",
      "Trained batch 1359 batch loss 0.611912251 epoch total loss 0.596002221\n",
      "Trained batch 1360 batch loss 0.516793609 epoch total loss 0.595943928\n",
      "Trained batch 1361 batch loss 0.531557798 epoch total loss 0.595896661\n",
      "Trained batch 1362 batch loss 0.502559423 epoch total loss 0.595828116\n",
      "Trained batch 1363 batch loss 0.573480844 epoch total loss 0.595811725\n",
      "Trained batch 1364 batch loss 0.511179268 epoch total loss 0.595749676\n",
      "Trained batch 1365 batch loss 0.572943747 epoch total loss 0.595732927\n",
      "Trained batch 1366 batch loss 0.637052357 epoch total loss 0.595763206\n",
      "Trained batch 1367 batch loss 0.610941887 epoch total loss 0.595774293\n",
      "Trained batch 1368 batch loss 0.566100955 epoch total loss 0.595752597\n",
      "Trained batch 1369 batch loss 0.577348709 epoch total loss 0.595739126\n",
      "Trained batch 1370 batch loss 0.628697157 epoch total loss 0.595763206\n",
      "Trained batch 1371 batch loss 0.74403441 epoch total loss 0.595871389\n",
      "Trained batch 1372 batch loss 0.674232721 epoch total loss 0.59592849\n",
      "Trained batch 1373 batch loss 0.619766235 epoch total loss 0.595945835\n",
      "Trained batch 1374 batch loss 0.588002563 epoch total loss 0.595940053\n",
      "Trained batch 1375 batch loss 0.61634618 epoch total loss 0.595954895\n",
      "Trained batch 1376 batch loss 0.67153573 epoch total loss 0.596009791\n",
      "Trained batch 1377 batch loss 0.662356675 epoch total loss 0.596058\n",
      "Trained batch 1378 batch loss 0.64604491 epoch total loss 0.596094251\n",
      "Trained batch 1379 batch loss 0.6417588 epoch total loss 0.596127391\n",
      "Trained batch 1380 batch loss 0.724783838 epoch total loss 0.596220613\n",
      "Trained batch 1381 batch loss 0.673770308 epoch total loss 0.59627682\n",
      "Trained batch 1382 batch loss 0.607316613 epoch total loss 0.596284747\n",
      "Trained batch 1383 batch loss 0.620509446 epoch total loss 0.596302271\n",
      "Trained batch 1384 batch loss 0.70671469 epoch total loss 0.596382082\n",
      "Trained batch 1385 batch loss 0.723252773 epoch total loss 0.596473694\n",
      "Trained batch 1386 batch loss 0.617598832 epoch total loss 0.596488893\n",
      "Trained batch 1387 batch loss 0.599332452 epoch total loss 0.59649092\n",
      "Trained batch 1388 batch loss 0.605863512 epoch total loss 0.596497655\n",
      "Trained batch 1389 batch loss 0.6258412 epoch total loss 0.596518815\n",
      "Trained batch 1390 batch loss 0.588038802 epoch total loss 0.596512675\n",
      "Trained batch 1391 batch loss 0.588052511 epoch total loss 0.596506655\n",
      "Trained batch 1392 batch loss 0.554873645 epoch total loss 0.596476734\n",
      "Trained batch 1393 batch loss 0.518974364 epoch total loss 0.596421063\n",
      "Trained batch 1394 batch loss 0.479518235 epoch total loss 0.596337199\n",
      "Trained batch 1395 batch loss 0.585604072 epoch total loss 0.59632951\n",
      "Trained batch 1396 batch loss 0.650766134 epoch total loss 0.596368551\n",
      "Trained batch 1397 batch loss 0.624104202 epoch total loss 0.59638834\n",
      "Trained batch 1398 batch loss 0.668614447 epoch total loss 0.59644\n",
      "Trained batch 1399 batch loss 0.617592156 epoch total loss 0.596455157\n",
      "Trained batch 1400 batch loss 0.658322275 epoch total loss 0.596499383\n",
      "Trained batch 1401 batch loss 0.633843303 epoch total loss 0.596526\n",
      "Trained batch 1402 batch loss 0.598627329 epoch total loss 0.596527517\n",
      "Trained batch 1403 batch loss 0.609904289 epoch total loss 0.596537113\n",
      "Trained batch 1404 batch loss 0.603797853 epoch total loss 0.596542299\n",
      "Trained batch 1405 batch loss 0.592464 epoch total loss 0.596539378\n",
      "Trained batch 1406 batch loss 0.618659616 epoch total loss 0.596555114\n",
      "Trained batch 1407 batch loss 0.59751904 epoch total loss 0.596555769\n",
      "Trained batch 1408 batch loss 0.703722119 epoch total loss 0.596631944\n",
      "Trained batch 1409 batch loss 0.64885956 epoch total loss 0.596668959\n",
      "Trained batch 1410 batch loss 0.569213271 epoch total loss 0.596649528\n",
      "Trained batch 1411 batch loss 0.560026467 epoch total loss 0.59662354\n",
      "Trained batch 1412 batch loss 0.554929852 epoch total loss 0.596594036\n",
      "Trained batch 1413 batch loss 0.569705725 epoch total loss 0.596574962\n",
      "Trained batch 1414 batch loss 0.525777 epoch total loss 0.596524894\n",
      "Trained batch 1415 batch loss 0.541563153 epoch total loss 0.596486032\n",
      "Trained batch 1416 batch loss 0.617500722 epoch total loss 0.596500874\n",
      "Trained batch 1417 batch loss 0.616666138 epoch total loss 0.596515119\n",
      "Trained batch 1418 batch loss 0.538455069 epoch total loss 0.596474171\n",
      "Trained batch 1419 batch loss 0.465502471 epoch total loss 0.596381843\n",
      "Trained batch 1420 batch loss 0.468861818 epoch total loss 0.596292078\n",
      "Trained batch 1421 batch loss 0.625423968 epoch total loss 0.596312582\n",
      "Trained batch 1422 batch loss 0.592733204 epoch total loss 0.59631\n",
      "Trained batch 1423 batch loss 0.657851517 epoch total loss 0.596353292\n",
      "Trained batch 1424 batch loss 0.531185031 epoch total loss 0.596307516\n",
      "Trained batch 1425 batch loss 0.516489327 epoch total loss 0.596251488\n",
      "Trained batch 1426 batch loss 0.591018856 epoch total loss 0.596247792\n",
      "Trained batch 1427 batch loss 0.540905774 epoch total loss 0.596209049\n",
      "Trained batch 1428 batch loss 0.535987 epoch total loss 0.596166849\n",
      "Trained batch 1429 batch loss 0.559205532 epoch total loss 0.596141\n",
      "Trained batch 1430 batch loss 0.541936576 epoch total loss 0.596103072\n",
      "Trained batch 1431 batch loss 0.609363556 epoch total loss 0.59611237\n",
      "Trained batch 1432 batch loss 0.474198699 epoch total loss 0.596027195\n",
      "Trained batch 1433 batch loss 0.452466369 epoch total loss 0.59592706\n",
      "Trained batch 1434 batch loss 0.45972538 epoch total loss 0.59583205\n",
      "Trained batch 1435 batch loss 0.445683151 epoch total loss 0.595727384\n",
      "Trained batch 1436 batch loss 0.460517377 epoch total loss 0.595633268\n",
      "Trained batch 1437 batch loss 0.533795178 epoch total loss 0.595590234\n",
      "Trained batch 1438 batch loss 0.630836725 epoch total loss 0.595614731\n",
      "Trained batch 1439 batch loss 0.646958232 epoch total loss 0.595650434\n",
      "Trained batch 1440 batch loss 0.688171387 epoch total loss 0.595714688\n",
      "Trained batch 1441 batch loss 0.802433252 epoch total loss 0.595858157\n",
      "Trained batch 1442 batch loss 0.841914296 epoch total loss 0.596028805\n",
      "Trained batch 1443 batch loss 0.7998299 epoch total loss 0.59617\n",
      "Trained batch 1444 batch loss 0.578354359 epoch total loss 0.59615767\n",
      "Trained batch 1445 batch loss 0.620544612 epoch total loss 0.596174538\n",
      "Trained batch 1446 batch loss 0.571357846 epoch total loss 0.596157372\n",
      "Trained batch 1447 batch loss 0.55965066 epoch total loss 0.596132159\n",
      "Trained batch 1448 batch loss 0.57305181 epoch total loss 0.596116185\n",
      "Trained batch 1449 batch loss 0.580287933 epoch total loss 0.596105278\n",
      "Trained batch 1450 batch loss 0.643149495 epoch total loss 0.596137702\n",
      "Trained batch 1451 batch loss 0.632615507 epoch total loss 0.596162856\n",
      "Trained batch 1452 batch loss 0.614615142 epoch total loss 0.596175551\n",
      "Trained batch 1453 batch loss 0.584652066 epoch total loss 0.596167624\n",
      "Trained batch 1454 batch loss 0.604541898 epoch total loss 0.596173406\n",
      "Trained batch 1455 batch loss 0.66743046 epoch total loss 0.596222341\n",
      "Trained batch 1456 batch loss 0.583555579 epoch total loss 0.596213639\n",
      "Trained batch 1457 batch loss 0.547460437 epoch total loss 0.596180201\n",
      "Trained batch 1458 batch loss 0.653540134 epoch total loss 0.596219599\n",
      "Trained batch 1459 batch loss 0.65504539 epoch total loss 0.596259892\n",
      "Trained batch 1460 batch loss 0.638566 epoch total loss 0.59628886\n",
      "Trained batch 1461 batch loss 0.595041513 epoch total loss 0.596287966\n",
      "Trained batch 1462 batch loss 0.578159928 epoch total loss 0.596275628\n",
      "Trained batch 1463 batch loss 0.559540749 epoch total loss 0.596250534\n",
      "Trained batch 1464 batch loss 0.589915 epoch total loss 0.596246183\n",
      "Trained batch 1465 batch loss 0.554664195 epoch total loss 0.596217811\n",
      "Trained batch 1466 batch loss 0.52043885 epoch total loss 0.596166134\n",
      "Trained batch 1467 batch loss 0.594105124 epoch total loss 0.596164763\n",
      "Trained batch 1468 batch loss 0.564734161 epoch total loss 0.596143365\n",
      "Trained batch 1469 batch loss 0.568449378 epoch total loss 0.59612447\n",
      "Trained batch 1470 batch loss 0.697288036 epoch total loss 0.596193254\n",
      "Trained batch 1471 batch loss 0.544859946 epoch total loss 0.596158385\n",
      "Trained batch 1472 batch loss 0.590920627 epoch total loss 0.596154809\n",
      "Trained batch 1473 batch loss 0.537341356 epoch total loss 0.596114933\n",
      "Trained batch 1474 batch loss 0.469736248 epoch total loss 0.596029162\n",
      "Trained batch 1475 batch loss 0.447196424 epoch total loss 0.595928252\n",
      "Trained batch 1476 batch loss 0.480332851 epoch total loss 0.59585\n",
      "Trained batch 1477 batch loss 0.608799756 epoch total loss 0.595858753\n",
      "Trained batch 1478 batch loss 0.657730222 epoch total loss 0.595900595\n",
      "Trained batch 1479 batch loss 0.661091208 epoch total loss 0.595944643\n",
      "Trained batch 1480 batch loss 0.671623528 epoch total loss 0.595995784\n",
      "Trained batch 1481 batch loss 0.606031835 epoch total loss 0.596002579\n",
      "Trained batch 1482 batch loss 0.617707312 epoch total loss 0.596017241\n",
      "Trained batch 1483 batch loss 0.540572524 epoch total loss 0.595979869\n",
      "Trained batch 1484 batch loss 0.707217693 epoch total loss 0.596054792\n",
      "Trained batch 1485 batch loss 0.623137891 epoch total loss 0.596073031\n",
      "Trained batch 1486 batch loss 0.607899964 epoch total loss 0.596081\n",
      "Trained batch 1487 batch loss 0.590494156 epoch total loss 0.596077263\n",
      "Trained batch 1488 batch loss 0.627043545 epoch total loss 0.596098065\n",
      "Trained batch 1489 batch loss 0.649938345 epoch total loss 0.596134245\n",
      "Trained batch 1490 batch loss 0.632972956 epoch total loss 0.596159\n",
      "Trained batch 1491 batch loss 0.578455091 epoch total loss 0.59614706\n",
      "Trained batch 1492 batch loss 0.649168193 epoch total loss 0.596182585\n",
      "Trained batch 1493 batch loss 0.705073655 epoch total loss 0.596255541\n",
      "Trained batch 1494 batch loss 0.685975909 epoch total loss 0.596315622\n",
      "Trained batch 1495 batch loss 0.632577181 epoch total loss 0.596339822\n",
      "Trained batch 1496 batch loss 0.589225531 epoch total loss 0.596335113\n",
      "Trained batch 1497 batch loss 0.579923093 epoch total loss 0.596324146\n",
      "Trained batch 1498 batch loss 0.648883879 epoch total loss 0.596359193\n",
      "Trained batch 1499 batch loss 0.595281839 epoch total loss 0.596358478\n",
      "Trained batch 1500 batch loss 0.65482831 epoch total loss 0.59639746\n",
      "Trained batch 1501 batch loss 0.595073 epoch total loss 0.596396565\n",
      "Trained batch 1502 batch loss 0.590931535 epoch total loss 0.596393\n",
      "Trained batch 1503 batch loss 0.651249886 epoch total loss 0.596429467\n",
      "Trained batch 1504 batch loss 0.646278 epoch total loss 0.596462607\n",
      "Trained batch 1505 batch loss 0.621950507 epoch total loss 0.596479535\n",
      "Trained batch 1506 batch loss 0.587462664 epoch total loss 0.596473575\n",
      "Trained batch 1507 batch loss 0.626657 epoch total loss 0.596493602\n",
      "Trained batch 1508 batch loss 0.600861609 epoch total loss 0.596496522\n",
      "Trained batch 1509 batch loss 0.573681295 epoch total loss 0.596481383\n",
      "Trained batch 1510 batch loss 0.618602872 epoch total loss 0.596496046\n",
      "Trained batch 1511 batch loss 0.603907764 epoch total loss 0.596500933\n",
      "Trained batch 1512 batch loss 0.55445087 epoch total loss 0.596473098\n",
      "Trained batch 1513 batch loss 0.63358593 epoch total loss 0.596497655\n",
      "Trained batch 1514 batch loss 0.682774663 epoch total loss 0.596554637\n",
      "Trained batch 1515 batch loss 0.589616 epoch total loss 0.596550047\n",
      "Trained batch 1516 batch loss 0.559037387 epoch total loss 0.596525311\n",
      "Trained batch 1517 batch loss 0.608532786 epoch total loss 0.596533179\n",
      "Trained batch 1518 batch loss 0.537522316 epoch total loss 0.596494317\n",
      "Trained batch 1519 batch loss 0.56240648 epoch total loss 0.596471846\n",
      "Trained batch 1520 batch loss 0.668549716 epoch total loss 0.596519291\n",
      "Trained batch 1521 batch loss 0.626921773 epoch total loss 0.596539259\n",
      "Trained batch 1522 batch loss 0.546927452 epoch total loss 0.596506715\n",
      "Trained batch 1523 batch loss 0.571409285 epoch total loss 0.596490204\n",
      "Trained batch 1524 batch loss 0.668078303 epoch total loss 0.596537173\n",
      "Trained batch 1525 batch loss 0.648488164 epoch total loss 0.596571267\n",
      "Trained batch 1526 batch loss 0.587863088 epoch total loss 0.596565604\n",
      "Trained batch 1527 batch loss 0.532385349 epoch total loss 0.596523583\n",
      "Trained batch 1528 batch loss 0.650742 epoch total loss 0.596559048\n",
      "Trained batch 1529 batch loss 0.592271745 epoch total loss 0.596556246\n",
      "Trained batch 1530 batch loss 0.649368584 epoch total loss 0.596590757\n",
      "Trained batch 1531 batch loss 0.700343 epoch total loss 0.596658528\n",
      "Trained batch 1532 batch loss 0.671001792 epoch total loss 0.596707046\n",
      "Trained batch 1533 batch loss 0.676073432 epoch total loss 0.596758842\n",
      "Trained batch 1534 batch loss 0.584631622 epoch total loss 0.596751\n",
      "Trained batch 1535 batch loss 0.55265516 epoch total loss 0.596722245\n",
      "Trained batch 1536 batch loss 0.564796329 epoch total loss 0.596701443\n",
      "Trained batch 1537 batch loss 0.673333049 epoch total loss 0.596751332\n",
      "Trained batch 1538 batch loss 0.583883464 epoch total loss 0.596742928\n",
      "Trained batch 1539 batch loss 0.605079293 epoch total loss 0.596748352\n",
      "Trained batch 1540 batch loss 0.587971866 epoch total loss 0.59674269\n",
      "Trained batch 1541 batch loss 0.561377645 epoch total loss 0.596719742\n",
      "Trained batch 1542 batch loss 0.627016544 epoch total loss 0.596739352\n",
      "Trained batch 1543 batch loss 0.576333404 epoch total loss 0.596726179\n",
      "Trained batch 1544 batch loss 0.556631386 epoch total loss 0.596700191\n",
      "Trained batch 1545 batch loss 0.584889531 epoch total loss 0.596692562\n",
      "Trained batch 1546 batch loss 0.665110886 epoch total loss 0.596736789\n",
      "Trained batch 1547 batch loss 0.556453288 epoch total loss 0.596710801\n",
      "Trained batch 1548 batch loss 0.602132 epoch total loss 0.596714258\n",
      "Trained batch 1549 batch loss 0.622421086 epoch total loss 0.596730888\n",
      "Trained batch 1550 batch loss 0.608701 epoch total loss 0.596738577\n",
      "Trained batch 1551 batch loss 0.631011486 epoch total loss 0.59676069\n",
      "Trained batch 1552 batch loss 0.672095716 epoch total loss 0.596809208\n",
      "Trained batch 1553 batch loss 0.627554536 epoch total loss 0.596829057\n",
      "Trained batch 1554 batch loss 0.616210938 epoch total loss 0.596841514\n",
      "Trained batch 1555 batch loss 0.60643 epoch total loss 0.596847653\n",
      "Trained batch 1556 batch loss 0.631614208 epoch total loss 0.59687\n",
      "Trained batch 1557 batch loss 0.716195166 epoch total loss 0.596946657\n",
      "Trained batch 1558 batch loss 0.653528929 epoch total loss 0.596982956\n",
      "Trained batch 1559 batch loss 0.612639308 epoch total loss 0.59699297\n",
      "Trained batch 1560 batch loss 0.659031689 epoch total loss 0.597032726\n",
      "Trained batch 1561 batch loss 0.5281564 epoch total loss 0.596988618\n",
      "Trained batch 1562 batch loss 0.682534635 epoch total loss 0.597043395\n",
      "Trained batch 1563 batch loss 0.639435887 epoch total loss 0.597070515\n",
      "Trained batch 1564 batch loss 0.49906832 epoch total loss 0.597007871\n",
      "Trained batch 1565 batch loss 0.560765386 epoch total loss 0.596984744\n",
      "Trained batch 1566 batch loss 0.722802043 epoch total loss 0.597065091\n",
      "Trained batch 1567 batch loss 0.596205533 epoch total loss 0.597064495\n",
      "Trained batch 1568 batch loss 0.614530861 epoch total loss 0.597075641\n",
      "Trained batch 1569 batch loss 0.627399683 epoch total loss 0.597094953\n",
      "Trained batch 1570 batch loss 0.646988928 epoch total loss 0.597126722\n",
      "Trained batch 1571 batch loss 0.68546766 epoch total loss 0.597183\n",
      "Trained batch 1572 batch loss 0.6915977 epoch total loss 0.597243\n",
      "Trained batch 1573 batch loss 0.694294155 epoch total loss 0.597304702\n",
      "Trained batch 1574 batch loss 0.70566982 epoch total loss 0.597373545\n",
      "Trained batch 1575 batch loss 0.688016236 epoch total loss 0.597431123\n",
      "Trained batch 1576 batch loss 0.540468216 epoch total loss 0.597394943\n",
      "Trained batch 1577 batch loss 0.594218552 epoch total loss 0.597393\n",
      "Trained batch 1578 batch loss 0.611036122 epoch total loss 0.597401559\n",
      "Trained batch 1579 batch loss 0.584571 epoch total loss 0.597393453\n",
      "Trained batch 1580 batch loss 0.595314 epoch total loss 0.597392201\n",
      "Trained batch 1581 batch loss 0.574536681 epoch total loss 0.597377717\n",
      "Trained batch 1582 batch loss 0.645364881 epoch total loss 0.597408056\n",
      "Trained batch 1583 batch loss 0.573191404 epoch total loss 0.597392738\n",
      "Trained batch 1584 batch loss 0.580682397 epoch total loss 0.597382188\n",
      "Trained batch 1585 batch loss 0.647373915 epoch total loss 0.597413778\n",
      "Trained batch 1586 batch loss 0.683109 epoch total loss 0.59746778\n",
      "Trained batch 1587 batch loss 0.633855283 epoch total loss 0.597490728\n",
      "Trained batch 1588 batch loss 0.662780762 epoch total loss 0.597531855\n",
      "Trained batch 1589 batch loss 0.710038781 epoch total loss 0.597602606\n",
      "Trained batch 1590 batch loss 0.663070261 epoch total loss 0.597643793\n",
      "Trained batch 1591 batch loss 0.669770718 epoch total loss 0.597689152\n",
      "Trained batch 1592 batch loss 0.64880693 epoch total loss 0.597721279\n",
      "Trained batch 1593 batch loss 0.692346752 epoch total loss 0.597780645\n",
      "Trained batch 1594 batch loss 0.714294553 epoch total loss 0.59785378\n",
      "Trained batch 1595 batch loss 0.738322914 epoch total loss 0.597941816\n",
      "Trained batch 1596 batch loss 0.677234888 epoch total loss 0.597991526\n",
      "Trained batch 1597 batch loss 0.680300295 epoch total loss 0.598043084\n",
      "Trained batch 1598 batch loss 0.669450343 epoch total loss 0.598087728\n",
      "Trained batch 1599 batch loss 0.635712564 epoch total loss 0.598111272\n",
      "Trained batch 1600 batch loss 0.659579039 epoch total loss 0.598149717\n",
      "Trained batch 1601 batch loss 0.653268933 epoch total loss 0.598184168\n",
      "Trained batch 1602 batch loss 0.650884688 epoch total loss 0.598217\n",
      "Trained batch 1603 batch loss 0.653504074 epoch total loss 0.598251522\n",
      "Trained batch 1604 batch loss 0.634601593 epoch total loss 0.598274171\n",
      "Trained batch 1605 batch loss 0.657375872 epoch total loss 0.598311\n",
      "Trained batch 1606 batch loss 0.636071801 epoch total loss 0.598334491\n",
      "Trained batch 1607 batch loss 0.673615694 epoch total loss 0.598381341\n",
      "Trained batch 1608 batch loss 0.660214782 epoch total loss 0.598419785\n",
      "Trained batch 1609 batch loss 0.646814227 epoch total loss 0.598449886\n",
      "Trained batch 1610 batch loss 0.670078397 epoch total loss 0.598494351\n",
      "Trained batch 1611 batch loss 0.619913578 epoch total loss 0.598507702\n",
      "Trained batch 1612 batch loss 0.61227 epoch total loss 0.598516166\n",
      "Trained batch 1613 batch loss 0.636319935 epoch total loss 0.598539591\n",
      "Trained batch 1614 batch loss 0.644525051 epoch total loss 0.598568082\n",
      "Trained batch 1615 batch loss 0.667471588 epoch total loss 0.598610759\n",
      "Trained batch 1616 batch loss 0.650026262 epoch total loss 0.598642588\n",
      "Trained batch 1617 batch loss 0.603189349 epoch total loss 0.598645449\n",
      "Trained batch 1618 batch loss 0.57980144 epoch total loss 0.598633766\n",
      "Trained batch 1619 batch loss 0.526160657 epoch total loss 0.598589\n",
      "Trained batch 1620 batch loss 0.577898622 epoch total loss 0.598576248\n",
      "Trained batch 1621 batch loss 0.660565197 epoch total loss 0.598614454\n",
      "Trained batch 1622 batch loss 0.482842118 epoch total loss 0.598543108\n",
      "Trained batch 1623 batch loss 0.557916582 epoch total loss 0.598518074\n",
      "Trained batch 1624 batch loss 0.473804474 epoch total loss 0.598441303\n",
      "Trained batch 1625 batch loss 0.50246 epoch total loss 0.598382235\n",
      "Trained batch 1626 batch loss 0.503409445 epoch total loss 0.598323822\n",
      "Trained batch 1627 batch loss 0.516543 epoch total loss 0.598273516\n",
      "Trained batch 1628 batch loss 0.524261475 epoch total loss 0.598228037\n",
      "Trained batch 1629 batch loss 0.600436449 epoch total loss 0.598229408\n",
      "Trained batch 1630 batch loss 0.554879189 epoch total loss 0.598202825\n",
      "Trained batch 1631 batch loss 0.464034617 epoch total loss 0.59812057\n",
      "Trained batch 1632 batch loss 0.424302459 epoch total loss 0.598014057\n",
      "Trained batch 1633 batch loss 0.398925781 epoch total loss 0.597892165\n",
      "Trained batch 1634 batch loss 0.417400688 epoch total loss 0.597781718\n",
      "Trained batch 1635 batch loss 0.491324872 epoch total loss 0.59771663\n",
      "Trained batch 1636 batch loss 0.430537611 epoch total loss 0.597614408\n",
      "Trained batch 1637 batch loss 0.38055104 epoch total loss 0.597481847\n",
      "Trained batch 1638 batch loss 0.477220833 epoch total loss 0.597408414\n",
      "Trained batch 1639 batch loss 0.592740774 epoch total loss 0.597405553\n",
      "Trained batch 1640 batch loss 0.629855275 epoch total loss 0.597425342\n",
      "Trained batch 1641 batch loss 0.627897859 epoch total loss 0.597443938\n",
      "Trained batch 1642 batch loss 0.648536444 epoch total loss 0.597475052\n",
      "Trained batch 1643 batch loss 0.537321091 epoch total loss 0.597438395\n",
      "Trained batch 1644 batch loss 0.593529105 epoch total loss 0.597436\n",
      "Trained batch 1645 batch loss 0.633749723 epoch total loss 0.597458065\n",
      "Trained batch 1646 batch loss 0.515638292 epoch total loss 0.597408354\n",
      "Trained batch 1647 batch loss 0.56763 epoch total loss 0.597390294\n",
      "Trained batch 1648 batch loss 0.55709213 epoch total loss 0.597365797\n",
      "Trained batch 1649 batch loss 0.595038712 epoch total loss 0.597364426\n",
      "Trained batch 1650 batch loss 0.607438922 epoch total loss 0.597370505\n",
      "Trained batch 1651 batch loss 0.606284857 epoch total loss 0.59737587\n",
      "Trained batch 1652 batch loss 0.600253761 epoch total loss 0.597377658\n",
      "Trained batch 1653 batch loss 0.556137383 epoch total loss 0.597352684\n",
      "Trained batch 1654 batch loss 0.537309 epoch total loss 0.597316384\n",
      "Trained batch 1655 batch loss 0.542681754 epoch total loss 0.597283363\n",
      "Trained batch 1656 batch loss 0.516975045 epoch total loss 0.597234845\n",
      "Trained batch 1657 batch loss 0.497738 epoch total loss 0.597174823\n",
      "Trained batch 1658 batch loss 0.567368448 epoch total loss 0.597156823\n",
      "Trained batch 1659 batch loss 0.574213 epoch total loss 0.597143054\n",
      "Trained batch 1660 batch loss 0.476329446 epoch total loss 0.597070217\n",
      "Trained batch 1661 batch loss 0.529385328 epoch total loss 0.597029448\n",
      "Trained batch 1662 batch loss 0.500832 epoch total loss 0.596971631\n",
      "Trained batch 1663 batch loss 0.553049147 epoch total loss 0.596945167\n",
      "Trained batch 1664 batch loss 0.521545827 epoch total loss 0.596899867\n",
      "Trained batch 1665 batch loss 0.547436774 epoch total loss 0.596870184\n",
      "Trained batch 1666 batch loss 0.551022649 epoch total loss 0.596842647\n",
      "Trained batch 1667 batch loss 0.555966675 epoch total loss 0.596818149\n",
      "Trained batch 1668 batch loss 0.554774702 epoch total loss 0.596792877\n",
      "Trained batch 1669 batch loss 0.494377643 epoch total loss 0.596731544\n",
      "Trained batch 1670 batch loss 0.564068615 epoch total loss 0.596712\n",
      "Trained batch 1671 batch loss 0.540137351 epoch total loss 0.596678138\n",
      "Trained batch 1672 batch loss 0.562084794 epoch total loss 0.596657455\n",
      "Trained batch 1673 batch loss 0.500383794 epoch total loss 0.596599877\n",
      "Trained batch 1674 batch loss 0.606459916 epoch total loss 0.596605778\n",
      "Trained batch 1675 batch loss 0.607841134 epoch total loss 0.596612513\n",
      "Trained batch 1676 batch loss 0.767923117 epoch total loss 0.596714735\n",
      "Trained batch 1677 batch loss 0.733871281 epoch total loss 0.596796513\n",
      "Trained batch 1678 batch loss 0.641473711 epoch total loss 0.596823156\n",
      "Trained batch 1679 batch loss 0.631895423 epoch total loss 0.596844\n",
      "Trained batch 1680 batch loss 0.570356846 epoch total loss 0.596828282\n",
      "Trained batch 1681 batch loss 0.612047851 epoch total loss 0.596837342\n",
      "Trained batch 1682 batch loss 0.605154097 epoch total loss 0.596842289\n",
      "Trained batch 1683 batch loss 0.668908656 epoch total loss 0.596885085\n",
      "Trained batch 1684 batch loss 0.683662891 epoch total loss 0.596936643\n",
      "Trained batch 1685 batch loss 0.655747771 epoch total loss 0.596971512\n",
      "Trained batch 1686 batch loss 0.640158892 epoch total loss 0.596997142\n",
      "Trained batch 1687 batch loss 0.586374104 epoch total loss 0.596990824\n",
      "Trained batch 1688 batch loss 0.616855323 epoch total loss 0.597002625\n",
      "Trained batch 1689 batch loss 0.59997189 epoch total loss 0.597004354\n",
      "Trained batch 1690 batch loss 0.505165696 epoch total loss 0.596950054\n",
      "Trained batch 1691 batch loss 0.544412732 epoch total loss 0.596919\n",
      "Trained batch 1692 batch loss 0.685006917 epoch total loss 0.596971035\n",
      "Trained batch 1693 batch loss 0.718295932 epoch total loss 0.597042739\n",
      "Trained batch 1694 batch loss 0.643141568 epoch total loss 0.597069919\n",
      "Trained batch 1695 batch loss 0.663249493 epoch total loss 0.59710896\n",
      "Trained batch 1696 batch loss 0.692901611 epoch total loss 0.597165465\n",
      "Trained batch 1697 batch loss 0.608655214 epoch total loss 0.597172201\n",
      "Trained batch 1698 batch loss 0.674462378 epoch total loss 0.597217739\n",
      "Trained batch 1699 batch loss 0.653318405 epoch total loss 0.59725076\n",
      "Trained batch 1700 batch loss 0.741624475 epoch total loss 0.597335696\n",
      "Trained batch 1701 batch loss 0.648469687 epoch total loss 0.597365737\n",
      "Trained batch 1702 batch loss 0.645646 epoch total loss 0.597394109\n",
      "Trained batch 1703 batch loss 0.646590889 epoch total loss 0.597423\n",
      "Trained batch 1704 batch loss 0.645201445 epoch total loss 0.597451031\n",
      "Trained batch 1705 batch loss 0.619456291 epoch total loss 0.597463965\n",
      "Trained batch 1706 batch loss 0.582414269 epoch total loss 0.597455084\n",
      "Trained batch 1707 batch loss 0.581530631 epoch total loss 0.597445786\n",
      "Trained batch 1708 batch loss 0.578313112 epoch total loss 0.59743458\n",
      "Trained batch 1709 batch loss 0.641272604 epoch total loss 0.59746027\n",
      "Trained batch 1710 batch loss 0.601076245 epoch total loss 0.597462356\n",
      "Trained batch 1711 batch loss 0.53722012 epoch total loss 0.597427189\n",
      "Trained batch 1712 batch loss 0.672239602 epoch total loss 0.59747088\n",
      "Trained batch 1713 batch loss 0.621454239 epoch total loss 0.597484887\n",
      "Trained batch 1714 batch loss 0.598768473 epoch total loss 0.597485602\n",
      "Trained batch 1715 batch loss 0.635267079 epoch total loss 0.597507656\n",
      "Trained batch 1716 batch loss 0.689354658 epoch total loss 0.597561121\n",
      "Trained batch 1717 batch loss 0.64631629 epoch total loss 0.597589552\n",
      "Trained batch 1718 batch loss 0.701852143 epoch total loss 0.59765029\n",
      "Trained batch 1719 batch loss 0.632564068 epoch total loss 0.597670615\n",
      "Trained batch 1720 batch loss 0.595807791 epoch total loss 0.597669542\n",
      "Trained batch 1721 batch loss 0.644792557 epoch total loss 0.5976969\n",
      "Trained batch 1722 batch loss 0.746762574 epoch total loss 0.597783446\n",
      "Trained batch 1723 batch loss 0.629413366 epoch total loss 0.597801745\n",
      "Trained batch 1724 batch loss 0.509869277 epoch total loss 0.597750783\n",
      "Trained batch 1725 batch loss 0.487012863 epoch total loss 0.597686589\n",
      "Trained batch 1726 batch loss 0.530124724 epoch total loss 0.597647488\n",
      "Trained batch 1727 batch loss 0.523079574 epoch total loss 0.597604275\n",
      "Trained batch 1728 batch loss 0.533819199 epoch total loss 0.597567379\n",
      "Trained batch 1729 batch loss 0.629063666 epoch total loss 0.597585559\n",
      "Trained batch 1730 batch loss 0.619075 epoch total loss 0.597597957\n",
      "Trained batch 1731 batch loss 0.628779173 epoch total loss 0.597615957\n",
      "Trained batch 1732 batch loss 0.599018216 epoch total loss 0.597616792\n",
      "Trained batch 1733 batch loss 0.595938683 epoch total loss 0.597615838\n",
      "Trained batch 1734 batch loss 0.566262722 epoch total loss 0.597597778\n",
      "Trained batch 1735 batch loss 0.569824934 epoch total loss 0.597581744\n",
      "Trained batch 1736 batch loss 0.486134291 epoch total loss 0.59751749\n",
      "Trained batch 1737 batch loss 0.489491642 epoch total loss 0.597455323\n",
      "Trained batch 1738 batch loss 0.432139665 epoch total loss 0.597360194\n",
      "Trained batch 1739 batch loss 0.547168553 epoch total loss 0.597331285\n",
      "Trained batch 1740 batch loss 0.613232315 epoch total loss 0.597340465\n",
      "Trained batch 1741 batch loss 0.64677453 epoch total loss 0.597368836\n",
      "Trained batch 1742 batch loss 0.735460401 epoch total loss 0.597448111\n",
      "Trained batch 1743 batch loss 0.599874914 epoch total loss 0.597449481\n",
      "Trained batch 1744 batch loss 0.640671611 epoch total loss 0.597474277\n",
      "Trained batch 1745 batch loss 0.67180562 epoch total loss 0.597516835\n",
      "Trained batch 1746 batch loss 0.616151333 epoch total loss 0.597527564\n",
      "Trained batch 1747 batch loss 0.589618385 epoch total loss 0.597523\n",
      "Trained batch 1748 batch loss 0.677888811 epoch total loss 0.597568929\n",
      "Trained batch 1749 batch loss 0.697240412 epoch total loss 0.597626\n",
      "Trained batch 1750 batch loss 0.667496562 epoch total loss 0.597665846\n",
      "Trained batch 1751 batch loss 0.583106399 epoch total loss 0.597657561\n",
      "Trained batch 1752 batch loss 0.608220696 epoch total loss 0.597663641\n",
      "Trained batch 1753 batch loss 0.620668471 epoch total loss 0.597676814\n",
      "Trained batch 1754 batch loss 0.568605721 epoch total loss 0.597660244\n",
      "Trained batch 1755 batch loss 0.679875493 epoch total loss 0.597707093\n",
      "Trained batch 1756 batch loss 0.7095505 epoch total loss 0.59777081\n",
      "Trained batch 1757 batch loss 0.6776402 epoch total loss 0.597816229\n",
      "Trained batch 1758 batch loss 0.521683633 epoch total loss 0.597772956\n",
      "Trained batch 1759 batch loss 0.606391251 epoch total loss 0.597777903\n",
      "Trained batch 1760 batch loss 0.588377833 epoch total loss 0.597772539\n",
      "Trained batch 1761 batch loss 0.525636673 epoch total loss 0.59773159\n",
      "Trained batch 1762 batch loss 0.57341373 epoch total loss 0.597717762\n",
      "Trained batch 1763 batch loss 0.549707 epoch total loss 0.597690523\n",
      "Trained batch 1764 batch loss 0.461044431 epoch total loss 0.597613096\n",
      "Trained batch 1765 batch loss 0.470171154 epoch total loss 0.597540915\n",
      "Trained batch 1766 batch loss 0.5919379 epoch total loss 0.597537696\n",
      "Trained batch 1767 batch loss 0.523961186 epoch total loss 0.597496033\n",
      "Trained batch 1768 batch loss 0.622431815 epoch total loss 0.597510159\n",
      "Trained batch 1769 batch loss 0.645017743 epoch total loss 0.597537\n",
      "Trained batch 1770 batch loss 0.571021914 epoch total loss 0.597522\n",
      "Trained batch 1771 batch loss 0.605612218 epoch total loss 0.59752661\n",
      "Trained batch 1772 batch loss 0.606449962 epoch total loss 0.597531617\n",
      "Trained batch 1773 batch loss 0.647406042 epoch total loss 0.59755981\n",
      "Trained batch 1774 batch loss 0.611814618 epoch total loss 0.597567856\n",
      "Trained batch 1775 batch loss 0.695523858 epoch total loss 0.59762305\n",
      "Trained batch 1776 batch loss 0.684887052 epoch total loss 0.597672224\n",
      "Trained batch 1777 batch loss 0.612602711 epoch total loss 0.597680569\n",
      "Trained batch 1778 batch loss 0.597828865 epoch total loss 0.597680628\n",
      "Trained batch 1779 batch loss 0.605555058 epoch total loss 0.597685099\n",
      "Trained batch 1780 batch loss 0.628261507 epoch total loss 0.597702265\n",
      "Trained batch 1781 batch loss 0.508707583 epoch total loss 0.597652256\n",
      "Trained batch 1782 batch loss 0.628739357 epoch total loss 0.597669721\n",
      "Trained batch 1783 batch loss 0.7027933 epoch total loss 0.59772867\n",
      "Trained batch 1784 batch loss 0.72944653 epoch total loss 0.59780252\n",
      "Trained batch 1785 batch loss 0.77403909 epoch total loss 0.597901285\n",
      "Trained batch 1786 batch loss 0.612336159 epoch total loss 0.597909331\n",
      "Trained batch 1787 batch loss 0.629714489 epoch total loss 0.597927153\n",
      "Trained batch 1788 batch loss 0.698958 epoch total loss 0.597983658\n",
      "Trained batch 1789 batch loss 0.572464883 epoch total loss 0.597969472\n",
      "Trained batch 1790 batch loss 0.630761504 epoch total loss 0.597987771\n",
      "Trained batch 1791 batch loss 0.535144508 epoch total loss 0.597952664\n",
      "Trained batch 1792 batch loss 0.580543756 epoch total loss 0.597942948\n",
      "Trained batch 1793 batch loss 0.590521097 epoch total loss 0.597938836\n",
      "Trained batch 1794 batch loss 0.586491406 epoch total loss 0.597932518\n",
      "Trained batch 1795 batch loss 0.461800098 epoch total loss 0.597856641\n",
      "Trained batch 1796 batch loss 0.575415969 epoch total loss 0.597844183\n",
      "Trained batch 1797 batch loss 0.650860965 epoch total loss 0.597873688\n",
      "Trained batch 1798 batch loss 0.631083727 epoch total loss 0.597892165\n",
      "Trained batch 1799 batch loss 0.60800004 epoch total loss 0.597897828\n",
      "Trained batch 1800 batch loss 0.584644675 epoch total loss 0.597890437\n",
      "Trained batch 1801 batch loss 0.528075814 epoch total loss 0.597851634\n",
      "Trained batch 1802 batch loss 0.524266839 epoch total loss 0.597810864\n",
      "Trained batch 1803 batch loss 0.539874613 epoch total loss 0.597778738\n",
      "Trained batch 1804 batch loss 0.454998434 epoch total loss 0.597699583\n",
      "Trained batch 1805 batch loss 0.529428 epoch total loss 0.597661734\n",
      "Trained batch 1806 batch loss 0.458594531 epoch total loss 0.597584724\n",
      "Trained batch 1807 batch loss 0.438120306 epoch total loss 0.59749651\n",
      "Trained batch 1808 batch loss 0.541415513 epoch total loss 0.597465456\n",
      "Trained batch 1809 batch loss 0.592339516 epoch total loss 0.597462595\n",
      "Trained batch 1810 batch loss 0.616820335 epoch total loss 0.597473264\n",
      "Trained batch 1811 batch loss 0.719638824 epoch total loss 0.597540736\n",
      "Trained batch 1812 batch loss 0.744135141 epoch total loss 0.59762162\n",
      "Trained batch 1813 batch loss 0.746763229 epoch total loss 0.597703874\n",
      "Trained batch 1814 batch loss 0.703110337 epoch total loss 0.597762\n",
      "Trained batch 1815 batch loss 0.793375969 epoch total loss 0.597869694\n",
      "Trained batch 1816 batch loss 0.651840448 epoch total loss 0.597899437\n",
      "Trained batch 1817 batch loss 0.628820717 epoch total loss 0.597916424\n",
      "Trained batch 1818 batch loss 0.697863519 epoch total loss 0.597971439\n",
      "Trained batch 1819 batch loss 0.657659054 epoch total loss 0.598004282\n",
      "Trained batch 1820 batch loss 0.719691575 epoch total loss 0.598071158\n",
      "Trained batch 1821 batch loss 0.681574285 epoch total loss 0.598117\n",
      "Trained batch 1822 batch loss 0.659131 epoch total loss 0.598150492\n",
      "Trained batch 1823 batch loss 0.600095749 epoch total loss 0.598151565\n",
      "Trained batch 1824 batch loss 0.639629364 epoch total loss 0.598174334\n",
      "Trained batch 1825 batch loss 0.611534238 epoch total loss 0.598181665\n",
      "Trained batch 1826 batch loss 0.570392907 epoch total loss 0.598166466\n",
      "Trained batch 1827 batch loss 0.614278913 epoch total loss 0.598175287\n",
      "Trained batch 1828 batch loss 0.675861 epoch total loss 0.598217785\n",
      "Trained batch 1829 batch loss 0.660321772 epoch total loss 0.5982517\n",
      "Trained batch 1830 batch loss 0.609365463 epoch total loss 0.59825778\n",
      "Trained batch 1831 batch loss 0.555500627 epoch total loss 0.598234475\n",
      "Trained batch 1832 batch loss 0.686384499 epoch total loss 0.598282576\n",
      "Trained batch 1833 batch loss 0.627053678 epoch total loss 0.598298311\n",
      "Trained batch 1834 batch loss 0.635631382 epoch total loss 0.598318636\n",
      "Trained batch 1835 batch loss 0.594163895 epoch total loss 0.598316371\n",
      "Trained batch 1836 batch loss 0.587586641 epoch total loss 0.59831053\n",
      "Trained batch 1837 batch loss 0.573884606 epoch total loss 0.598297238\n",
      "Trained batch 1838 batch loss 0.574181914 epoch total loss 0.598284125\n",
      "Trained batch 1839 batch loss 0.555129528 epoch total loss 0.598260701\n",
      "Trained batch 1840 batch loss 0.545619071 epoch total loss 0.59823209\n",
      "Trained batch 1841 batch loss 0.505307734 epoch total loss 0.598181605\n",
      "Trained batch 1842 batch loss 0.644652307 epoch total loss 0.598206818\n",
      "Trained batch 1843 batch loss 0.619592428 epoch total loss 0.598218441\n",
      "Trained batch 1844 batch loss 0.619063258 epoch total loss 0.598229706\n",
      "Trained batch 1845 batch loss 0.652367115 epoch total loss 0.598259032\n",
      "Trained batch 1846 batch loss 0.643399596 epoch total loss 0.598283529\n",
      "Trained batch 1847 batch loss 0.643417597 epoch total loss 0.598307967\n",
      "Trained batch 1848 batch loss 0.649652481 epoch total loss 0.598335743\n",
      "Trained batch 1849 batch loss 0.613651395 epoch total loss 0.598344\n",
      "Trained batch 1850 batch loss 0.610216856 epoch total loss 0.598350465\n",
      "Trained batch 1851 batch loss 0.564666927 epoch total loss 0.598332286\n",
      "Trained batch 1852 batch loss 0.600462079 epoch total loss 0.598333418\n",
      "Trained batch 1853 batch loss 0.626721144 epoch total loss 0.598348737\n",
      "Trained batch 1854 batch loss 0.657885969 epoch total loss 0.598380864\n",
      "Trained batch 1855 batch loss 0.527658463 epoch total loss 0.598342717\n",
      "Trained batch 1856 batch loss 0.520779788 epoch total loss 0.598300934\n",
      "Trained batch 1857 batch loss 0.489712209 epoch total loss 0.598242462\n",
      "Trained batch 1858 batch loss 0.502035 epoch total loss 0.598190725\n",
      "Trained batch 1859 batch loss 0.626401782 epoch total loss 0.598205864\n",
      "Trained batch 1860 batch loss 0.598561704 epoch total loss 0.598206043\n",
      "Trained batch 1861 batch loss 0.736184 epoch total loss 0.598280191\n",
      "Trained batch 1862 batch loss 0.67704165 epoch total loss 0.598322451\n",
      "Trained batch 1863 batch loss 0.654763818 epoch total loss 0.59835279\n",
      "Trained batch 1864 batch loss 0.610630274 epoch total loss 0.598359346\n",
      "Trained batch 1865 batch loss 0.559244573 epoch total loss 0.598338366\n",
      "Trained batch 1866 batch loss 0.627075 epoch total loss 0.598353744\n",
      "Trained batch 1867 batch loss 0.551857471 epoch total loss 0.598328829\n",
      "Trained batch 1868 batch loss 0.63678211 epoch total loss 0.598349452\n",
      "Trained batch 1869 batch loss 0.627223492 epoch total loss 0.59836489\n",
      "Trained batch 1870 batch loss 0.596554339 epoch total loss 0.598363936\n",
      "Trained batch 1871 batch loss 0.676875234 epoch total loss 0.598405898\n",
      "Trained batch 1872 batch loss 0.588311732 epoch total loss 0.598400474\n",
      "Trained batch 1873 batch loss 0.518003166 epoch total loss 0.598357499\n",
      "Trained batch 1874 batch loss 0.589401305 epoch total loss 0.59835273\n",
      "Trained batch 1875 batch loss 0.780535579 epoch total loss 0.598449886\n",
      "Trained batch 1876 batch loss 0.737050831 epoch total loss 0.598523736\n",
      "Trained batch 1877 batch loss 0.65350157 epoch total loss 0.598553\n",
      "Trained batch 1878 batch loss 0.592927 epoch total loss 0.59855\n",
      "Trained batch 1879 batch loss 0.59905082 epoch total loss 0.59855026\n",
      "Trained batch 1880 batch loss 0.576333463 epoch total loss 0.598538399\n",
      "Trained batch 1881 batch loss 0.789329 epoch total loss 0.598639846\n",
      "Trained batch 1882 batch loss 0.633879781 epoch total loss 0.598658562\n",
      "Trained batch 1883 batch loss 0.703825951 epoch total loss 0.598714411\n",
      "Trained batch 1884 batch loss 0.684574 epoch total loss 0.59876\n",
      "Trained batch 1885 batch loss 0.680365622 epoch total loss 0.598803341\n",
      "Trained batch 1886 batch loss 0.600797892 epoch total loss 0.598804414\n",
      "Trained batch 1887 batch loss 0.610527277 epoch total loss 0.598810554\n",
      "Trained batch 1888 batch loss 0.664760828 epoch total loss 0.598845541\n",
      "Trained batch 1889 batch loss 0.541274905 epoch total loss 0.598815\n",
      "Trained batch 1890 batch loss 0.630757272 epoch total loss 0.598831952\n",
      "Trained batch 1891 batch loss 0.560538411 epoch total loss 0.598811686\n",
      "Trained batch 1892 batch loss 0.575630367 epoch total loss 0.598799467\n",
      "Trained batch 1893 batch loss 0.568478346 epoch total loss 0.598783433\n",
      "Trained batch 1894 batch loss 0.626941502 epoch total loss 0.598798335\n",
      "Trained batch 1895 batch loss 0.635247588 epoch total loss 0.598817587\n",
      "Trained batch 1896 batch loss 0.605516434 epoch total loss 0.598821044\n",
      "Trained batch 1897 batch loss 0.621511757 epoch total loss 0.598833\n",
      "Trained batch 1898 batch loss 0.621259809 epoch total loss 0.598844767\n",
      "Trained batch 1899 batch loss 0.629016519 epoch total loss 0.598860681\n",
      "Trained batch 1900 batch loss 0.569676578 epoch total loss 0.598845363\n",
      "Trained batch 1901 batch loss 0.555858493 epoch total loss 0.598822773\n",
      "Trained batch 1902 batch loss 0.587458432 epoch total loss 0.598816752\n",
      "Trained batch 1903 batch loss 0.60481745 epoch total loss 0.598819911\n",
      "Trained batch 1904 batch loss 0.577223063 epoch total loss 0.598808587\n",
      "Trained batch 1905 batch loss 0.564950585 epoch total loss 0.598790824\n",
      "Trained batch 1906 batch loss 0.618330538 epoch total loss 0.598801076\n",
      "Trained batch 1907 batch loss 0.573816657 epoch total loss 0.598787963\n",
      "Trained batch 1908 batch loss 0.595321715 epoch total loss 0.598786175\n",
      "Trained batch 1909 batch loss 0.596388102 epoch total loss 0.598784924\n",
      "Trained batch 1910 batch loss 0.568525076 epoch total loss 0.598769069\n",
      "Trained batch 1911 batch loss 0.531420887 epoch total loss 0.598733783\n",
      "Trained batch 1912 batch loss 0.591249108 epoch total loss 0.598729908\n",
      "Trained batch 1913 batch loss 0.609997094 epoch total loss 0.598735809\n",
      "Trained batch 1914 batch loss 0.600026071 epoch total loss 0.598736465\n",
      "Trained batch 1915 batch loss 0.625374854 epoch total loss 0.598750353\n",
      "Trained batch 1916 batch loss 0.647599 epoch total loss 0.598775864\n",
      "Trained batch 1917 batch loss 0.607212543 epoch total loss 0.598780215\n",
      "Trained batch 1918 batch loss 0.638720393 epoch total loss 0.598801\n",
      "Trained batch 1919 batch loss 0.633812904 epoch total loss 0.598819256\n",
      "Trained batch 1920 batch loss 0.593713164 epoch total loss 0.598816633\n",
      "Trained batch 1921 batch loss 0.563403964 epoch total loss 0.598798156\n",
      "Trained batch 1922 batch loss 0.657292724 epoch total loss 0.598828614\n",
      "Trained batch 1923 batch loss 0.564555168 epoch total loss 0.598810792\n",
      "Trained batch 1924 batch loss 0.656594753 epoch total loss 0.598840833\n",
      "Trained batch 1925 batch loss 0.640272677 epoch total loss 0.59886235\n",
      "Trained batch 1926 batch loss 0.631188631 epoch total loss 0.598879158\n",
      "Trained batch 1927 batch loss 0.561650932 epoch total loss 0.598859847\n",
      "Trained batch 1928 batch loss 0.592265546 epoch total loss 0.598856449\n",
      "Trained batch 1929 batch loss 0.649554 epoch total loss 0.598882735\n",
      "Trained batch 1930 batch loss 0.687773824 epoch total loss 0.59892875\n",
      "Trained batch 1931 batch loss 0.582419693 epoch total loss 0.598920166\n",
      "Trained batch 1932 batch loss 0.647134125 epoch total loss 0.598945141\n",
      "Trained batch 1933 batch loss 0.564128399 epoch total loss 0.598927081\n",
      "Trained batch 1934 batch loss 0.647521377 epoch total loss 0.598952174\n",
      "Trained batch 1935 batch loss 0.62230891 epoch total loss 0.598964274\n",
      "Trained batch 1936 batch loss 0.690657616 epoch total loss 0.59901166\n",
      "Trained batch 1937 batch loss 0.598074 epoch total loss 0.599011123\n",
      "Trained batch 1938 batch loss 0.525654495 epoch total loss 0.598973274\n",
      "Trained batch 1939 batch loss 0.559460878 epoch total loss 0.598952889\n",
      "Trained batch 1940 batch loss 0.605026901 epoch total loss 0.598956\n",
      "Trained batch 1941 batch loss 0.603123546 epoch total loss 0.598958135\n",
      "Trained batch 1942 batch loss 0.642966926 epoch total loss 0.598980784\n",
      "Trained batch 1943 batch loss 0.553244114 epoch total loss 0.598957241\n",
      "Trained batch 1944 batch loss 0.648931861 epoch total loss 0.59898293\n",
      "Trained batch 1945 batch loss 0.64691174 epoch total loss 0.599007607\n",
      "Trained batch 1946 batch loss 0.587024331 epoch total loss 0.599001467\n",
      "Trained batch 1947 batch loss 0.621416271 epoch total loss 0.599013031\n",
      "Trained batch 1948 batch loss 0.572825193 epoch total loss 0.59899956\n",
      "Trained batch 1949 batch loss 0.525005877 epoch total loss 0.598961651\n",
      "Trained batch 1950 batch loss 0.515942037 epoch total loss 0.598919094\n",
      "Trained batch 1951 batch loss 0.544998467 epoch total loss 0.598891497\n",
      "Trained batch 1952 batch loss 0.623169839 epoch total loss 0.598903894\n",
      "Trained batch 1953 batch loss 0.598715365 epoch total loss 0.598903835\n",
      "Trained batch 1954 batch loss 0.607969344 epoch total loss 0.598908424\n",
      "Trained batch 1955 batch loss 0.640084922 epoch total loss 0.598929524\n",
      "Trained batch 1956 batch loss 0.59175539 epoch total loss 0.598925889\n",
      "Trained batch 1957 batch loss 0.558732629 epoch total loss 0.598905325\n",
      "Trained batch 1958 batch loss 0.537914693 epoch total loss 0.598874211\n",
      "Trained batch 1959 batch loss 0.52494216 epoch total loss 0.598836422\n",
      "Trained batch 1960 batch loss 0.537172735 epoch total loss 0.598805\n",
      "Trained batch 1961 batch loss 0.536857545 epoch total loss 0.59877342\n",
      "Trained batch 1962 batch loss 0.521978259 epoch total loss 0.59873426\n",
      "Trained batch 1963 batch loss 0.495065838 epoch total loss 0.598681509\n",
      "Trained batch 1964 batch loss 0.516692042 epoch total loss 0.598639786\n",
      "Trained batch 1965 batch loss 0.577093422 epoch total loss 0.598628819\n",
      "Trained batch 1966 batch loss 0.484394044 epoch total loss 0.598570704\n",
      "Trained batch 1967 batch loss 0.468928665 epoch total loss 0.598504782\n",
      "Trained batch 1968 batch loss 0.563524485 epoch total loss 0.59848696\n",
      "Trained batch 1969 batch loss 0.614147842 epoch total loss 0.598494947\n",
      "Trained batch 1970 batch loss 0.443605214 epoch total loss 0.598416328\n",
      "Trained batch 1971 batch loss 0.661038697 epoch total loss 0.598448038\n",
      "Trained batch 1972 batch loss 0.564932764 epoch total loss 0.598431051\n",
      "Trained batch 1973 batch loss 0.568318427 epoch total loss 0.598415852\n",
      "Trained batch 1974 batch loss 0.536453068 epoch total loss 0.59838444\n",
      "Trained batch 1975 batch loss 0.502242684 epoch total loss 0.598335743\n",
      "Trained batch 1976 batch loss 0.505607903 epoch total loss 0.598288834\n",
      "Trained batch 1977 batch loss 0.610971391 epoch total loss 0.598295271\n",
      "Trained batch 1978 batch loss 0.556730568 epoch total loss 0.598274231\n",
      "Trained batch 1979 batch loss 0.587308407 epoch total loss 0.598268688\n",
      "Trained batch 1980 batch loss 0.696279764 epoch total loss 0.598318219\n",
      "Trained batch 1981 batch loss 0.658001 epoch total loss 0.59834832\n",
      "Trained batch 1982 batch loss 0.584225953 epoch total loss 0.598341167\n",
      "Trained batch 1983 batch loss 0.634955347 epoch total loss 0.598359704\n",
      "Trained batch 1984 batch loss 0.61468035 epoch total loss 0.59836787\n",
      "Trained batch 1985 batch loss 0.65995 epoch total loss 0.598398864\n",
      "Trained batch 1986 batch loss 0.59172982 epoch total loss 0.598395467\n",
      "Trained batch 1987 batch loss 0.574130177 epoch total loss 0.598383248\n",
      "Trained batch 1988 batch loss 0.568288207 epoch total loss 0.598368108\n",
      "Trained batch 1989 batch loss 0.573401809 epoch total loss 0.598355532\n",
      "Trained batch 1990 batch loss 0.649588406 epoch total loss 0.598381221\n",
      "Trained batch 1991 batch loss 0.535393357 epoch total loss 0.598349631\n",
      "Trained batch 1992 batch loss 0.566799879 epoch total loss 0.598333776\n",
      "Trained batch 1993 batch loss 0.585313857 epoch total loss 0.598327219\n",
      "Trained batch 1994 batch loss 0.523668826 epoch total loss 0.598289788\n",
      "Trained batch 1995 batch loss 0.555178881 epoch total loss 0.598268211\n",
      "Trained batch 1996 batch loss 0.620490789 epoch total loss 0.598279297\n",
      "Trained batch 1997 batch loss 0.633419752 epoch total loss 0.59829694\n",
      "Trained batch 1998 batch loss 0.623650312 epoch total loss 0.598309636\n",
      "Trained batch 1999 batch loss 0.609597206 epoch total loss 0.598315299\n",
      "Trained batch 2000 batch loss 0.579678953 epoch total loss 0.598305941\n",
      "Trained batch 2001 batch loss 0.552809656 epoch total loss 0.598283231\n",
      "Trained batch 2002 batch loss 0.523533046 epoch total loss 0.598245919\n",
      "Trained batch 2003 batch loss 0.418653756 epoch total loss 0.598156273\n",
      "Trained batch 2004 batch loss 0.529232502 epoch total loss 0.598121881\n",
      "Trained batch 2005 batch loss 0.552737355 epoch total loss 0.598099232\n",
      "Trained batch 2006 batch loss 0.506567895 epoch total loss 0.598053634\n",
      "Trained batch 2007 batch loss 0.534864783 epoch total loss 0.598022163\n",
      "Trained batch 2008 batch loss 0.545412302 epoch total loss 0.597995937\n",
      "Trained batch 2009 batch loss 0.518457651 epoch total loss 0.597956359\n",
      "Trained batch 2010 batch loss 0.590359032 epoch total loss 0.597952545\n",
      "Trained batch 2011 batch loss 0.625595629 epoch total loss 0.597966313\n",
      "Trained batch 2012 batch loss 0.601029158 epoch total loss 0.597967863\n",
      "Trained batch 2013 batch loss 0.693945944 epoch total loss 0.598015547\n",
      "Trained batch 2014 batch loss 0.617000222 epoch total loss 0.598024964\n",
      "Trained batch 2015 batch loss 0.498796403 epoch total loss 0.597975671\n",
      "Trained batch 2016 batch loss 0.56816256 epoch total loss 0.597960889\n",
      "Trained batch 2017 batch loss 0.589662 epoch total loss 0.597956777\n",
      "Trained batch 2018 batch loss 0.619126379 epoch total loss 0.597967267\n",
      "Trained batch 2019 batch loss 0.618060589 epoch total loss 0.597977221\n",
      "Trained batch 2020 batch loss 0.598649383 epoch total loss 0.597977579\n",
      "Trained batch 2021 batch loss 0.642296731 epoch total loss 0.597999513\n",
      "Trained batch 2022 batch loss 0.688364863 epoch total loss 0.598044217\n",
      "Trained batch 2023 batch loss 0.737306178 epoch total loss 0.59811306\n",
      "Trained batch 2024 batch loss 0.80478406 epoch total loss 0.598215163\n",
      "Trained batch 2025 batch loss 0.717471719 epoch total loss 0.598274052\n",
      "Trained batch 2026 batch loss 0.687562287 epoch total loss 0.59831816\n",
      "Trained batch 2027 batch loss 0.719290376 epoch total loss 0.598377824\n",
      "Trained batch 2028 batch loss 0.588172078 epoch total loss 0.598372757\n",
      "Trained batch 2029 batch loss 0.548925757 epoch total loss 0.598348439\n",
      "Trained batch 2030 batch loss 0.505843163 epoch total loss 0.598302841\n",
      "Trained batch 2031 batch loss 0.526887774 epoch total loss 0.598267674\n",
      "Trained batch 2032 batch loss 0.570881546 epoch total loss 0.598254204\n",
      "Trained batch 2033 batch loss 0.667918742 epoch total loss 0.598288536\n",
      "Trained batch 2034 batch loss 0.675679684 epoch total loss 0.598326564\n",
      "Trained batch 2035 batch loss 0.74492 epoch total loss 0.598398566\n",
      "Trained batch 2036 batch loss 0.703375816 epoch total loss 0.598450124\n",
      "Trained batch 2037 batch loss 0.667466044 epoch total loss 0.598484039\n",
      "Trained batch 2038 batch loss 0.623677135 epoch total loss 0.598496377\n",
      "Trained batch 2039 batch loss 0.537568092 epoch total loss 0.598466516\n",
      "Trained batch 2040 batch loss 0.555079937 epoch total loss 0.598445237\n",
      "Trained batch 2041 batch loss 0.576876044 epoch total loss 0.598434687\n",
      "Trained batch 2042 batch loss 0.522636235 epoch total loss 0.598397493\n",
      "Trained batch 2043 batch loss 0.591992736 epoch total loss 0.598394394\n",
      "Trained batch 2044 batch loss 0.601311207 epoch total loss 0.598395824\n",
      "Trained batch 2045 batch loss 0.601579189 epoch total loss 0.598397374\n",
      "Trained batch 2046 batch loss 0.679640174 epoch total loss 0.59843713\n",
      "Trained batch 2047 batch loss 0.58662504 epoch total loss 0.598431349\n",
      "Trained batch 2048 batch loss 0.620833218 epoch total loss 0.598442316\n",
      "Trained batch 2049 batch loss 0.615341485 epoch total loss 0.598450541\n",
      "Trained batch 2050 batch loss 0.542405784 epoch total loss 0.598423183\n",
      "Trained batch 2051 batch loss 0.609517574 epoch total loss 0.598428607\n",
      "Trained batch 2052 batch loss 0.565094709 epoch total loss 0.598412335\n",
      "Trained batch 2053 batch loss 0.576664507 epoch total loss 0.598401725\n",
      "Trained batch 2054 batch loss 0.510748267 epoch total loss 0.598359048\n",
      "Trained batch 2055 batch loss 0.559420884 epoch total loss 0.598340154\n",
      "Trained batch 2056 batch loss 0.618667 epoch total loss 0.598350048\n",
      "Trained batch 2057 batch loss 0.593983889 epoch total loss 0.598347902\n",
      "Trained batch 2058 batch loss 0.601935506 epoch total loss 0.598349631\n",
      "Trained batch 2059 batch loss 0.598322 epoch total loss 0.598349631\n",
      "Trained batch 2060 batch loss 0.504796624 epoch total loss 0.598304152\n",
      "Trained batch 2061 batch loss 0.53735137 epoch total loss 0.598274589\n",
      "Trained batch 2062 batch loss 0.609055 epoch total loss 0.598279774\n",
      "Trained batch 2063 batch loss 0.506703854 epoch total loss 0.598235428\n",
      "Trained batch 2064 batch loss 0.524728298 epoch total loss 0.598199844\n",
      "Trained batch 2065 batch loss 0.585279703 epoch total loss 0.598193586\n",
      "Trained batch 2066 batch loss 0.516038477 epoch total loss 0.59815383\n",
      "Trained batch 2067 batch loss 0.714651763 epoch total loss 0.598210156\n",
      "Trained batch 2068 batch loss 0.670900047 epoch total loss 0.598245263\n",
      "Trained batch 2069 batch loss 0.65325743 epoch total loss 0.598271847\n",
      "Trained batch 2070 batch loss 0.574552774 epoch total loss 0.598260403\n",
      "Trained batch 2071 batch loss 0.55739677 epoch total loss 0.598240674\n",
      "Trained batch 2072 batch loss 0.594416618 epoch total loss 0.598238766\n",
      "Trained batch 2073 batch loss 0.539605379 epoch total loss 0.598210454\n",
      "Trained batch 2074 batch loss 0.527450621 epoch total loss 0.59817636\n",
      "Trained batch 2075 batch loss 0.451560557 epoch total loss 0.598105729\n",
      "Trained batch 2076 batch loss 0.525454283 epoch total loss 0.598070741\n",
      "Trained batch 2077 batch loss 0.408842623 epoch total loss 0.597979605\n",
      "Trained batch 2078 batch loss 0.398986548 epoch total loss 0.597883821\n",
      "Trained batch 2079 batch loss 0.446306437 epoch total loss 0.597810924\n",
      "Trained batch 2080 batch loss 0.527986288 epoch total loss 0.597777307\n",
      "Trained batch 2081 batch loss 0.548055 epoch total loss 0.597753465\n",
      "Trained batch 2082 batch loss 0.506183922 epoch total loss 0.597709477\n",
      "Trained batch 2083 batch loss 0.68953383 epoch total loss 0.597753584\n",
      "Trained batch 2084 batch loss 0.696089566 epoch total loss 0.597800732\n",
      "Trained batch 2085 batch loss 0.578070939 epoch total loss 0.597791314\n",
      "Trained batch 2086 batch loss 0.538480759 epoch total loss 0.597762883\n",
      "Trained batch 2087 batch loss 0.626003087 epoch total loss 0.597776413\n",
      "Trained batch 2088 batch loss 0.630012929 epoch total loss 0.597791851\n",
      "Trained batch 2089 batch loss 0.614986122 epoch total loss 0.597800076\n",
      "Trained batch 2090 batch loss 0.632324815 epoch total loss 0.597816586\n",
      "Trained batch 2091 batch loss 0.577120841 epoch total loss 0.597806692\n",
      "Trained batch 2092 batch loss 0.477355868 epoch total loss 0.597749114\n",
      "Trained batch 2093 batch loss 0.527337313 epoch total loss 0.597715437\n",
      "Trained batch 2094 batch loss 0.507391095 epoch total loss 0.597672343\n",
      "Trained batch 2095 batch loss 0.600741148 epoch total loss 0.597673774\n",
      "Trained batch 2096 batch loss 0.574500263 epoch total loss 0.597662687\n",
      "Trained batch 2097 batch loss 0.565642416 epoch total loss 0.597647488\n",
      "Trained batch 2098 batch loss 0.657761216 epoch total loss 0.597676098\n",
      "Trained batch 2099 batch loss 0.619409621 epoch total loss 0.59768641\n",
      "Trained batch 2100 batch loss 0.584000587 epoch total loss 0.597679913\n",
      "Trained batch 2101 batch loss 0.57504648 epoch total loss 0.597669125\n",
      "Trained batch 2102 batch loss 0.54697 epoch total loss 0.597645044\n",
      "Trained batch 2103 batch loss 0.610872209 epoch total loss 0.597651303\n",
      "Trained batch 2104 batch loss 0.585642219 epoch total loss 0.59764564\n",
      "Trained batch 2105 batch loss 0.613478 epoch total loss 0.597653151\n",
      "Trained batch 2106 batch loss 0.636530936 epoch total loss 0.597671628\n",
      "Trained batch 2107 batch loss 0.631744564 epoch total loss 0.597687781\n",
      "Trained batch 2108 batch loss 0.566744566 epoch total loss 0.597673118\n",
      "Trained batch 2109 batch loss 0.542645 epoch total loss 0.597647\n",
      "Trained batch 2110 batch loss 0.534501433 epoch total loss 0.59761709\n",
      "Trained batch 2111 batch loss 0.588986456 epoch total loss 0.597613\n",
      "Trained batch 2112 batch loss 0.629613101 epoch total loss 0.597628176\n",
      "Trained batch 2113 batch loss 0.598028302 epoch total loss 0.597628355\n",
      "Trained batch 2114 batch loss 0.613565326 epoch total loss 0.597635865\n",
      "Trained batch 2115 batch loss 0.543267667 epoch total loss 0.597610116\n",
      "Trained batch 2116 batch loss 0.64738214 epoch total loss 0.59763366\n",
      "Trained batch 2117 batch loss 0.613654554 epoch total loss 0.59764123\n",
      "Trained batch 2118 batch loss 0.635291278 epoch total loss 0.597658932\n",
      "Trained batch 2119 batch loss 0.537739873 epoch total loss 0.59763068\n",
      "Trained batch 2120 batch loss 0.498000741 epoch total loss 0.597583711\n",
      "Trained batch 2121 batch loss 0.513635278 epoch total loss 0.597544134\n",
      "Trained batch 2122 batch loss 0.511799335 epoch total loss 0.597503722\n",
      "Trained batch 2123 batch loss 0.676662683 epoch total loss 0.597541034\n",
      "Trained batch 2124 batch loss 0.64036876 epoch total loss 0.597561181\n",
      "Trained batch 2125 batch loss 0.585310578 epoch total loss 0.597555459\n",
      "Trained batch 2126 batch loss 0.582468271 epoch total loss 0.597548366\n",
      "Trained batch 2127 batch loss 0.660080433 epoch total loss 0.597577751\n",
      "Trained batch 2128 batch loss 0.603988111 epoch total loss 0.597580731\n",
      "Trained batch 2129 batch loss 0.62734282 epoch total loss 0.597594738\n",
      "Trained batch 2130 batch loss 0.581240416 epoch total loss 0.597587049\n",
      "Trained batch 2131 batch loss 0.632113338 epoch total loss 0.597603261\n",
      "Trained batch 2132 batch loss 0.646203518 epoch total loss 0.59762609\n",
      "Trained batch 2133 batch loss 0.650971651 epoch total loss 0.597651124\n",
      "Trained batch 2134 batch loss 0.626013041 epoch total loss 0.597664356\n",
      "Trained batch 2135 batch loss 0.514960706 epoch total loss 0.597625673\n",
      "Trained batch 2136 batch loss 0.501982391 epoch total loss 0.59758085\n",
      "Trained batch 2137 batch loss 0.508785307 epoch total loss 0.597539306\n",
      "Trained batch 2138 batch loss 0.539814353 epoch total loss 0.597512305\n",
      "Trained batch 2139 batch loss 0.488143295 epoch total loss 0.597461164\n",
      "Trained batch 2140 batch loss 0.574784279 epoch total loss 0.597450614\n",
      "Trained batch 2141 batch loss 0.637101 epoch total loss 0.597469151\n",
      "Trained batch 2142 batch loss 0.6543957 epoch total loss 0.597495735\n",
      "Trained batch 2143 batch loss 0.64040494 epoch total loss 0.597515702\n",
      "Trained batch 2144 batch loss 0.685319066 epoch total loss 0.597556651\n",
      "Trained batch 2145 batch loss 0.69201225 epoch total loss 0.597600698\n",
      "Trained batch 2146 batch loss 0.647760153 epoch total loss 0.597624063\n",
      "Trained batch 2147 batch loss 0.569720685 epoch total loss 0.59761107\n",
      "Trained batch 2148 batch loss 0.701711 epoch total loss 0.597659469\n",
      "Trained batch 2149 batch loss 0.643786669 epoch total loss 0.597681\n",
      "Trained batch 2150 batch loss 0.667370439 epoch total loss 0.597713351\n",
      "Trained batch 2151 batch loss 0.592407 epoch total loss 0.597710907\n",
      "Trained batch 2152 batch loss 0.592277 epoch total loss 0.597708404\n",
      "Trained batch 2153 batch loss 0.63104707 epoch total loss 0.597723901\n",
      "Trained batch 2154 batch loss 0.646987557 epoch total loss 0.59774673\n",
      "Trained batch 2155 batch loss 0.596525788 epoch total loss 0.597746193\n",
      "Trained batch 2156 batch loss 0.611806214 epoch total loss 0.59775275\n",
      "Trained batch 2157 batch loss 0.665964544 epoch total loss 0.5977844\n",
      "Trained batch 2158 batch loss 0.576388597 epoch total loss 0.597774446\n",
      "Trained batch 2159 batch loss 0.663850784 epoch total loss 0.597805083\n",
      "Trained batch 2160 batch loss 0.640084386 epoch total loss 0.597824633\n",
      "Trained batch 2161 batch loss 0.604703 epoch total loss 0.597827852\n",
      "Trained batch 2162 batch loss 0.668386102 epoch total loss 0.597860456\n",
      "Trained batch 2163 batch loss 0.606486261 epoch total loss 0.597864449\n",
      "Trained batch 2164 batch loss 0.701219678 epoch total loss 0.597912192\n",
      "Trained batch 2165 batch loss 0.639947116 epoch total loss 0.597931564\n",
      "Trained batch 2166 batch loss 0.576375425 epoch total loss 0.59792161\n",
      "Trained batch 2167 batch loss 0.647762418 epoch total loss 0.597944617\n",
      "Trained batch 2168 batch loss 0.660518944 epoch total loss 0.597973466\n",
      "Trained batch 2169 batch loss 0.598963916 epoch total loss 0.597973943\n",
      "Trained batch 2170 batch loss 0.547671616 epoch total loss 0.597950816\n",
      "Trained batch 2171 batch loss 0.623452127 epoch total loss 0.597962499\n",
      "Trained batch 2172 batch loss 0.609779119 epoch total loss 0.597967923\n",
      "Trained batch 2173 batch loss 0.554462194 epoch total loss 0.597947896\n",
      "Trained batch 2174 batch loss 0.606511354 epoch total loss 0.597951889\n",
      "Trained batch 2175 batch loss 0.599647641 epoch total loss 0.597952664\n",
      "Trained batch 2176 batch loss 0.56976223 epoch total loss 0.59793967\n",
      "Trained batch 2177 batch loss 0.569996774 epoch total loss 0.597926795\n",
      "Trained batch 2178 batch loss 0.543565512 epoch total loss 0.597901821\n",
      "Trained batch 2179 batch loss 0.506110668 epoch total loss 0.59785974\n",
      "Trained batch 2180 batch loss 0.643181443 epoch total loss 0.597880483\n",
      "Trained batch 2181 batch loss 0.620721102 epoch total loss 0.597891\n",
      "Trained batch 2182 batch loss 0.574367762 epoch total loss 0.597880185\n",
      "Trained batch 2183 batch loss 0.560824573 epoch total loss 0.597863197\n",
      "Trained batch 2184 batch loss 0.598565578 epoch total loss 0.597863495\n",
      "Trained batch 2185 batch loss 0.526216507 epoch total loss 0.597830713\n",
      "Trained batch 2186 batch loss 0.624414086 epoch total loss 0.597842872\n",
      "Trained batch 2187 batch loss 0.693303704 epoch total loss 0.597886562\n",
      "Trained batch 2188 batch loss 0.595354199 epoch total loss 0.59788537\n",
      "Trained batch 2189 batch loss 0.694512188 epoch total loss 0.597929478\n",
      "Trained batch 2190 batch loss 0.678586543 epoch total loss 0.597966313\n",
      "Trained batch 2191 batch loss 0.651234865 epoch total loss 0.597990632\n",
      "Trained batch 2192 batch loss 0.63177067 epoch total loss 0.598006\n",
      "Trained batch 2193 batch loss 0.629695475 epoch total loss 0.598020434\n",
      "Trained batch 2194 batch loss 0.70644778 epoch total loss 0.598069847\n",
      "Trained batch 2195 batch loss 0.648808062 epoch total loss 0.598093\n",
      "Trained batch 2196 batch loss 0.660449505 epoch total loss 0.598121345\n",
      "Trained batch 2197 batch loss 0.662920713 epoch total loss 0.598150849\n",
      "Trained batch 2198 batch loss 0.700174153 epoch total loss 0.598197281\n",
      "Trained batch 2199 batch loss 0.603099406 epoch total loss 0.598199546\n",
      "Trained batch 2200 batch loss 0.560930371 epoch total loss 0.598182619\n",
      "Trained batch 2201 batch loss 0.564459562 epoch total loss 0.598167241\n",
      "Trained batch 2202 batch loss 0.552154541 epoch total loss 0.598146379\n",
      "Trained batch 2203 batch loss 0.630620718 epoch total loss 0.598161101\n",
      "Trained batch 2204 batch loss 0.577758551 epoch total loss 0.598151863\n",
      "Trained batch 2205 batch loss 0.551363528 epoch total loss 0.598130643\n",
      "Trained batch 2206 batch loss 0.488056391 epoch total loss 0.598080754\n",
      "Trained batch 2207 batch loss 0.526410043 epoch total loss 0.59804821\n",
      "Trained batch 2208 batch loss 0.513671756 epoch total loss 0.59801\n",
      "Trained batch 2209 batch loss 0.590999305 epoch total loss 0.598006845\n",
      "Trained batch 2210 batch loss 0.587015867 epoch total loss 0.598001838\n",
      "Trained batch 2211 batch loss 0.581741691 epoch total loss 0.597994506\n",
      "Trained batch 2212 batch loss 0.600241721 epoch total loss 0.59799552\n",
      "Trained batch 2213 batch loss 0.561111212 epoch total loss 0.59797889\n",
      "Trained batch 2214 batch loss 0.665642679 epoch total loss 0.598009467\n",
      "Trained batch 2215 batch loss 0.614295125 epoch total loss 0.598016798\n",
      "Trained batch 2216 batch loss 0.548038185 epoch total loss 0.597994268\n",
      "Trained batch 2217 batch loss 0.598373532 epoch total loss 0.597994447\n",
      "Trained batch 2218 batch loss 0.547056913 epoch total loss 0.597971439\n",
      "Trained batch 2219 batch loss 0.577866733 epoch total loss 0.597962379\n",
      "Trained batch 2220 batch loss 0.661652863 epoch total loss 0.597991049\n",
      "Trained batch 2221 batch loss 0.620207787 epoch total loss 0.598001063\n",
      "Trained batch 2222 batch loss 0.711964846 epoch total loss 0.598052323\n",
      "Trained batch 2223 batch loss 0.653177321 epoch total loss 0.598077178\n",
      "Trained batch 2224 batch loss 0.655593395 epoch total loss 0.598103046\n",
      "Trained batch 2225 batch loss 0.648567796 epoch total loss 0.598125696\n",
      "Trained batch 2226 batch loss 0.662827373 epoch total loss 0.598154783\n",
      "Trained batch 2227 batch loss 0.577536881 epoch total loss 0.598145545\n",
      "Trained batch 2228 batch loss 0.604026377 epoch total loss 0.598148167\n",
      "Trained batch 2229 batch loss 0.609643161 epoch total loss 0.598153293\n",
      "Trained batch 2230 batch loss 0.584339261 epoch total loss 0.598147094\n",
      "Trained batch 2231 batch loss 0.561656952 epoch total loss 0.598130763\n",
      "Trained batch 2232 batch loss 0.538358152 epoch total loss 0.59810394\n",
      "Trained batch 2233 batch loss 0.54242754 epoch total loss 0.598079\n",
      "Trained batch 2234 batch loss 0.640584767 epoch total loss 0.598098099\n",
      "Trained batch 2235 batch loss 0.745516241 epoch total loss 0.598164\n",
      "Trained batch 2236 batch loss 0.598318458 epoch total loss 0.598164082\n",
      "Trained batch 2237 batch loss 0.667890191 epoch total loss 0.598195255\n",
      "Trained batch 2238 batch loss 0.726937354 epoch total loss 0.598252773\n",
      "Trained batch 2239 batch loss 0.704669356 epoch total loss 0.598300278\n",
      "Trained batch 2240 batch loss 0.733775437 epoch total loss 0.598360777\n",
      "Trained batch 2241 batch loss 0.701306462 epoch total loss 0.598406672\n",
      "Trained batch 2242 batch loss 0.529822588 epoch total loss 0.598376095\n",
      "Trained batch 2243 batch loss 0.574893713 epoch total loss 0.598365664\n",
      "Trained batch 2244 batch loss 0.55339241 epoch total loss 0.598345578\n",
      "Trained batch 2245 batch loss 0.728155 epoch total loss 0.598403394\n",
      "Trained batch 2246 batch loss 0.637899876 epoch total loss 0.598421037\n",
      "Trained batch 2247 batch loss 0.632921398 epoch total loss 0.598436356\n",
      "Trained batch 2248 batch loss 0.650904775 epoch total loss 0.598459721\n",
      "Trained batch 2249 batch loss 0.644958317 epoch total loss 0.598480344\n",
      "Trained batch 2250 batch loss 0.693500161 epoch total loss 0.598522544\n",
      "Trained batch 2251 batch loss 0.644192874 epoch total loss 0.598542869\n",
      "Trained batch 2252 batch loss 0.66202873 epoch total loss 0.598571\n",
      "Trained batch 2253 batch loss 0.591580391 epoch total loss 0.598567903\n",
      "Trained batch 2254 batch loss 0.456127197 epoch total loss 0.598504722\n",
      "Trained batch 2255 batch loss 0.487748086 epoch total loss 0.598455608\n",
      "Trained batch 2256 batch loss 0.493318826 epoch total loss 0.598409\n",
      "Trained batch 2257 batch loss 0.600200951 epoch total loss 0.598409832\n",
      "Trained batch 2258 batch loss 0.627627552 epoch total loss 0.598422766\n",
      "Trained batch 2259 batch loss 0.623120606 epoch total loss 0.598433733\n",
      "Trained batch 2260 batch loss 0.596391678 epoch total loss 0.598432839\n",
      "Trained batch 2261 batch loss 0.673659 epoch total loss 0.598466158\n",
      "Trained batch 2262 batch loss 0.663951933 epoch total loss 0.598495066\n",
      "Trained batch 2263 batch loss 0.512206078 epoch total loss 0.598457\n",
      "Trained batch 2264 batch loss 0.582669854 epoch total loss 0.59845\n",
      "Trained batch 2265 batch loss 0.764461458 epoch total loss 0.598523259\n",
      "Trained batch 2266 batch loss 0.811454356 epoch total loss 0.598617196\n",
      "Trained batch 2267 batch loss 0.700251341 epoch total loss 0.598662\n",
      "Trained batch 2268 batch loss 0.744968355 epoch total loss 0.598726511\n",
      "Trained batch 2269 batch loss 0.6724509 epoch total loss 0.598759055\n",
      "Trained batch 2270 batch loss 0.575010777 epoch total loss 0.598748565\n",
      "Trained batch 2271 batch loss 0.561488211 epoch total loss 0.598732173\n",
      "Trained batch 2272 batch loss 0.591844678 epoch total loss 0.598729074\n",
      "Trained batch 2273 batch loss 0.552408457 epoch total loss 0.598708689\n",
      "Trained batch 2274 batch loss 0.459286451 epoch total loss 0.598647356\n",
      "Trained batch 2275 batch loss 0.463435173 epoch total loss 0.59858793\n",
      "Trained batch 2276 batch loss 0.472473383 epoch total loss 0.598532498\n",
      "Trained batch 2277 batch loss 0.548528731 epoch total loss 0.598510563\n",
      "Trained batch 2278 batch loss 0.617073894 epoch total loss 0.598518729\n",
      "Trained batch 2279 batch loss 0.631895304 epoch total loss 0.598533332\n",
      "Trained batch 2280 batch loss 0.541440725 epoch total loss 0.598508298\n",
      "Trained batch 2281 batch loss 0.389795154 epoch total loss 0.598416746\n",
      "Trained batch 2282 batch loss 0.423027 epoch total loss 0.598339915\n",
      "Trained batch 2283 batch loss 0.462372839 epoch total loss 0.59828037\n",
      "Trained batch 2284 batch loss 0.467928141 epoch total loss 0.598223269\n",
      "Trained batch 2285 batch loss 0.499342114 epoch total loss 0.59818\n",
      "Trained batch 2286 batch loss 0.482144833 epoch total loss 0.598129272\n",
      "Trained batch 2287 batch loss 0.558481574 epoch total loss 0.598111928\n",
      "Trained batch 2288 batch loss 0.539892077 epoch total loss 0.598086476\n",
      "Trained batch 2289 batch loss 0.559319913 epoch total loss 0.598069549\n",
      "Trained batch 2290 batch loss 0.513586342 epoch total loss 0.598032653\n",
      "Trained batch 2291 batch loss 0.493410021 epoch total loss 0.597987\n",
      "Trained batch 2292 batch loss 0.670246363 epoch total loss 0.598018527\n",
      "Trained batch 2293 batch loss 0.625389218 epoch total loss 0.598030448\n",
      "Trained batch 2294 batch loss 0.567513168 epoch total loss 0.598017156\n",
      "Trained batch 2295 batch loss 0.643367231 epoch total loss 0.598036885\n",
      "Trained batch 2296 batch loss 0.617784798 epoch total loss 0.598045468\n",
      "Trained batch 2297 batch loss 0.586186051 epoch total loss 0.598040342\n",
      "Trained batch 2298 batch loss 0.620542705 epoch total loss 0.598050058\n",
      "Trained batch 2299 batch loss 0.613996 epoch total loss 0.598057032\n",
      "Trained batch 2300 batch loss 0.63676846 epoch total loss 0.59807384\n",
      "Trained batch 2301 batch loss 0.61647439 epoch total loss 0.598081827\n",
      "Trained batch 2302 batch loss 0.557662189 epoch total loss 0.598064244\n",
      "Trained batch 2303 batch loss 0.629202962 epoch total loss 0.598077774\n",
      "Trained batch 2304 batch loss 0.632607818 epoch total loss 0.598092735\n",
      "Trained batch 2305 batch loss 0.628853202 epoch total loss 0.598106086\n",
      "Trained batch 2306 batch loss 0.615282297 epoch total loss 0.598113537\n",
      "Trained batch 2307 batch loss 0.613146663 epoch total loss 0.598120034\n",
      "Trained batch 2308 batch loss 0.660037756 epoch total loss 0.598146856\n",
      "Trained batch 2309 batch loss 0.621676207 epoch total loss 0.598157048\n",
      "Trained batch 2310 batch loss 0.641408145 epoch total loss 0.598175764\n",
      "Trained batch 2311 batch loss 0.64125061 epoch total loss 0.59819442\n",
      "Trained batch 2312 batch loss 0.538003862 epoch total loss 0.598168373\n",
      "Trained batch 2313 batch loss 0.521407247 epoch total loss 0.598135114\n",
      "Trained batch 2314 batch loss 0.534192204 epoch total loss 0.598107517\n",
      "Trained batch 2315 batch loss 0.640204787 epoch total loss 0.598125696\n",
      "Trained batch 2316 batch loss 0.516685545 epoch total loss 0.598090529\n",
      "Trained batch 2317 batch loss 0.547293186 epoch total loss 0.598068595\n",
      "Trained batch 2318 batch loss 0.50904119 epoch total loss 0.59803021\n",
      "Trained batch 2319 batch loss 0.511452317 epoch total loss 0.597992897\n",
      "Trained batch 2320 batch loss 0.52574116 epoch total loss 0.597961724\n",
      "Trained batch 2321 batch loss 0.600053847 epoch total loss 0.597962677\n",
      "Trained batch 2322 batch loss 0.548893392 epoch total loss 0.597941577\n",
      "Trained batch 2323 batch loss 0.515041947 epoch total loss 0.597905874\n",
      "Trained batch 2324 batch loss 0.572935939 epoch total loss 0.597895086\n",
      "Trained batch 2325 batch loss 0.537019789 epoch total loss 0.59786886\n",
      "Trained batch 2326 batch loss 0.619139969 epoch total loss 0.597878039\n",
      "Trained batch 2327 batch loss 0.559946358 epoch total loss 0.597861707\n",
      "Trained batch 2328 batch loss 0.576797545 epoch total loss 0.597852647\n",
      "Trained batch 2329 batch loss 0.570707083 epoch total loss 0.597841\n",
      "Trained batch 2330 batch loss 0.535845816 epoch total loss 0.597814441\n",
      "Trained batch 2331 batch loss 0.564027309 epoch total loss 0.597799957\n",
      "Trained batch 2332 batch loss 0.505361617 epoch total loss 0.59776032\n",
      "Trained batch 2333 batch loss 0.498088628 epoch total loss 0.597717583\n",
      "Trained batch 2334 batch loss 0.543999434 epoch total loss 0.597694516\n",
      "Trained batch 2335 batch loss 0.656642437 epoch total loss 0.597719789\n",
      "Trained batch 2336 batch loss 0.658705413 epoch total loss 0.597745895\n",
      "Trained batch 2337 batch loss 0.631652951 epoch total loss 0.597760379\n",
      "Trained batch 2338 batch loss 0.65612787 epoch total loss 0.597785354\n",
      "Trained batch 2339 batch loss 0.605110526 epoch total loss 0.597788513\n",
      "Trained batch 2340 batch loss 0.618075252 epoch total loss 0.597797155\n",
      "Trained batch 2341 batch loss 0.590611517 epoch total loss 0.597794056\n",
      "Trained batch 2342 batch loss 0.589277864 epoch total loss 0.59779042\n",
      "Trained batch 2343 batch loss 0.670844316 epoch total loss 0.597821593\n",
      "Trained batch 2344 batch loss 0.70415926 epoch total loss 0.597866952\n",
      "Trained batch 2345 batch loss 0.64924 epoch total loss 0.597888887\n",
      "Trained batch 2346 batch loss 0.553492665 epoch total loss 0.597869933\n",
      "Trained batch 2347 batch loss 0.577675104 epoch total loss 0.59786135\n",
      "Trained batch 2348 batch loss 0.634676576 epoch total loss 0.597876966\n",
      "Trained batch 2349 batch loss 0.588491201 epoch total loss 0.597873032\n",
      "Trained batch 2350 batch loss 0.62078476 epoch total loss 0.597882748\n",
      "Trained batch 2351 batch loss 0.653657556 epoch total loss 0.59790647\n",
      "Trained batch 2352 batch loss 0.622639537 epoch total loss 0.597917\n",
      "Trained batch 2353 batch loss 0.578246117 epoch total loss 0.597908616\n",
      "Trained batch 2354 batch loss 0.575101078 epoch total loss 0.59789896\n",
      "Trained batch 2355 batch loss 0.632444739 epoch total loss 0.597913623\n",
      "Trained batch 2356 batch loss 0.611815274 epoch total loss 0.597919524\n",
      "Trained batch 2357 batch loss 0.643769145 epoch total loss 0.597938955\n",
      "Trained batch 2358 batch loss 0.634503841 epoch total loss 0.597954512\n",
      "Trained batch 2359 batch loss 0.671533525 epoch total loss 0.597985685\n",
      "Trained batch 2360 batch loss 0.666577458 epoch total loss 0.598014772\n",
      "Trained batch 2361 batch loss 0.682785153 epoch total loss 0.598050654\n",
      "Trained batch 2362 batch loss 0.672480285 epoch total loss 0.598082125\n",
      "Trained batch 2363 batch loss 0.633809626 epoch total loss 0.598097265\n",
      "Trained batch 2364 batch loss 0.675329566 epoch total loss 0.598129928\n",
      "Trained batch 2365 batch loss 0.604131 epoch total loss 0.598132432\n",
      "Trained batch 2366 batch loss 0.636522949 epoch total loss 0.598148644\n",
      "Trained batch 2367 batch loss 0.659144223 epoch total loss 0.598174453\n",
      "Trained batch 2368 batch loss 0.607722 epoch total loss 0.598178446\n",
      "Trained batch 2369 batch loss 0.570027053 epoch total loss 0.598166585\n",
      "Trained batch 2370 batch loss 0.572054327 epoch total loss 0.598155558\n",
      "Trained batch 2371 batch loss 0.61059165 epoch total loss 0.598160803\n",
      "Trained batch 2372 batch loss 0.599345207 epoch total loss 0.59816128\n",
      "Trained batch 2373 batch loss 0.536098599 epoch total loss 0.598135173\n",
      "Trained batch 2374 batch loss 0.589309573 epoch total loss 0.598131478\n",
      "Trained batch 2375 batch loss 0.627696812 epoch total loss 0.598143935\n",
      "Trained batch 2376 batch loss 0.608735323 epoch total loss 0.598148406\n",
      "Trained batch 2377 batch loss 0.621530235 epoch total loss 0.59815824\n",
      "Trained batch 2378 batch loss 0.62140584 epoch total loss 0.598168\n",
      "Trained batch 2379 batch loss 0.578902721 epoch total loss 0.598159909\n",
      "Trained batch 2380 batch loss 0.65754813 epoch total loss 0.598184884\n",
      "Trained batch 2381 batch loss 0.613962054 epoch total loss 0.598191559\n",
      "Trained batch 2382 batch loss 0.607214 epoch total loss 0.598195314\n",
      "Trained batch 2383 batch loss 0.571104646 epoch total loss 0.59818393\n",
      "Trained batch 2384 batch loss 0.547293425 epoch total loss 0.598162532\n",
      "Trained batch 2385 batch loss 0.62975049 epoch total loss 0.598175824\n",
      "Trained batch 2386 batch loss 0.622476578 epoch total loss 0.598185956\n",
      "Trained batch 2387 batch loss 0.562564611 epoch total loss 0.598171055\n",
      "Trained batch 2388 batch loss 0.656743586 epoch total loss 0.598195612\n",
      "Trained batch 2389 batch loss 0.559100032 epoch total loss 0.598179221\n",
      "Trained batch 2390 batch loss 0.517986417 epoch total loss 0.598145664\n",
      "Trained batch 2391 batch loss 0.531591713 epoch total loss 0.598117828\n",
      "Trained batch 2392 batch loss 0.565198779 epoch total loss 0.59810406\n",
      "Trained batch 2393 batch loss 0.676041603 epoch total loss 0.598136604\n",
      "Trained batch 2394 batch loss 0.623733222 epoch total loss 0.598147333\n",
      "Trained batch 2395 batch loss 0.54979974 epoch total loss 0.598127127\n",
      "Trained batch 2396 batch loss 0.578829467 epoch total loss 0.59811908\n",
      "Trained batch 2397 batch loss 0.597641945 epoch total loss 0.598118901\n",
      "Trained batch 2398 batch loss 0.510290146 epoch total loss 0.598082304\n",
      "Trained batch 2399 batch loss 0.520022035 epoch total loss 0.59804976\n",
      "Trained batch 2400 batch loss 0.526786 epoch total loss 0.59802\n",
      "Trained batch 2401 batch loss 0.535531938 epoch total loss 0.59799397\n",
      "Trained batch 2402 batch loss 0.635542631 epoch total loss 0.598009586\n",
      "Trained batch 2403 batch loss 0.57430625 epoch total loss 0.597999752\n",
      "Trained batch 2404 batch loss 0.55648303 epoch total loss 0.597982526\n",
      "Trained batch 2405 batch loss 0.623925686 epoch total loss 0.597993255\n",
      "Trained batch 2406 batch loss 0.712157965 epoch total loss 0.5980407\n",
      "Trained batch 2407 batch loss 0.562242925 epoch total loss 0.598025858\n",
      "Trained batch 2408 batch loss 0.597982109 epoch total loss 0.598025858\n",
      "Trained batch 2409 batch loss 0.493821442 epoch total loss 0.597982585\n",
      "Trained batch 2410 batch loss 0.53147155 epoch total loss 0.597955\n",
      "Trained batch 2411 batch loss 0.523499668 epoch total loss 0.597924113\n",
      "Trained batch 2412 batch loss 0.59803617 epoch total loss 0.597924173\n",
      "Trained batch 2413 batch loss 0.582051873 epoch total loss 0.597917616\n",
      "Trained batch 2414 batch loss 0.583438396 epoch total loss 0.597911596\n",
      "Trained batch 2415 batch loss 0.608378887 epoch total loss 0.597915947\n",
      "Trained batch 2416 batch loss 0.578240335 epoch total loss 0.597907841\n",
      "Trained batch 2417 batch loss 0.580428421 epoch total loss 0.597900569\n",
      "Trained batch 2418 batch loss 0.596081257 epoch total loss 0.597899854\n",
      "Trained batch 2419 batch loss 0.634127 epoch total loss 0.597914815\n",
      "Trained batch 2420 batch loss 0.601112843 epoch total loss 0.597916126\n",
      "Trained batch 2421 batch loss 0.601589561 epoch total loss 0.597917616\n",
      "Trained batch 2422 batch loss 0.453145176 epoch total loss 0.597857833\n",
      "Trained batch 2423 batch loss 0.513448358 epoch total loss 0.597823\n",
      "Trained batch 2424 batch loss 0.534352183 epoch total loss 0.597796798\n",
      "Trained batch 2425 batch loss 0.53704685 epoch total loss 0.597771704\n",
      "Trained batch 2426 batch loss 0.544379 epoch total loss 0.59774977\n",
      "Trained batch 2427 batch loss 0.56176424 epoch total loss 0.597734928\n",
      "Trained batch 2428 batch loss 0.540008664 epoch total loss 0.597711146\n",
      "Trained batch 2429 batch loss 0.498761773 epoch total loss 0.597670436\n",
      "Trained batch 2430 batch loss 0.485654235 epoch total loss 0.597624302\n",
      "Trained batch 2431 batch loss 0.606324196 epoch total loss 0.597627878\n",
      "Trained batch 2432 batch loss 0.65346843 epoch total loss 0.597650826\n",
      "Trained batch 2433 batch loss 0.552738249 epoch total loss 0.597632349\n",
      "Trained batch 2434 batch loss 0.643752098 epoch total loss 0.597651362\n",
      "Trained batch 2435 batch loss 0.657074571 epoch total loss 0.597675741\n",
      "Trained batch 2436 batch loss 0.670155883 epoch total loss 0.597705483\n",
      "Trained batch 2437 batch loss 0.783531725 epoch total loss 0.597781777\n",
      "Trained batch 2438 batch loss 0.724551439 epoch total loss 0.597833812\n",
      "Trained batch 2439 batch loss 0.685673 epoch total loss 0.597869813\n",
      "Trained batch 2440 batch loss 0.63395679 epoch total loss 0.597884595\n",
      "Trained batch 2441 batch loss 0.587410748 epoch total loss 0.597880304\n",
      "Trained batch 2442 batch loss 0.711374283 epoch total loss 0.597926795\n",
      "Trained batch 2443 batch loss 0.614407718 epoch total loss 0.597933531\n",
      "Trained batch 2444 batch loss 0.606731713 epoch total loss 0.597937107\n",
      "Trained batch 2445 batch loss 0.575384259 epoch total loss 0.597927928\n",
      "Trained batch 2446 batch loss 0.617117226 epoch total loss 0.597935736\n",
      "Trained batch 2447 batch loss 0.520277202 epoch total loss 0.597903967\n",
      "Trained batch 2448 batch loss 0.483091056 epoch total loss 0.597857058\n",
      "Trained batch 2449 batch loss 0.574744284 epoch total loss 0.597847581\n",
      "Trained batch 2450 batch loss 0.623591781 epoch total loss 0.597858071\n",
      "Trained batch 2451 batch loss 0.57755363 epoch total loss 0.597849786\n",
      "Trained batch 2452 batch loss 0.609023869 epoch total loss 0.597854316\n",
      "Trained batch 2453 batch loss 0.669091403 epoch total loss 0.597883344\n",
      "Trained batch 2454 batch loss 0.673657715 epoch total loss 0.597914279\n",
      "Trained batch 2455 batch loss 0.636646032 epoch total loss 0.59793\n",
      "Trained batch 2456 batch loss 0.596724927 epoch total loss 0.597929537\n",
      "Trained batch 2457 batch loss 0.589480519 epoch total loss 0.59792608\n",
      "Trained batch 2458 batch loss 0.565683782 epoch total loss 0.597912967\n",
      "Trained batch 2459 batch loss 0.554920912 epoch total loss 0.597895503\n",
      "Trained batch 2460 batch loss 0.523880363 epoch total loss 0.597865403\n",
      "Trained batch 2461 batch loss 0.546571195 epoch total loss 0.597844601\n",
      "Trained batch 2462 batch loss 0.510769308 epoch total loss 0.597809196\n",
      "Trained batch 2463 batch loss 0.495078862 epoch total loss 0.597767532\n",
      "Trained batch 2464 batch loss 0.495419621 epoch total loss 0.597725928\n",
      "Trained batch 2465 batch loss 0.60048455 epoch total loss 0.59772706\n",
      "Trained batch 2466 batch loss 0.591700375 epoch total loss 0.597724617\n",
      "Trained batch 2467 batch loss 0.573733628 epoch total loss 0.597714901\n",
      "Trained batch 2468 batch loss 0.528968394 epoch total loss 0.597687\n",
      "Trained batch 2469 batch loss 0.465291 epoch total loss 0.597633421\n",
      "Trained batch 2470 batch loss 0.539340854 epoch total loss 0.597609818\n",
      "Trained batch 2471 batch loss 0.578323364 epoch total loss 0.597602\n",
      "Trained batch 2472 batch loss 0.634567618 epoch total loss 0.597617\n",
      "Trained batch 2473 batch loss 0.770047069 epoch total loss 0.597686648\n",
      "Trained batch 2474 batch loss 0.648397207 epoch total loss 0.597707152\n",
      "Trained batch 2475 batch loss 0.742766261 epoch total loss 0.597765803\n",
      "Trained batch 2476 batch loss 0.659524322 epoch total loss 0.597790718\n",
      "Trained batch 2477 batch loss 0.539776206 epoch total loss 0.597767353\n",
      "Trained batch 2478 batch loss 0.523252606 epoch total loss 0.597737253\n",
      "Trained batch 2479 batch loss 0.47368449 epoch total loss 0.597687185\n",
      "Trained batch 2480 batch loss 0.500696301 epoch total loss 0.597648084\n",
      "Trained batch 2481 batch loss 0.470301747 epoch total loss 0.597596765\n",
      "Trained batch 2482 batch loss 0.665469408 epoch total loss 0.597624123\n",
      "Trained batch 2483 batch loss 0.681372046 epoch total loss 0.597657859\n",
      "Trained batch 2484 batch loss 0.653087139 epoch total loss 0.597680211\n",
      "Trained batch 2485 batch loss 0.713169217 epoch total loss 0.597726643\n",
      "Trained batch 2486 batch loss 0.640185714 epoch total loss 0.59774369\n",
      "Trained batch 2487 batch loss 0.592634618 epoch total loss 0.597741663\n",
      "Trained batch 2488 batch loss 0.65153569 epoch total loss 0.59776324\n",
      "Trained batch 2489 batch loss 0.629160166 epoch total loss 0.597775877\n",
      "Trained batch 2490 batch loss 0.601525128 epoch total loss 0.597777367\n",
      "Trained batch 2491 batch loss 0.576826453 epoch total loss 0.597768962\n",
      "Trained batch 2492 batch loss 0.649435461 epoch total loss 0.597789705\n",
      "Trained batch 2493 batch loss 0.641886234 epoch total loss 0.597807348\n",
      "Trained batch 2494 batch loss 0.575264335 epoch total loss 0.597798347\n",
      "Trained batch 2495 batch loss 0.594277382 epoch total loss 0.597796917\n",
      "Trained batch 2496 batch loss 0.689266384 epoch total loss 0.597833514\n",
      "Trained batch 2497 batch loss 0.582437813 epoch total loss 0.597827375\n",
      "Trained batch 2498 batch loss 0.605002403 epoch total loss 0.597830236\n",
      "Trained batch 2499 batch loss 0.584025621 epoch total loss 0.597824693\n",
      "Trained batch 2500 batch loss 0.559101224 epoch total loss 0.597809196\n",
      "Trained batch 2501 batch loss 0.589630425 epoch total loss 0.597805917\n",
      "Trained batch 2502 batch loss 0.635816395 epoch total loss 0.597821116\n",
      "Trained batch 2503 batch loss 0.601816177 epoch total loss 0.597822726\n",
      "Trained batch 2504 batch loss 0.59939605 epoch total loss 0.597823322\n",
      "Trained batch 2505 batch loss 0.617744684 epoch total loss 0.597831309\n",
      "Trained batch 2506 batch loss 0.58192873 epoch total loss 0.597824931\n",
      "Trained batch 2507 batch loss 0.567732334 epoch total loss 0.597812951\n",
      "Trained batch 2508 batch loss 0.629758716 epoch total loss 0.597825706\n",
      "Trained batch 2509 batch loss 0.60840261 epoch total loss 0.597829878\n",
      "Trained batch 2510 batch loss 0.600498617 epoch total loss 0.597830951\n",
      "Trained batch 2511 batch loss 0.628896832 epoch total loss 0.597843289\n",
      "Trained batch 2512 batch loss 0.668194711 epoch total loss 0.597871304\n",
      "Trained batch 2513 batch loss 0.632971346 epoch total loss 0.597885311\n",
      "Trained batch 2514 batch loss 0.638354361 epoch total loss 0.597901344\n",
      "Trained batch 2515 batch loss 0.568072259 epoch total loss 0.597889543\n",
      "Trained batch 2516 batch loss 0.595435 epoch total loss 0.597888529\n",
      "Trained batch 2517 batch loss 0.649402142 epoch total loss 0.597909033\n",
      "Trained batch 2518 batch loss 0.592306137 epoch total loss 0.597906768\n",
      "Trained batch 2519 batch loss 0.591345727 epoch total loss 0.597904146\n",
      "Trained batch 2520 batch loss 0.626587033 epoch total loss 0.59791553\n",
      "Trained batch 2521 batch loss 0.621727 epoch total loss 0.597925\n",
      "Trained batch 2522 batch loss 0.6649791 epoch total loss 0.597951591\n",
      "Trained batch 2523 batch loss 0.633367 epoch total loss 0.597965658\n",
      "Trained batch 2524 batch loss 0.588200629 epoch total loss 0.597961783\n",
      "Trained batch 2525 batch loss 0.585296214 epoch total loss 0.597956836\n",
      "Trained batch 2526 batch loss 0.620306 epoch total loss 0.597965658\n",
      "Trained batch 2527 batch loss 0.560939848 epoch total loss 0.597951\n",
      "Trained batch 2528 batch loss 0.662757516 epoch total loss 0.597976625\n",
      "Trained batch 2529 batch loss 0.652271688 epoch total loss 0.597998083\n",
      "Trained batch 2530 batch loss 0.658461392 epoch total loss 0.598022\n",
      "Trained batch 2531 batch loss 0.576654 epoch total loss 0.59801352\n",
      "Trained batch 2532 batch loss 0.558316 epoch total loss 0.597997844\n",
      "Trained batch 2533 batch loss 0.651525855 epoch total loss 0.598019\n",
      "Trained batch 2534 batch loss 0.603340268 epoch total loss 0.59802109\n",
      "Trained batch 2535 batch loss 0.61488533 epoch total loss 0.598027766\n",
      "Trained batch 2536 batch loss 0.56980449 epoch total loss 0.59801662\n",
      "Trained batch 2537 batch loss 0.530176044 epoch total loss 0.597989857\n",
      "Trained batch 2538 batch loss 0.564526 epoch total loss 0.597976744\n",
      "Trained batch 2539 batch loss 0.706001878 epoch total loss 0.598019302\n",
      "Trained batch 2540 batch loss 0.661267638 epoch total loss 0.598044157\n",
      "Trained batch 2541 batch loss 0.666013181 epoch total loss 0.59807092\n",
      "Trained batch 2542 batch loss 0.634598374 epoch total loss 0.598085344\n",
      "Trained batch 2543 batch loss 0.615310907 epoch total loss 0.598092139\n",
      "Trained batch 2544 batch loss 0.599890172 epoch total loss 0.598092794\n",
      "Trained batch 2545 batch loss 0.579227328 epoch total loss 0.598085403\n",
      "Trained batch 2546 batch loss 0.636642337 epoch total loss 0.598100543\n",
      "Trained batch 2547 batch loss 0.467444241 epoch total loss 0.598049223\n",
      "Trained batch 2548 batch loss 0.546180785 epoch total loss 0.598028839\n",
      "Trained batch 2549 batch loss 0.476584256 epoch total loss 0.597981155\n",
      "Trained batch 2550 batch loss 0.503253579 epoch total loss 0.597944\n",
      "Trained batch 2551 batch loss 0.547124743 epoch total loss 0.597924113\n",
      "Trained batch 2552 batch loss 0.501243591 epoch total loss 0.597886205\n",
      "Trained batch 2553 batch loss 0.608348906 epoch total loss 0.597890377\n",
      "Trained batch 2554 batch loss 0.548491061 epoch total loss 0.597871\n",
      "Trained batch 2555 batch loss 0.631353915 epoch total loss 0.597884119\n",
      "Trained batch 2556 batch loss 0.616994917 epoch total loss 0.597891569\n",
      "Trained batch 2557 batch loss 0.63467747 epoch total loss 0.597905934\n",
      "Trained batch 2558 batch loss 0.608688235 epoch total loss 0.597910106\n",
      "Trained batch 2559 batch loss 0.621678054 epoch total loss 0.597919405\n",
      "Trained batch 2560 batch loss 0.618174255 epoch total loss 0.597927332\n",
      "Trained batch 2561 batch loss 0.627941191 epoch total loss 0.597939074\n",
      "Trained batch 2562 batch loss 0.668052793 epoch total loss 0.597966433\n",
      "Trained batch 2563 batch loss 0.62650454 epoch total loss 0.597977519\n",
      "Trained batch 2564 batch loss 0.689433694 epoch total loss 0.598013222\n",
      "Trained batch 2565 batch loss 0.633256435 epoch total loss 0.598027\n",
      "Trained batch 2566 batch loss 0.654663503 epoch total loss 0.598049045\n",
      "Trained batch 2567 batch loss 0.560589135 epoch total loss 0.598034441\n",
      "Trained batch 2568 batch loss 0.590755641 epoch total loss 0.59803158\n",
      "Trained batch 2569 batch loss 0.554300964 epoch total loss 0.598014593\n",
      "Trained batch 2570 batch loss 0.596584857 epoch total loss 0.598014\n",
      "Trained batch 2571 batch loss 0.58696121 epoch total loss 0.598009706\n",
      "Trained batch 2572 batch loss 0.543251336 epoch total loss 0.597988367\n",
      "Trained batch 2573 batch loss 0.516811609 epoch total loss 0.597956836\n",
      "Trained batch 2574 batch loss 0.513126 epoch total loss 0.597923934\n",
      "Trained batch 2575 batch loss 0.575685143 epoch total loss 0.597915292\n",
      "Trained batch 2576 batch loss 0.617752373 epoch total loss 0.597923\n",
      "Trained batch 2577 batch loss 0.673575461 epoch total loss 0.597952366\n",
      "Trained batch 2578 batch loss 0.616034627 epoch total loss 0.597959399\n",
      "Trained batch 2579 batch loss 0.559400856 epoch total loss 0.597944438\n",
      "Trained batch 2580 batch loss 0.559530914 epoch total loss 0.597929597\n",
      "Trained batch 2581 batch loss 0.607292295 epoch total loss 0.597933233\n",
      "Trained batch 2582 batch loss 0.598042727 epoch total loss 0.597933233\n",
      "Trained batch 2583 batch loss 0.498930275 epoch total loss 0.597894907\n",
      "Trained batch 2584 batch loss 0.602274477 epoch total loss 0.597896636\n",
      "Trained batch 2585 batch loss 0.523749709 epoch total loss 0.597867966\n",
      "Trained batch 2586 batch loss 0.548334479 epoch total loss 0.597848773\n",
      "Trained batch 2587 batch loss 0.599376142 epoch total loss 0.597849369\n",
      "Trained batch 2588 batch loss 0.591470718 epoch total loss 0.597846925\n",
      "Trained batch 2589 batch loss 0.575657487 epoch total loss 0.597838342\n",
      "Trained batch 2590 batch loss 0.569971263 epoch total loss 0.597827554\n",
      "Trained batch 2591 batch loss 0.53338027 epoch total loss 0.597802699\n",
      "Trained batch 2592 batch loss 0.612352133 epoch total loss 0.597808301\n",
      "Trained batch 2593 batch loss 0.503887296 epoch total loss 0.597772062\n",
      "Trained batch 2594 batch loss 0.555827618 epoch total loss 0.597755849\n",
      "Trained batch 2595 batch loss 0.565130472 epoch total loss 0.597743332\n",
      "Trained batch 2596 batch loss 0.639353335 epoch total loss 0.597759366\n",
      "Trained batch 2597 batch loss 0.547335625 epoch total loss 0.59774\n",
      "Trained batch 2598 batch loss 0.499847919 epoch total loss 0.597702324\n",
      "Trained batch 2599 batch loss 0.601394057 epoch total loss 0.597703755\n",
      "Trained batch 2600 batch loss 0.612909913 epoch total loss 0.597709596\n",
      "Trained batch 2601 batch loss 0.669946551 epoch total loss 0.597737372\n",
      "Trained batch 2602 batch loss 0.593852043 epoch total loss 0.597735882\n",
      "Trained batch 2603 batch loss 0.716716409 epoch total loss 0.597781539\n",
      "Trained batch 2604 batch loss 0.621159315 epoch total loss 0.597790539\n",
      "Trained batch 2605 batch loss 0.58417064 epoch total loss 0.597785354\n",
      "Trained batch 2606 batch loss 0.629949033 epoch total loss 0.597797692\n",
      "Trained batch 2607 batch loss 0.632803917 epoch total loss 0.597811162\n",
      "Trained batch 2608 batch loss 0.503498197 epoch total loss 0.597775\n",
      "Trained batch 2609 batch loss 0.552010894 epoch total loss 0.597757459\n",
      "Trained batch 2610 batch loss 0.594008088 epoch total loss 0.597756\n",
      "Trained batch 2611 batch loss 0.577067494 epoch total loss 0.597748101\n",
      "Trained batch 2612 batch loss 0.530708909 epoch total loss 0.597722411\n",
      "Trained batch 2613 batch loss 0.489289671 epoch total loss 0.597680926\n",
      "Trained batch 2614 batch loss 0.486112475 epoch total loss 0.597638249\n",
      "Trained batch 2615 batch loss 0.503071249 epoch total loss 0.597602069\n",
      "Trained batch 2616 batch loss 0.635612965 epoch total loss 0.597616613\n",
      "Trained batch 2617 batch loss 0.53300786 epoch total loss 0.597591877\n",
      "Trained batch 2618 batch loss 0.530260861 epoch total loss 0.597566187\n",
      "Trained batch 2619 batch loss 0.51019007 epoch total loss 0.597532809\n",
      "Trained batch 2620 batch loss 0.52094382 epoch total loss 0.597503543\n",
      "Trained batch 2621 batch loss 0.519594193 epoch total loss 0.59747386\n",
      "Trained batch 2622 batch loss 0.527550161 epoch total loss 0.597447217\n",
      "Trained batch 2623 batch loss 0.485173434 epoch total loss 0.59740442\n",
      "Trained batch 2624 batch loss 0.48827 epoch total loss 0.597362816\n",
      "Trained batch 2625 batch loss 0.611433685 epoch total loss 0.59736824\n",
      "Trained batch 2626 batch loss 0.45518139 epoch total loss 0.59731406\n",
      "Trained batch 2627 batch loss 0.522125363 epoch total loss 0.59728545\n",
      "Trained batch 2628 batch loss 0.596066177 epoch total loss 0.597285\n",
      "Trained batch 2629 batch loss 0.583172441 epoch total loss 0.597279608\n",
      "Trained batch 2630 batch loss 0.655125082 epoch total loss 0.597301602\n",
      "Trained batch 2631 batch loss 0.756345 epoch total loss 0.597362041\n",
      "Trained batch 2632 batch loss 0.570742607 epoch total loss 0.597351968\n",
      "Trained batch 2633 batch loss 0.571988 epoch total loss 0.597342312\n",
      "Trained batch 2634 batch loss 0.564036787 epoch total loss 0.597329736\n",
      "Trained batch 2635 batch loss 0.546478 epoch total loss 0.597310424\n",
      "Trained batch 2636 batch loss 0.629826844 epoch total loss 0.597322762\n",
      "Trained batch 2637 batch loss 0.570554316 epoch total loss 0.597312629\n",
      "Trained batch 2638 batch loss 0.599616528 epoch total loss 0.597313523\n",
      "Trained batch 2639 batch loss 0.576792181 epoch total loss 0.597305715\n",
      "Trained batch 2640 batch loss 0.536551118 epoch total loss 0.597282708\n",
      "Trained batch 2641 batch loss 0.567266524 epoch total loss 0.597271323\n",
      "Trained batch 2642 batch loss 0.578016639 epoch total loss 0.597264051\n",
      "Trained batch 2643 batch loss 0.584507 epoch total loss 0.597259164\n",
      "Trained batch 2644 batch loss 0.700570762 epoch total loss 0.597298265\n",
      "Trained batch 2645 batch loss 0.576696873 epoch total loss 0.597290456\n",
      "Trained batch 2646 batch loss 0.587051094 epoch total loss 0.597286582\n",
      "Trained batch 2647 batch loss 0.590639293 epoch total loss 0.597284079\n",
      "Trained batch 2648 batch loss 0.69519347 epoch total loss 0.597321093\n",
      "Trained batch 2649 batch loss 0.501664758 epoch total loss 0.597285\n",
      "Trained batch 2650 batch loss 0.591153681 epoch total loss 0.597282648\n",
      "Trained batch 2651 batch loss 0.614150107 epoch total loss 0.597289\n",
      "Trained batch 2652 batch loss 0.595424592 epoch total loss 0.597288311\n",
      "Trained batch 2653 batch loss 0.577068567 epoch total loss 0.597280681\n",
      "Trained batch 2654 batch loss 0.577284038 epoch total loss 0.597273171\n",
      "Trained batch 2655 batch loss 0.610492706 epoch total loss 0.597278118\n",
      "Trained batch 2656 batch loss 0.580416262 epoch total loss 0.5972718\n",
      "Trained batch 2657 batch loss 0.612700164 epoch total loss 0.597277582\n",
      "Trained batch 2658 batch loss 0.576972365 epoch total loss 0.597269952\n",
      "Trained batch 2659 batch loss 0.55650866 epoch total loss 0.597254634\n",
      "Trained batch 2660 batch loss 0.525444388 epoch total loss 0.597227633\n",
      "Trained batch 2661 batch loss 0.568045259 epoch total loss 0.597216666\n",
      "Trained batch 2662 batch loss 0.565926909 epoch total loss 0.597204864\n",
      "Trained batch 2663 batch loss 0.614568 epoch total loss 0.597211421\n",
      "Trained batch 2664 batch loss 0.593672335 epoch total loss 0.59721005\n",
      "Trained batch 2665 batch loss 0.719669461 epoch total loss 0.597256064\n",
      "Trained batch 2666 batch loss 0.650389254 epoch total loss 0.597276\n",
      "Trained batch 2667 batch loss 0.632625818 epoch total loss 0.597289205\n",
      "Trained batch 2668 batch loss 0.699160039 epoch total loss 0.597327411\n",
      "Trained batch 2669 batch loss 0.631027102 epoch total loss 0.597340047\n",
      "Trained batch 2670 batch loss 0.637725413 epoch total loss 0.597355127\n",
      "Trained batch 2671 batch loss 0.630783677 epoch total loss 0.597367644\n",
      "Trained batch 2672 batch loss 0.636698961 epoch total loss 0.597382367\n",
      "Trained batch 2673 batch loss 0.619410515 epoch total loss 0.597390592\n",
      "Trained batch 2674 batch loss 0.624785364 epoch total loss 0.597400844\n",
      "Trained batch 2675 batch loss 0.614182949 epoch total loss 0.597407103\n",
      "Trained batch 2676 batch loss 0.656226218 epoch total loss 0.597429097\n",
      "Trained batch 2677 batch loss 0.592684209 epoch total loss 0.597427309\n",
      "Trained batch 2678 batch loss 0.647078812 epoch total loss 0.597445846\n",
      "Trained batch 2679 batch loss 0.553888202 epoch total loss 0.597429574\n",
      "Trained batch 2680 batch loss 0.575805187 epoch total loss 0.597421467\n",
      "Trained batch 2681 batch loss 0.629793525 epoch total loss 0.597433567\n",
      "Trained batch 2682 batch loss 0.544561744 epoch total loss 0.597413838\n",
      "Trained batch 2683 batch loss 0.600347281 epoch total loss 0.597414911\n",
      "Trained batch 2684 batch loss 0.573342621 epoch total loss 0.59740597\n",
      "Trained batch 2685 batch loss 0.542120576 epoch total loss 0.597385347\n",
      "Trained batch 2686 batch loss 0.515537858 epoch total loss 0.597354889\n",
      "Trained batch 2687 batch loss 0.419041395 epoch total loss 0.597288549\n",
      "Trained batch 2688 batch loss 0.552013516 epoch total loss 0.597271681\n",
      "Trained batch 2689 batch loss 0.677447736 epoch total loss 0.597301543\n",
      "Trained batch 2690 batch loss 0.583135068 epoch total loss 0.597296238\n",
      "Trained batch 2691 batch loss 0.605337918 epoch total loss 0.597299218\n",
      "Trained batch 2692 batch loss 0.599561691 epoch total loss 0.597300112\n",
      "Trained batch 2693 batch loss 0.579526722 epoch total loss 0.597293496\n",
      "Trained batch 2694 batch loss 0.543868303 epoch total loss 0.597273648\n",
      "Trained batch 2695 batch loss 0.562462 epoch total loss 0.597260714\n",
      "Trained batch 2696 batch loss 0.569651425 epoch total loss 0.597250521\n",
      "Trained batch 2697 batch loss 0.62304008 epoch total loss 0.597260058\n",
      "Trained batch 2698 batch loss 0.563275576 epoch total loss 0.597247481\n",
      "Trained batch 2699 batch loss 0.621862352 epoch total loss 0.597256541\n",
      "Trained batch 2700 batch loss 0.553531706 epoch total loss 0.597240388\n",
      "Trained batch 2701 batch loss 0.549993157 epoch total loss 0.597222924\n",
      "Trained batch 2702 batch loss 0.535169542 epoch total loss 0.5972\n",
      "Trained batch 2703 batch loss 0.539913476 epoch total loss 0.597178757\n",
      "Trained batch 2704 batch loss 0.54966253 epoch total loss 0.597161174\n",
      "Trained batch 2705 batch loss 0.557105303 epoch total loss 0.597146392\n",
      "Trained batch 2706 batch loss 0.553452134 epoch total loss 0.597130239\n",
      "Trained batch 2707 batch loss 0.523679495 epoch total loss 0.597103119\n",
      "Trained batch 2708 batch loss 0.521566629 epoch total loss 0.597075224\n",
      "Trained batch 2709 batch loss 0.557106853 epoch total loss 0.597060502\n",
      "Trained batch 2710 batch loss 0.563458681 epoch total loss 0.597048104\n",
      "Trained batch 2711 batch loss 0.660639048 epoch total loss 0.597071588\n",
      "Trained batch 2712 batch loss 0.653674603 epoch total loss 0.59709245\n",
      "Trained batch 2713 batch loss 0.636834919 epoch total loss 0.597107112\n",
      "Trained batch 2714 batch loss 0.577055514 epoch total loss 0.597099721\n",
      "Trained batch 2715 batch loss 0.646104038 epoch total loss 0.597117722\n",
      "Trained batch 2716 batch loss 0.569802761 epoch total loss 0.597107708\n",
      "Trained batch 2717 batch loss 0.570885956 epoch total loss 0.597098053\n",
      "Trained batch 2718 batch loss 0.589725494 epoch total loss 0.59709537\n",
      "Trained batch 2719 batch loss 0.633481 epoch total loss 0.597108722\n",
      "Trained batch 2720 batch loss 0.617036521 epoch total loss 0.597116053\n",
      "Trained batch 2721 batch loss 0.625788 epoch total loss 0.597126544\n",
      "Trained batch 2722 batch loss 0.556654811 epoch total loss 0.597111702\n",
      "Trained batch 2723 batch loss 0.608626306 epoch total loss 0.597115934\n",
      "Trained batch 2724 batch loss 0.470540017 epoch total loss 0.597069442\n",
      "Trained batch 2725 batch loss 0.600640416 epoch total loss 0.597070754\n",
      "Trained batch 2726 batch loss 0.57505095 epoch total loss 0.597062707\n",
      "Trained batch 2727 batch loss 0.537332177 epoch total loss 0.597040772\n",
      "Trained batch 2728 batch loss 0.604986429 epoch total loss 0.597043693\n",
      "Trained batch 2729 batch loss 0.54188174 epoch total loss 0.597023487\n",
      "Trained batch 2730 batch loss 0.545213819 epoch total loss 0.597004473\n",
      "Trained batch 2731 batch loss 0.51636982 epoch total loss 0.596974969\n",
      "Trained batch 2732 batch loss 0.577970564 epoch total loss 0.596968\n",
      "Trained batch 2733 batch loss 0.616215 epoch total loss 0.596975088\n",
      "Trained batch 2734 batch loss 0.543666601 epoch total loss 0.596955597\n",
      "Trained batch 2735 batch loss 0.705345333 epoch total loss 0.596995175\n",
      "Trained batch 2736 batch loss 0.504233599 epoch total loss 0.596961319\n",
      "Trained batch 2737 batch loss 0.694440603 epoch total loss 0.596996903\n",
      "Trained batch 2738 batch loss 0.480366677 epoch total loss 0.596954346\n",
      "Trained batch 2739 batch loss 0.515536189 epoch total loss 0.596924603\n",
      "Trained batch 2740 batch loss 0.437773764 epoch total loss 0.596866488\n",
      "Trained batch 2741 batch loss 0.48549208 epoch total loss 0.596825838\n",
      "Trained batch 2742 batch loss 0.489466 epoch total loss 0.596786737\n",
      "Trained batch 2743 batch loss 0.484483778 epoch total loss 0.596745789\n",
      "Trained batch 2744 batch loss 0.562459171 epoch total loss 0.596733272\n",
      "Trained batch 2745 batch loss 0.499505192 epoch total loss 0.596697867\n",
      "Trained batch 2746 batch loss 0.541777074 epoch total loss 0.59667784\n",
      "Trained batch 2747 batch loss 0.542935669 epoch total loss 0.596658289\n",
      "Trained batch 2748 batch loss 0.610281229 epoch total loss 0.596663237\n",
      "Trained batch 2749 batch loss 0.585112274 epoch total loss 0.596659064\n",
      "Trained batch 2750 batch loss 0.601961613 epoch total loss 0.596661\n",
      "Trained batch 2751 batch loss 0.546880841 epoch total loss 0.596642852\n",
      "Trained batch 2752 batch loss 0.625941217 epoch total loss 0.596653521\n",
      "Trained batch 2753 batch loss 0.635311604 epoch total loss 0.596667528\n",
      "Trained batch 2754 batch loss 0.687620699 epoch total loss 0.596700549\n",
      "Trained batch 2755 batch loss 0.625305295 epoch total loss 0.596711\n",
      "Trained batch 2756 batch loss 0.590460122 epoch total loss 0.596708715\n",
      "Trained batch 2757 batch loss 0.601887047 epoch total loss 0.596710563\n",
      "Trained batch 2758 batch loss 0.706255555 epoch total loss 0.596750319\n",
      "Trained batch 2759 batch loss 0.624854147 epoch total loss 0.596760511\n",
      "Trained batch 2760 batch loss 0.510360897 epoch total loss 0.596729219\n",
      "Trained batch 2761 batch loss 0.517841816 epoch total loss 0.596700668\n",
      "Trained batch 2762 batch loss 0.571930647 epoch total loss 0.596691668\n",
      "Trained batch 2763 batch loss 0.517169237 epoch total loss 0.596662879\n",
      "Trained batch 2764 batch loss 0.578691721 epoch total loss 0.596656442\n",
      "Trained batch 2765 batch loss 0.627775073 epoch total loss 0.596667707\n",
      "Trained batch 2766 batch loss 0.635803938 epoch total loss 0.596681833\n",
      "Trained batch 2767 batch loss 0.635534644 epoch total loss 0.5966959\n",
      "Trained batch 2768 batch loss 0.66151 epoch total loss 0.596719265\n",
      "Trained batch 2769 batch loss 0.648959875 epoch total loss 0.59673816\n",
      "Trained batch 2770 batch loss 0.610318363 epoch total loss 0.596743047\n",
      "Trained batch 2771 batch loss 0.619748116 epoch total loss 0.596751332\n",
      "Trained batch 2772 batch loss 0.620403 epoch total loss 0.596759856\n",
      "Trained batch 2773 batch loss 0.557718575 epoch total loss 0.596745789\n",
      "Trained batch 2774 batch loss 0.549552202 epoch total loss 0.596728802\n",
      "Trained batch 2775 batch loss 0.597888291 epoch total loss 0.596729219\n",
      "Trained batch 2776 batch loss 0.50776577 epoch total loss 0.596697211\n",
      "Epoch 4 train loss 0.596697211265564\n",
      "Validated batch 1 batch loss 0.613013446\n",
      "Validated batch 2 batch loss 0.638973176\n",
      "Validated batch 3 batch loss 0.552031219\n",
      "Validated batch 4 batch loss 0.628667951\n",
      "Validated batch 5 batch loss 0.578746378\n",
      "Validated batch 6 batch loss 0.620259523\n",
      "Validated batch 7 batch loss 0.548774719\n",
      "Validated batch 8 batch loss 0.612865567\n",
      "Validated batch 9 batch loss 0.645521045\n",
      "Validated batch 10 batch loss 0.671657801\n",
      "Validated batch 11 batch loss 0.727806628\n",
      "Validated batch 12 batch loss 0.573297739\n",
      "Validated batch 13 batch loss 0.543375\n",
      "Validated batch 14 batch loss 0.607975483\n",
      "Validated batch 15 batch loss 0.617669404\n",
      "Validated batch 16 batch loss 0.631793857\n",
      "Validated batch 17 batch loss 0.586177289\n",
      "Validated batch 18 batch loss 0.644128561\n",
      "Validated batch 19 batch loss 0.598854184\n",
      "Validated batch 20 batch loss 0.616774917\n",
      "Validated batch 21 batch loss 0.595587909\n",
      "Validated batch 22 batch loss 0.646484315\n",
      "Validated batch 23 batch loss 0.574896693\n",
      "Validated batch 24 batch loss 0.548139632\n",
      "Validated batch 25 batch loss 0.650373876\n",
      "Validated batch 26 batch loss 0.718701899\n",
      "Validated batch 27 batch loss 0.590192676\n",
      "Validated batch 28 batch loss 0.549482\n",
      "Validated batch 29 batch loss 0.659045219\n",
      "Validated batch 30 batch loss 0.609234929\n",
      "Validated batch 31 batch loss 0.626582205\n",
      "Validated batch 32 batch loss 0.644084156\n",
      "Validated batch 33 batch loss 0.615782\n",
      "Validated batch 34 batch loss 0.672814786\n",
      "Validated batch 35 batch loss 0.555306911\n",
      "Validated batch 36 batch loss 0.487095416\n",
      "Validated batch 37 batch loss 0.646158695\n",
      "Validated batch 38 batch loss 0.626679242\n",
      "Validated batch 39 batch loss 0.601282\n",
      "Validated batch 40 batch loss 0.542890906\n",
      "Validated batch 41 batch loss 0.607390702\n",
      "Validated batch 42 batch loss 0.636962056\n",
      "Validated batch 43 batch loss 0.576927304\n",
      "Validated batch 44 batch loss 0.633056104\n",
      "Validated batch 45 batch loss 0.584675193\n",
      "Validated batch 46 batch loss 0.63011235\n",
      "Validated batch 47 batch loss 0.558942556\n",
      "Validated batch 48 batch loss 0.626829147\n",
      "Validated batch 49 batch loss 0.642750442\n",
      "Validated batch 50 batch loss 0.521694601\n",
      "Validated batch 51 batch loss 0.66239357\n",
      "Validated batch 52 batch loss 0.496181756\n",
      "Validated batch 53 batch loss 0.548052371\n",
      "Validated batch 54 batch loss 0.611857295\n",
      "Validated batch 55 batch loss 0.621725857\n",
      "Validated batch 56 batch loss 0.509185255\n",
      "Validated batch 57 batch loss 0.710285068\n",
      "Validated batch 58 batch loss 0.544067562\n",
      "Validated batch 59 batch loss 0.547561\n",
      "Validated batch 60 batch loss 0.585974\n",
      "Validated batch 61 batch loss 0.516619802\n",
      "Validated batch 62 batch loss 0.499766409\n",
      "Validated batch 63 batch loss 0.523242\n",
      "Validated batch 64 batch loss 0.603595853\n",
      "Validated batch 65 batch loss 0.59919858\n",
      "Validated batch 66 batch loss 0.526761055\n",
      "Validated batch 67 batch loss 0.541568577\n",
      "Validated batch 68 batch loss 0.554210067\n",
      "Validated batch 69 batch loss 0.645264626\n",
      "Validated batch 70 batch loss 0.506346047\n",
      "Validated batch 71 batch loss 0.540942311\n",
      "Validated batch 72 batch loss 0.638679385\n",
      "Validated batch 73 batch loss 0.548590422\n",
      "Validated batch 74 batch loss 0.573238671\n",
      "Validated batch 75 batch loss 0.650906801\n",
      "Validated batch 76 batch loss 0.632663369\n",
      "Validated batch 77 batch loss 0.643688858\n",
      "Validated batch 78 batch loss 0.587640285\n",
      "Validated batch 79 batch loss 0.588398874\n",
      "Validated batch 80 batch loss 0.599495232\n",
      "Validated batch 81 batch loss 0.66856575\n",
      "Validated batch 82 batch loss 0.583691895\n",
      "Validated batch 83 batch loss 0.500181556\n",
      "Validated batch 84 batch loss 0.502487719\n",
      "Validated batch 85 batch loss 0.510402381\n",
      "Validated batch 86 batch loss 0.634769678\n",
      "Validated batch 87 batch loss 0.577908158\n",
      "Validated batch 88 batch loss 0.511778355\n",
      "Validated batch 89 batch loss 0.596670747\n",
      "Validated batch 90 batch loss 0.618801236\n",
      "Validated batch 91 batch loss 0.686017394\n",
      "Validated batch 92 batch loss 0.638438821\n",
      "Validated batch 93 batch loss 0.550328135\n",
      "Validated batch 94 batch loss 0.595357597\n",
      "Validated batch 95 batch loss 0.527518511\n",
      "Validated batch 96 batch loss 0.539455652\n",
      "Validated batch 97 batch loss 0.57041204\n",
      "Validated batch 98 batch loss 0.572538435\n",
      "Validated batch 99 batch loss 0.590079486\n",
      "Validated batch 100 batch loss 0.531541824\n",
      "Validated batch 101 batch loss 0.596372962\n",
      "Validated batch 102 batch loss 0.591205776\n",
      "Validated batch 103 batch loss 0.591412723\n",
      "Validated batch 104 batch loss 0.61395061\n",
      "Validated batch 105 batch loss 0.567292929\n",
      "Validated batch 106 batch loss 0.634184957\n",
      "Validated batch 107 batch loss 0.562955797\n",
      "Validated batch 108 batch loss 0.576707\n",
      "Validated batch 109 batch loss 0.657126069\n",
      "Validated batch 110 batch loss 0.539135754\n",
      "Validated batch 111 batch loss 0.533633053\n",
      "Validated batch 112 batch loss 0.609526038\n",
      "Validated batch 113 batch loss 0.687462\n",
      "Validated batch 114 batch loss 0.470327675\n",
      "Validated batch 115 batch loss 0.677575707\n",
      "Validated batch 116 batch loss 0.571454167\n",
      "Validated batch 117 batch loss 0.522529483\n",
      "Validated batch 118 batch loss 0.615270734\n",
      "Validated batch 119 batch loss 0.571194887\n",
      "Validated batch 120 batch loss 0.558250546\n",
      "Validated batch 121 batch loss 0.617994368\n",
      "Validated batch 122 batch loss 0.569803715\n",
      "Validated batch 123 batch loss 0.538520277\n",
      "Validated batch 124 batch loss 0.614765763\n",
      "Validated batch 125 batch loss 0.66313386\n",
      "Validated batch 126 batch loss 0.678516626\n",
      "Validated batch 127 batch loss 0.547023535\n",
      "Validated batch 128 batch loss 0.682522058\n",
      "Validated batch 129 batch loss 0.5688169\n",
      "Validated batch 130 batch loss 0.480267107\n",
      "Validated batch 131 batch loss 0.617226481\n",
      "Validated batch 132 batch loss 0.651520669\n",
      "Validated batch 133 batch loss 0.581042886\n",
      "Validated batch 134 batch loss 0.597728133\n",
      "Validated batch 135 batch loss 0.608226717\n",
      "Validated batch 136 batch loss 0.472185612\n",
      "Validated batch 137 batch loss 0.611606419\n",
      "Validated batch 138 batch loss 0.580860794\n",
      "Validated batch 139 batch loss 0.589135647\n",
      "Validated batch 140 batch loss 0.625763\n",
      "Validated batch 141 batch loss 0.650033951\n",
      "Validated batch 142 batch loss 0.559914052\n",
      "Validated batch 143 batch loss 0.48891446\n",
      "Validated batch 144 batch loss 0.598505855\n",
      "Validated batch 145 batch loss 0.598823607\n",
      "Validated batch 146 batch loss 0.529317796\n",
      "Validated batch 147 batch loss 0.544726372\n",
      "Validated batch 148 batch loss 0.596364617\n",
      "Validated batch 149 batch loss 0.55459094\n",
      "Validated batch 150 batch loss 0.605295897\n",
      "Validated batch 151 batch loss 0.611946523\n",
      "Validated batch 152 batch loss 0.590280473\n",
      "Validated batch 153 batch loss 0.517387807\n",
      "Validated batch 154 batch loss 0.630261242\n",
      "Validated batch 155 batch loss 0.468278885\n",
      "Validated batch 156 batch loss 0.673220634\n",
      "Validated batch 157 batch loss 0.657806098\n",
      "Validated batch 158 batch loss 0.6142115\n",
      "Validated batch 159 batch loss 0.558580458\n",
      "Validated batch 160 batch loss 0.503723562\n",
      "Validated batch 161 batch loss 0.54423213\n",
      "Validated batch 162 batch loss 0.503111064\n",
      "Validated batch 163 batch loss 0.623711348\n",
      "Validated batch 164 batch loss 0.629885316\n",
      "Validated batch 165 batch loss 0.603571177\n",
      "Validated batch 166 batch loss 0.680796802\n",
      "Validated batch 167 batch loss 0.606386\n",
      "Validated batch 168 batch loss 0.680781722\n",
      "Validated batch 169 batch loss 0.662659287\n",
      "Validated batch 170 batch loss 0.695460737\n",
      "Validated batch 171 batch loss 0.528552353\n",
      "Validated batch 172 batch loss 0.628597856\n",
      "Validated batch 173 batch loss 0.681303\n",
      "Validated batch 174 batch loss 0.648434877\n",
      "Validated batch 175 batch loss 0.571950555\n",
      "Validated batch 176 batch loss 0.574249387\n",
      "Validated batch 177 batch loss 0.74741286\n",
      "Validated batch 178 batch loss 0.526551187\n",
      "Validated batch 179 batch loss 0.604263246\n",
      "Validated batch 180 batch loss 0.623422205\n",
      "Validated batch 181 batch loss 0.508786738\n",
      "Validated batch 182 batch loss 0.428422838\n",
      "Validated batch 183 batch loss 0.525524139\n",
      "Validated batch 184 batch loss 0.636928082\n",
      "Validated batch 185 batch loss 0.541995823\n",
      "Validated batch 186 batch loss 0.577033699\n",
      "Validated batch 187 batch loss 0.630929708\n",
      "Validated batch 188 batch loss 0.605181754\n",
      "Validated batch 189 batch loss 0.556183457\n",
      "Validated batch 190 batch loss 0.597790182\n",
      "Validated batch 191 batch loss 0.535722673\n",
      "Validated batch 192 batch loss 0.561246455\n",
      "Validated batch 193 batch loss 0.611507654\n",
      "Validated batch 194 batch loss 0.560349345\n",
      "Validated batch 195 batch loss 0.603506327\n",
      "Validated batch 196 batch loss 0.638267398\n",
      "Validated batch 197 batch loss 0.628527641\n",
      "Validated batch 198 batch loss 0.638625\n",
      "Validated batch 199 batch loss 0.58394\n",
      "Validated batch 200 batch loss 0.668663859\n",
      "Validated batch 201 batch loss 0.575900257\n",
      "Validated batch 202 batch loss 0.635519385\n",
      "Validated batch 203 batch loss 0.573697388\n",
      "Validated batch 204 batch loss 0.561020851\n",
      "Validated batch 205 batch loss 0.583600163\n",
      "Validated batch 206 batch loss 0.595879495\n",
      "Validated batch 207 batch loss 0.615877748\n",
      "Validated batch 208 batch loss 0.558247209\n",
      "Validated batch 209 batch loss 0.566356778\n",
      "Validated batch 210 batch loss 0.636915565\n",
      "Validated batch 211 batch loss 0.668562651\n",
      "Validated batch 212 batch loss 0.622709632\n",
      "Validated batch 213 batch loss 0.673594\n",
      "Validated batch 214 batch loss 0.595754325\n",
      "Validated batch 215 batch loss 0.59445262\n",
      "Validated batch 216 batch loss 0.656838834\n",
      "Validated batch 217 batch loss 0.587513745\n",
      "Validated batch 218 batch loss 0.767129421\n",
      "Validated batch 219 batch loss 0.547703207\n",
      "Validated batch 220 batch loss 0.517785072\n",
      "Validated batch 221 batch loss 0.623397887\n",
      "Validated batch 222 batch loss 0.575546741\n",
      "Validated batch 223 batch loss 0.596025586\n",
      "Validated batch 224 batch loss 0.468413085\n",
      "Validated batch 225 batch loss 0.4965702\n",
      "Validated batch 226 batch loss 0.611985147\n",
      "Validated batch 227 batch loss 0.636116803\n",
      "Validated batch 228 batch loss 0.645171463\n",
      "Validated batch 229 batch loss 0.541530252\n",
      "Validated batch 230 batch loss 0.555888772\n",
      "Validated batch 231 batch loss 0.595417738\n",
      "Validated batch 232 batch loss 0.583200932\n",
      "Validated batch 233 batch loss 0.579873443\n",
      "Validated batch 234 batch loss 0.555321455\n",
      "Validated batch 235 batch loss 0.56487453\n",
      "Validated batch 236 batch loss 0.591425538\n",
      "Validated batch 237 batch loss 0.575691223\n",
      "Validated batch 238 batch loss 0.503595352\n",
      "Validated batch 239 batch loss 0.628555059\n",
      "Validated batch 240 batch loss 0.526908755\n",
      "Validated batch 241 batch loss 0.624976158\n",
      "Validated batch 242 batch loss 0.655811\n",
      "Validated batch 243 batch loss 0.561193883\n",
      "Validated batch 244 batch loss 0.624788642\n",
      "Validated batch 245 batch loss 0.619868\n",
      "Validated batch 246 batch loss 0.638123214\n",
      "Validated batch 247 batch loss 0.674431205\n",
      "Validated batch 248 batch loss 0.603453338\n",
      "Validated batch 249 batch loss 0.677822769\n",
      "Validated batch 250 batch loss 0.571111619\n",
      "Validated batch 251 batch loss 0.60707134\n",
      "Validated batch 252 batch loss 0.583518386\n",
      "Validated batch 253 batch loss 0.671405792\n",
      "Validated batch 254 batch loss 0.690239787\n",
      "Validated batch 255 batch loss 0.583372593\n",
      "Validated batch 256 batch loss 0.604000688\n",
      "Validated batch 257 batch loss 0.670533478\n",
      "Validated batch 258 batch loss 0.604512572\n",
      "Validated batch 259 batch loss 0.634492457\n",
      "Validated batch 260 batch loss 0.624163628\n",
      "Validated batch 261 batch loss 0.675483882\n",
      "Validated batch 262 batch loss 0.641233504\n",
      "Validated batch 263 batch loss 0.66244936\n",
      "Validated batch 264 batch loss 0.602868855\n",
      "Validated batch 265 batch loss 0.624229193\n",
      "Validated batch 266 batch loss 0.49273783\n",
      "Validated batch 267 batch loss 0.577929616\n",
      "Validated batch 268 batch loss 0.589283943\n",
      "Validated batch 269 batch loss 0.614961863\n",
      "Validated batch 270 batch loss 0.628744483\n",
      "Validated batch 271 batch loss 0.671905637\n",
      "Validated batch 272 batch loss 0.600816727\n",
      "Validated batch 273 batch loss 0.559567094\n",
      "Validated batch 274 batch loss 0.585808456\n",
      "Validated batch 275 batch loss 0.622758389\n",
      "Validated batch 276 batch loss 0.582286477\n",
      "Validated batch 277 batch loss 0.586502075\n",
      "Validated batch 278 batch loss 0.539316297\n",
      "Validated batch 279 batch loss 0.589361548\n",
      "Validated batch 280 batch loss 0.579795\n",
      "Validated batch 281 batch loss 0.628187537\n",
      "Validated batch 282 batch loss 0.631663799\n",
      "Validated batch 283 batch loss 0.581265628\n",
      "Validated batch 284 batch loss 0.549523175\n",
      "Validated batch 285 batch loss 0.603410959\n",
      "Validated batch 286 batch loss 0.652668953\n",
      "Validated batch 287 batch loss 0.683274806\n",
      "Validated batch 288 batch loss 0.695054173\n",
      "Validated batch 289 batch loss 0.5887869\n",
      "Validated batch 290 batch loss 0.497466683\n",
      "Validated batch 291 batch loss 0.597025335\n",
      "Validated batch 292 batch loss 0.612509727\n",
      "Validated batch 293 batch loss 0.626119494\n",
      "Validated batch 294 batch loss 0.578075588\n",
      "Validated batch 295 batch loss 0.624873\n",
      "Validated batch 296 batch loss 0.624194443\n",
      "Validated batch 297 batch loss 0.641649663\n",
      "Validated batch 298 batch loss 0.625891566\n",
      "Validated batch 299 batch loss 0.570384681\n",
      "Validated batch 300 batch loss 0.59786886\n",
      "Validated batch 301 batch loss 0.438020349\n",
      "Validated batch 302 batch loss 0.515363097\n",
      "Validated batch 303 batch loss 0.612958133\n",
      "Validated batch 304 batch loss 0.559044659\n",
      "Validated batch 305 batch loss 0.57283473\n",
      "Validated batch 306 batch loss 0.520761728\n",
      "Validated batch 307 batch loss 0.58279705\n",
      "Validated batch 308 batch loss 0.621909916\n",
      "Validated batch 309 batch loss 0.653855324\n",
      "Validated batch 310 batch loss 0.609565318\n",
      "Validated batch 311 batch loss 0.545161963\n",
      "Validated batch 312 batch loss 0.504720926\n",
      "Validated batch 313 batch loss 0.618415356\n",
      "Validated batch 314 batch loss 0.574884415\n",
      "Validated batch 315 batch loss 0.653709114\n",
      "Validated batch 316 batch loss 0.663436949\n",
      "Validated batch 317 batch loss 0.574284315\n",
      "Validated batch 318 batch loss 0.653338552\n",
      "Validated batch 319 batch loss 0.597324252\n",
      "Validated batch 320 batch loss 0.559267282\n",
      "Validated batch 321 batch loss 0.583072245\n",
      "Validated batch 322 batch loss 0.509944499\n",
      "Validated batch 323 batch loss 0.623899\n",
      "Validated batch 324 batch loss 0.643878\n",
      "Validated batch 325 batch loss 0.62275368\n",
      "Validated batch 326 batch loss 0.573002\n",
      "Validated batch 327 batch loss 0.685942292\n",
      "Validated batch 328 batch loss 0.517268777\n",
      "Validated batch 329 batch loss 0.599236965\n",
      "Validated batch 330 batch loss 0.57939893\n",
      "Validated batch 331 batch loss 0.552688122\n",
      "Validated batch 332 batch loss 0.493849248\n",
      "Validated batch 333 batch loss 0.611303926\n",
      "Validated batch 334 batch loss 0.622638404\n",
      "Validated batch 335 batch loss 0.576697767\n",
      "Validated batch 336 batch loss 0.664176583\n",
      "Validated batch 337 batch loss 0.610396\n",
      "Validated batch 338 batch loss 0.53470397\n",
      "Validated batch 339 batch loss 0.519672751\n",
      "Validated batch 340 batch loss 0.556405842\n",
      "Validated batch 341 batch loss 0.655113876\n",
      "Validated batch 342 batch loss 0.609055936\n",
      "Validated batch 343 batch loss 0.604763806\n",
      "Validated batch 344 batch loss 0.600637615\n",
      "Validated batch 345 batch loss 0.654344857\n",
      "Validated batch 346 batch loss 0.64185828\n",
      "Validated batch 347 batch loss 0.562687933\n",
      "Validated batch 348 batch loss 0.648876488\n",
      "Validated batch 349 batch loss 0.594278455\n",
      "Validated batch 350 batch loss 0.52325356\n",
      "Validated batch 351 batch loss 0.652863\n",
      "Validated batch 352 batch loss 0.541768372\n",
      "Validated batch 353 batch loss 0.572977245\n",
      "Validated batch 354 batch loss 0.624787092\n",
      "Validated batch 355 batch loss 0.565712452\n",
      "Validated batch 356 batch loss 0.612181425\n",
      "Validated batch 357 batch loss 0.555698574\n",
      "Validated batch 358 batch loss 0.656953454\n",
      "Validated batch 359 batch loss 0.620256662\n",
      "Validated batch 360 batch loss 0.606777549\n",
      "Validated batch 361 batch loss 0.641133428\n",
      "Validated batch 362 batch loss 0.738849163\n",
      "Validated batch 363 batch loss 0.709700704\n",
      "Validated batch 364 batch loss 0.637465417\n",
      "Validated batch 365 batch loss 0.590489626\n",
      "Validated batch 366 batch loss 0.633205295\n",
      "Validated batch 367 batch loss 0.59597373\n",
      "Validated batch 368 batch loss 0.54292\n",
      "Validated batch 369 batch loss 0.522772551\n",
      "Epoch 4 val loss 0.5959016680717468\n",
      "Model /aiffel/mpii/weights/shg_model-epoch-4-loss-0.5959.h5 saved.\n",
      "Start epoch 5 with learning rate 0.0007\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 0.60802567 epoch total loss 0.60802567\n",
      "Trained batch 2 batch loss 0.594799042 epoch total loss 0.601412356\n",
      "Trained batch 3 batch loss 0.55838418 epoch total loss 0.587069631\n",
      "Trained batch 4 batch loss 0.541583478 epoch total loss 0.575698078\n",
      "Trained batch 5 batch loss 0.594545841 epoch total loss 0.579467654\n",
      "Trained batch 6 batch loss 0.539244235 epoch total loss 0.572763741\n",
      "Trained batch 7 batch loss 0.509271324 epoch total loss 0.563693404\n",
      "Trained batch 8 batch loss 0.55396682 epoch total loss 0.562477589\n",
      "Trained batch 9 batch loss 0.576880574 epoch total loss 0.564077914\n",
      "Trained batch 10 batch loss 0.532072306 epoch total loss 0.560877323\n",
      "Trained batch 11 batch loss 0.532292068 epoch total loss 0.55827862\n",
      "Trained batch 12 batch loss 0.605329275 epoch total loss 0.562199533\n",
      "Trained batch 13 batch loss 0.55306828 epoch total loss 0.561497092\n",
      "Trained batch 14 batch loss 0.624692798 epoch total loss 0.566011071\n",
      "Trained batch 15 batch loss 0.532746315 epoch total loss 0.563793421\n",
      "Trained batch 16 batch loss 0.515541077 epoch total loss 0.560777664\n",
      "Trained batch 17 batch loss 0.568742037 epoch total loss 0.561246157\n",
      "Trained batch 18 batch loss 0.465880215 epoch total loss 0.555948\n",
      "Trained batch 19 batch loss 0.452178925 epoch total loss 0.550486505\n",
      "Trained batch 20 batch loss 0.407290041 epoch total loss 0.543326735\n",
      "Trained batch 21 batch loss 0.488195688 epoch total loss 0.540701389\n",
      "Trained batch 22 batch loss 0.489919752 epoch total loss 0.53839314\n",
      "Trained batch 23 batch loss 0.617416 epoch total loss 0.54182893\n",
      "Trained batch 24 batch loss 0.703829467 epoch total loss 0.548579\n",
      "Trained batch 25 batch loss 0.603208661 epoch total loss 0.550764143\n",
      "Trained batch 26 batch loss 0.558912933 epoch total loss 0.551077604\n",
      "Trained batch 27 batch loss 0.56887126 epoch total loss 0.551736593\n",
      "Trained batch 28 batch loss 0.582842946 epoch total loss 0.552847564\n",
      "Trained batch 29 batch loss 0.639419436 epoch total loss 0.555832744\n",
      "Trained batch 30 batch loss 0.54814589 epoch total loss 0.555576503\n",
      "Trained batch 31 batch loss 0.531775951 epoch total loss 0.554808795\n",
      "Trained batch 32 batch loss 0.583303094 epoch total loss 0.555699229\n",
      "Trained batch 33 batch loss 0.51286149 epoch total loss 0.5544011\n",
      "Trained batch 34 batch loss 0.463915497 epoch total loss 0.551739752\n",
      "Trained batch 35 batch loss 0.519166827 epoch total loss 0.550809085\n",
      "Trained batch 36 batch loss 0.549051285 epoch total loss 0.550760269\n",
      "Trained batch 37 batch loss 0.518749535 epoch total loss 0.549895108\n",
      "Trained batch 38 batch loss 0.582310081 epoch total loss 0.55074811\n",
      "Trained batch 39 batch loss 0.54848361 epoch total loss 0.550690055\n",
      "Trained batch 40 batch loss 0.596509 epoch total loss 0.551835537\n",
      "Trained batch 41 batch loss 0.586770713 epoch total loss 0.552687645\n",
      "Trained batch 42 batch loss 0.548683107 epoch total loss 0.552592278\n",
      "Trained batch 43 batch loss 0.6317904 epoch total loss 0.554434061\n",
      "Trained batch 44 batch loss 0.624195457 epoch total loss 0.556019545\n",
      "Trained batch 45 batch loss 0.602785408 epoch total loss 0.557058811\n",
      "Trained batch 46 batch loss 0.583426 epoch total loss 0.557631969\n",
      "Trained batch 47 batch loss 0.558977723 epoch total loss 0.557660639\n",
      "Trained batch 48 batch loss 0.563778341 epoch total loss 0.557788074\n",
      "Trained batch 49 batch loss 0.624465823 epoch total loss 0.559148848\n",
      "Trained batch 50 batch loss 0.578414142 epoch total loss 0.559534132\n",
      "Trained batch 51 batch loss 0.571998119 epoch total loss 0.559778571\n",
      "Trained batch 52 batch loss 0.632588387 epoch total loss 0.561178744\n",
      "Trained batch 53 batch loss 0.58531183 epoch total loss 0.561634064\n",
      "Trained batch 54 batch loss 0.651015341 epoch total loss 0.563289285\n",
      "Trained batch 55 batch loss 0.590122163 epoch total loss 0.563777149\n",
      "Trained batch 56 batch loss 0.529421508 epoch total loss 0.563163638\n",
      "Trained batch 57 batch loss 0.506553054 epoch total loss 0.562170446\n",
      "Trained batch 58 batch loss 0.586598396 epoch total loss 0.562591612\n",
      "Trained batch 59 batch loss 0.575953841 epoch total loss 0.56281811\n",
      "Trained batch 60 batch loss 0.531274319 epoch total loss 0.562292337\n",
      "Trained batch 61 batch loss 0.487559587 epoch total loss 0.561067224\n",
      "Trained batch 62 batch loss 0.469463706 epoch total loss 0.559589744\n",
      "Trained batch 63 batch loss 0.471223533 epoch total loss 0.558187068\n",
      "Trained batch 64 batch loss 0.486301631 epoch total loss 0.557063878\n",
      "Trained batch 65 batch loss 0.494149208 epoch total loss 0.556095958\n",
      "Trained batch 66 batch loss 0.570399821 epoch total loss 0.55631268\n",
      "Trained batch 67 batch loss 0.585726738 epoch total loss 0.556751728\n",
      "Trained batch 68 batch loss 0.511949182 epoch total loss 0.556092799\n",
      "Trained batch 69 batch loss 0.569705248 epoch total loss 0.55629009\n",
      "Trained batch 70 batch loss 0.555286229 epoch total loss 0.556275785\n",
      "Trained batch 71 batch loss 0.5325 epoch total loss 0.555940926\n",
      "Trained batch 72 batch loss 0.495109469 epoch total loss 0.55509603\n",
      "Trained batch 73 batch loss 0.550890803 epoch total loss 0.555038452\n",
      "Trained batch 74 batch loss 0.499731928 epoch total loss 0.55429107\n",
      "Trained batch 75 batch loss 0.55416 epoch total loss 0.554289341\n",
      "Trained batch 76 batch loss 0.58376807 epoch total loss 0.554677188\n",
      "Trained batch 77 batch loss 0.458060205 epoch total loss 0.553422451\n",
      "Trained batch 78 batch loss 0.52739948 epoch total loss 0.553088844\n",
      "Trained batch 79 batch loss 0.536073387 epoch total loss 0.552873433\n",
      "Trained batch 80 batch loss 0.717358589 epoch total loss 0.554929495\n",
      "Trained batch 81 batch loss 0.598276 epoch total loss 0.555464625\n",
      "Trained batch 82 batch loss 0.6318416 epoch total loss 0.556396\n",
      "Trained batch 83 batch loss 0.605651677 epoch total loss 0.556989491\n",
      "Trained batch 84 batch loss 0.59766531 epoch total loss 0.55747366\n",
      "Trained batch 85 batch loss 0.563111305 epoch total loss 0.55754\n",
      "Trained batch 86 batch loss 0.56831497 epoch total loss 0.557665288\n",
      "Trained batch 87 batch loss 0.519640148 epoch total loss 0.557228208\n",
      "Trained batch 88 batch loss 0.522501111 epoch total loss 0.556833625\n",
      "Trained batch 89 batch loss 0.547402561 epoch total loss 0.556727648\n",
      "Trained batch 90 batch loss 0.604131341 epoch total loss 0.557254314\n",
      "Trained batch 91 batch loss 0.519473195 epoch total loss 0.556839168\n",
      "Trained batch 92 batch loss 0.552754223 epoch total loss 0.556794763\n",
      "Trained batch 93 batch loss 0.502520502 epoch total loss 0.556211174\n",
      "Trained batch 94 batch loss 0.563341856 epoch total loss 0.55628705\n",
      "Trained batch 95 batch loss 0.483933657 epoch total loss 0.555525422\n",
      "Trained batch 96 batch loss 0.427148849 epoch total loss 0.554188192\n",
      "Trained batch 97 batch loss 0.591544151 epoch total loss 0.554573298\n",
      "Trained batch 98 batch loss 0.522631764 epoch total loss 0.554247379\n",
      "Trained batch 99 batch loss 0.411408097 epoch total loss 0.55280453\n",
      "Trained batch 100 batch loss 0.480759948 epoch total loss 0.552084088\n",
      "Trained batch 101 batch loss 0.540313423 epoch total loss 0.551967561\n",
      "Trained batch 102 batch loss 0.538043737 epoch total loss 0.551831067\n",
      "Trained batch 103 batch loss 0.489635408 epoch total loss 0.551227212\n",
      "Trained batch 104 batch loss 0.517543554 epoch total loss 0.55090332\n",
      "Trained batch 105 batch loss 0.459188282 epoch total loss 0.550029814\n",
      "Trained batch 106 batch loss 0.49163124 epoch total loss 0.549478889\n",
      "Trained batch 107 batch loss 0.559068084 epoch total loss 0.549568474\n",
      "Trained batch 108 batch loss 0.527404249 epoch total loss 0.549363256\n",
      "Trained batch 109 batch loss 0.490573555 epoch total loss 0.548823953\n",
      "Trained batch 110 batch loss 0.549124897 epoch total loss 0.548826694\n",
      "Trained batch 111 batch loss 0.525072396 epoch total loss 0.548612654\n",
      "Trained batch 112 batch loss 0.521378219 epoch total loss 0.548369527\n",
      "Trained batch 113 batch loss 0.540554881 epoch total loss 0.548300326\n",
      "Trained batch 114 batch loss 0.714042068 epoch total loss 0.549754202\n",
      "Trained batch 115 batch loss 0.659967422 epoch total loss 0.550712585\n",
      "Trained batch 116 batch loss 0.721940756 epoch total loss 0.552188694\n",
      "Trained batch 117 batch loss 0.636240959 epoch total loss 0.55290705\n",
      "Trained batch 118 batch loss 0.516238213 epoch total loss 0.552596271\n",
      "Trained batch 119 batch loss 0.501617432 epoch total loss 0.552167892\n",
      "Trained batch 120 batch loss 0.67544353 epoch total loss 0.553195179\n",
      "Trained batch 121 batch loss 0.604440391 epoch total loss 0.55361867\n",
      "Trained batch 122 batch loss 0.656403601 epoch total loss 0.554461181\n",
      "Trained batch 123 batch loss 0.619596601 epoch total loss 0.554990768\n",
      "Trained batch 124 batch loss 0.542594 epoch total loss 0.554890811\n",
      "Trained batch 125 batch loss 0.584899783 epoch total loss 0.555130839\n",
      "Trained batch 126 batch loss 0.57643038 epoch total loss 0.555299938\n",
      "Trained batch 127 batch loss 0.614692 epoch total loss 0.555767596\n",
      "Trained batch 128 batch loss 0.584059 epoch total loss 0.55598861\n",
      "Trained batch 129 batch loss 0.440484017 epoch total loss 0.555093229\n",
      "Trained batch 130 batch loss 0.584679246 epoch total loss 0.555320799\n",
      "Trained batch 131 batch loss 0.729221225 epoch total loss 0.556648254\n",
      "Trained batch 132 batch loss 0.676926553 epoch total loss 0.557559431\n",
      "Trained batch 133 batch loss 0.632935703 epoch total loss 0.558126152\n",
      "Trained batch 134 batch loss 0.655497611 epoch total loss 0.558852792\n",
      "Trained batch 135 batch loss 0.603693604 epoch total loss 0.559184968\n",
      "Trained batch 136 batch loss 0.594945967 epoch total loss 0.559447885\n",
      "Trained batch 137 batch loss 0.569923937 epoch total loss 0.559524357\n",
      "Trained batch 138 batch loss 0.742486835 epoch total loss 0.560850143\n",
      "Trained batch 139 batch loss 0.67568028 epoch total loss 0.561676323\n",
      "Trained batch 140 batch loss 0.596919 epoch total loss 0.561928034\n",
      "Trained batch 141 batch loss 0.660090327 epoch total loss 0.562624156\n",
      "Trained batch 142 batch loss 0.567479432 epoch total loss 0.56265837\n",
      "Trained batch 143 batch loss 0.650692403 epoch total loss 0.563274\n",
      "Trained batch 144 batch loss 0.551052928 epoch total loss 0.563189209\n",
      "Trained batch 145 batch loss 0.529811382 epoch total loss 0.562958956\n",
      "Trained batch 146 batch loss 0.592376411 epoch total loss 0.563160479\n",
      "Trained batch 147 batch loss 0.533447742 epoch total loss 0.56295836\n",
      "Trained batch 148 batch loss 0.546381354 epoch total loss 0.562846303\n",
      "Trained batch 149 batch loss 0.564619839 epoch total loss 0.562858224\n",
      "Trained batch 150 batch loss 0.616704762 epoch total loss 0.563217223\n",
      "Trained batch 151 batch loss 0.643777311 epoch total loss 0.563750744\n",
      "Trained batch 152 batch loss 0.597710431 epoch total loss 0.563974142\n",
      "Trained batch 153 batch loss 0.590200067 epoch total loss 0.564145565\n",
      "Trained batch 154 batch loss 0.644628882 epoch total loss 0.564668179\n",
      "Trained batch 155 batch loss 0.614485502 epoch total loss 0.564989567\n",
      "Trained batch 156 batch loss 0.695518 epoch total loss 0.565826297\n",
      "Trained batch 157 batch loss 0.650654078 epoch total loss 0.566366673\n",
      "Trained batch 158 batch loss 0.555463076 epoch total loss 0.56629765\n",
      "Trained batch 159 batch loss 0.627701759 epoch total loss 0.566683829\n",
      "Trained batch 160 batch loss 0.617282331 epoch total loss 0.567000031\n",
      "Trained batch 161 batch loss 0.673395932 epoch total loss 0.567660868\n",
      "Trained batch 162 batch loss 0.533558547 epoch total loss 0.567450404\n",
      "Trained batch 163 batch loss 0.556597114 epoch total loss 0.567383766\n",
      "Trained batch 164 batch loss 0.547243118 epoch total loss 0.567261\n",
      "Trained batch 165 batch loss 0.529909253 epoch total loss 0.567034602\n",
      "Trained batch 166 batch loss 0.498924375 epoch total loss 0.566624284\n",
      "Trained batch 167 batch loss 0.552646756 epoch total loss 0.566540599\n",
      "Trained batch 168 batch loss 0.510991 epoch total loss 0.56621\n",
      "Trained batch 169 batch loss 0.655762374 epoch total loss 0.566739857\n",
      "Trained batch 170 batch loss 0.663948894 epoch total loss 0.567311704\n",
      "Trained batch 171 batch loss 0.612742543 epoch total loss 0.567577362\n",
      "Trained batch 172 batch loss 0.586159825 epoch total loss 0.567685366\n",
      "Trained batch 173 batch loss 0.54095763 epoch total loss 0.56753087\n",
      "Trained batch 174 batch loss 0.549126387 epoch total loss 0.567425072\n",
      "Trained batch 175 batch loss 0.590139 epoch total loss 0.567554891\n",
      "Trained batch 176 batch loss 0.629210293 epoch total loss 0.567905188\n",
      "Trained batch 177 batch loss 0.586380839 epoch total loss 0.568009615\n",
      "Trained batch 178 batch loss 0.584136367 epoch total loss 0.568100214\n",
      "Trained batch 179 batch loss 0.561180055 epoch total loss 0.568061531\n",
      "Trained batch 180 batch loss 0.537539959 epoch total loss 0.567891955\n",
      "Trained batch 181 batch loss 0.576622367 epoch total loss 0.567940176\n",
      "Trained batch 182 batch loss 0.595721364 epoch total loss 0.568092823\n",
      "Trained batch 183 batch loss 0.707585335 epoch total loss 0.568855107\n",
      "Trained batch 184 batch loss 0.663485587 epoch total loss 0.569369376\n",
      "Trained batch 185 batch loss 0.671542227 epoch total loss 0.569921613\n",
      "Trained batch 186 batch loss 0.658509254 epoch total loss 0.570397913\n",
      "Trained batch 187 batch loss 0.612251699 epoch total loss 0.570621729\n",
      "Trained batch 188 batch loss 0.581163466 epoch total loss 0.570677757\n",
      "Trained batch 189 batch loss 0.578866839 epoch total loss 0.57072109\n",
      "Trained batch 190 batch loss 0.55641824 epoch total loss 0.570645809\n",
      "Trained batch 191 batch loss 0.559667468 epoch total loss 0.57058835\n",
      "Trained batch 192 batch loss 0.544813335 epoch total loss 0.570454121\n",
      "Trained batch 193 batch loss 0.582889 epoch total loss 0.570518553\n",
      "Trained batch 194 batch loss 0.493300676 epoch total loss 0.570120513\n",
      "Trained batch 195 batch loss 0.617373466 epoch total loss 0.570362806\n",
      "Trained batch 196 batch loss 0.676044345 epoch total loss 0.570902\n",
      "Trained batch 197 batch loss 0.628143549 epoch total loss 0.571192563\n",
      "Trained batch 198 batch loss 0.646572292 epoch total loss 0.571573257\n",
      "Trained batch 199 batch loss 0.666008234 epoch total loss 0.57204783\n",
      "Trained batch 200 batch loss 0.707781434 epoch total loss 0.572726488\n",
      "Trained batch 201 batch loss 0.767932653 epoch total loss 0.573697627\n",
      "Trained batch 202 batch loss 0.649045229 epoch total loss 0.574070632\n",
      "Trained batch 203 batch loss 0.551486433 epoch total loss 0.57395941\n",
      "Trained batch 204 batch loss 0.502713919 epoch total loss 0.573610127\n",
      "Trained batch 205 batch loss 0.525338173 epoch total loss 0.573374689\n",
      "Trained batch 206 batch loss 0.710435808 epoch total loss 0.57404\n",
      "Trained batch 207 batch loss 0.597376287 epoch total loss 0.574152768\n",
      "Trained batch 208 batch loss 0.575627 epoch total loss 0.574159861\n",
      "Trained batch 209 batch loss 0.576114357 epoch total loss 0.574169159\n",
      "Trained batch 210 batch loss 0.676485896 epoch total loss 0.574656367\n",
      "Trained batch 211 batch loss 0.624033332 epoch total loss 0.574890375\n",
      "Trained batch 212 batch loss 0.674447238 epoch total loss 0.57536\n",
      "Trained batch 213 batch loss 0.669832826 epoch total loss 0.575803518\n",
      "Trained batch 214 batch loss 0.519217968 epoch total loss 0.575539112\n",
      "Trained batch 215 batch loss 0.557962418 epoch total loss 0.575457335\n",
      "Trained batch 216 batch loss 0.426400602 epoch total loss 0.574767232\n",
      "Trained batch 217 batch loss 0.535097182 epoch total loss 0.574584424\n",
      "Trained batch 218 batch loss 0.631878674 epoch total loss 0.574847281\n",
      "Trained batch 219 batch loss 0.514592052 epoch total loss 0.574572146\n",
      "Trained batch 220 batch loss 0.600662529 epoch total loss 0.574690759\n",
      "Trained batch 221 batch loss 0.551064968 epoch total loss 0.574583828\n",
      "Trained batch 222 batch loss 0.518190563 epoch total loss 0.574329793\n",
      "Trained batch 223 batch loss 0.413474083 epoch total loss 0.573608458\n",
      "Trained batch 224 batch loss 0.423657 epoch total loss 0.572939038\n",
      "Trained batch 225 batch loss 0.515719414 epoch total loss 0.572684705\n",
      "Trained batch 226 batch loss 0.426496029 epoch total loss 0.572037876\n",
      "Trained batch 227 batch loss 0.508666813 epoch total loss 0.571758747\n",
      "Trained batch 228 batch loss 0.492071569 epoch total loss 0.571409166\n",
      "Trained batch 229 batch loss 0.493752509 epoch total loss 0.571070135\n",
      "Trained batch 230 batch loss 0.522130251 epoch total loss 0.570857286\n",
      "Trained batch 231 batch loss 0.58343184 epoch total loss 0.570911765\n",
      "Trained batch 232 batch loss 0.580024362 epoch total loss 0.570951\n",
      "Trained batch 233 batch loss 0.622468472 epoch total loss 0.571172118\n",
      "Trained batch 234 batch loss 0.566486 epoch total loss 0.571152031\n",
      "Trained batch 235 batch loss 0.495662868 epoch total loss 0.570830822\n",
      "Trained batch 236 batch loss 0.580462277 epoch total loss 0.570871651\n",
      "Trained batch 237 batch loss 0.674880385 epoch total loss 0.57131052\n",
      "Trained batch 238 batch loss 0.651158 epoch total loss 0.571646\n",
      "Trained batch 239 batch loss 0.59375155 epoch total loss 0.571738482\n",
      "Trained batch 240 batch loss 0.640177131 epoch total loss 0.57202363\n",
      "Trained batch 241 batch loss 0.584492385 epoch total loss 0.572075367\n",
      "Trained batch 242 batch loss 0.67403847 epoch total loss 0.572496712\n",
      "Trained batch 243 batch loss 0.594965816 epoch total loss 0.572589219\n",
      "Trained batch 244 batch loss 0.499339938 epoch total loss 0.572289\n",
      "Trained batch 245 batch loss 0.473419785 epoch total loss 0.571885467\n",
      "Trained batch 246 batch loss 0.535863876 epoch total loss 0.571739\n",
      "Trained batch 247 batch loss 0.523093641 epoch total loss 0.571542\n",
      "Trained batch 248 batch loss 0.569463611 epoch total loss 0.57153362\n",
      "Trained batch 249 batch loss 0.548339486 epoch total loss 0.571440458\n",
      "Trained batch 250 batch loss 0.648295164 epoch total loss 0.571747899\n",
      "Trained batch 251 batch loss 0.609279394 epoch total loss 0.571897447\n",
      "Trained batch 252 batch loss 0.663551569 epoch total loss 0.572261214\n",
      "Trained batch 253 batch loss 0.607133269 epoch total loss 0.572399\n",
      "Trained batch 254 batch loss 0.595885873 epoch total loss 0.572491527\n",
      "Trained batch 255 batch loss 0.604328692 epoch total loss 0.572616339\n",
      "Trained batch 256 batch loss 0.595804393 epoch total loss 0.572706938\n",
      "Trained batch 257 batch loss 0.606516957 epoch total loss 0.572838485\n",
      "Trained batch 258 batch loss 0.584566116 epoch total loss 0.572883964\n",
      "Trained batch 259 batch loss 0.471439183 epoch total loss 0.572492242\n",
      "Trained batch 260 batch loss 0.550890326 epoch total loss 0.572409153\n",
      "Trained batch 261 batch loss 0.617171824 epoch total loss 0.572580695\n",
      "Trained batch 262 batch loss 0.507843077 epoch total loss 0.572333574\n",
      "Trained batch 263 batch loss 0.589674354 epoch total loss 0.572399557\n",
      "Trained batch 264 batch loss 0.680917799 epoch total loss 0.57281059\n",
      "Trained batch 265 batch loss 0.715295732 epoch total loss 0.573348284\n",
      "Trained batch 266 batch loss 0.681048751 epoch total loss 0.573753178\n",
      "Trained batch 267 batch loss 0.634656668 epoch total loss 0.573981285\n",
      "Trained batch 268 batch loss 0.723893106 epoch total loss 0.574540675\n",
      "Trained batch 269 batch loss 0.561351299 epoch total loss 0.57449162\n",
      "Trained batch 270 batch loss 0.677743673 epoch total loss 0.574874103\n",
      "Trained batch 271 batch loss 0.642860711 epoch total loss 0.575125\n",
      "Trained batch 272 batch loss 0.576777875 epoch total loss 0.575131059\n",
      "Trained batch 273 batch loss 0.603348613 epoch total loss 0.575234413\n",
      "Trained batch 274 batch loss 0.601082563 epoch total loss 0.575328827\n",
      "Trained batch 275 batch loss 0.573291302 epoch total loss 0.575321376\n",
      "Trained batch 276 batch loss 0.537979662 epoch total loss 0.575186074\n",
      "Trained batch 277 batch loss 0.510701656 epoch total loss 0.574953258\n",
      "Trained batch 278 batch loss 0.486462593 epoch total loss 0.574634969\n",
      "Trained batch 279 batch loss 0.614814639 epoch total loss 0.574779\n",
      "Trained batch 280 batch loss 0.594401419 epoch total loss 0.574849069\n",
      "Trained batch 281 batch loss 0.574180365 epoch total loss 0.574846625\n",
      "Trained batch 282 batch loss 0.575360835 epoch total loss 0.574848473\n",
      "Trained batch 283 batch loss 0.486665905 epoch total loss 0.57453686\n",
      "Trained batch 284 batch loss 0.53600955 epoch total loss 0.5744012\n",
      "Trained batch 285 batch loss 0.525845 epoch total loss 0.57423085\n",
      "Trained batch 286 batch loss 0.500634491 epoch total loss 0.573973536\n",
      "Trained batch 287 batch loss 0.476403892 epoch total loss 0.573633611\n",
      "Trained batch 288 batch loss 0.504765689 epoch total loss 0.573394477\n",
      "Trained batch 289 batch loss 0.437531114 epoch total loss 0.572924316\n",
      "Trained batch 290 batch loss 0.455131799 epoch total loss 0.57251817\n",
      "Trained batch 291 batch loss 0.538348913 epoch total loss 0.572400749\n",
      "Trained batch 292 batch loss 0.714426577 epoch total loss 0.572887182\n",
      "Trained batch 293 batch loss 0.71382153 epoch total loss 0.573368192\n",
      "Trained batch 294 batch loss 0.776006937 epoch total loss 0.5740574\n",
      "Trained batch 295 batch loss 0.673095167 epoch total loss 0.574393094\n",
      "Trained batch 296 batch loss 0.717466831 epoch total loss 0.574876487\n",
      "Trained batch 297 batch loss 0.742981613 epoch total loss 0.575442493\n",
      "Trained batch 298 batch loss 0.619241178 epoch total loss 0.575589478\n",
      "Trained batch 299 batch loss 0.565488458 epoch total loss 0.575555682\n",
      "Trained batch 300 batch loss 0.726873755 epoch total loss 0.576060057\n",
      "Trained batch 301 batch loss 0.6469419 epoch total loss 0.576295555\n",
      "Trained batch 302 batch loss 0.718451 epoch total loss 0.576766253\n",
      "Trained batch 303 batch loss 0.631126 epoch total loss 0.576945662\n",
      "Trained batch 304 batch loss 0.736649215 epoch total loss 0.577470958\n",
      "Trained batch 305 batch loss 0.727673709 epoch total loss 0.577963471\n",
      "Trained batch 306 batch loss 0.705079794 epoch total loss 0.578378856\n",
      "Trained batch 307 batch loss 0.633858681 epoch total loss 0.578559577\n",
      "Trained batch 308 batch loss 0.573757768 epoch total loss 0.578544\n",
      "Trained batch 309 batch loss 0.576443613 epoch total loss 0.578537226\n",
      "Trained batch 310 batch loss 0.524949908 epoch total loss 0.578364372\n",
      "Trained batch 311 batch loss 0.463339269 epoch total loss 0.577994466\n",
      "Trained batch 312 batch loss 0.501386046 epoch total loss 0.577748954\n",
      "Trained batch 313 batch loss 0.63069576 epoch total loss 0.577918112\n",
      "Trained batch 314 batch loss 0.666854441 epoch total loss 0.578201354\n",
      "Trained batch 315 batch loss 0.646069348 epoch total loss 0.578416824\n",
      "Trained batch 316 batch loss 0.680589259 epoch total loss 0.57874012\n",
      "Trained batch 317 batch loss 0.652856171 epoch total loss 0.578973949\n",
      "Trained batch 318 batch loss 0.680378079 epoch total loss 0.579292834\n",
      "Trained batch 319 batch loss 0.637730718 epoch total loss 0.579476\n",
      "Trained batch 320 batch loss 0.594793141 epoch total loss 0.579523861\n",
      "Trained batch 321 batch loss 0.599036336 epoch total loss 0.579584599\n",
      "Trained batch 322 batch loss 0.582668424 epoch total loss 0.579594195\n",
      "Trained batch 323 batch loss 0.631983638 epoch total loss 0.579756439\n",
      "Trained batch 324 batch loss 0.621115923 epoch total loss 0.579884052\n",
      "Trained batch 325 batch loss 0.55928272 epoch total loss 0.579820633\n",
      "Trained batch 326 batch loss 0.557694554 epoch total loss 0.579752803\n",
      "Trained batch 327 batch loss 0.557658553 epoch total loss 0.579685211\n",
      "Trained batch 328 batch loss 0.670535147 epoch total loss 0.579962194\n",
      "Trained batch 329 batch loss 0.593783438 epoch total loss 0.580004215\n",
      "Trained batch 330 batch loss 0.5493536 epoch total loss 0.579911292\n",
      "Trained batch 331 batch loss 0.560343802 epoch total loss 0.579852164\n",
      "Trained batch 332 batch loss 0.566649616 epoch total loss 0.579812407\n",
      "Trained batch 333 batch loss 0.553326905 epoch total loss 0.579732895\n",
      "Trained batch 334 batch loss 0.588541865 epoch total loss 0.5797593\n",
      "Trained batch 335 batch loss 0.577554226 epoch total loss 0.579752743\n",
      "Trained batch 336 batch loss 0.619825 epoch total loss 0.579872\n",
      "Trained batch 337 batch loss 0.636935115 epoch total loss 0.580041289\n",
      "Trained batch 338 batch loss 0.574766695 epoch total loss 0.580025733\n",
      "Trained batch 339 batch loss 0.602892816 epoch total loss 0.580093145\n",
      "Trained batch 340 batch loss 0.567861259 epoch total loss 0.580057144\n",
      "Trained batch 341 batch loss 0.552780092 epoch total loss 0.579977155\n",
      "Trained batch 342 batch loss 0.619335592 epoch total loss 0.580092251\n",
      "Trained batch 343 batch loss 0.65174669 epoch total loss 0.580301166\n",
      "Trained batch 344 batch loss 0.634916842 epoch total loss 0.580459952\n",
      "Trained batch 345 batch loss 0.598146379 epoch total loss 0.580511212\n",
      "Trained batch 346 batch loss 0.577545822 epoch total loss 0.580502629\n",
      "Trained batch 347 batch loss 0.518519521 epoch total loss 0.580324054\n",
      "Trained batch 348 batch loss 0.52068013 epoch total loss 0.580152631\n",
      "Trained batch 349 batch loss 0.618784726 epoch total loss 0.580263317\n",
      "Trained batch 350 batch loss 0.638266861 epoch total loss 0.580429\n",
      "Trained batch 351 batch loss 0.597703218 epoch total loss 0.580478251\n",
      "Trained batch 352 batch loss 0.700524271 epoch total loss 0.580819309\n",
      "Trained batch 353 batch loss 0.563161969 epoch total loss 0.580769241\n",
      "Trained batch 354 batch loss 0.58025825 epoch total loss 0.58076781\n",
      "Trained batch 355 batch loss 0.504977822 epoch total loss 0.580554307\n",
      "Trained batch 356 batch loss 0.625467539 epoch total loss 0.58068049\n",
      "Trained batch 357 batch loss 0.579866767 epoch total loss 0.580678225\n",
      "Trained batch 358 batch loss 0.634747 epoch total loss 0.580829263\n",
      "Trained batch 359 batch loss 0.574067175 epoch total loss 0.580810428\n",
      "Trained batch 360 batch loss 0.608708143 epoch total loss 0.580887914\n",
      "Trained batch 361 batch loss 0.591772795 epoch total loss 0.580918\n",
      "Trained batch 362 batch loss 0.633058429 epoch total loss 0.581062078\n",
      "Trained batch 363 batch loss 0.578828812 epoch total loss 0.58105588\n",
      "Trained batch 364 batch loss 0.614860058 epoch total loss 0.581148744\n",
      "Trained batch 365 batch loss 0.686125338 epoch total loss 0.581436396\n",
      "Trained batch 366 batch loss 0.692888677 epoch total loss 0.581740856\n",
      "Trained batch 367 batch loss 0.709653139 epoch total loss 0.582089424\n",
      "Trained batch 368 batch loss 0.5970366 epoch total loss 0.58213\n",
      "Trained batch 369 batch loss 0.527962744 epoch total loss 0.581983268\n",
      "Trained batch 370 batch loss 0.551039457 epoch total loss 0.581899583\n",
      "Trained batch 371 batch loss 0.59070015 epoch total loss 0.581923306\n",
      "Trained batch 372 batch loss 0.673569143 epoch total loss 0.582169712\n",
      "Trained batch 373 batch loss 0.614142954 epoch total loss 0.582255363\n",
      "Trained batch 374 batch loss 0.541672468 epoch total loss 0.582146883\n",
      "Trained batch 375 batch loss 0.590279698 epoch total loss 0.582168579\n",
      "Trained batch 376 batch loss 0.611833811 epoch total loss 0.582247496\n",
      "Trained batch 377 batch loss 0.628572822 epoch total loss 0.582370341\n",
      "Trained batch 378 batch loss 0.597728848 epoch total loss 0.582411\n",
      "Trained batch 379 batch loss 0.607217431 epoch total loss 0.582476437\n",
      "Trained batch 380 batch loss 0.59779048 epoch total loss 0.582516789\n",
      "Trained batch 381 batch loss 0.560343921 epoch total loss 0.582458556\n",
      "Trained batch 382 batch loss 0.642037272 epoch total loss 0.582614541\n",
      "Trained batch 383 batch loss 0.581186 epoch total loss 0.582610846\n",
      "Trained batch 384 batch loss 0.546840727 epoch total loss 0.582517684\n",
      "Trained batch 385 batch loss 0.56063664 epoch total loss 0.58246088\n",
      "Trained batch 386 batch loss 0.664309621 epoch total loss 0.582672894\n",
      "Trained batch 387 batch loss 0.625577688 epoch total loss 0.582783759\n",
      "Trained batch 388 batch loss 0.60906595 epoch total loss 0.582851529\n",
      "Trained batch 389 batch loss 0.605845809 epoch total loss 0.582910657\n",
      "Trained batch 390 batch loss 0.527631044 epoch total loss 0.582768917\n",
      "Trained batch 391 batch loss 0.558686495 epoch total loss 0.582707345\n",
      "Trained batch 392 batch loss 0.593364239 epoch total loss 0.582734525\n",
      "Trained batch 393 batch loss 0.56664592 epoch total loss 0.582693577\n",
      "Trained batch 394 batch loss 0.606461227 epoch total loss 0.582753897\n",
      "Trained batch 395 batch loss 0.519940674 epoch total loss 0.582594872\n",
      "Trained batch 396 batch loss 0.533860087 epoch total loss 0.582471848\n",
      "Trained batch 397 batch loss 0.610580564 epoch total loss 0.582542658\n",
      "Trained batch 398 batch loss 0.575840235 epoch total loss 0.58252579\n",
      "Trained batch 399 batch loss 0.608688593 epoch total loss 0.582591355\n",
      "Trained batch 400 batch loss 0.595250964 epoch total loss 0.582623\n",
      "Trained batch 401 batch loss 0.5964275 epoch total loss 0.582657397\n",
      "Trained batch 402 batch loss 0.617280543 epoch total loss 0.582743526\n",
      "Trained batch 403 batch loss 0.645717 epoch total loss 0.582899809\n",
      "Trained batch 404 batch loss 0.58582592 epoch total loss 0.582907\n",
      "Trained batch 405 batch loss 0.713470936 epoch total loss 0.583229423\n",
      "Trained batch 406 batch loss 0.682845592 epoch total loss 0.583474755\n",
      "Trained batch 407 batch loss 0.537763894 epoch total loss 0.58336246\n",
      "Trained batch 408 batch loss 0.473366588 epoch total loss 0.583092868\n",
      "Trained batch 409 batch loss 0.567364216 epoch total loss 0.583054423\n",
      "Trained batch 410 batch loss 0.651013851 epoch total loss 0.583220184\n",
      "Trained batch 411 batch loss 0.6025545 epoch total loss 0.583267272\n",
      "Trained batch 412 batch loss 0.560501277 epoch total loss 0.583212\n",
      "Trained batch 413 batch loss 0.627711296 epoch total loss 0.583319724\n",
      "Trained batch 414 batch loss 0.596938431 epoch total loss 0.583352625\n",
      "Trained batch 415 batch loss 0.646961331 epoch total loss 0.583505929\n",
      "Trained batch 416 batch loss 0.56668812 epoch total loss 0.583465457\n",
      "Trained batch 417 batch loss 0.588654101 epoch total loss 0.583477914\n",
      "Trained batch 418 batch loss 0.526408672 epoch total loss 0.58334136\n",
      "Trained batch 419 batch loss 0.584050417 epoch total loss 0.583343089\n",
      "Trained batch 420 batch loss 0.583072484 epoch total loss 0.583342433\n",
      "Trained batch 421 batch loss 0.709010899 epoch total loss 0.583640933\n",
      "Trained batch 422 batch loss 0.576637864 epoch total loss 0.583624363\n",
      "Trained batch 423 batch loss 0.635628879 epoch total loss 0.583747327\n",
      "Trained batch 424 batch loss 0.684468389 epoch total loss 0.583984852\n",
      "Trained batch 425 batch loss 0.598784864 epoch total loss 0.584019661\n",
      "Trained batch 426 batch loss 0.585082173 epoch total loss 0.584022164\n",
      "Trained batch 427 batch loss 0.580985785 epoch total loss 0.584015\n",
      "Trained batch 428 batch loss 0.643772 epoch total loss 0.584154665\n",
      "Trained batch 429 batch loss 0.661717057 epoch total loss 0.584335446\n",
      "Trained batch 430 batch loss 0.625750184 epoch total loss 0.584431767\n",
      "Trained batch 431 batch loss 0.582991958 epoch total loss 0.58442843\n",
      "Trained batch 432 batch loss 0.669412374 epoch total loss 0.584625125\n",
      "Trained batch 433 batch loss 0.558488488 epoch total loss 0.584564745\n",
      "Trained batch 434 batch loss 0.499946386 epoch total loss 0.584369779\n",
      "Trained batch 435 batch loss 0.560352206 epoch total loss 0.584314585\n",
      "Trained batch 436 batch loss 0.652603 epoch total loss 0.584471166\n",
      "Trained batch 437 batch loss 0.564452291 epoch total loss 0.58442539\n",
      "Trained batch 438 batch loss 0.555543065 epoch total loss 0.584359407\n",
      "Trained batch 439 batch loss 0.493621 epoch total loss 0.584152758\n",
      "Trained batch 440 batch loss 0.530690312 epoch total loss 0.584031284\n",
      "Trained batch 441 batch loss 0.584309638 epoch total loss 0.58403194\n",
      "Trained batch 442 batch loss 0.681507349 epoch total loss 0.584252477\n",
      "Trained batch 443 batch loss 0.594079137 epoch total loss 0.58427465\n",
      "Trained batch 444 batch loss 0.635871649 epoch total loss 0.584390879\n",
      "Trained batch 445 batch loss 0.595487952 epoch total loss 0.584415793\n",
      "Trained batch 446 batch loss 0.622715056 epoch total loss 0.584501684\n",
      "Trained batch 447 batch loss 0.581460595 epoch total loss 0.584494829\n",
      "Trained batch 448 batch loss 0.617351413 epoch total loss 0.584568143\n",
      "Trained batch 449 batch loss 0.573932648 epoch total loss 0.58454448\n",
      "Trained batch 450 batch loss 0.585085273 epoch total loss 0.584545672\n",
      "Trained batch 451 batch loss 0.549438238 epoch total loss 0.584467828\n",
      "Trained batch 452 batch loss 0.615606904 epoch total loss 0.584536731\n",
      "Trained batch 453 batch loss 0.614266157 epoch total loss 0.584602356\n",
      "Trained batch 454 batch loss 0.666300416 epoch total loss 0.584782243\n",
      "Trained batch 455 batch loss 0.665535033 epoch total loss 0.584959745\n",
      "Trained batch 456 batch loss 0.582975328 epoch total loss 0.584955394\n",
      "Trained batch 457 batch loss 0.523458302 epoch total loss 0.584820867\n",
      "Trained batch 458 batch loss 0.54080677 epoch total loss 0.584724724\n",
      "Trained batch 459 batch loss 0.539136 epoch total loss 0.584625363\n",
      "Trained batch 460 batch loss 0.573717237 epoch total loss 0.5846017\n",
      "Trained batch 461 batch loss 0.506849289 epoch total loss 0.584433\n",
      "Trained batch 462 batch loss 0.594726562 epoch total loss 0.584455311\n",
      "Trained batch 463 batch loss 0.603720248 epoch total loss 0.584496915\n",
      "Trained batch 464 batch loss 0.453837633 epoch total loss 0.584215283\n",
      "Trained batch 465 batch loss 0.447413296 epoch total loss 0.583921134\n",
      "Trained batch 466 batch loss 0.558685899 epoch total loss 0.583866954\n",
      "Trained batch 467 batch loss 0.67897588 epoch total loss 0.584070623\n",
      "Trained batch 468 batch loss 0.510859668 epoch total loss 0.58391422\n",
      "Trained batch 469 batch loss 0.579559147 epoch total loss 0.583904922\n",
      "Trained batch 470 batch loss 0.529070318 epoch total loss 0.583788276\n",
      "Trained batch 471 batch loss 0.549006522 epoch total loss 0.583714426\n",
      "Trained batch 472 batch loss 0.553150415 epoch total loss 0.583649695\n",
      "Trained batch 473 batch loss 0.501830935 epoch total loss 0.583476722\n",
      "Trained batch 474 batch loss 0.468778372 epoch total loss 0.583234787\n",
      "Trained batch 475 batch loss 0.546672702 epoch total loss 0.583157778\n",
      "Trained batch 476 batch loss 0.547383785 epoch total loss 0.583082616\n",
      "Trained batch 477 batch loss 0.577554822 epoch total loss 0.583071\n",
      "Trained batch 478 batch loss 0.51532191 epoch total loss 0.582929313\n",
      "Trained batch 479 batch loss 0.628258705 epoch total loss 0.583023906\n",
      "Trained batch 480 batch loss 0.553300321 epoch total loss 0.582962036\n",
      "Trained batch 481 batch loss 0.561843097 epoch total loss 0.582918108\n",
      "Trained batch 482 batch loss 0.594266534 epoch total loss 0.582941651\n",
      "Trained batch 483 batch loss 0.598454416 epoch total loss 0.582973778\n",
      "Trained batch 484 batch loss 0.618553 epoch total loss 0.583047271\n",
      "Trained batch 485 batch loss 0.588985562 epoch total loss 0.583059549\n",
      "Trained batch 486 batch loss 0.542124748 epoch total loss 0.582975268\n",
      "Trained batch 487 batch loss 0.652237773 epoch total loss 0.583117545\n",
      "Trained batch 488 batch loss 0.529711306 epoch total loss 0.583008111\n",
      "Trained batch 489 batch loss 0.578872681 epoch total loss 0.582999706\n",
      "Trained batch 490 batch loss 0.526213 epoch total loss 0.582883835\n",
      "Trained batch 491 batch loss 0.540389955 epoch total loss 0.582797229\n",
      "Trained batch 492 batch loss 0.60027343 epoch total loss 0.582832754\n",
      "Trained batch 493 batch loss 0.613366365 epoch total loss 0.582894742\n",
      "Trained batch 494 batch loss 0.649587512 epoch total loss 0.583029747\n",
      "Trained batch 495 batch loss 0.603247106 epoch total loss 0.583070576\n",
      "Trained batch 496 batch loss 0.690061 epoch total loss 0.583286285\n",
      "Trained batch 497 batch loss 0.611088574 epoch total loss 0.583342195\n",
      "Trained batch 498 batch loss 0.644118309 epoch total loss 0.583464205\n",
      "Trained batch 499 batch loss 0.567260802 epoch total loss 0.58343178\n",
      "Trained batch 500 batch loss 0.52635324 epoch total loss 0.583317637\n",
      "Trained batch 501 batch loss 0.566622734 epoch total loss 0.583284318\n",
      "Trained batch 502 batch loss 0.536197841 epoch total loss 0.583190501\n",
      "Trained batch 503 batch loss 0.527734756 epoch total loss 0.583080232\n",
      "Trained batch 504 batch loss 0.515229106 epoch total loss 0.582945645\n",
      "Trained batch 505 batch loss 0.619402468 epoch total loss 0.583017826\n",
      "Trained batch 506 batch loss 0.547451913 epoch total loss 0.582947552\n",
      "Trained batch 507 batch loss 0.629331 epoch total loss 0.583039045\n",
      "Trained batch 508 batch loss 0.684718966 epoch total loss 0.583239198\n",
      "Trained batch 509 batch loss 0.624846697 epoch total loss 0.583321\n",
      "Trained batch 510 batch loss 0.609066725 epoch total loss 0.58337146\n",
      "Trained batch 511 batch loss 0.671753228 epoch total loss 0.583544433\n",
      "Trained batch 512 batch loss 0.650500894 epoch total loss 0.583675206\n",
      "Trained batch 513 batch loss 0.685069144 epoch total loss 0.583872855\n",
      "Trained batch 514 batch loss 0.695066333 epoch total loss 0.58408916\n",
      "Trained batch 515 batch loss 0.592605948 epoch total loss 0.58410573\n",
      "Trained batch 516 batch loss 0.668265104 epoch total loss 0.584268868\n",
      "Trained batch 517 batch loss 0.657332242 epoch total loss 0.584410131\n",
      "Trained batch 518 batch loss 0.553452194 epoch total loss 0.584350407\n",
      "Trained batch 519 batch loss 0.553024471 epoch total loss 0.584290087\n",
      "Trained batch 520 batch loss 0.524241567 epoch total loss 0.584174573\n",
      "Trained batch 521 batch loss 0.510982692 epoch total loss 0.584034085\n",
      "Trained batch 522 batch loss 0.59278518 epoch total loss 0.584050834\n",
      "Trained batch 523 batch loss 0.550951481 epoch total loss 0.583987594\n",
      "Trained batch 524 batch loss 0.547814786 epoch total loss 0.583918571\n",
      "Trained batch 525 batch loss 0.475659221 epoch total loss 0.583712339\n",
      "Trained batch 526 batch loss 0.473246694 epoch total loss 0.583502293\n",
      "Trained batch 527 batch loss 0.488143325 epoch total loss 0.583321333\n",
      "Trained batch 528 batch loss 0.503246307 epoch total loss 0.583169639\n",
      "Trained batch 529 batch loss 0.498729825 epoch total loss 0.58301\n",
      "Trained batch 530 batch loss 0.551718831 epoch total loss 0.582951\n",
      "Trained batch 531 batch loss 0.576567948 epoch total loss 0.582938969\n",
      "Trained batch 532 batch loss 0.585437775 epoch total loss 0.582943678\n",
      "Trained batch 533 batch loss 0.506900132 epoch total loss 0.582801\n",
      "Trained batch 534 batch loss 0.674396157 epoch total loss 0.582972527\n",
      "Trained batch 535 batch loss 0.610242724 epoch total loss 0.583023489\n",
      "Trained batch 536 batch loss 0.542207122 epoch total loss 0.582947373\n",
      "Trained batch 537 batch loss 0.634656191 epoch total loss 0.583043635\n",
      "Trained batch 538 batch loss 0.665847778 epoch total loss 0.583197474\n",
      "Trained batch 539 batch loss 0.630837083 epoch total loss 0.583285868\n",
      "Trained batch 540 batch loss 0.629341066 epoch total loss 0.583371162\n",
      "Trained batch 541 batch loss 0.516875565 epoch total loss 0.583248258\n",
      "Trained batch 542 batch loss 0.577793896 epoch total loss 0.583238184\n",
      "Trained batch 543 batch loss 0.573184371 epoch total loss 0.583219647\n",
      "Trained batch 544 batch loss 0.509936154 epoch total loss 0.583084941\n",
      "Trained batch 545 batch loss 0.564865112 epoch total loss 0.583051503\n",
      "Trained batch 546 batch loss 0.58009845 epoch total loss 0.583046079\n",
      "Trained batch 547 batch loss 0.558225811 epoch total loss 0.58300072\n",
      "Trained batch 548 batch loss 0.645256162 epoch total loss 0.583114326\n",
      "Trained batch 549 batch loss 0.596848965 epoch total loss 0.58313936\n",
      "Trained batch 550 batch loss 0.482127 epoch total loss 0.582955718\n",
      "Trained batch 551 batch loss 0.461658746 epoch total loss 0.582735598\n",
      "Trained batch 552 batch loss 0.409847528 epoch total loss 0.582422376\n",
      "Trained batch 553 batch loss 0.439795583 epoch total loss 0.582164466\n",
      "Trained batch 554 batch loss 0.453580558 epoch total loss 0.581932366\n",
      "Trained batch 555 batch loss 0.521161675 epoch total loss 0.581822872\n",
      "Trained batch 556 batch loss 0.579044461 epoch total loss 0.581817865\n",
      "Trained batch 557 batch loss 0.63215661 epoch total loss 0.581908226\n",
      "Trained batch 558 batch loss 0.678955197 epoch total loss 0.582082152\n",
      "Trained batch 559 batch loss 0.634481847 epoch total loss 0.58217591\n",
      "Trained batch 560 batch loss 0.62234807 epoch total loss 0.582247674\n",
      "Trained batch 561 batch loss 0.672894478 epoch total loss 0.582409203\n",
      "Trained batch 562 batch loss 0.632635117 epoch total loss 0.58249855\n",
      "Trained batch 563 batch loss 0.588639498 epoch total loss 0.582509518\n",
      "Trained batch 564 batch loss 0.654387891 epoch total loss 0.582636952\n",
      "Trained batch 565 batch loss 0.649264574 epoch total loss 0.58275485\n",
      "Trained batch 566 batch loss 0.690946341 epoch total loss 0.582946\n",
      "Trained batch 567 batch loss 0.682328761 epoch total loss 0.5831213\n",
      "Trained batch 568 batch loss 0.610408068 epoch total loss 0.583169401\n",
      "Trained batch 569 batch loss 0.65441525 epoch total loss 0.58329457\n",
      "Trained batch 570 batch loss 0.589581132 epoch total loss 0.583305597\n",
      "Trained batch 571 batch loss 0.600458622 epoch total loss 0.583335638\n",
      "Trained batch 572 batch loss 0.61572969 epoch total loss 0.583392262\n",
      "Trained batch 573 batch loss 0.49777326 epoch total loss 0.583242834\n",
      "Trained batch 574 batch loss 0.542223811 epoch total loss 0.583171427\n",
      "Trained batch 575 batch loss 0.560484767 epoch total loss 0.583131969\n",
      "Trained batch 576 batch loss 0.590658963 epoch total loss 0.583145\n",
      "Trained batch 577 batch loss 0.517424941 epoch total loss 0.583031118\n",
      "Trained batch 578 batch loss 0.605717659 epoch total loss 0.583070397\n",
      "Trained batch 579 batch loss 0.629367888 epoch total loss 0.583150327\n",
      "Trained batch 580 batch loss 0.596590161 epoch total loss 0.583173513\n",
      "Trained batch 581 batch loss 0.591084957 epoch total loss 0.583187163\n",
      "Trained batch 582 batch loss 0.582594633 epoch total loss 0.58318609\n",
      "Trained batch 583 batch loss 0.540279031 epoch total loss 0.583112478\n",
      "Trained batch 584 batch loss 0.597254932 epoch total loss 0.583136737\n",
      "Trained batch 585 batch loss 0.585013151 epoch total loss 0.583139956\n",
      "Trained batch 586 batch loss 0.583423 epoch total loss 0.583140433\n",
      "Trained batch 587 batch loss 0.531945944 epoch total loss 0.583053231\n",
      "Trained batch 588 batch loss 0.546068907 epoch total loss 0.582990348\n",
      "Trained batch 589 batch loss 0.509060264 epoch total loss 0.582864881\n",
      "Trained batch 590 batch loss 0.565652907 epoch total loss 0.582835674\n",
      "Trained batch 591 batch loss 0.604528368 epoch total loss 0.582872391\n",
      "Trained batch 592 batch loss 0.576173902 epoch total loss 0.582861066\n",
      "Trained batch 593 batch loss 0.60228467 epoch total loss 0.582893848\n",
      "Trained batch 594 batch loss 0.526928544 epoch total loss 0.582799613\n",
      "Trained batch 595 batch loss 0.527897894 epoch total loss 0.582707286\n",
      "Trained batch 596 batch loss 0.562934339 epoch total loss 0.582674146\n",
      "Trained batch 597 batch loss 0.631748199 epoch total loss 0.582756341\n",
      "Trained batch 598 batch loss 0.546500742 epoch total loss 0.582695723\n",
      "Trained batch 599 batch loss 0.595679045 epoch total loss 0.582717359\n",
      "Trained batch 600 batch loss 0.564545155 epoch total loss 0.58268708\n",
      "Trained batch 601 batch loss 0.637328804 epoch total loss 0.582778\n",
      "Trained batch 602 batch loss 0.543974876 epoch total loss 0.582713544\n",
      "Trained batch 603 batch loss 0.529218614 epoch total loss 0.582624793\n",
      "Trained batch 604 batch loss 0.622251391 epoch total loss 0.582690418\n",
      "Trained batch 605 batch loss 0.541476607 epoch total loss 0.58262229\n",
      "Trained batch 606 batch loss 0.478164494 epoch total loss 0.582449913\n",
      "Trained batch 607 batch loss 0.526291549 epoch total loss 0.582357407\n",
      "Trained batch 608 batch loss 0.561582208 epoch total loss 0.582323253\n",
      "Trained batch 609 batch loss 0.604804873 epoch total loss 0.582360148\n",
      "Trained batch 610 batch loss 0.516736031 epoch total loss 0.582252502\n",
      "Trained batch 611 batch loss 0.60870564 epoch total loss 0.582295835\n",
      "Trained batch 612 batch loss 0.599771142 epoch total loss 0.582324386\n",
      "Trained batch 613 batch loss 0.583141208 epoch total loss 0.582325697\n",
      "Trained batch 614 batch loss 0.673124194 epoch total loss 0.582473576\n",
      "Trained batch 615 batch loss 0.600793183 epoch total loss 0.582503378\n",
      "Trained batch 616 batch loss 0.701303899 epoch total loss 0.582696199\n",
      "Trained batch 617 batch loss 0.591874719 epoch total loss 0.582711101\n",
      "Trained batch 618 batch loss 0.553595 epoch total loss 0.582663953\n",
      "Trained batch 619 batch loss 0.534738779 epoch total loss 0.582586527\n",
      "Trained batch 620 batch loss 0.544817388 epoch total loss 0.582525611\n",
      "Trained batch 621 batch loss 0.532132268 epoch total loss 0.582444489\n",
      "Trained batch 622 batch loss 0.510741889 epoch total loss 0.582329214\n",
      "Trained batch 623 batch loss 0.535038829 epoch total loss 0.582253277\n",
      "Trained batch 624 batch loss 0.530777276 epoch total loss 0.582170844\n",
      "Trained batch 625 batch loss 0.550928712 epoch total loss 0.582120836\n",
      "Trained batch 626 batch loss 0.574191868 epoch total loss 0.5821082\n",
      "Trained batch 627 batch loss 0.615184546 epoch total loss 0.58216089\n",
      "Trained batch 628 batch loss 0.583843708 epoch total loss 0.582163572\n",
      "Trained batch 629 batch loss 0.619988263 epoch total loss 0.582223713\n",
      "Trained batch 630 batch loss 0.615835249 epoch total loss 0.58227706\n",
      "Trained batch 631 batch loss 0.524973 epoch total loss 0.582186282\n",
      "Trained batch 632 batch loss 0.4594329 epoch total loss 0.58199203\n",
      "Trained batch 633 batch loss 0.52234292 epoch total loss 0.581897795\n",
      "Trained batch 634 batch loss 0.534971952 epoch total loss 0.581823766\n",
      "Trained batch 635 batch loss 0.541326404 epoch total loss 0.58176\n",
      "Trained batch 636 batch loss 0.562276959 epoch total loss 0.581729412\n",
      "Trained batch 637 batch loss 0.653232 epoch total loss 0.581841648\n",
      "Trained batch 638 batch loss 0.673653901 epoch total loss 0.581985533\n",
      "Trained batch 639 batch loss 0.653597832 epoch total loss 0.58209759\n",
      "Trained batch 640 batch loss 0.650503516 epoch total loss 0.582204461\n",
      "Trained batch 641 batch loss 0.570792079 epoch total loss 0.582186699\n",
      "Trained batch 642 batch loss 0.589853525 epoch total loss 0.58219862\n",
      "Trained batch 643 batch loss 0.564918101 epoch total loss 0.582171738\n",
      "Trained batch 644 batch loss 0.479941934 epoch total loss 0.582013\n",
      "Trained batch 645 batch loss 0.429803073 epoch total loss 0.581777036\n",
      "Trained batch 646 batch loss 0.350377113 epoch total loss 0.581418812\n",
      "Trained batch 647 batch loss 0.380345047 epoch total loss 0.581108034\n",
      "Trained batch 648 batch loss 0.394614071 epoch total loss 0.580820262\n",
      "Trained batch 649 batch loss 0.421663195 epoch total loss 0.580575\n",
      "Trained batch 650 batch loss 0.471477568 epoch total loss 0.580407143\n",
      "Trained batch 651 batch loss 0.483930111 epoch total loss 0.580258906\n",
      "Trained batch 652 batch loss 0.526958704 epoch total loss 0.580177188\n",
      "Trained batch 653 batch loss 0.546925306 epoch total loss 0.580126286\n",
      "Trained batch 654 batch loss 0.563785434 epoch total loss 0.580101252\n",
      "Trained batch 655 batch loss 0.490548 epoch total loss 0.579964519\n",
      "Trained batch 656 batch loss 0.540782869 epoch total loss 0.579904795\n",
      "Trained batch 657 batch loss 0.604706645 epoch total loss 0.579942524\n",
      "Trained batch 658 batch loss 0.580594182 epoch total loss 0.579943538\n",
      "Trained batch 659 batch loss 0.595344186 epoch total loss 0.579966903\n",
      "Trained batch 660 batch loss 0.604761124 epoch total loss 0.580004454\n",
      "Trained batch 661 batch loss 0.557376 epoch total loss 0.579970241\n",
      "Trained batch 662 batch loss 0.622675538 epoch total loss 0.580034733\n",
      "Trained batch 663 batch loss 0.649194598 epoch total loss 0.580139041\n",
      "Trained batch 664 batch loss 0.598683059 epoch total loss 0.580167\n",
      "Trained batch 665 batch loss 0.683707714 epoch total loss 0.580322742\n",
      "Trained batch 666 batch loss 0.68999356 epoch total loss 0.58048743\n",
      "Trained batch 667 batch loss 0.679956794 epoch total loss 0.580636561\n",
      "Trained batch 668 batch loss 0.682551801 epoch total loss 0.580789149\n",
      "Trained batch 669 batch loss 0.650298 epoch total loss 0.58089304\n",
      "Trained batch 670 batch loss 0.680455446 epoch total loss 0.581041634\n",
      "Trained batch 671 batch loss 0.666079164 epoch total loss 0.581168354\n",
      "Trained batch 672 batch loss 0.595761657 epoch total loss 0.58119005\n",
      "Trained batch 673 batch loss 0.563344479 epoch total loss 0.581163585\n",
      "Trained batch 674 batch loss 0.528879821 epoch total loss 0.581086\n",
      "Trained batch 675 batch loss 0.592687964 epoch total loss 0.581103146\n",
      "Trained batch 676 batch loss 0.569596827 epoch total loss 0.581086159\n",
      "Trained batch 677 batch loss 0.627360523 epoch total loss 0.581154466\n",
      "Trained batch 678 batch loss 0.603592277 epoch total loss 0.581187606\n",
      "Trained batch 679 batch loss 0.627534091 epoch total loss 0.581255853\n",
      "Trained batch 680 batch loss 0.575468063 epoch total loss 0.58124733\n",
      "Trained batch 681 batch loss 0.586651444 epoch total loss 0.581255257\n",
      "Trained batch 682 batch loss 0.580447257 epoch total loss 0.581254065\n",
      "Trained batch 683 batch loss 0.579818428 epoch total loss 0.581252\n",
      "Trained batch 684 batch loss 0.532395959 epoch total loss 0.581180573\n",
      "Trained batch 685 batch loss 0.516980588 epoch total loss 0.581086814\n",
      "Trained batch 686 batch loss 0.567020595 epoch total loss 0.58106631\n",
      "Trained batch 687 batch loss 0.525190353 epoch total loss 0.58098495\n",
      "Trained batch 688 batch loss 0.504251778 epoch total loss 0.58087343\n",
      "Trained batch 689 batch loss 0.524669528 epoch total loss 0.580791831\n",
      "Trained batch 690 batch loss 0.501936674 epoch total loss 0.580677509\n",
      "Trained batch 691 batch loss 0.583901882 epoch total loss 0.580682158\n",
      "Trained batch 692 batch loss 0.525538683 epoch total loss 0.580602467\n",
      "Trained batch 693 batch loss 0.59801656 epoch total loss 0.58062762\n",
      "Trained batch 694 batch loss 0.57451874 epoch total loss 0.580618799\n",
      "Trained batch 695 batch loss 0.556894481 epoch total loss 0.580584645\n",
      "Trained batch 696 batch loss 0.590831101 epoch total loss 0.580599368\n",
      "Trained batch 697 batch loss 0.581860542 epoch total loss 0.580601156\n",
      "Trained batch 698 batch loss 0.55251193 epoch total loss 0.580560923\n",
      "Trained batch 699 batch loss 0.444841594 epoch total loss 0.58036679\n",
      "Trained batch 700 batch loss 0.514007747 epoch total loss 0.580272\n",
      "Trained batch 701 batch loss 0.633404732 epoch total loss 0.580347776\n",
      "Trained batch 702 batch loss 0.673466265 epoch total loss 0.580480397\n",
      "Trained batch 703 batch loss 0.61690408 epoch total loss 0.580532253\n",
      "Trained batch 704 batch loss 0.535441279 epoch total loss 0.580468178\n",
      "Trained batch 705 batch loss 0.554257154 epoch total loss 0.580431\n",
      "Trained batch 706 batch loss 0.576453745 epoch total loss 0.580425382\n",
      "Trained batch 707 batch loss 0.593598604 epoch total loss 0.580444\n",
      "Trained batch 708 batch loss 0.45995456 epoch total loss 0.580273807\n",
      "Trained batch 709 batch loss 0.566120565 epoch total loss 0.580253899\n",
      "Trained batch 710 batch loss 0.565520406 epoch total loss 0.580233097\n",
      "Trained batch 711 batch loss 0.592016876 epoch total loss 0.580249667\n",
      "Trained batch 712 batch loss 0.51043576 epoch total loss 0.580151618\n",
      "Trained batch 713 batch loss 0.570737481 epoch total loss 0.580138445\n",
      "Trained batch 714 batch loss 0.655727565 epoch total loss 0.580244303\n",
      "Trained batch 715 batch loss 0.670802772 epoch total loss 0.580370963\n",
      "Trained batch 716 batch loss 0.532736063 epoch total loss 0.580304444\n",
      "Trained batch 717 batch loss 0.530908287 epoch total loss 0.580235541\n",
      "Trained batch 718 batch loss 0.510543585 epoch total loss 0.580138505\n",
      "Trained batch 719 batch loss 0.513510644 epoch total loss 0.580045819\n",
      "Trained batch 720 batch loss 0.516812503 epoch total loss 0.579958\n",
      "Trained batch 721 batch loss 0.543236375 epoch total loss 0.57990706\n",
      "Trained batch 722 batch loss 0.57690686 epoch total loss 0.579902947\n",
      "Trained batch 723 batch loss 0.556206286 epoch total loss 0.579870164\n",
      "Trained batch 724 batch loss 0.615109324 epoch total loss 0.579918861\n",
      "Trained batch 725 batch loss 0.525293529 epoch total loss 0.579843521\n",
      "Trained batch 726 batch loss 0.555592299 epoch total loss 0.579810083\n",
      "Trained batch 727 batch loss 0.576795757 epoch total loss 0.57980597\n",
      "Trained batch 728 batch loss 0.595001042 epoch total loss 0.579826832\n",
      "Trained batch 729 batch loss 0.567840099 epoch total loss 0.579810381\n",
      "Trained batch 730 batch loss 0.494024605 epoch total loss 0.579692841\n",
      "Trained batch 731 batch loss 0.488786459 epoch total loss 0.579568505\n",
      "Trained batch 732 batch loss 0.585239887 epoch total loss 0.579576254\n",
      "Trained batch 733 batch loss 0.492722511 epoch total loss 0.57945776\n",
      "Trained batch 734 batch loss 0.555079937 epoch total loss 0.57942456\n",
      "Trained batch 735 batch loss 0.59565872 epoch total loss 0.579446673\n",
      "Trained batch 736 batch loss 0.543975532 epoch total loss 0.579398513\n",
      "Trained batch 737 batch loss 0.556141138 epoch total loss 0.579366922\n",
      "Trained batch 738 batch loss 0.738340259 epoch total loss 0.579582334\n",
      "Trained batch 739 batch loss 0.702153862 epoch total loss 0.579748213\n",
      "Trained batch 740 batch loss 0.742234409 epoch total loss 0.579967797\n",
      "Trained batch 741 batch loss 0.741713405 epoch total loss 0.580186069\n",
      "Trained batch 742 batch loss 0.691407859 epoch total loss 0.580336\n",
      "Trained batch 743 batch loss 0.599073827 epoch total loss 0.580361187\n",
      "Trained batch 744 batch loss 0.452449024 epoch total loss 0.580189228\n",
      "Trained batch 745 batch loss 0.592461765 epoch total loss 0.580205739\n",
      "Trained batch 746 batch loss 0.502987325 epoch total loss 0.580102205\n",
      "Trained batch 747 batch loss 0.525954485 epoch total loss 0.580029726\n",
      "Trained batch 748 batch loss 0.486668646 epoch total loss 0.579904914\n",
      "Trained batch 749 batch loss 0.592075527 epoch total loss 0.579921126\n",
      "Trained batch 750 batch loss 0.673048317 epoch total loss 0.580045283\n",
      "Trained batch 751 batch loss 0.619580328 epoch total loss 0.580097914\n",
      "Trained batch 752 batch loss 0.592586398 epoch total loss 0.580114543\n",
      "Trained batch 753 batch loss 0.606969476 epoch total loss 0.580150187\n",
      "Trained batch 754 batch loss 0.573806286 epoch total loss 0.580141723\n",
      "Trained batch 755 batch loss 0.623658955 epoch total loss 0.580199361\n",
      "Trained batch 756 batch loss 0.619262099 epoch total loss 0.580251038\n",
      "Trained batch 757 batch loss 0.662549615 epoch total loss 0.580359757\n",
      "Trained batch 758 batch loss 0.688649416 epoch total loss 0.580502629\n",
      "Trained batch 759 batch loss 0.710429788 epoch total loss 0.580673814\n",
      "Trained batch 760 batch loss 0.650078475 epoch total loss 0.580765128\n",
      "Trained batch 761 batch loss 0.678951561 epoch total loss 0.580894172\n",
      "Trained batch 762 batch loss 0.641903281 epoch total loss 0.580974221\n",
      "Trained batch 763 batch loss 0.5582636 epoch total loss 0.580944479\n",
      "Trained batch 764 batch loss 0.59303093 epoch total loss 0.580960274\n",
      "Trained batch 765 batch loss 0.631682396 epoch total loss 0.581026554\n",
      "Trained batch 766 batch loss 0.571508408 epoch total loss 0.581014156\n",
      "Trained batch 767 batch loss 0.518031359 epoch total loss 0.580932\n",
      "Trained batch 768 batch loss 0.589122415 epoch total loss 0.58094269\n",
      "Trained batch 769 batch loss 0.568541229 epoch total loss 0.580926538\n",
      "Trained batch 770 batch loss 0.622424245 epoch total loss 0.58098048\n",
      "Trained batch 771 batch loss 0.681193471 epoch total loss 0.581110418\n",
      "Trained batch 772 batch loss 0.594117105 epoch total loss 0.581127286\n",
      "Trained batch 773 batch loss 0.540138841 epoch total loss 0.581074238\n",
      "Trained batch 774 batch loss 0.606204808 epoch total loss 0.581106722\n",
      "Trained batch 775 batch loss 0.57501477 epoch total loss 0.581098855\n",
      "Trained batch 776 batch loss 0.542599082 epoch total loss 0.581049204\n",
      "Trained batch 777 batch loss 0.52128154 epoch total loss 0.580972314\n",
      "Trained batch 778 batch loss 0.619352102 epoch total loss 0.581021607\n",
      "Trained batch 779 batch loss 0.542544127 epoch total loss 0.580972195\n",
      "Trained batch 780 batch loss 0.543277144 epoch total loss 0.580923915\n",
      "Trained batch 781 batch loss 0.620625138 epoch total loss 0.580974758\n",
      "Trained batch 782 batch loss 0.59074831 epoch total loss 0.580987275\n",
      "Trained batch 783 batch loss 0.629073143 epoch total loss 0.581048667\n",
      "Trained batch 784 batch loss 0.632026911 epoch total loss 0.581113636\n",
      "Trained batch 785 batch loss 0.520279884 epoch total loss 0.58103621\n",
      "Trained batch 786 batch loss 0.524771214 epoch total loss 0.580964625\n",
      "Trained batch 787 batch loss 0.510868967 epoch total loss 0.580875516\n",
      "Trained batch 788 batch loss 0.501166403 epoch total loss 0.580774367\n",
      "Trained batch 789 batch loss 0.524231136 epoch total loss 0.580702722\n",
      "Trained batch 790 batch loss 0.435331315 epoch total loss 0.580518723\n",
      "Trained batch 791 batch loss 0.46024251 epoch total loss 0.580366611\n",
      "Trained batch 792 batch loss 0.510206819 epoch total loss 0.580278039\n",
      "Trained batch 793 batch loss 0.751632571 epoch total loss 0.580494106\n",
      "Trained batch 794 batch loss 0.777926624 epoch total loss 0.580742776\n",
      "Trained batch 795 batch loss 0.714901805 epoch total loss 0.580911517\n",
      "Trained batch 796 batch loss 0.663119256 epoch total loss 0.581014752\n",
      "Trained batch 797 batch loss 0.567208648 epoch total loss 0.580997467\n",
      "Trained batch 798 batch loss 0.554385543 epoch total loss 0.580964088\n",
      "Trained batch 799 batch loss 0.500316501 epoch total loss 0.580863118\n",
      "Trained batch 800 batch loss 0.540536582 epoch total loss 0.580812693\n",
      "Trained batch 801 batch loss 0.511319 epoch total loss 0.580725968\n",
      "Trained batch 802 batch loss 0.68742615 epoch total loss 0.580859\n",
      "Trained batch 803 batch loss 0.469894707 epoch total loss 0.580720842\n",
      "Trained batch 804 batch loss 0.575891256 epoch total loss 0.580714881\n",
      "Trained batch 805 batch loss 0.501333296 epoch total loss 0.580616236\n",
      "Trained batch 806 batch loss 0.521110117 epoch total loss 0.580542445\n",
      "Trained batch 807 batch loss 0.531619668 epoch total loss 0.580481827\n",
      "Trained batch 808 batch loss 0.576747119 epoch total loss 0.580477178\n",
      "Trained batch 809 batch loss 0.607062936 epoch total loss 0.58051\n",
      "Trained batch 810 batch loss 0.579822 epoch total loss 0.580509186\n",
      "Trained batch 811 batch loss 0.564062476 epoch total loss 0.58048892\n",
      "Trained batch 812 batch loss 0.57349 epoch total loss 0.580480278\n",
      "Trained batch 813 batch loss 0.551604629 epoch total loss 0.580444813\n",
      "Trained batch 814 batch loss 0.607992172 epoch total loss 0.580478668\n",
      "Trained batch 815 batch loss 0.548894 epoch total loss 0.580439866\n",
      "Trained batch 816 batch loss 0.616878092 epoch total loss 0.580484569\n",
      "Trained batch 817 batch loss 0.638743877 epoch total loss 0.580555856\n",
      "Trained batch 818 batch loss 0.589956462 epoch total loss 0.58056736\n",
      "Trained batch 819 batch loss 0.476433367 epoch total loss 0.580440223\n",
      "Trained batch 820 batch loss 0.499576211 epoch total loss 0.580341578\n",
      "Trained batch 821 batch loss 0.54189539 epoch total loss 0.580294788\n",
      "Trained batch 822 batch loss 0.523424208 epoch total loss 0.580225587\n",
      "Trained batch 823 batch loss 0.51908648 epoch total loss 0.58015126\n",
      "Trained batch 824 batch loss 0.579115689 epoch total loss 0.58015\n",
      "Trained batch 825 batch loss 0.525643826 epoch total loss 0.580083907\n",
      "Trained batch 826 batch loss 0.702159464 epoch total loss 0.580231726\n",
      "Trained batch 827 batch loss 0.671762526 epoch total loss 0.580342352\n",
      "Trained batch 828 batch loss 0.581863165 epoch total loss 0.5803442\n",
      "Trained batch 829 batch loss 0.421008945 epoch total loss 0.580152035\n",
      "Trained batch 830 batch loss 0.571834862 epoch total loss 0.580141962\n",
      "Trained batch 831 batch loss 0.599731684 epoch total loss 0.580165565\n",
      "Trained batch 832 batch loss 0.568771362 epoch total loss 0.580151856\n",
      "Trained batch 833 batch loss 0.64103 epoch total loss 0.580224931\n",
      "Trained batch 834 batch loss 0.644563 epoch total loss 0.58030206\n",
      "Trained batch 835 batch loss 0.422104895 epoch total loss 0.580112636\n",
      "Trained batch 836 batch loss 0.471811146 epoch total loss 0.579983056\n",
      "Trained batch 837 batch loss 0.629167318 epoch total loss 0.580041826\n",
      "Trained batch 838 batch loss 0.667741 epoch total loss 0.580146492\n",
      "Trained batch 839 batch loss 0.678586423 epoch total loss 0.580263853\n",
      "Trained batch 840 batch loss 0.617575049 epoch total loss 0.580308259\n",
      "Trained batch 841 batch loss 0.678284168 epoch total loss 0.580424786\n",
      "Trained batch 842 batch loss 0.579771519 epoch total loss 0.580424\n",
      "Trained batch 843 batch loss 0.51905179 epoch total loss 0.580351174\n",
      "Trained batch 844 batch loss 0.452343494 epoch total loss 0.58019948\n",
      "Trained batch 845 batch loss 0.480684668 epoch total loss 0.580081761\n",
      "Trained batch 846 batch loss 0.469753444 epoch total loss 0.579951346\n",
      "Trained batch 847 batch loss 0.463148504 epoch total loss 0.579813421\n",
      "Trained batch 848 batch loss 0.559188366 epoch total loss 0.579789042\n",
      "Trained batch 849 batch loss 0.469567657 epoch total loss 0.579659283\n",
      "Trained batch 850 batch loss 0.446949184 epoch total loss 0.579503119\n",
      "Trained batch 851 batch loss 0.488842517 epoch total loss 0.579396605\n",
      "Trained batch 852 batch loss 0.491566896 epoch total loss 0.579293489\n",
      "Trained batch 853 batch loss 0.48344177 epoch total loss 0.579181135\n",
      "Trained batch 854 batch loss 0.464844 epoch total loss 0.579047263\n",
      "Trained batch 855 batch loss 0.536165535 epoch total loss 0.578997076\n",
      "Trained batch 856 batch loss 0.545072258 epoch total loss 0.578957438\n",
      "Trained batch 857 batch loss 0.539043784 epoch total loss 0.578910887\n",
      "Trained batch 858 batch loss 0.532477319 epoch total loss 0.578856766\n",
      "Trained batch 859 batch loss 0.599267364 epoch total loss 0.578880489\n",
      "Trained batch 860 batch loss 0.542081118 epoch total loss 0.578837752\n",
      "Trained batch 861 batch loss 0.560807347 epoch total loss 0.578816831\n",
      "Trained batch 862 batch loss 0.594408035 epoch total loss 0.578834891\n",
      "Trained batch 863 batch loss 0.58556217 epoch total loss 0.5788427\n",
      "Trained batch 864 batch loss 0.625980794 epoch total loss 0.578897238\n",
      "Trained batch 865 batch loss 0.535621822 epoch total loss 0.578847229\n",
      "Trained batch 866 batch loss 0.656511426 epoch total loss 0.578936934\n",
      "Trained batch 867 batch loss 0.537482619 epoch total loss 0.578889072\n",
      "Trained batch 868 batch loss 0.532144964 epoch total loss 0.578835249\n",
      "Trained batch 869 batch loss 0.578001261 epoch total loss 0.578834295\n",
      "Trained batch 870 batch loss 0.602192163 epoch total loss 0.578861117\n",
      "Trained batch 871 batch loss 0.688676596 epoch total loss 0.578987241\n",
      "Trained batch 872 batch loss 0.60818 epoch total loss 0.579020739\n",
      "Trained batch 873 batch loss 0.55170393 epoch total loss 0.578989446\n",
      "Trained batch 874 batch loss 0.496731222 epoch total loss 0.57889533\n",
      "Trained batch 875 batch loss 0.464642882 epoch total loss 0.578764737\n",
      "Trained batch 876 batch loss 0.565485477 epoch total loss 0.578749537\n",
      "Trained batch 877 batch loss 0.524179161 epoch total loss 0.57868731\n",
      "Trained batch 878 batch loss 0.506040275 epoch total loss 0.578604579\n",
      "Trained batch 879 batch loss 0.604036927 epoch total loss 0.578633547\n",
      "Trained batch 880 batch loss 0.661811471 epoch total loss 0.578728\n",
      "Trained batch 881 batch loss 0.569853723 epoch total loss 0.578717947\n",
      "Trained batch 882 batch loss 0.561019778 epoch total loss 0.57869786\n",
      "Trained batch 883 batch loss 0.680970252 epoch total loss 0.578813732\n",
      "Trained batch 884 batch loss 0.548439205 epoch total loss 0.57877934\n",
      "Trained batch 885 batch loss 0.565709472 epoch total loss 0.578764617\n",
      "Trained batch 886 batch loss 0.628934085 epoch total loss 0.578821182\n",
      "Trained batch 887 batch loss 0.619996786 epoch total loss 0.578867614\n",
      "Trained batch 888 batch loss 0.652335286 epoch total loss 0.578950346\n",
      "Trained batch 889 batch loss 0.651691735 epoch total loss 0.579032123\n",
      "Trained batch 890 batch loss 0.619542599 epoch total loss 0.579077721\n",
      "Trained batch 891 batch loss 0.637885153 epoch total loss 0.579143703\n",
      "Trained batch 892 batch loss 0.611633122 epoch total loss 0.579180121\n",
      "Trained batch 893 batch loss 0.630314231 epoch total loss 0.579237342\n",
      "Trained batch 894 batch loss 0.612419844 epoch total loss 0.579274476\n",
      "Trained batch 895 batch loss 0.673638165 epoch total loss 0.579379916\n",
      "Trained batch 896 batch loss 0.682652891 epoch total loss 0.579495251\n",
      "Trained batch 897 batch loss 0.597480297 epoch total loss 0.579515278\n",
      "Trained batch 898 batch loss 0.56180948 epoch total loss 0.579495549\n",
      "Trained batch 899 batch loss 0.544172943 epoch total loss 0.579456329\n",
      "Trained batch 900 batch loss 0.562848806 epoch total loss 0.579437852\n",
      "Trained batch 901 batch loss 0.545183122 epoch total loss 0.579399824\n",
      "Trained batch 902 batch loss 0.592964709 epoch total loss 0.579414845\n",
      "Trained batch 903 batch loss 0.55713433 epoch total loss 0.579390168\n",
      "Trained batch 904 batch loss 0.540245414 epoch total loss 0.579346836\n",
      "Trained batch 905 batch loss 0.49224779 epoch total loss 0.579250634\n",
      "Trained batch 906 batch loss 0.501337469 epoch total loss 0.579164624\n",
      "Trained batch 907 batch loss 0.505831182 epoch total loss 0.5790838\n",
      "Trained batch 908 batch loss 0.537617624 epoch total loss 0.579038084\n",
      "Trained batch 909 batch loss 0.612690508 epoch total loss 0.579075098\n",
      "Trained batch 910 batch loss 0.576625347 epoch total loss 0.579072416\n",
      "Trained batch 911 batch loss 0.63361752 epoch total loss 0.579132259\n",
      "Trained batch 912 batch loss 0.615122795 epoch total loss 0.579171717\n",
      "Trained batch 913 batch loss 0.588873327 epoch total loss 0.579182327\n",
      "Trained batch 914 batch loss 0.600072384 epoch total loss 0.579205215\n",
      "Trained batch 915 batch loss 0.595514059 epoch total loss 0.579223037\n",
      "Trained batch 916 batch loss 0.616952062 epoch total loss 0.579264224\n",
      "Trained batch 917 batch loss 0.547431469 epoch total loss 0.579229474\n",
      "Trained batch 918 batch loss 0.492285967 epoch total loss 0.579134822\n",
      "Trained batch 919 batch loss 0.590163648 epoch total loss 0.579146802\n",
      "Trained batch 920 batch loss 0.572723806 epoch total loss 0.579139829\n",
      "Trained batch 921 batch loss 0.526795268 epoch total loss 0.579083\n",
      "Trained batch 922 batch loss 0.456924409 epoch total loss 0.578950524\n",
      "Trained batch 923 batch loss 0.504143834 epoch total loss 0.578869462\n",
      "Trained batch 924 batch loss 0.53279984 epoch total loss 0.578819573\n",
      "Trained batch 925 batch loss 0.53869921 epoch total loss 0.578776181\n",
      "Trained batch 926 batch loss 0.50208354 epoch total loss 0.57869339\n",
      "Trained batch 927 batch loss 0.475501239 epoch total loss 0.578582048\n",
      "Trained batch 928 batch loss 0.478512108 epoch total loss 0.578474224\n",
      "Trained batch 929 batch loss 0.496723831 epoch total loss 0.578386247\n",
      "Trained batch 930 batch loss 0.581189573 epoch total loss 0.578389227\n",
      "Trained batch 931 batch loss 0.437995136 epoch total loss 0.578238428\n",
      "Trained batch 932 batch loss 0.510297656 epoch total loss 0.578165531\n",
      "Trained batch 933 batch loss 0.497666329 epoch total loss 0.578079283\n",
      "Trained batch 934 batch loss 0.534308672 epoch total loss 0.578032374\n",
      "Trained batch 935 batch loss 0.428599179 epoch total loss 0.577872574\n",
      "Trained batch 936 batch loss 0.517274141 epoch total loss 0.577807844\n",
      "Trained batch 937 batch loss 0.655898333 epoch total loss 0.577891171\n",
      "Trained batch 938 batch loss 0.530820489 epoch total loss 0.577841\n",
      "Trained batch 939 batch loss 0.691757321 epoch total loss 0.577962279\n",
      "Trained batch 940 batch loss 0.544939697 epoch total loss 0.577927172\n",
      "Trained batch 941 batch loss 0.598863959 epoch total loss 0.577949405\n",
      "Trained batch 942 batch loss 0.535665274 epoch total loss 0.577904522\n",
      "Trained batch 943 batch loss 0.556346655 epoch total loss 0.577881634\n",
      "Trained batch 944 batch loss 0.596041381 epoch total loss 0.577900887\n",
      "Trained batch 945 batch loss 0.571686447 epoch total loss 0.57789433\n",
      "Trained batch 946 batch loss 0.560159147 epoch total loss 0.577875614\n",
      "Trained batch 947 batch loss 0.548853755 epoch total loss 0.577845\n",
      "Trained batch 948 batch loss 0.524249 epoch total loss 0.577788413\n",
      "Trained batch 949 batch loss 0.51626575 epoch total loss 0.577723563\n",
      "Trained batch 950 batch loss 0.527951658 epoch total loss 0.57767117\n",
      "Trained batch 951 batch loss 0.565300941 epoch total loss 0.577658176\n",
      "Trained batch 952 batch loss 0.587353 epoch total loss 0.577668309\n",
      "Trained batch 953 batch loss 0.620325804 epoch total loss 0.577713072\n",
      "Trained batch 954 batch loss 0.575593114 epoch total loss 0.577710867\n",
      "Trained batch 955 batch loss 0.663408637 epoch total loss 0.577800572\n",
      "Trained batch 956 batch loss 0.675909221 epoch total loss 0.577903211\n",
      "Trained batch 957 batch loss 0.641171396 epoch total loss 0.577969313\n",
      "Trained batch 958 batch loss 0.580881 epoch total loss 0.577972353\n",
      "Trained batch 959 batch loss 0.639998615 epoch total loss 0.578037\n",
      "Trained batch 960 batch loss 0.652750909 epoch total loss 0.578114867\n",
      "Trained batch 961 batch loss 0.601597726 epoch total loss 0.578139365\n",
      "Trained batch 962 batch loss 0.627652764 epoch total loss 0.578190804\n",
      "Trained batch 963 batch loss 0.651310444 epoch total loss 0.57826674\n",
      "Trained batch 964 batch loss 0.619886 epoch total loss 0.578309894\n",
      "Trained batch 965 batch loss 0.655343294 epoch total loss 0.578389704\n",
      "Trained batch 966 batch loss 0.649728775 epoch total loss 0.578463554\n",
      "Trained batch 967 batch loss 0.551177859 epoch total loss 0.578435302\n",
      "Trained batch 968 batch loss 0.5055722 epoch total loss 0.57836\n",
      "Trained batch 969 batch loss 0.557509601 epoch total loss 0.578338444\n",
      "Trained batch 970 batch loss 0.556909382 epoch total loss 0.578316331\n",
      "Trained batch 971 batch loss 0.508937478 epoch total loss 0.578244865\n",
      "Trained batch 972 batch loss 0.528973877 epoch total loss 0.578194201\n",
      "Trained batch 973 batch loss 0.435994208 epoch total loss 0.57804805\n",
      "Trained batch 974 batch loss 0.541158676 epoch total loss 0.578010142\n",
      "Trained batch 975 batch loss 0.498884857 epoch total loss 0.577929\n",
      "Trained batch 976 batch loss 0.462539017 epoch total loss 0.577810764\n",
      "Trained batch 977 batch loss 0.52557373 epoch total loss 0.577757299\n",
      "Trained batch 978 batch loss 0.52491951 epoch total loss 0.577703238\n",
      "Trained batch 979 batch loss 0.520881712 epoch total loss 0.577645183\n",
      "Trained batch 980 batch loss 0.506053269 epoch total loss 0.577572107\n",
      "Trained batch 981 batch loss 0.421576977 epoch total loss 0.577413082\n",
      "Trained batch 982 batch loss 0.400453389 epoch total loss 0.577232897\n",
      "Trained batch 983 batch loss 0.420597494 epoch total loss 0.577073574\n",
      "Trained batch 984 batch loss 0.473694354 epoch total loss 0.576968491\n",
      "Trained batch 985 batch loss 0.469525516 epoch total loss 0.576859415\n",
      "Trained batch 986 batch loss 0.383524805 epoch total loss 0.576663375\n",
      "Trained batch 987 batch loss 0.488921404 epoch total loss 0.576574445\n",
      "Trained batch 988 batch loss 0.461614639 epoch total loss 0.576458097\n",
      "Trained batch 989 batch loss 0.643569529 epoch total loss 0.576525927\n",
      "Trained batch 990 batch loss 0.606181741 epoch total loss 0.576555908\n",
      "Trained batch 991 batch loss 0.635637641 epoch total loss 0.576615512\n",
      "Trained batch 992 batch loss 0.538483858 epoch total loss 0.576577067\n",
      "Trained batch 993 batch loss 0.508179486 epoch total loss 0.576508224\n",
      "Trained batch 994 batch loss 0.608068228 epoch total loss 0.57654\n",
      "Trained batch 995 batch loss 0.551442623 epoch total loss 0.576514781\n",
      "Trained batch 996 batch loss 0.537718177 epoch total loss 0.576475799\n",
      "Trained batch 997 batch loss 0.613552928 epoch total loss 0.576513\n",
      "Trained batch 998 batch loss 0.556137204 epoch total loss 0.576492608\n",
      "Trained batch 999 batch loss 0.523312926 epoch total loss 0.576439381\n",
      "Trained batch 1000 batch loss 0.565950453 epoch total loss 0.57642889\n",
      "Trained batch 1001 batch loss 0.50809747 epoch total loss 0.576360643\n",
      "Trained batch 1002 batch loss 0.607686162 epoch total loss 0.576391876\n",
      "Trained batch 1003 batch loss 0.56816411 epoch total loss 0.57638371\n",
      "Trained batch 1004 batch loss 0.650780141 epoch total loss 0.576457798\n",
      "Trained batch 1005 batch loss 0.663544536 epoch total loss 0.576544464\n",
      "Trained batch 1006 batch loss 0.538335264 epoch total loss 0.576506495\n",
      "Trained batch 1007 batch loss 0.566537 epoch total loss 0.576496542\n",
      "Trained batch 1008 batch loss 0.601551831 epoch total loss 0.576521456\n",
      "Trained batch 1009 batch loss 0.616417527 epoch total loss 0.576561\n",
      "Trained batch 1010 batch loss 0.572664 epoch total loss 0.5765571\n",
      "Trained batch 1011 batch loss 0.56029433 epoch total loss 0.576541066\n",
      "Trained batch 1012 batch loss 0.537150443 epoch total loss 0.576502144\n",
      "Trained batch 1013 batch loss 0.526907742 epoch total loss 0.576453209\n",
      "Trained batch 1014 batch loss 0.600108504 epoch total loss 0.576476514\n",
      "Trained batch 1015 batch loss 0.587750375 epoch total loss 0.57648766\n",
      "Trained batch 1016 batch loss 0.580781579 epoch total loss 0.576491892\n",
      "Trained batch 1017 batch loss 0.566608846 epoch total loss 0.576482177\n",
      "Trained batch 1018 batch loss 0.577813566 epoch total loss 0.576483488\n",
      "Trained batch 1019 batch loss 0.524670839 epoch total loss 0.576432586\n",
      "Trained batch 1020 batch loss 0.532984853 epoch total loss 0.576389968\n",
      "Trained batch 1021 batch loss 0.503666282 epoch total loss 0.576318741\n",
      "Trained batch 1022 batch loss 0.4980959 epoch total loss 0.576242208\n",
      "Trained batch 1023 batch loss 0.534255385 epoch total loss 0.576201141\n",
      "Trained batch 1024 batch loss 0.562983871 epoch total loss 0.576188266\n",
      "Trained batch 1025 batch loss 0.580833852 epoch total loss 0.576192796\n",
      "Trained batch 1026 batch loss 0.664105952 epoch total loss 0.576278508\n",
      "Trained batch 1027 batch loss 0.57136023 epoch total loss 0.57627368\n",
      "Trained batch 1028 batch loss 0.553202629 epoch total loss 0.576251268\n",
      "Trained batch 1029 batch loss 0.620203376 epoch total loss 0.576293945\n",
      "Trained batch 1030 batch loss 0.553506851 epoch total loss 0.576271832\n",
      "Trained batch 1031 batch loss 0.519294262 epoch total loss 0.576216578\n",
      "Trained batch 1032 batch loss 0.542715788 epoch total loss 0.576184094\n",
      "Trained batch 1033 batch loss 0.449678957 epoch total loss 0.576061666\n",
      "Trained batch 1034 batch loss 0.451357454 epoch total loss 0.575941086\n",
      "Trained batch 1035 batch loss 0.384759426 epoch total loss 0.575756371\n",
      "Trained batch 1036 batch loss 0.495885 epoch total loss 0.575679302\n",
      "Trained batch 1037 batch loss 0.370644182 epoch total loss 0.575481594\n",
      "Trained batch 1038 batch loss 0.415553868 epoch total loss 0.575327516\n",
      "Trained batch 1039 batch loss 0.503286362 epoch total loss 0.575258195\n",
      "Trained batch 1040 batch loss 0.473219037 epoch total loss 0.57516\n",
      "Trained batch 1041 batch loss 0.60201323 epoch total loss 0.575185835\n",
      "Trained batch 1042 batch loss 0.651969671 epoch total loss 0.575259507\n",
      "Trained batch 1043 batch loss 0.551650047 epoch total loss 0.575236857\n",
      "Trained batch 1044 batch loss 0.454896688 epoch total loss 0.575121582\n",
      "Trained batch 1045 batch loss 0.627855 epoch total loss 0.575172067\n",
      "Trained batch 1046 batch loss 0.717733622 epoch total loss 0.575308323\n",
      "Trained batch 1047 batch loss 0.611983955 epoch total loss 0.57534337\n",
      "Trained batch 1048 batch loss 0.572364092 epoch total loss 0.575340569\n",
      "Trained batch 1049 batch loss 0.632567465 epoch total loss 0.575395107\n",
      "Trained batch 1050 batch loss 0.718823075 epoch total loss 0.575531721\n",
      "Trained batch 1051 batch loss 0.676733911 epoch total loss 0.575628042\n",
      "Trained batch 1052 batch loss 0.733635306 epoch total loss 0.575778246\n",
      "Trained batch 1053 batch loss 0.656000674 epoch total loss 0.575854421\n",
      "Trained batch 1054 batch loss 0.676032364 epoch total loss 0.57594943\n",
      "Trained batch 1055 batch loss 0.576578736 epoch total loss 0.575950086\n",
      "Trained batch 1056 batch loss 0.614374042 epoch total loss 0.575986445\n",
      "Trained batch 1057 batch loss 0.560972154 epoch total loss 0.575972259\n",
      "Trained batch 1058 batch loss 0.571254611 epoch total loss 0.575967789\n",
      "Trained batch 1059 batch loss 0.633483 epoch total loss 0.576022089\n",
      "Trained batch 1060 batch loss 0.557449222 epoch total loss 0.576004565\n",
      "Trained batch 1061 batch loss 0.600098133 epoch total loss 0.576027274\n",
      "Trained batch 1062 batch loss 0.565651119 epoch total loss 0.576017499\n",
      "Trained batch 1063 batch loss 0.601660132 epoch total loss 0.576041639\n",
      "Trained batch 1064 batch loss 0.634208739 epoch total loss 0.576096356\n",
      "Trained batch 1065 batch loss 0.638189554 epoch total loss 0.576154649\n",
      "Trained batch 1066 batch loss 0.605644 epoch total loss 0.576182306\n",
      "Trained batch 1067 batch loss 0.688108206 epoch total loss 0.57628721\n",
      "Trained batch 1068 batch loss 0.645340681 epoch total loss 0.576351821\n",
      "Trained batch 1069 batch loss 0.691773713 epoch total loss 0.576459825\n",
      "Trained batch 1070 batch loss 0.640257 epoch total loss 0.57651943\n",
      "Trained batch 1071 batch loss 0.614040315 epoch total loss 0.576554418\n",
      "Trained batch 1072 batch loss 0.640935719 epoch total loss 0.576614499\n",
      "Trained batch 1073 batch loss 0.721688628 epoch total loss 0.576749682\n",
      "Trained batch 1074 batch loss 0.662554204 epoch total loss 0.576829553\n",
      "Trained batch 1075 batch loss 0.684123039 epoch total loss 0.57692939\n",
      "Trained batch 1076 batch loss 0.655738115 epoch total loss 0.577002645\n",
      "Trained batch 1077 batch loss 0.737054348 epoch total loss 0.577151299\n",
      "Trained batch 1078 batch loss 0.672412395 epoch total loss 0.577239633\n",
      "Trained batch 1079 batch loss 0.656321 epoch total loss 0.577312946\n",
      "Trained batch 1080 batch loss 0.607198656 epoch total loss 0.577340603\n",
      "Trained batch 1081 batch loss 0.658325315 epoch total loss 0.577415526\n",
      "Trained batch 1082 batch loss 0.586102724 epoch total loss 0.577423573\n",
      "Trained batch 1083 batch loss 0.625800371 epoch total loss 0.577468216\n",
      "Trained batch 1084 batch loss 0.640516222 epoch total loss 0.577526331\n",
      "Trained batch 1085 batch loss 0.667891502 epoch total loss 0.577609658\n",
      "Trained batch 1086 batch loss 0.658996 epoch total loss 0.577684581\n",
      "Trained batch 1087 batch loss 0.673257828 epoch total loss 0.577772558\n",
      "Trained batch 1088 batch loss 0.579814553 epoch total loss 0.577774465\n",
      "Trained batch 1089 batch loss 0.51388669 epoch total loss 0.577715814\n",
      "Trained batch 1090 batch loss 0.58693862 epoch total loss 0.577724218\n",
      "Trained batch 1091 batch loss 0.585534751 epoch total loss 0.577731371\n",
      "Trained batch 1092 batch loss 0.521202505 epoch total loss 0.577679574\n",
      "Trained batch 1093 batch loss 0.593725443 epoch total loss 0.577694297\n",
      "Trained batch 1094 batch loss 0.556009769 epoch total loss 0.577674508\n",
      "Trained batch 1095 batch loss 0.558687449 epoch total loss 0.577657163\n",
      "Trained batch 1096 batch loss 0.542425931 epoch total loss 0.577625036\n",
      "Trained batch 1097 batch loss 0.596958041 epoch total loss 0.577642679\n",
      "Trained batch 1098 batch loss 0.583739579 epoch total loss 0.577648222\n",
      "Trained batch 1099 batch loss 0.639846265 epoch total loss 0.577704787\n",
      "Trained batch 1100 batch loss 0.57895422 epoch total loss 0.577706\n",
      "Trained batch 1101 batch loss 0.606330752 epoch total loss 0.577731967\n",
      "Trained batch 1102 batch loss 0.658501565 epoch total loss 0.577805281\n",
      "Trained batch 1103 batch loss 0.642961264 epoch total loss 0.577864289\n",
      "Trained batch 1104 batch loss 0.666344941 epoch total loss 0.577944458\n",
      "Trained batch 1105 batch loss 0.655745685 epoch total loss 0.578014851\n",
      "Trained batch 1106 batch loss 0.629864 epoch total loss 0.578061759\n",
      "Trained batch 1107 batch loss 0.632787466 epoch total loss 0.578111231\n",
      "Trained batch 1108 batch loss 0.608092904 epoch total loss 0.578138292\n",
      "Trained batch 1109 batch loss 0.625875115 epoch total loss 0.578181326\n",
      "Trained batch 1110 batch loss 0.588841915 epoch total loss 0.578190923\n",
      "Trained batch 1111 batch loss 0.569983363 epoch total loss 0.578183591\n",
      "Trained batch 1112 batch loss 0.549492717 epoch total loss 0.578157783\n",
      "Trained batch 1113 batch loss 0.581901848 epoch total loss 0.57816112\n",
      "Trained batch 1114 batch loss 0.589060187 epoch total loss 0.578170896\n",
      "Trained batch 1115 batch loss 0.584608912 epoch total loss 0.578176677\n",
      "Trained batch 1116 batch loss 0.534326255 epoch total loss 0.578137338\n",
      "Trained batch 1117 batch loss 0.651130736 epoch total loss 0.578202724\n",
      "Trained batch 1118 batch loss 0.634628892 epoch total loss 0.57825321\n",
      "Trained batch 1119 batch loss 0.52182436 epoch total loss 0.578202784\n",
      "Trained batch 1120 batch loss 0.620598912 epoch total loss 0.578240633\n",
      "Trained batch 1121 batch loss 0.588346362 epoch total loss 0.578249633\n",
      "Trained batch 1122 batch loss 0.583489776 epoch total loss 0.578254282\n",
      "Trained batch 1123 batch loss 0.611618578 epoch total loss 0.578284\n",
      "Trained batch 1124 batch loss 0.593199492 epoch total loss 0.578297317\n",
      "Trained batch 1125 batch loss 0.57482928 epoch total loss 0.578294218\n",
      "Trained batch 1126 batch loss 0.542225778 epoch total loss 0.57826221\n",
      "Trained batch 1127 batch loss 0.579121828 epoch total loss 0.578262925\n",
      "Trained batch 1128 batch loss 0.595525146 epoch total loss 0.578278244\n",
      "Trained batch 1129 batch loss 0.596093118 epoch total loss 0.578294\n",
      "Trained batch 1130 batch loss 0.607534051 epoch total loss 0.578319907\n",
      "Trained batch 1131 batch loss 0.651709 epoch total loss 0.578384817\n",
      "Trained batch 1132 batch loss 0.584084928 epoch total loss 0.578389823\n",
      "Trained batch 1133 batch loss 0.553523302 epoch total loss 0.578367889\n",
      "Trained batch 1134 batch loss 0.628405809 epoch total loss 0.578412056\n",
      "Trained batch 1135 batch loss 0.648876727 epoch total loss 0.578474104\n",
      "Trained batch 1136 batch loss 0.560045958 epoch total loss 0.578457892\n",
      "Trained batch 1137 batch loss 0.577204406 epoch total loss 0.578456819\n",
      "Trained batch 1138 batch loss 0.463197321 epoch total loss 0.578355491\n",
      "Trained batch 1139 batch loss 0.529760659 epoch total loss 0.578312874\n",
      "Trained batch 1140 batch loss 0.452484548 epoch total loss 0.578202546\n",
      "Trained batch 1141 batch loss 0.478098691 epoch total loss 0.578114808\n",
      "Trained batch 1142 batch loss 0.541121066 epoch total loss 0.578082383\n",
      "Trained batch 1143 batch loss 0.573271 epoch total loss 0.578078151\n",
      "Trained batch 1144 batch loss 0.646934688 epoch total loss 0.578138351\n",
      "Trained batch 1145 batch loss 0.579483628 epoch total loss 0.578139484\n",
      "Trained batch 1146 batch loss 0.681545 epoch total loss 0.578229725\n",
      "Trained batch 1147 batch loss 0.619881511 epoch total loss 0.578266\n",
      "Trained batch 1148 batch loss 0.586333871 epoch total loss 0.578273\n",
      "Trained batch 1149 batch loss 0.654118717 epoch total loss 0.57833904\n",
      "Trained batch 1150 batch loss 0.651196361 epoch total loss 0.57840234\n",
      "Trained batch 1151 batch loss 0.602126598 epoch total loss 0.578422964\n",
      "Trained batch 1152 batch loss 0.740405202 epoch total loss 0.578563571\n",
      "Trained batch 1153 batch loss 0.553652823 epoch total loss 0.578542\n",
      "Trained batch 1154 batch loss 0.564319611 epoch total loss 0.578529656\n",
      "Trained batch 1155 batch loss 0.636891723 epoch total loss 0.578580201\n",
      "Trained batch 1156 batch loss 0.589595735 epoch total loss 0.578589737\n",
      "Trained batch 1157 batch loss 0.684544086 epoch total loss 0.57868135\n",
      "Trained batch 1158 batch loss 0.575759113 epoch total loss 0.578678787\n",
      "Trained batch 1159 batch loss 0.586016238 epoch total loss 0.578685105\n",
      "Trained batch 1160 batch loss 0.549272418 epoch total loss 0.578659713\n",
      "Trained batch 1161 batch loss 0.644178152 epoch total loss 0.578716159\n",
      "Trained batch 1162 batch loss 0.693615496 epoch total loss 0.578815043\n",
      "Trained batch 1163 batch loss 0.563182771 epoch total loss 0.578801572\n",
      "Trained batch 1164 batch loss 0.588559926 epoch total loss 0.57881\n",
      "Trained batch 1165 batch loss 0.617967486 epoch total loss 0.578843594\n",
      "Trained batch 1166 batch loss 0.633508325 epoch total loss 0.578890443\n",
      "Trained batch 1167 batch loss 0.689451516 epoch total loss 0.578985214\n",
      "Trained batch 1168 batch loss 0.620803893 epoch total loss 0.579021\n",
      "Trained batch 1169 batch loss 0.534184158 epoch total loss 0.578982651\n",
      "Trained batch 1170 batch loss 0.622850478 epoch total loss 0.579020143\n",
      "Trained batch 1171 batch loss 0.563844621 epoch total loss 0.579007149\n",
      "Trained batch 1172 batch loss 0.540762365 epoch total loss 0.578974545\n",
      "Trained batch 1173 batch loss 0.5794034 epoch total loss 0.578974903\n",
      "Trained batch 1174 batch loss 0.637679338 epoch total loss 0.579024911\n",
      "Trained batch 1175 batch loss 0.631455183 epoch total loss 0.579069555\n",
      "Trained batch 1176 batch loss 0.587622583 epoch total loss 0.579076886\n",
      "Trained batch 1177 batch loss 0.598165333 epoch total loss 0.579093039\n",
      "Trained batch 1178 batch loss 0.554919183 epoch total loss 0.579072535\n",
      "Trained batch 1179 batch loss 0.521556079 epoch total loss 0.579023778\n",
      "Trained batch 1180 batch loss 0.647290111 epoch total loss 0.579081595\n",
      "Trained batch 1181 batch loss 0.767386317 epoch total loss 0.579241037\n",
      "Trained batch 1182 batch loss 0.726513445 epoch total loss 0.579365611\n",
      "Trained batch 1183 batch loss 0.688152075 epoch total loss 0.579457581\n",
      "Trained batch 1184 batch loss 0.646276832 epoch total loss 0.579514086\n",
      "Trained batch 1185 batch loss 0.614648163 epoch total loss 0.57954371\n",
      "Trained batch 1186 batch loss 0.591479123 epoch total loss 0.579553783\n",
      "Trained batch 1187 batch loss 0.557896078 epoch total loss 0.579535544\n",
      "Trained batch 1188 batch loss 0.53747052 epoch total loss 0.579500139\n",
      "Trained batch 1189 batch loss 0.47947067 epoch total loss 0.579416037\n",
      "Trained batch 1190 batch loss 0.456683636 epoch total loss 0.579312861\n",
      "Trained batch 1191 batch loss 0.410577744 epoch total loss 0.579171181\n",
      "Trained batch 1192 batch loss 0.436824799 epoch total loss 0.579051793\n",
      "Trained batch 1193 batch loss 0.575667441 epoch total loss 0.579049\n",
      "Trained batch 1194 batch loss 0.603782237 epoch total loss 0.579069674\n",
      "Trained batch 1195 batch loss 0.566393 epoch total loss 0.579059064\n",
      "Trained batch 1196 batch loss 0.484462053 epoch total loss 0.578979969\n",
      "Trained batch 1197 batch loss 0.418505728 epoch total loss 0.578845918\n",
      "Trained batch 1198 batch loss 0.395062834 epoch total loss 0.578692496\n",
      "Trained batch 1199 batch loss 0.419994324 epoch total loss 0.578560114\n",
      "Trained batch 1200 batch loss 0.456287861 epoch total loss 0.57845825\n",
      "Trained batch 1201 batch loss 0.418302655 epoch total loss 0.578324854\n",
      "Trained batch 1202 batch loss 0.495882064 epoch total loss 0.578256309\n",
      "Trained batch 1203 batch loss 0.484249771 epoch total loss 0.578178167\n",
      "Trained batch 1204 batch loss 0.504247069 epoch total loss 0.578116775\n",
      "Trained batch 1205 batch loss 0.580422521 epoch total loss 0.578118742\n",
      "Trained batch 1206 batch loss 0.513914287 epoch total loss 0.578065455\n",
      "Trained batch 1207 batch loss 0.567329049 epoch total loss 0.578056574\n",
      "Trained batch 1208 batch loss 0.645249605 epoch total loss 0.578112245\n",
      "Trained batch 1209 batch loss 0.558589041 epoch total loss 0.578096092\n",
      "Trained batch 1210 batch loss 0.537706614 epoch total loss 0.578062713\n",
      "Trained batch 1211 batch loss 0.581023574 epoch total loss 0.578065097\n",
      "Trained batch 1212 batch loss 0.622394323 epoch total loss 0.578101695\n",
      "Trained batch 1213 batch loss 0.587320924 epoch total loss 0.578109324\n",
      "Trained batch 1214 batch loss 0.618831217 epoch total loss 0.578142822\n",
      "Trained batch 1215 batch loss 0.62218225 epoch total loss 0.578179121\n",
      "Trained batch 1216 batch loss 0.647793531 epoch total loss 0.578236341\n",
      "Trained batch 1217 batch loss 0.564279 epoch total loss 0.578224838\n",
      "Trained batch 1218 batch loss 0.569378555 epoch total loss 0.578217626\n",
      "Trained batch 1219 batch loss 0.612484932 epoch total loss 0.578245699\n",
      "Trained batch 1220 batch loss 0.600897789 epoch total loss 0.578264296\n",
      "Trained batch 1221 batch loss 0.617598474 epoch total loss 0.578296483\n",
      "Trained batch 1222 batch loss 0.597077787 epoch total loss 0.57831192\n",
      "Trained batch 1223 batch loss 0.499939799 epoch total loss 0.578247845\n",
      "Trained batch 1224 batch loss 0.581748426 epoch total loss 0.578250647\n",
      "Trained batch 1225 batch loss 0.613360941 epoch total loss 0.578279316\n",
      "Trained batch 1226 batch loss 0.585705638 epoch total loss 0.578285336\n",
      "Trained batch 1227 batch loss 0.543439388 epoch total loss 0.578256965\n",
      "Trained batch 1228 batch loss 0.575054526 epoch total loss 0.578254402\n",
      "Trained batch 1229 batch loss 0.570449293 epoch total loss 0.578248\n",
      "Trained batch 1230 batch loss 0.563855231 epoch total loss 0.578236282\n",
      "Trained batch 1231 batch loss 0.543561041 epoch total loss 0.578208148\n",
      "Trained batch 1232 batch loss 0.642659068 epoch total loss 0.578260422\n",
      "Trained batch 1233 batch loss 0.645569444 epoch total loss 0.578315\n",
      "Trained batch 1234 batch loss 0.679439485 epoch total loss 0.578397\n",
      "Trained batch 1235 batch loss 0.617116332 epoch total loss 0.578428328\n",
      "Trained batch 1236 batch loss 0.663610399 epoch total loss 0.578497291\n",
      "Trained batch 1237 batch loss 0.644256 epoch total loss 0.578550398\n",
      "Trained batch 1238 batch loss 0.613306165 epoch total loss 0.578578472\n",
      "Trained batch 1239 batch loss 0.676748753 epoch total loss 0.578657687\n",
      "Trained batch 1240 batch loss 0.599190772 epoch total loss 0.578674257\n",
      "Trained batch 1241 batch loss 0.625342 epoch total loss 0.578711867\n",
      "Trained batch 1242 batch loss 0.642461181 epoch total loss 0.578763187\n",
      "Trained batch 1243 batch loss 0.627238154 epoch total loss 0.578802228\n",
      "Trained batch 1244 batch loss 0.602729797 epoch total loss 0.57882148\n",
      "Trained batch 1245 batch loss 0.633259892 epoch total loss 0.57886517\n",
      "Trained batch 1246 batch loss 0.641293287 epoch total loss 0.578915298\n",
      "Trained batch 1247 batch loss 0.533336699 epoch total loss 0.578878701\n",
      "Trained batch 1248 batch loss 0.582534552 epoch total loss 0.578881621\n",
      "Trained batch 1249 batch loss 0.558530807 epoch total loss 0.578865349\n",
      "Trained batch 1250 batch loss 0.583196878 epoch total loss 0.578868806\n",
      "Trained batch 1251 batch loss 0.620319963 epoch total loss 0.578901947\n",
      "Trained batch 1252 batch loss 0.521074176 epoch total loss 0.578855693\n",
      "Trained batch 1253 batch loss 0.533062577 epoch total loss 0.578819156\n",
      "Trained batch 1254 batch loss 0.539343536 epoch total loss 0.578787744\n",
      "Trained batch 1255 batch loss 0.460946649 epoch total loss 0.578693807\n",
      "Trained batch 1256 batch loss 0.493557066 epoch total loss 0.578626037\n",
      "Trained batch 1257 batch loss 0.558163166 epoch total loss 0.578609765\n",
      "Trained batch 1258 batch loss 0.585429668 epoch total loss 0.578615189\n",
      "Trained batch 1259 batch loss 0.635178924 epoch total loss 0.578660131\n",
      "Trained batch 1260 batch loss 0.589722 epoch total loss 0.578668892\n",
      "Trained batch 1261 batch loss 0.552310944 epoch total loss 0.578648\n",
      "Trained batch 1262 batch loss 0.572622836 epoch total loss 0.578643203\n",
      "Trained batch 1263 batch loss 0.573910952 epoch total loss 0.578639448\n",
      "Trained batch 1264 batch loss 0.632853448 epoch total loss 0.578682363\n",
      "Trained batch 1265 batch loss 0.595294237 epoch total loss 0.578695476\n",
      "Trained batch 1266 batch loss 0.649065435 epoch total loss 0.578751087\n",
      "Trained batch 1267 batch loss 0.55163157 epoch total loss 0.578729689\n",
      "Trained batch 1268 batch loss 0.521654606 epoch total loss 0.578684688\n",
      "Trained batch 1269 batch loss 0.512190282 epoch total loss 0.578632295\n",
      "Trained batch 1270 batch loss 0.556135893 epoch total loss 0.578614593\n",
      "Trained batch 1271 batch loss 0.519860685 epoch total loss 0.578568339\n",
      "Trained batch 1272 batch loss 0.551983237 epoch total loss 0.578547418\n",
      "Trained batch 1273 batch loss 0.544471204 epoch total loss 0.578520715\n",
      "Trained batch 1274 batch loss 0.519596398 epoch total loss 0.578474462\n",
      "Trained batch 1275 batch loss 0.482235044 epoch total loss 0.578398943\n",
      "Trained batch 1276 batch loss 0.454375267 epoch total loss 0.578301728\n",
      "Trained batch 1277 batch loss 0.475945413 epoch total loss 0.578221619\n",
      "Trained batch 1278 batch loss 0.435465038 epoch total loss 0.57810992\n",
      "Trained batch 1279 batch loss 0.467681289 epoch total loss 0.578023553\n",
      "Trained batch 1280 batch loss 0.485514194 epoch total loss 0.577951312\n",
      "Trained batch 1281 batch loss 0.513667405 epoch total loss 0.577901125\n",
      "Trained batch 1282 batch loss 0.537530184 epoch total loss 0.577869594\n",
      "Trained batch 1283 batch loss 0.5639956 epoch total loss 0.577858806\n",
      "Trained batch 1284 batch loss 0.530057311 epoch total loss 0.577821612\n",
      "Trained batch 1285 batch loss 0.52498734 epoch total loss 0.577780426\n",
      "Trained batch 1286 batch loss 0.563438058 epoch total loss 0.577769279\n",
      "Trained batch 1287 batch loss 0.486291081 epoch total loss 0.577698171\n",
      "Trained batch 1288 batch loss 0.496820301 epoch total loss 0.577635407\n",
      "Trained batch 1289 batch loss 0.569278359 epoch total loss 0.577628911\n",
      "Trained batch 1290 batch loss 0.49363336 epoch total loss 0.577563822\n",
      "Trained batch 1291 batch loss 0.545989633 epoch total loss 0.577539325\n",
      "Trained batch 1292 batch loss 0.571149766 epoch total loss 0.577534378\n",
      "Trained batch 1293 batch loss 0.55803144 epoch total loss 0.577519298\n",
      "Trained batch 1294 batch loss 0.68467176 epoch total loss 0.577602148\n",
      "Trained batch 1295 batch loss 0.554966748 epoch total loss 0.577584684\n",
      "Trained batch 1296 batch loss 0.603036642 epoch total loss 0.577604294\n",
      "Trained batch 1297 batch loss 0.659226775 epoch total loss 0.577667236\n",
      "Trained batch 1298 batch loss 0.572007596 epoch total loss 0.577662885\n",
      "Trained batch 1299 batch loss 0.65382725 epoch total loss 0.577721536\n",
      "Trained batch 1300 batch loss 0.707204521 epoch total loss 0.577821136\n",
      "Trained batch 1301 batch loss 0.539142907 epoch total loss 0.577791393\n",
      "Trained batch 1302 batch loss 0.475417793 epoch total loss 0.577712774\n",
      "Trained batch 1303 batch loss 0.545624733 epoch total loss 0.577688158\n",
      "Trained batch 1304 batch loss 0.518740177 epoch total loss 0.577642918\n",
      "Trained batch 1305 batch loss 0.567849 epoch total loss 0.577635467\n",
      "Trained batch 1306 batch loss 0.558393419 epoch total loss 0.577620745\n",
      "Trained batch 1307 batch loss 0.578697145 epoch total loss 0.57762152\n",
      "Trained batch 1308 batch loss 0.612931371 epoch total loss 0.57764852\n",
      "Trained batch 1309 batch loss 0.522868752 epoch total loss 0.577606678\n",
      "Trained batch 1310 batch loss 0.541013956 epoch total loss 0.577578783\n",
      "Trained batch 1311 batch loss 0.512256444 epoch total loss 0.577528954\n",
      "Trained batch 1312 batch loss 0.592841268 epoch total loss 0.577540636\n",
      "Trained batch 1313 batch loss 0.527567387 epoch total loss 0.577502549\n",
      "Trained batch 1314 batch loss 0.555816 epoch total loss 0.577486038\n",
      "Trained batch 1315 batch loss 0.501107633 epoch total loss 0.577427924\n",
      "Trained batch 1316 batch loss 0.55539149 epoch total loss 0.577411234\n",
      "Trained batch 1317 batch loss 0.584398568 epoch total loss 0.577416539\n",
      "Trained batch 1318 batch loss 0.510398865 epoch total loss 0.577365696\n",
      "Trained batch 1319 batch loss 0.602288365 epoch total loss 0.577384591\n",
      "Trained batch 1320 batch loss 0.506168902 epoch total loss 0.577330589\n",
      "Trained batch 1321 batch loss 0.549481809 epoch total loss 0.577309549\n",
      "Trained batch 1322 batch loss 0.583803833 epoch total loss 0.577314436\n",
      "Trained batch 1323 batch loss 0.625676394 epoch total loss 0.577351034\n",
      "Trained batch 1324 batch loss 0.617774367 epoch total loss 0.577381551\n",
      "Trained batch 1325 batch loss 0.473659933 epoch total loss 0.577303231\n",
      "Trained batch 1326 batch loss 0.519401133 epoch total loss 0.5772596\n",
      "Trained batch 1327 batch loss 0.551389396 epoch total loss 0.577240109\n",
      "Trained batch 1328 batch loss 0.620222509 epoch total loss 0.577272475\n",
      "Trained batch 1329 batch loss 0.604923368 epoch total loss 0.577293277\n",
      "Trained batch 1330 batch loss 0.489694923 epoch total loss 0.577227414\n",
      "Trained batch 1331 batch loss 0.602893949 epoch total loss 0.577246726\n",
      "Trained batch 1332 batch loss 0.560644805 epoch total loss 0.577234268\n",
      "Trained batch 1333 batch loss 0.60458976 epoch total loss 0.577254832\n",
      "Trained batch 1334 batch loss 0.69401741 epoch total loss 0.577342331\n",
      "Trained batch 1335 batch loss 0.6286605 epoch total loss 0.577380776\n",
      "Trained batch 1336 batch loss 0.568663239 epoch total loss 0.577374279\n",
      "Trained batch 1337 batch loss 0.558641791 epoch total loss 0.577360272\n",
      "Trained batch 1338 batch loss 0.635668039 epoch total loss 0.577403843\n",
      "Trained batch 1339 batch loss 0.53642422 epoch total loss 0.577373266\n",
      "Trained batch 1340 batch loss 0.555734694 epoch total loss 0.577357113\n",
      "Trained batch 1341 batch loss 0.559370875 epoch total loss 0.577343702\n",
      "Trained batch 1342 batch loss 0.510424733 epoch total loss 0.577293813\n",
      "Trained batch 1343 batch loss 0.559059918 epoch total loss 0.577280283\n",
      "Trained batch 1344 batch loss 0.571666956 epoch total loss 0.577276111\n",
      "Trained batch 1345 batch loss 0.639550388 epoch total loss 0.577322364\n",
      "Trained batch 1346 batch loss 0.630812645 epoch total loss 0.57736212\n",
      "Trained batch 1347 batch loss 0.559977174 epoch total loss 0.577349186\n",
      "Trained batch 1348 batch loss 0.560816288 epoch total loss 0.577336907\n",
      "Trained batch 1349 batch loss 0.605682135 epoch total loss 0.577357948\n",
      "Trained batch 1350 batch loss 0.57349968 epoch total loss 0.577355\n",
      "Trained batch 1351 batch loss 0.635130584 epoch total loss 0.577397823\n",
      "Trained batch 1352 batch loss 0.648013711 epoch total loss 0.577450037\n",
      "Trained batch 1353 batch loss 0.528123498 epoch total loss 0.577413619\n",
      "Trained batch 1354 batch loss 0.599878907 epoch total loss 0.577430189\n",
      "Trained batch 1355 batch loss 0.595802069 epoch total loss 0.577443719\n",
      "Trained batch 1356 batch loss 0.577824891 epoch total loss 0.577444\n",
      "Trained batch 1357 batch loss 0.555328846 epoch total loss 0.577427745\n",
      "Trained batch 1358 batch loss 0.586620688 epoch total loss 0.57743454\n",
      "Trained batch 1359 batch loss 0.607318163 epoch total loss 0.577456474\n",
      "Trained batch 1360 batch loss 0.59395647 epoch total loss 0.577468574\n",
      "Trained batch 1361 batch loss 0.708231091 epoch total loss 0.577564716\n",
      "Trained batch 1362 batch loss 0.621553242 epoch total loss 0.577597\n",
      "Trained batch 1363 batch loss 0.534241 epoch total loss 0.577565193\n",
      "Trained batch 1364 batch loss 0.516173542 epoch total loss 0.577520192\n",
      "Trained batch 1365 batch loss 0.450697094 epoch total loss 0.577427268\n",
      "Trained batch 1366 batch loss 0.457243085 epoch total loss 0.577339292\n",
      "Trained batch 1367 batch loss 0.520114779 epoch total loss 0.577297449\n",
      "Trained batch 1368 batch loss 0.667376518 epoch total loss 0.577363253\n",
      "Trained batch 1369 batch loss 0.489262193 epoch total loss 0.57729888\n",
      "Trained batch 1370 batch loss 0.586351752 epoch total loss 0.577305496\n",
      "Trained batch 1371 batch loss 0.601016521 epoch total loss 0.577322841\n",
      "Trained batch 1372 batch loss 0.511584163 epoch total loss 0.577274919\n",
      "Trained batch 1373 batch loss 0.546363354 epoch total loss 0.577252388\n",
      "Trained batch 1374 batch loss 0.533738375 epoch total loss 0.577220738\n",
      "Trained batch 1375 batch loss 0.574955285 epoch total loss 0.577219129\n",
      "Trained batch 1376 batch loss 0.604306221 epoch total loss 0.577238798\n",
      "Trained batch 1377 batch loss 0.6109851 epoch total loss 0.577263296\n",
      "Trained batch 1378 batch loss 0.631791949 epoch total loss 0.577302814\n",
      "Trained batch 1379 batch loss 0.585160434 epoch total loss 0.577308536\n",
      "Trained batch 1380 batch loss 0.702189207 epoch total loss 0.577399\n",
      "Trained batch 1381 batch loss 0.616774678 epoch total loss 0.577427566\n",
      "Trained batch 1382 batch loss 0.56434 epoch total loss 0.577418089\n",
      "Trained batch 1383 batch loss 0.615536869 epoch total loss 0.577445626\n",
      "Trained batch 1384 batch loss 0.690638483 epoch total loss 0.577527404\n",
      "Trained batch 1385 batch loss 0.681975901 epoch total loss 0.577602804\n",
      "Trained batch 1386 batch loss 0.661879897 epoch total loss 0.5776636\n",
      "Trained batch 1387 batch loss 0.711303294 epoch total loss 0.577759922\n",
      "Trained batch 1388 batch loss 0.656220496 epoch total loss 0.577816486\n",
      "Trained batch 1389 batch loss 0.590576947 epoch total loss 0.577825665\n",
      "Trained batch 1390 batch loss 0.583499432 epoch total loss 0.577829719\n",
      "Trained batch 1391 batch loss 0.624338448 epoch total loss 0.577863157\n",
      "Trained batch 1392 batch loss 0.540466 epoch total loss 0.577836335\n",
      "Trained batch 1393 batch loss 0.522577703 epoch total loss 0.577796638\n",
      "Trained batch 1394 batch loss 0.586311102 epoch total loss 0.577802777\n",
      "Trained batch 1395 batch loss 0.532309592 epoch total loss 0.577770114\n",
      "Trained batch 1396 batch loss 0.482821852 epoch total loss 0.577702105\n",
      "Trained batch 1397 batch loss 0.531711459 epoch total loss 0.577669203\n",
      "Trained batch 1398 batch loss 0.699575186 epoch total loss 0.577756405\n",
      "Trained batch 1399 batch loss 0.682579339 epoch total loss 0.577831328\n",
      "Trained batch 1400 batch loss 0.629261613 epoch total loss 0.577868104\n",
      "Trained batch 1401 batch loss 0.599987268 epoch total loss 0.57788384\n",
      "Trained batch 1402 batch loss 0.739322364 epoch total loss 0.577999\n",
      "Trained batch 1403 batch loss 0.553621888 epoch total loss 0.577981651\n",
      "Trained batch 1404 batch loss 0.591536283 epoch total loss 0.577991307\n",
      "Trained batch 1405 batch loss 0.646322727 epoch total loss 0.578039944\n",
      "Trained batch 1406 batch loss 0.592847 epoch total loss 0.578050435\n",
      "Trained batch 1407 batch loss 0.600959599 epoch total loss 0.578066766\n",
      "Trained batch 1408 batch loss 0.584139466 epoch total loss 0.578071058\n",
      "Trained batch 1409 batch loss 0.588647723 epoch total loss 0.578078568\n",
      "Trained batch 1410 batch loss 0.6683 epoch total loss 0.578142524\n",
      "Trained batch 1411 batch loss 0.607111156 epoch total loss 0.578163087\n",
      "Trained batch 1412 batch loss 0.600891292 epoch total loss 0.578179181\n",
      "Trained batch 1413 batch loss 0.547064424 epoch total loss 0.578157127\n",
      "Trained batch 1414 batch loss 0.622893572 epoch total loss 0.578188777\n",
      "Trained batch 1415 batch loss 0.574468255 epoch total loss 0.578186095\n",
      "Trained batch 1416 batch loss 0.586623847 epoch total loss 0.578192055\n",
      "Trained batch 1417 batch loss 0.487703234 epoch total loss 0.578128219\n",
      "Trained batch 1418 batch loss 0.54862082 epoch total loss 0.578107417\n",
      "Trained batch 1419 batch loss 0.567758501 epoch total loss 0.578100145\n",
      "Trained batch 1420 batch loss 0.526689 epoch total loss 0.578063905\n",
      "Trained batch 1421 batch loss 0.602784932 epoch total loss 0.57808131\n",
      "Trained batch 1422 batch loss 0.651998818 epoch total loss 0.578133285\n",
      "Trained batch 1423 batch loss 0.616820455 epoch total loss 0.578160465\n",
      "Trained batch 1424 batch loss 0.542039871 epoch total loss 0.578135133\n",
      "Trained batch 1425 batch loss 0.54507184 epoch total loss 0.578111887\n",
      "Trained batch 1426 batch loss 0.564149797 epoch total loss 0.578102112\n",
      "Trained batch 1427 batch loss 0.539772749 epoch total loss 0.57807523\n",
      "Trained batch 1428 batch loss 0.523707271 epoch total loss 0.578037143\n",
      "Trained batch 1429 batch loss 0.624767542 epoch total loss 0.578069866\n",
      "Trained batch 1430 batch loss 0.50334394 epoch total loss 0.578017592\n",
      "Trained batch 1431 batch loss 0.487947762 epoch total loss 0.57795471\n",
      "Trained batch 1432 batch loss 0.459462672 epoch total loss 0.577872\n",
      "Trained batch 1433 batch loss 0.462892264 epoch total loss 0.577791691\n",
      "Trained batch 1434 batch loss 0.464903563 epoch total loss 0.577713\n",
      "Trained batch 1435 batch loss 0.449293613 epoch total loss 0.577623487\n",
      "Trained batch 1436 batch loss 0.575277269 epoch total loss 0.577621818\n",
      "Trained batch 1437 batch loss 0.665004134 epoch total loss 0.577682614\n",
      "Trained batch 1438 batch loss 0.682971299 epoch total loss 0.577755868\n",
      "Trained batch 1439 batch loss 0.698932707 epoch total loss 0.57784003\n",
      "Trained batch 1440 batch loss 0.5639503 epoch total loss 0.577830434\n",
      "Trained batch 1441 batch loss 0.651557505 epoch total loss 0.577881575\n",
      "Trained batch 1442 batch loss 0.617307782 epoch total loss 0.577908933\n",
      "Trained batch 1443 batch loss 0.623242557 epoch total loss 0.577940345\n",
      "Trained batch 1444 batch loss 0.6260342 epoch total loss 0.577973664\n",
      "Trained batch 1445 batch loss 0.670541525 epoch total loss 0.578037679\n",
      "Trained batch 1446 batch loss 0.732030809 epoch total loss 0.578144193\n",
      "Trained batch 1447 batch loss 0.575636148 epoch total loss 0.578142464\n",
      "Trained batch 1448 batch loss 0.596819162 epoch total loss 0.578155339\n",
      "Trained batch 1449 batch loss 0.554878414 epoch total loss 0.578139305\n",
      "Trained batch 1450 batch loss 0.650400519 epoch total loss 0.578189135\n",
      "Trained batch 1451 batch loss 0.645148575 epoch total loss 0.578235269\n",
      "Trained batch 1452 batch loss 0.620106339 epoch total loss 0.578264117\n",
      "Trained batch 1453 batch loss 0.68974191 epoch total loss 0.578340828\n",
      "Trained batch 1454 batch loss 0.557735384 epoch total loss 0.578326643\n",
      "Trained batch 1455 batch loss 0.617170274 epoch total loss 0.578353345\n",
      "Trained batch 1456 batch loss 0.554345191 epoch total loss 0.578336835\n",
      "Trained batch 1457 batch loss 0.482397795 epoch total loss 0.578271031\n",
      "Trained batch 1458 batch loss 0.516854584 epoch total loss 0.578228891\n",
      "Trained batch 1459 batch loss 0.577606797 epoch total loss 0.578228474\n",
      "Trained batch 1460 batch loss 0.488278866 epoch total loss 0.578166902\n",
      "Trained batch 1461 batch loss 0.464328915 epoch total loss 0.578089\n",
      "Trained batch 1462 batch loss 0.446053326 epoch total loss 0.577998698\n",
      "Trained batch 1463 batch loss 0.564860046 epoch total loss 0.577989697\n",
      "Trained batch 1464 batch loss 0.55006814 epoch total loss 0.577970624\n",
      "Trained batch 1465 batch loss 0.581861734 epoch total loss 0.577973247\n",
      "Trained batch 1466 batch loss 0.649800777 epoch total loss 0.578022242\n",
      "Trained batch 1467 batch loss 0.563315809 epoch total loss 0.578012228\n",
      "Trained batch 1468 batch loss 0.619522214 epoch total loss 0.578040481\n",
      "Trained batch 1469 batch loss 0.596113682 epoch total loss 0.578052759\n",
      "Trained batch 1470 batch loss 0.625055432 epoch total loss 0.578084767\n",
      "Trained batch 1471 batch loss 0.633267403 epoch total loss 0.578122258\n",
      "Trained batch 1472 batch loss 0.67235446 epoch total loss 0.578186274\n",
      "Trained batch 1473 batch loss 0.57314229 epoch total loss 0.578182817\n",
      "Trained batch 1474 batch loss 0.56615746 epoch total loss 0.57817471\n",
      "Trained batch 1475 batch loss 0.555630267 epoch total loss 0.578159392\n",
      "Trained batch 1476 batch loss 0.505782962 epoch total loss 0.578110337\n",
      "Trained batch 1477 batch loss 0.509551108 epoch total loss 0.578063905\n",
      "Trained batch 1478 batch loss 0.640162587 epoch total loss 0.578105927\n",
      "Trained batch 1479 batch loss 0.594529033 epoch total loss 0.578117\n",
      "Trained batch 1480 batch loss 0.693468 epoch total loss 0.578195\n",
      "Trained batch 1481 batch loss 0.639106512 epoch total loss 0.578236103\n",
      "Trained batch 1482 batch loss 0.63279587 epoch total loss 0.578272939\n",
      "Trained batch 1483 batch loss 0.741534591 epoch total loss 0.578383\n",
      "Trained batch 1484 batch loss 0.672133 epoch total loss 0.57844615\n",
      "Trained batch 1485 batch loss 0.695285857 epoch total loss 0.578524888\n",
      "Trained batch 1486 batch loss 0.541423142 epoch total loss 0.578499913\n",
      "Trained batch 1487 batch loss 0.6648224 epoch total loss 0.578557968\n",
      "Trained batch 1488 batch loss 0.605398834 epoch total loss 0.578575969\n",
      "Trained batch 1489 batch loss 0.617344141 epoch total loss 0.578602076\n",
      "Trained batch 1490 batch loss 0.61891067 epoch total loss 0.578629076\n",
      "Trained batch 1491 batch loss 0.592919528 epoch total loss 0.578638673\n",
      "Trained batch 1492 batch loss 0.519313574 epoch total loss 0.578598857\n",
      "Trained batch 1493 batch loss 0.524305701 epoch total loss 0.578562498\n",
      "Trained batch 1494 batch loss 0.470410109 epoch total loss 0.578490078\n",
      "Trained batch 1495 batch loss 0.540533304 epoch total loss 0.578464687\n",
      "Trained batch 1496 batch loss 0.65856868 epoch total loss 0.578518271\n",
      "Trained batch 1497 batch loss 0.587280095 epoch total loss 0.578524113\n",
      "Trained batch 1498 batch loss 0.637906551 epoch total loss 0.57856375\n",
      "Trained batch 1499 batch loss 0.645753384 epoch total loss 0.578608572\n",
      "Trained batch 1500 batch loss 0.657425642 epoch total loss 0.578661084\n",
      "Trained batch 1501 batch loss 0.622963 epoch total loss 0.578690648\n",
      "Trained batch 1502 batch loss 0.608454943 epoch total loss 0.578710437\n",
      "Trained batch 1503 batch loss 0.542530358 epoch total loss 0.578686357\n",
      "Trained batch 1504 batch loss 0.544321239 epoch total loss 0.578663528\n",
      "Trained batch 1505 batch loss 0.5043661 epoch total loss 0.578614175\n",
      "Trained batch 1506 batch loss 0.595407367 epoch total loss 0.578625321\n",
      "Trained batch 1507 batch loss 0.48393786 epoch total loss 0.578562498\n",
      "Trained batch 1508 batch loss 0.475117058 epoch total loss 0.578493893\n",
      "Trained batch 1509 batch loss 0.453003943 epoch total loss 0.578410745\n",
      "Trained batch 1510 batch loss 0.519851387 epoch total loss 0.578371942\n",
      "Trained batch 1511 batch loss 0.55304414 epoch total loss 0.578355193\n",
      "Trained batch 1512 batch loss 0.542587519 epoch total loss 0.57833153\n",
      "Trained batch 1513 batch loss 0.521956384 epoch total loss 0.578294277\n",
      "Trained batch 1514 batch loss 0.554249763 epoch total loss 0.578278422\n",
      "Trained batch 1515 batch loss 0.50501442 epoch total loss 0.57823\n",
      "Trained batch 1516 batch loss 0.538185239 epoch total loss 0.578203619\n",
      "Trained batch 1517 batch loss 0.589915693 epoch total loss 0.578211367\n",
      "Trained batch 1518 batch loss 0.545629144 epoch total loss 0.578189909\n",
      "Trained batch 1519 batch loss 0.461505234 epoch total loss 0.578113079\n",
      "Trained batch 1520 batch loss 0.468269229 epoch total loss 0.578040779\n",
      "Trained batch 1521 batch loss 0.410115927 epoch total loss 0.577930391\n",
      "Trained batch 1522 batch loss 0.599125445 epoch total loss 0.577944279\n",
      "Trained batch 1523 batch loss 0.625981152 epoch total loss 0.57797581\n",
      "Trained batch 1524 batch loss 0.650886714 epoch total loss 0.578023672\n",
      "Trained batch 1525 batch loss 0.561263382 epoch total loss 0.578012705\n",
      "Trained batch 1526 batch loss 0.60000366 epoch total loss 0.57802707\n",
      "Trained batch 1527 batch loss 0.598233044 epoch total loss 0.578040302\n",
      "Trained batch 1528 batch loss 0.566351593 epoch total loss 0.578032672\n",
      "Trained batch 1529 batch loss 0.601229429 epoch total loss 0.578047812\n",
      "Trained batch 1530 batch loss 0.562618315 epoch total loss 0.578037739\n",
      "Trained batch 1531 batch loss 0.599626 epoch total loss 0.578051865\n",
      "Trained batch 1532 batch loss 0.626589775 epoch total loss 0.578083515\n",
      "Trained batch 1533 batch loss 0.607933462 epoch total loss 0.578103\n",
      "Trained batch 1534 batch loss 0.543222189 epoch total loss 0.578080237\n",
      "Trained batch 1535 batch loss 0.511817098 epoch total loss 0.578037083\n",
      "Trained batch 1536 batch loss 0.504205287 epoch total loss 0.577989042\n",
      "Trained batch 1537 batch loss 0.531499445 epoch total loss 0.577958763\n",
      "Trained batch 1538 batch loss 0.534457147 epoch total loss 0.57793051\n",
      "Trained batch 1539 batch loss 0.598857641 epoch total loss 0.5779441\n",
      "Trained batch 1540 batch loss 0.572835743 epoch total loss 0.577940762\n",
      "Trained batch 1541 batch loss 0.539964557 epoch total loss 0.577916145\n",
      "Trained batch 1542 batch loss 0.569191456 epoch total loss 0.577910483\n",
      "Trained batch 1543 batch loss 0.587519944 epoch total loss 0.577916741\n",
      "Trained batch 1544 batch loss 0.725122571 epoch total loss 0.578012049\n",
      "Trained batch 1545 batch loss 0.640516281 epoch total loss 0.578052521\n",
      "Trained batch 1546 batch loss 0.643228292 epoch total loss 0.578094661\n",
      "Trained batch 1547 batch loss 0.599411309 epoch total loss 0.57810849\n",
      "Trained batch 1548 batch loss 0.611282289 epoch total loss 0.578129888\n",
      "Trained batch 1549 batch loss 0.590258241 epoch total loss 0.578137755\n",
      "Trained batch 1550 batch loss 0.688959539 epoch total loss 0.578209221\n",
      "Trained batch 1551 batch loss 0.64792949 epoch total loss 0.578254163\n",
      "Trained batch 1552 batch loss 0.611620128 epoch total loss 0.578275681\n",
      "Trained batch 1553 batch loss 0.660680652 epoch total loss 0.578328788\n",
      "Trained batch 1554 batch loss 0.671746254 epoch total loss 0.57838887\n",
      "Trained batch 1555 batch loss 0.67554605 epoch total loss 0.578451395\n",
      "Trained batch 1556 batch loss 0.602881134 epoch total loss 0.578467071\n",
      "Trained batch 1557 batch loss 0.648825228 epoch total loss 0.578512251\n",
      "Trained batch 1558 batch loss 0.691531241 epoch total loss 0.57858479\n",
      "Trained batch 1559 batch loss 0.701726615 epoch total loss 0.578663766\n",
      "Trained batch 1560 batch loss 0.562917 epoch total loss 0.578653693\n",
      "Trained batch 1561 batch loss 0.561927259 epoch total loss 0.578643\n",
      "Trained batch 1562 batch loss 0.607198417 epoch total loss 0.578661263\n",
      "Trained batch 1563 batch loss 0.577459574 epoch total loss 0.578660488\n",
      "Trained batch 1564 batch loss 0.680457234 epoch total loss 0.578725576\n",
      "Trained batch 1565 batch loss 0.590126038 epoch total loss 0.578732908\n",
      "Trained batch 1566 batch loss 0.502240717 epoch total loss 0.578684032\n",
      "Trained batch 1567 batch loss 0.458192021 epoch total loss 0.578607142\n",
      "Trained batch 1568 batch loss 0.495038033 epoch total loss 0.578553855\n",
      "Trained batch 1569 batch loss 0.441051692 epoch total loss 0.578466237\n",
      "Trained batch 1570 batch loss 0.480584085 epoch total loss 0.57840389\n",
      "Trained batch 1571 batch loss 0.467092752 epoch total loss 0.578333\n",
      "Trained batch 1572 batch loss 0.487927586 epoch total loss 0.578275502\n",
      "Trained batch 1573 batch loss 0.596528053 epoch total loss 0.578287125\n",
      "Trained batch 1574 batch loss 0.592020869 epoch total loss 0.578295887\n",
      "Trained batch 1575 batch loss 0.706666708 epoch total loss 0.578377366\n",
      "Trained batch 1576 batch loss 0.687491179 epoch total loss 0.578446627\n",
      "Trained batch 1577 batch loss 0.599697471 epoch total loss 0.578460097\n",
      "Trained batch 1578 batch loss 0.714283347 epoch total loss 0.578546166\n",
      "Trained batch 1579 batch loss 0.597450495 epoch total loss 0.578558147\n",
      "Trained batch 1580 batch loss 0.633315325 epoch total loss 0.578592837\n",
      "Trained batch 1581 batch loss 0.473492116 epoch total loss 0.578526318\n",
      "Trained batch 1582 batch loss 0.519532263 epoch total loss 0.578489065\n",
      "Trained batch 1583 batch loss 0.492754489 epoch total loss 0.578434885\n",
      "Trained batch 1584 batch loss 0.448350966 epoch total loss 0.578352749\n",
      "Trained batch 1585 batch loss 0.568356037 epoch total loss 0.578346431\n",
      "Trained batch 1586 batch loss 0.588176489 epoch total loss 0.57835269\n",
      "Trained batch 1587 batch loss 0.678935409 epoch total loss 0.578416049\n",
      "Trained batch 1588 batch loss 0.521714926 epoch total loss 0.578380346\n",
      "Trained batch 1589 batch loss 0.628716052 epoch total loss 0.578412056\n",
      "Trained batch 1590 batch loss 0.694719374 epoch total loss 0.578485191\n",
      "Trained batch 1591 batch loss 0.516680896 epoch total loss 0.578446329\n",
      "Trained batch 1592 batch loss 0.550449252 epoch total loss 0.578428745\n",
      "Trained batch 1593 batch loss 0.646558285 epoch total loss 0.578471541\n",
      "Trained batch 1594 batch loss 0.634579897 epoch total loss 0.578506708\n",
      "Trained batch 1595 batch loss 0.687704086 epoch total loss 0.578575194\n",
      "Trained batch 1596 batch loss 0.6975981 epoch total loss 0.5786497\n",
      "Trained batch 1597 batch loss 0.560193598 epoch total loss 0.578638136\n",
      "Trained batch 1598 batch loss 0.604326606 epoch total loss 0.57865423\n",
      "Trained batch 1599 batch loss 0.575956523 epoch total loss 0.578652501\n",
      "Trained batch 1600 batch loss 0.604229 epoch total loss 0.578668535\n",
      "Trained batch 1601 batch loss 0.589905739 epoch total loss 0.578675508\n",
      "Trained batch 1602 batch loss 0.526085317 epoch total loss 0.578642666\n",
      "Trained batch 1603 batch loss 0.524092197 epoch total loss 0.578608692\n",
      "Trained batch 1604 batch loss 0.476867437 epoch total loss 0.578545272\n",
      "Trained batch 1605 batch loss 0.542669237 epoch total loss 0.578522861\n",
      "Trained batch 1606 batch loss 0.536690116 epoch total loss 0.578496814\n",
      "Trained batch 1607 batch loss 0.60752207 epoch total loss 0.578514934\n",
      "Trained batch 1608 batch loss 0.564289331 epoch total loss 0.578506052\n",
      "Trained batch 1609 batch loss 0.61070931 epoch total loss 0.57852608\n",
      "Trained batch 1610 batch loss 0.537988842 epoch total loss 0.578500867\n",
      "Trained batch 1611 batch loss 0.506870568 epoch total loss 0.578456402\n",
      "Trained batch 1612 batch loss 0.485426039 epoch total loss 0.578398705\n",
      "Trained batch 1613 batch loss 0.470919311 epoch total loss 0.578332067\n",
      "Trained batch 1614 batch loss 0.523516357 epoch total loss 0.578298151\n",
      "Trained batch 1615 batch loss 0.432943255 epoch total loss 0.578208089\n",
      "Trained batch 1616 batch loss 0.479934692 epoch total loss 0.578147292\n",
      "Trained batch 1617 batch loss 0.46108675 epoch total loss 0.578074872\n",
      "Trained batch 1618 batch loss 0.640757561 epoch total loss 0.578113616\n",
      "Trained batch 1619 batch loss 0.655825853 epoch total loss 0.578161597\n",
      "Trained batch 1620 batch loss 0.576091409 epoch total loss 0.578160346\n",
      "Trained batch 1621 batch loss 0.572229862 epoch total loss 0.57815665\n",
      "Trained batch 1622 batch loss 0.642610312 epoch total loss 0.578196406\n",
      "Trained batch 1623 batch loss 0.71197 epoch total loss 0.57827884\n",
      "Trained batch 1624 batch loss 0.69907403 epoch total loss 0.578353226\n",
      "Trained batch 1625 batch loss 0.678468704 epoch total loss 0.578414857\n",
      "Trained batch 1626 batch loss 0.641348839 epoch total loss 0.578453541\n",
      "Trained batch 1627 batch loss 0.638621092 epoch total loss 0.578490555\n",
      "Trained batch 1628 batch loss 0.605415881 epoch total loss 0.578507066\n",
      "Trained batch 1629 batch loss 0.58312124 epoch total loss 0.578509927\n",
      "Trained batch 1630 batch loss 0.627470136 epoch total loss 0.578539908\n",
      "Trained batch 1631 batch loss 0.563190103 epoch total loss 0.57853049\n",
      "Trained batch 1632 batch loss 0.557123959 epoch total loss 0.578517377\n",
      "Trained batch 1633 batch loss 0.513725877 epoch total loss 0.57847774\n",
      "Trained batch 1634 batch loss 0.517283559 epoch total loss 0.578440249\n",
      "Trained batch 1635 batch loss 0.637938142 epoch total loss 0.578476667\n",
      "Trained batch 1636 batch loss 0.552276373 epoch total loss 0.578460634\n",
      "Trained batch 1637 batch loss 0.559711874 epoch total loss 0.57844913\n",
      "Trained batch 1638 batch loss 0.626252115 epoch total loss 0.578478336\n",
      "Trained batch 1639 batch loss 0.575390458 epoch total loss 0.578476489\n",
      "Trained batch 1640 batch loss 0.540026665 epoch total loss 0.578453\n",
      "Trained batch 1641 batch loss 0.547110856 epoch total loss 0.578433931\n",
      "Trained batch 1642 batch loss 0.620255947 epoch total loss 0.578459382\n",
      "Trained batch 1643 batch loss 0.614021659 epoch total loss 0.578481\n",
      "Trained batch 1644 batch loss 0.548620701 epoch total loss 0.578462899\n",
      "Trained batch 1645 batch loss 0.614118934 epoch total loss 0.578484595\n",
      "Trained batch 1646 batch loss 0.590951443 epoch total loss 0.578492165\n",
      "Trained batch 1647 batch loss 0.555407882 epoch total loss 0.578478158\n",
      "Trained batch 1648 batch loss 0.694808781 epoch total loss 0.578548729\n",
      "Trained batch 1649 batch loss 0.567313671 epoch total loss 0.578541934\n",
      "Trained batch 1650 batch loss 0.506189525 epoch total loss 0.578498065\n",
      "Trained batch 1651 batch loss 0.525242329 epoch total loss 0.578465819\n",
      "Trained batch 1652 batch loss 0.52671504 epoch total loss 0.578434527\n",
      "Trained batch 1653 batch loss 0.505934119 epoch total loss 0.578390658\n",
      "Trained batch 1654 batch loss 0.635539949 epoch total loss 0.578425169\n",
      "Trained batch 1655 batch loss 0.566210568 epoch total loss 0.578417838\n",
      "Trained batch 1656 batch loss 0.488120317 epoch total loss 0.578363299\n",
      "Trained batch 1657 batch loss 0.524487078 epoch total loss 0.578330755\n",
      "Trained batch 1658 batch loss 0.538774788 epoch total loss 0.578306913\n",
      "Trained batch 1659 batch loss 0.474050343 epoch total loss 0.57824403\n",
      "Trained batch 1660 batch loss 0.689219296 epoch total loss 0.578310907\n",
      "Trained batch 1661 batch loss 0.560891569 epoch total loss 0.578300416\n",
      "Trained batch 1662 batch loss 0.4685179 epoch total loss 0.578234375\n",
      "Trained batch 1663 batch loss 0.512120187 epoch total loss 0.578194618\n",
      "Trained batch 1664 batch loss 0.479343325 epoch total loss 0.578135252\n",
      "Trained batch 1665 batch loss 0.56016171 epoch total loss 0.578124464\n",
      "Trained batch 1666 batch loss 0.606436074 epoch total loss 0.578141451\n",
      "Trained batch 1667 batch loss 0.550073504 epoch total loss 0.578124583\n",
      "Trained batch 1668 batch loss 0.501256704 epoch total loss 0.578078508\n",
      "Trained batch 1669 batch loss 0.596430898 epoch total loss 0.578089535\n",
      "Trained batch 1670 batch loss 0.487105846 epoch total loss 0.578035057\n",
      "Trained batch 1671 batch loss 0.544696808 epoch total loss 0.578015089\n",
      "Trained batch 1672 batch loss 0.562550426 epoch total loss 0.57800585\n",
      "Trained batch 1673 batch loss 0.499304771 epoch total loss 0.577958822\n",
      "Trained batch 1674 batch loss 0.505956769 epoch total loss 0.577915847\n",
      "Trained batch 1675 batch loss 0.542184591 epoch total loss 0.577894509\n",
      "Trained batch 1676 batch loss 0.548089266 epoch total loss 0.577876687\n",
      "Trained batch 1677 batch loss 0.566381276 epoch total loss 0.577869892\n",
      "Trained batch 1678 batch loss 0.606689095 epoch total loss 0.577887058\n",
      "Trained batch 1679 batch loss 0.590641141 epoch total loss 0.577894628\n",
      "Trained batch 1680 batch loss 0.561680913 epoch total loss 0.577885\n",
      "Trained batch 1681 batch loss 0.574807227 epoch total loss 0.577883184\n",
      "Trained batch 1682 batch loss 0.564787269 epoch total loss 0.577875376\n",
      "Trained batch 1683 batch loss 0.631065309 epoch total loss 0.577906966\n",
      "Trained batch 1684 batch loss 0.554186523 epoch total loss 0.5778929\n",
      "Trained batch 1685 batch loss 0.596406817 epoch total loss 0.577903926\n",
      "Trained batch 1686 batch loss 0.616655648 epoch total loss 0.577926874\n",
      "Trained batch 1687 batch loss 0.643216 epoch total loss 0.577965558\n",
      "Trained batch 1688 batch loss 0.626998305 epoch total loss 0.577994585\n",
      "Trained batch 1689 batch loss 0.609971225 epoch total loss 0.578013539\n",
      "Trained batch 1690 batch loss 0.636511922 epoch total loss 0.57804817\n",
      "Trained batch 1691 batch loss 0.574389696 epoch total loss 0.578046\n",
      "Trained batch 1692 batch loss 0.576831818 epoch total loss 0.578045309\n",
      "Trained batch 1693 batch loss 0.551749587 epoch total loss 0.578029811\n",
      "Trained batch 1694 batch loss 0.552696466 epoch total loss 0.578014791\n",
      "Trained batch 1695 batch loss 0.638987362 epoch total loss 0.578050792\n",
      "Trained batch 1696 batch loss 0.657489657 epoch total loss 0.578097641\n",
      "Trained batch 1697 batch loss 0.710517466 epoch total loss 0.578175664\n",
      "Trained batch 1698 batch loss 0.670712054 epoch total loss 0.578230143\n",
      "Trained batch 1699 batch loss 0.565892339 epoch total loss 0.578222871\n",
      "Trained batch 1700 batch loss 0.581465781 epoch total loss 0.578224838\n",
      "Trained batch 1701 batch loss 0.540728152 epoch total loss 0.578202784\n",
      "Trained batch 1702 batch loss 0.455196083 epoch total loss 0.578130484\n",
      "Trained batch 1703 batch loss 0.43532154 epoch total loss 0.57804662\n",
      "Trained batch 1704 batch loss 0.500523269 epoch total loss 0.578001142\n",
      "Trained batch 1705 batch loss 0.548469782 epoch total loss 0.577983797\n",
      "Trained batch 1706 batch loss 0.55754441 epoch total loss 0.577971816\n",
      "Trained batch 1707 batch loss 0.537884176 epoch total loss 0.577948391\n",
      "Trained batch 1708 batch loss 0.587998688 epoch total loss 0.577954233\n",
      "Trained batch 1709 batch loss 0.698780954 epoch total loss 0.578025\n",
      "Trained batch 1710 batch loss 0.668427944 epoch total loss 0.578077853\n",
      "Trained batch 1711 batch loss 0.585576713 epoch total loss 0.578082204\n",
      "Trained batch 1712 batch loss 0.586344242 epoch total loss 0.578087091\n",
      "Trained batch 1713 batch loss 0.520854 epoch total loss 0.578053653\n",
      "Trained batch 1714 batch loss 0.586989105 epoch total loss 0.578058898\n",
      "Trained batch 1715 batch loss 0.588742852 epoch total loss 0.578065097\n",
      "Trained batch 1716 batch loss 0.513056159 epoch total loss 0.578027248\n",
      "Trained batch 1717 batch loss 0.553303838 epoch total loss 0.578012824\n",
      "Trained batch 1718 batch loss 0.626068056 epoch total loss 0.578040779\n",
      "Trained batch 1719 batch loss 0.561851621 epoch total loss 0.578031361\n",
      "Trained batch 1720 batch loss 0.610722303 epoch total loss 0.578050315\n",
      "Trained batch 1721 batch loss 0.522717178 epoch total loss 0.578018188\n",
      "Trained batch 1722 batch loss 0.670184731 epoch total loss 0.578071713\n",
      "Trained batch 1723 batch loss 0.553399682 epoch total loss 0.578057349\n",
      "Trained batch 1724 batch loss 0.640442967 epoch total loss 0.578093588\n",
      "Trained batch 1725 batch loss 0.675964355 epoch total loss 0.578150272\n",
      "Trained batch 1726 batch loss 0.666377842 epoch total loss 0.578201413\n",
      "Trained batch 1727 batch loss 0.574908853 epoch total loss 0.578199506\n",
      "Trained batch 1728 batch loss 0.490048259 epoch total loss 0.578148484\n",
      "Trained batch 1729 batch loss 0.452750146 epoch total loss 0.578075945\n",
      "Trained batch 1730 batch loss 0.578401625 epoch total loss 0.578076184\n",
      "Trained batch 1731 batch loss 0.53398 epoch total loss 0.578050733\n",
      "Trained batch 1732 batch loss 0.461155117 epoch total loss 0.57798326\n",
      "Trained batch 1733 batch loss 0.552301943 epoch total loss 0.577968419\n",
      "Trained batch 1734 batch loss 0.643633783 epoch total loss 0.578006268\n",
      "Trained batch 1735 batch loss 0.564681411 epoch total loss 0.577998579\n",
      "Trained batch 1736 batch loss 0.593215644 epoch total loss 0.57800734\n",
      "Trained batch 1737 batch loss 0.612966537 epoch total loss 0.578027487\n",
      "Trained batch 1738 batch loss 0.638403714 epoch total loss 0.578062236\n",
      "Trained batch 1739 batch loss 0.770373344 epoch total loss 0.578172863\n",
      "Trained batch 1740 batch loss 0.667700291 epoch total loss 0.578224301\n",
      "Trained batch 1741 batch loss 0.636187911 epoch total loss 0.578257561\n",
      "Trained batch 1742 batch loss 0.649303615 epoch total loss 0.57829839\n",
      "Trained batch 1743 batch loss 0.656001925 epoch total loss 0.578343\n",
      "Trained batch 1744 batch loss 0.679943919 epoch total loss 0.578401208\n",
      "Trained batch 1745 batch loss 0.639516175 epoch total loss 0.578436255\n",
      "Trained batch 1746 batch loss 0.583709657 epoch total loss 0.578439236\n",
      "Trained batch 1747 batch loss 0.660560608 epoch total loss 0.578486264\n",
      "Trained batch 1748 batch loss 0.623942375 epoch total loss 0.578512251\n",
      "Trained batch 1749 batch loss 0.575655699 epoch total loss 0.578510642\n",
      "Trained batch 1750 batch loss 0.6456213 epoch total loss 0.578549\n",
      "Trained batch 1751 batch loss 0.56880641 epoch total loss 0.578543425\n",
      "Trained batch 1752 batch loss 0.598602951 epoch total loss 0.578554869\n",
      "Trained batch 1753 batch loss 0.623345613 epoch total loss 0.578580439\n",
      "Trained batch 1754 batch loss 0.563773215 epoch total loss 0.578572035\n",
      "Trained batch 1755 batch loss 0.55294013 epoch total loss 0.578557372\n",
      "Trained batch 1756 batch loss 0.633622289 epoch total loss 0.578588724\n",
      "Trained batch 1757 batch loss 0.606149077 epoch total loss 0.5786044\n",
      "Trained batch 1758 batch loss 0.579658151 epoch total loss 0.578605\n",
      "Trained batch 1759 batch loss 0.576503813 epoch total loss 0.578603804\n",
      "Trained batch 1760 batch loss 0.608591139 epoch total loss 0.578620851\n",
      "Trained batch 1761 batch loss 0.571553826 epoch total loss 0.578616798\n",
      "Trained batch 1762 batch loss 0.511227548 epoch total loss 0.578578591\n",
      "Trained batch 1763 batch loss 0.614213526 epoch total loss 0.578598797\n",
      "Trained batch 1764 batch loss 0.517205834 epoch total loss 0.578564\n",
      "Trained batch 1765 batch loss 0.516034126 epoch total loss 0.578528583\n",
      "Trained batch 1766 batch loss 0.511772752 epoch total loss 0.578490734\n",
      "Trained batch 1767 batch loss 0.474901408 epoch total loss 0.578432143\n",
      "Trained batch 1768 batch loss 0.448079437 epoch total loss 0.578358412\n",
      "Trained batch 1769 batch loss 0.467529088 epoch total loss 0.578295767\n",
      "Trained batch 1770 batch loss 0.486235738 epoch total loss 0.578243732\n",
      "Trained batch 1771 batch loss 0.696009934 epoch total loss 0.578310192\n",
      "Trained batch 1772 batch loss 0.592247963 epoch total loss 0.578318119\n",
      "Trained batch 1773 batch loss 0.64857626 epoch total loss 0.578357697\n",
      "Trained batch 1774 batch loss 0.563986421 epoch total loss 0.57834959\n",
      "Trained batch 1775 batch loss 0.655843556 epoch total loss 0.578393281\n",
      "Trained batch 1776 batch loss 0.706879556 epoch total loss 0.578465641\n",
      "Trained batch 1777 batch loss 0.636048496 epoch total loss 0.578498065\n",
      "Trained batch 1778 batch loss 0.616610527 epoch total loss 0.578519523\n",
      "Trained batch 1779 batch loss 0.662719071 epoch total loss 0.578566849\n",
      "Trained batch 1780 batch loss 0.618160129 epoch total loss 0.578589082\n",
      "Trained batch 1781 batch loss 0.537636757 epoch total loss 0.578566074\n",
      "Trained batch 1782 batch loss 0.580990255 epoch total loss 0.578567386\n",
      "Trained batch 1783 batch loss 0.573088109 epoch total loss 0.578564346\n",
      "Trained batch 1784 batch loss 0.54148078 epoch total loss 0.578543544\n",
      "Trained batch 1785 batch loss 0.614356458 epoch total loss 0.578563631\n",
      "Trained batch 1786 batch loss 0.60745275 epoch total loss 0.578579783\n",
      "Trained batch 1787 batch loss 0.534532785 epoch total loss 0.578555167\n",
      "Trained batch 1788 batch loss 0.562104404 epoch total loss 0.578546\n",
      "Trained batch 1789 batch loss 0.524875402 epoch total loss 0.578516\n",
      "Trained batch 1790 batch loss 0.517101109 epoch total loss 0.578481674\n",
      "Trained batch 1791 batch loss 0.474304616 epoch total loss 0.57842356\n",
      "Trained batch 1792 batch loss 0.591914952 epoch total loss 0.57843107\n",
      "Trained batch 1793 batch loss 0.622803271 epoch total loss 0.578455806\n",
      "Trained batch 1794 batch loss 0.610262692 epoch total loss 0.578473508\n",
      "Trained batch 1795 batch loss 0.647545218 epoch total loss 0.578512\n",
      "Trained batch 1796 batch loss 0.519356906 epoch total loss 0.578479111\n",
      "Trained batch 1797 batch loss 0.573734581 epoch total loss 0.578476489\n",
      "Trained batch 1798 batch loss 0.484589577 epoch total loss 0.578424275\n",
      "Trained batch 1799 batch loss 0.528206527 epoch total loss 0.57839638\n",
      "Trained batch 1800 batch loss 0.639014721 epoch total loss 0.578430057\n",
      "Trained batch 1801 batch loss 0.630768776 epoch total loss 0.578459084\n",
      "Trained batch 1802 batch loss 0.626589179 epoch total loss 0.578485787\n",
      "Trained batch 1803 batch loss 0.603030503 epoch total loss 0.578499377\n",
      "Trained batch 1804 batch loss 0.533030391 epoch total loss 0.578474224\n",
      "Trained batch 1805 batch loss 0.510564804 epoch total loss 0.578436613\n",
      "Trained batch 1806 batch loss 0.4957093 epoch total loss 0.578390837\n",
      "Trained batch 1807 batch loss 0.515791833 epoch total loss 0.578356147\n",
      "Trained batch 1808 batch loss 0.592228532 epoch total loss 0.578363895\n",
      "Trained batch 1809 batch loss 0.632781386 epoch total loss 0.578394\n",
      "Trained batch 1810 batch loss 0.573156953 epoch total loss 0.578391075\n",
      "Trained batch 1811 batch loss 0.515495479 epoch total loss 0.578356326\n",
      "Trained batch 1812 batch loss 0.565731943 epoch total loss 0.578349352\n",
      "Trained batch 1813 batch loss 0.670069873 epoch total loss 0.578399897\n",
      "Trained batch 1814 batch loss 0.568747818 epoch total loss 0.578394592\n",
      "Trained batch 1815 batch loss 0.587541342 epoch total loss 0.578399599\n",
      "Trained batch 1816 batch loss 0.535743296 epoch total loss 0.578376114\n",
      "Trained batch 1817 batch loss 0.443950564 epoch total loss 0.578302145\n",
      "Trained batch 1818 batch loss 0.494020939 epoch total loss 0.578255773\n",
      "Trained batch 1819 batch loss 0.588466346 epoch total loss 0.578261435\n",
      "Trained batch 1820 batch loss 0.668933332 epoch total loss 0.578311265\n",
      "Trained batch 1821 batch loss 0.610411465 epoch total loss 0.578328848\n",
      "Trained batch 1822 batch loss 0.539551 epoch total loss 0.578307569\n",
      "Trained batch 1823 batch loss 0.608402312 epoch total loss 0.57832408\n",
      "Trained batch 1824 batch loss 0.568709612 epoch total loss 0.578318834\n",
      "Trained batch 1825 batch loss 0.593257 epoch total loss 0.578327\n",
      "Trained batch 1826 batch loss 0.59135592 epoch total loss 0.578334093\n",
      "Trained batch 1827 batch loss 0.561377764 epoch total loss 0.578324854\n",
      "Trained batch 1828 batch loss 0.483612031 epoch total loss 0.578273058\n",
      "Trained batch 1829 batch loss 0.502950847 epoch total loss 0.578231871\n",
      "Trained batch 1830 batch loss 0.526025534 epoch total loss 0.578203321\n",
      "Trained batch 1831 batch loss 0.516059399 epoch total loss 0.578169405\n",
      "Trained batch 1832 batch loss 0.514493048 epoch total loss 0.578134656\n",
      "Trained batch 1833 batch loss 0.561713874 epoch total loss 0.578125715\n",
      "Trained batch 1834 batch loss 0.52164197 epoch total loss 0.5780949\n",
      "Trained batch 1835 batch loss 0.54578656 epoch total loss 0.578077316\n",
      "Trained batch 1836 batch loss 0.524105072 epoch total loss 0.578047872\n",
      "Trained batch 1837 batch loss 0.501194239 epoch total loss 0.578006\n",
      "Trained batch 1838 batch loss 0.490707934 epoch total loss 0.577958584\n",
      "Trained batch 1839 batch loss 0.450950056 epoch total loss 0.577889502\n",
      "Trained batch 1840 batch loss 0.606977224 epoch total loss 0.577905297\n",
      "Trained batch 1841 batch loss 0.578949094 epoch total loss 0.577905834\n",
      "Trained batch 1842 batch loss 0.543718398 epoch total loss 0.577887297\n",
      "Trained batch 1843 batch loss 0.576309264 epoch total loss 0.577886403\n",
      "Trained batch 1844 batch loss 0.482306361 epoch total loss 0.577834606\n",
      "Trained batch 1845 batch loss 0.474455029 epoch total loss 0.577778578\n",
      "Trained batch 1846 batch loss 0.482000381 epoch total loss 0.577726722\n",
      "Trained batch 1847 batch loss 0.587843776 epoch total loss 0.577732205\n",
      "Trained batch 1848 batch loss 0.544882774 epoch total loss 0.577714443\n",
      "Trained batch 1849 batch loss 0.578930795 epoch total loss 0.577715158\n",
      "Trained batch 1850 batch loss 0.577633619 epoch total loss 0.577715099\n",
      "Trained batch 1851 batch loss 0.676905394 epoch total loss 0.577768683\n",
      "Trained batch 1852 batch loss 0.586586535 epoch total loss 0.577773392\n",
      "Trained batch 1853 batch loss 0.605569899 epoch total loss 0.577788413\n",
      "Trained batch 1854 batch loss 0.607252181 epoch total loss 0.577804327\n",
      "Trained batch 1855 batch loss 0.636712134 epoch total loss 0.577836096\n",
      "Trained batch 1856 batch loss 0.58512789 epoch total loss 0.57784003\n",
      "Trained batch 1857 batch loss 0.571723521 epoch total loss 0.577836752\n",
      "Trained batch 1858 batch loss 0.539735317 epoch total loss 0.577816248\n",
      "Trained batch 1859 batch loss 0.557032943 epoch total loss 0.577805102\n",
      "Trained batch 1860 batch loss 0.583656609 epoch total loss 0.577808201\n",
      "Trained batch 1861 batch loss 0.625158668 epoch total loss 0.577833652\n",
      "Trained batch 1862 batch loss 0.587326109 epoch total loss 0.577838719\n",
      "Trained batch 1863 batch loss 0.503802598 epoch total loss 0.577798963\n",
      "Trained batch 1864 batch loss 0.536886334 epoch total loss 0.577776968\n",
      "Trained batch 1865 batch loss 0.510883689 epoch total loss 0.577741146\n",
      "Trained batch 1866 batch loss 0.454473644 epoch total loss 0.577675045\n",
      "Trained batch 1867 batch loss 0.506552219 epoch total loss 0.577636957\n",
      "Trained batch 1868 batch loss 0.530124903 epoch total loss 0.577611566\n",
      "Trained batch 1869 batch loss 0.554577708 epoch total loss 0.577599227\n",
      "Trained batch 1870 batch loss 0.612525165 epoch total loss 0.577617943\n",
      "Trained batch 1871 batch loss 0.549998462 epoch total loss 0.577603161\n",
      "Trained batch 1872 batch loss 0.430638403 epoch total loss 0.577524662\n",
      "Trained batch 1873 batch loss 0.543279648 epoch total loss 0.577506423\n",
      "Trained batch 1874 batch loss 0.668442369 epoch total loss 0.577554941\n",
      "Trained batch 1875 batch loss 0.600778162 epoch total loss 0.577567399\n",
      "Trained batch 1876 batch loss 0.62959069 epoch total loss 0.577595115\n",
      "Trained batch 1877 batch loss 0.598845363 epoch total loss 0.577606499\n",
      "Trained batch 1878 batch loss 0.615806878 epoch total loss 0.577626824\n",
      "Trained batch 1879 batch loss 0.540475309 epoch total loss 0.577607095\n",
      "Trained batch 1880 batch loss 0.590905249 epoch total loss 0.577614188\n",
      "Trained batch 1881 batch loss 0.569328964 epoch total loss 0.577609777\n",
      "Trained batch 1882 batch loss 0.685415149 epoch total loss 0.577667058\n",
      "Trained batch 1883 batch loss 0.62352 epoch total loss 0.577691436\n",
      "Trained batch 1884 batch loss 0.660826504 epoch total loss 0.577735543\n",
      "Trained batch 1885 batch loss 0.560661554 epoch total loss 0.577726483\n",
      "Trained batch 1886 batch loss 0.583408594 epoch total loss 0.577729464\n",
      "Trained batch 1887 batch loss 0.683642805 epoch total loss 0.577785552\n",
      "Trained batch 1888 batch loss 0.618301153 epoch total loss 0.577807\n",
      "Trained batch 1889 batch loss 0.606523514 epoch total loss 0.577822268\n",
      "Trained batch 1890 batch loss 0.5447613 epoch total loss 0.577804804\n",
      "Trained batch 1891 batch loss 0.581348121 epoch total loss 0.577806652\n",
      "Trained batch 1892 batch loss 0.509016335 epoch total loss 0.577770293\n",
      "Trained batch 1893 batch loss 0.513776124 epoch total loss 0.577736497\n",
      "Trained batch 1894 batch loss 0.649568856 epoch total loss 0.577774405\n",
      "Trained batch 1895 batch loss 0.564612687 epoch total loss 0.577767432\n",
      "Trained batch 1896 batch loss 0.633717239 epoch total loss 0.577796876\n",
      "Trained batch 1897 batch loss 0.569985926 epoch total loss 0.577792764\n",
      "Trained batch 1898 batch loss 0.618424416 epoch total loss 0.577814162\n",
      "Trained batch 1899 batch loss 0.724420369 epoch total loss 0.57789135\n",
      "Trained batch 1900 batch loss 0.623264432 epoch total loss 0.577915251\n",
      "Trained batch 1901 batch loss 0.572469234 epoch total loss 0.57791239\n",
      "Trained batch 1902 batch loss 0.651802897 epoch total loss 0.577951252\n",
      "Trained batch 1903 batch loss 0.617827117 epoch total loss 0.577972233\n",
      "Trained batch 1904 batch loss 0.524697781 epoch total loss 0.577944219\n",
      "Trained batch 1905 batch loss 0.504906058 epoch total loss 0.577905834\n",
      "Trained batch 1906 batch loss 0.531850874 epoch total loss 0.577881694\n",
      "Trained batch 1907 batch loss 0.574105 epoch total loss 0.577879727\n",
      "Trained batch 1908 batch loss 0.621180952 epoch total loss 0.577902436\n",
      "Trained batch 1909 batch loss 0.594833434 epoch total loss 0.577911317\n",
      "Trained batch 1910 batch loss 0.569115579 epoch total loss 0.577906668\n",
      "Trained batch 1911 batch loss 0.54596 epoch total loss 0.57789\n",
      "Trained batch 1912 batch loss 0.650772035 epoch total loss 0.577928126\n",
      "Trained batch 1913 batch loss 0.599140227 epoch total loss 0.577939212\n",
      "Trained batch 1914 batch loss 0.667943835 epoch total loss 0.57798624\n",
      "Trained batch 1915 batch loss 0.670042813 epoch total loss 0.578034282\n",
      "Trained batch 1916 batch loss 0.615765214 epoch total loss 0.578053951\n",
      "Trained batch 1917 batch loss 0.583491445 epoch total loss 0.578056812\n",
      "Trained batch 1918 batch loss 0.596669376 epoch total loss 0.578066528\n",
      "Trained batch 1919 batch loss 0.643215775 epoch total loss 0.578100443\n",
      "Trained batch 1920 batch loss 0.568895519 epoch total loss 0.578095615\n",
      "Trained batch 1921 batch loss 0.646671951 epoch total loss 0.578131378\n",
      "Trained batch 1922 batch loss 0.607523322 epoch total loss 0.578146636\n",
      "Trained batch 1923 batch loss 0.551394522 epoch total loss 0.578132749\n",
      "Trained batch 1924 batch loss 0.700370789 epoch total loss 0.578196228\n",
      "Trained batch 1925 batch loss 0.654592395 epoch total loss 0.578235924\n",
      "Trained batch 1926 batch loss 0.630429924 epoch total loss 0.578263\n",
      "Trained batch 1927 batch loss 0.601929665 epoch total loss 0.578275263\n",
      "Trained batch 1928 batch loss 0.5746454 epoch total loss 0.578273356\n",
      "Trained batch 1929 batch loss 0.645041108 epoch total loss 0.578307927\n",
      "Trained batch 1930 batch loss 0.564340472 epoch total loss 0.578300714\n",
      "Trained batch 1931 batch loss 0.582278728 epoch total loss 0.578302741\n",
      "Trained batch 1932 batch loss 0.631907403 epoch total loss 0.578330517\n",
      "Trained batch 1933 batch loss 0.692273796 epoch total loss 0.578389466\n",
      "Trained batch 1934 batch loss 0.569525123 epoch total loss 0.578384936\n",
      "Trained batch 1935 batch loss 0.670322895 epoch total loss 0.578432441\n",
      "Trained batch 1936 batch loss 0.605819941 epoch total loss 0.578446567\n",
      "Trained batch 1937 batch loss 0.568850935 epoch total loss 0.57844162\n",
      "Trained batch 1938 batch loss 0.580146849 epoch total loss 0.578442514\n",
      "Trained batch 1939 batch loss 0.597773075 epoch total loss 0.578452468\n",
      "Trained batch 1940 batch loss 0.457108051 epoch total loss 0.578389943\n",
      "Trained batch 1941 batch loss 0.390467525 epoch total loss 0.578293145\n",
      "Trained batch 1942 batch loss 0.474718153 epoch total loss 0.578239858\n",
      "Trained batch 1943 batch loss 0.547842801 epoch total loss 0.578224182\n",
      "Trained batch 1944 batch loss 0.55703634 epoch total loss 0.578213274\n",
      "Trained batch 1945 batch loss 0.529067 epoch total loss 0.578188\n",
      "Trained batch 1946 batch loss 0.532692313 epoch total loss 0.578164637\n",
      "Trained batch 1947 batch loss 0.543215454 epoch total loss 0.578146696\n",
      "Trained batch 1948 batch loss 0.627920628 epoch total loss 0.578172266\n",
      "Trained batch 1949 batch loss 0.663171828 epoch total loss 0.578215897\n",
      "Trained batch 1950 batch loss 0.601452351 epoch total loss 0.578227818\n",
      "Trained batch 1951 batch loss 0.602876484 epoch total loss 0.578240454\n",
      "Trained batch 1952 batch loss 0.526647747 epoch total loss 0.578214\n",
      "Trained batch 1953 batch loss 0.483940303 epoch total loss 0.57816571\n",
      "Trained batch 1954 batch loss 0.445314348 epoch total loss 0.578097701\n",
      "Trained batch 1955 batch loss 0.464127094 epoch total loss 0.578039408\n",
      "Trained batch 1956 batch loss 0.460910201 epoch total loss 0.577979505\n",
      "Trained batch 1957 batch loss 0.561495423 epoch total loss 0.577971101\n",
      "Trained batch 1958 batch loss 0.495948672 epoch total loss 0.577929258\n",
      "Trained batch 1959 batch loss 0.699830651 epoch total loss 0.577991486\n",
      "Trained batch 1960 batch loss 0.686987579 epoch total loss 0.578047097\n",
      "Trained batch 1961 batch loss 0.626163423 epoch total loss 0.578071654\n",
      "Trained batch 1962 batch loss 0.726073503 epoch total loss 0.578147113\n",
      "Trained batch 1963 batch loss 0.80467236 epoch total loss 0.578262508\n",
      "Trained batch 1964 batch loss 0.691497684 epoch total loss 0.578320146\n",
      "Trained batch 1965 batch loss 0.507776082 epoch total loss 0.578284264\n",
      "Trained batch 1966 batch loss 0.626291513 epoch total loss 0.578308702\n",
      "Trained batch 1967 batch loss 0.596562088 epoch total loss 0.578318\n",
      "Trained batch 1968 batch loss 0.552717 epoch total loss 0.578305\n",
      "Trained batch 1969 batch loss 0.507166445 epoch total loss 0.578268886\n",
      "Trained batch 1970 batch loss 0.635499299 epoch total loss 0.578298\n",
      "Trained batch 1971 batch loss 0.585085452 epoch total loss 0.57830137\n",
      "Trained batch 1972 batch loss 0.60924232 epoch total loss 0.578317106\n",
      "Trained batch 1973 batch loss 0.616402745 epoch total loss 0.578336418\n",
      "Trained batch 1974 batch loss 0.600941598 epoch total loss 0.578347862\n",
      "Trained batch 1975 batch loss 0.556538 epoch total loss 0.578336835\n",
      "Trained batch 1976 batch loss 0.565210164 epoch total loss 0.578330159\n",
      "Trained batch 1977 batch loss 0.582956553 epoch total loss 0.578332543\n",
      "Trained batch 1978 batch loss 0.614299178 epoch total loss 0.578350663\n",
      "Trained batch 1979 batch loss 0.581858635 epoch total loss 0.578352511\n",
      "Trained batch 1980 batch loss 0.675407 epoch total loss 0.578401506\n",
      "Trained batch 1981 batch loss 0.557921231 epoch total loss 0.578391135\n",
      "Trained batch 1982 batch loss 0.511202514 epoch total loss 0.578357279\n",
      "Trained batch 1983 batch loss 0.627188563 epoch total loss 0.578381896\n",
      "Trained batch 1984 batch loss 0.525340259 epoch total loss 0.578355193\n",
      "Trained batch 1985 batch loss 0.605357707 epoch total loss 0.578368783\n",
      "Trained batch 1986 batch loss 0.499604404 epoch total loss 0.578329146\n",
      "Trained batch 1987 batch loss 0.531533182 epoch total loss 0.578305542\n",
      "Trained batch 1988 batch loss 0.550086379 epoch total loss 0.578291357\n",
      "Trained batch 1989 batch loss 0.549821794 epoch total loss 0.578277\n",
      "Trained batch 1990 batch loss 0.640411496 epoch total loss 0.578308225\n",
      "Trained batch 1991 batch loss 0.61921674 epoch total loss 0.578328788\n",
      "Trained batch 1992 batch loss 0.5703 epoch total loss 0.578324795\n",
      "Trained batch 1993 batch loss 0.523902774 epoch total loss 0.578297496\n",
      "Trained batch 1994 batch loss 0.464032352 epoch total loss 0.578240156\n",
      "Trained batch 1995 batch loss 0.477323622 epoch total loss 0.578189552\n",
      "Trained batch 1996 batch loss 0.476504534 epoch total loss 0.578138649\n",
      "Trained batch 1997 batch loss 0.490904331 epoch total loss 0.5780949\n",
      "Trained batch 1998 batch loss 0.443703949 epoch total loss 0.578027666\n",
      "Trained batch 1999 batch loss 0.455293477 epoch total loss 0.577966273\n",
      "Trained batch 2000 batch loss 0.497053623 epoch total loss 0.577925861\n",
      "Trained batch 2001 batch loss 0.539670587 epoch total loss 0.577906728\n",
      "Trained batch 2002 batch loss 0.566548526 epoch total loss 0.577901065\n",
      "Trained batch 2003 batch loss 0.448427022 epoch total loss 0.577836454\n",
      "Trained batch 2004 batch loss 0.539319515 epoch total loss 0.577817202\n",
      "Trained batch 2005 batch loss 0.582065821 epoch total loss 0.577819288\n",
      "Trained batch 2006 batch loss 0.554043233 epoch total loss 0.577807486\n",
      "Trained batch 2007 batch loss 0.509237945 epoch total loss 0.577773333\n",
      "Trained batch 2008 batch loss 0.464520037 epoch total loss 0.577716887\n",
      "Trained batch 2009 batch loss 0.40968129 epoch total loss 0.577633262\n",
      "Trained batch 2010 batch loss 0.571916461 epoch total loss 0.577630401\n",
      "Trained batch 2011 batch loss 0.515449166 epoch total loss 0.577599525\n",
      "Trained batch 2012 batch loss 0.661583245 epoch total loss 0.577641249\n",
      "Trained batch 2013 batch loss 0.65196 epoch total loss 0.577678204\n",
      "Trained batch 2014 batch loss 0.507855475 epoch total loss 0.577643514\n",
      "Trained batch 2015 batch loss 0.490562201 epoch total loss 0.5776003\n",
      "Trained batch 2016 batch loss 0.63914 epoch total loss 0.577630818\n",
      "Trained batch 2017 batch loss 0.612613499 epoch total loss 0.577648222\n",
      "Trained batch 2018 batch loss 0.463386297 epoch total loss 0.577591598\n",
      "Trained batch 2019 batch loss 0.507377386 epoch total loss 0.577556789\n",
      "Trained batch 2020 batch loss 0.509640396 epoch total loss 0.577523172\n",
      "Trained batch 2021 batch loss 0.49366951 epoch total loss 0.577481687\n",
      "Trained batch 2022 batch loss 0.525807679 epoch total loss 0.577456117\n",
      "Trained batch 2023 batch loss 0.562473476 epoch total loss 0.577448666\n",
      "Trained batch 2024 batch loss 0.556869447 epoch total loss 0.577438533\n",
      "Trained batch 2025 batch loss 0.557357252 epoch total loss 0.577428639\n",
      "Trained batch 2026 batch loss 0.548987865 epoch total loss 0.577414572\n",
      "Trained batch 2027 batch loss 0.633721054 epoch total loss 0.577442348\n",
      "Trained batch 2028 batch loss 0.608638585 epoch total loss 0.577457726\n",
      "Trained batch 2029 batch loss 0.65089 epoch total loss 0.577493906\n",
      "Trained batch 2030 batch loss 0.715217769 epoch total loss 0.577561736\n",
      "Trained batch 2031 batch loss 0.686320961 epoch total loss 0.577615261\n",
      "Trained batch 2032 batch loss 0.59516567 epoch total loss 0.577623904\n",
      "Trained batch 2033 batch loss 0.583417237 epoch total loss 0.577626765\n",
      "Trained batch 2034 batch loss 0.548620224 epoch total loss 0.57761246\n",
      "Trained batch 2035 batch loss 0.527743101 epoch total loss 0.577587962\n",
      "Trained batch 2036 batch loss 0.569981933 epoch total loss 0.577584207\n",
      "Trained batch 2037 batch loss 0.573578298 epoch total loss 0.57758224\n",
      "Trained batch 2038 batch loss 0.530476451 epoch total loss 0.577559173\n",
      "Trained batch 2039 batch loss 0.631082833 epoch total loss 0.577585399\n",
      "Trained batch 2040 batch loss 0.540889621 epoch total loss 0.577567399\n",
      "Trained batch 2041 batch loss 0.548687339 epoch total loss 0.577553272\n",
      "Trained batch 2042 batch loss 0.529487 epoch total loss 0.577529788\n",
      "Trained batch 2043 batch loss 0.669932127 epoch total loss 0.577574968\n",
      "Trained batch 2044 batch loss 0.651475191 epoch total loss 0.577611148\n",
      "Trained batch 2045 batch loss 0.559971631 epoch total loss 0.577602506\n",
      "Trained batch 2046 batch loss 0.556575835 epoch total loss 0.577592194\n",
      "Trained batch 2047 batch loss 0.703692377 epoch total loss 0.577653825\n",
      "Trained batch 2048 batch loss 0.662021101 epoch total loss 0.577695\n",
      "Trained batch 2049 batch loss 0.705851614 epoch total loss 0.577757537\n",
      "Trained batch 2050 batch loss 0.662788808 epoch total loss 0.577799\n",
      "Trained batch 2051 batch loss 0.715662897 epoch total loss 0.577866256\n",
      "Trained batch 2052 batch loss 0.795855403 epoch total loss 0.577972531\n",
      "Trained batch 2053 batch loss 0.736401856 epoch total loss 0.578049719\n",
      "Trained batch 2054 batch loss 0.596827626 epoch total loss 0.578058839\n",
      "Trained batch 2055 batch loss 0.530367613 epoch total loss 0.578035653\n",
      "Trained batch 2056 batch loss 0.459142506 epoch total loss 0.577977836\n",
      "Trained batch 2057 batch loss 0.498456657 epoch total loss 0.577939153\n",
      "Trained batch 2058 batch loss 0.559902906 epoch total loss 0.577930391\n",
      "Trained batch 2059 batch loss 0.608008444 epoch total loss 0.577945\n",
      "Trained batch 2060 batch loss 0.559818387 epoch total loss 0.577936232\n",
      "Trained batch 2061 batch loss 0.496345431 epoch total loss 0.577896595\n",
      "Trained batch 2062 batch loss 0.553499043 epoch total loss 0.577884793\n",
      "Trained batch 2063 batch loss 0.480235904 epoch total loss 0.577837408\n",
      "Trained batch 2064 batch loss 0.472480088 epoch total loss 0.577786386\n",
      "Trained batch 2065 batch loss 0.54679209 epoch total loss 0.577771366\n",
      "Trained batch 2066 batch loss 0.456205904 epoch total loss 0.577712536\n",
      "Trained batch 2067 batch loss 0.516478181 epoch total loss 0.577682912\n",
      "Trained batch 2068 batch loss 0.520310163 epoch total loss 0.577655137\n",
      "Trained batch 2069 batch loss 0.525045931 epoch total loss 0.577629685\n",
      "Trained batch 2070 batch loss 0.457663864 epoch total loss 0.57757175\n",
      "Trained batch 2071 batch loss 0.452970266 epoch total loss 0.577511609\n",
      "Trained batch 2072 batch loss 0.475062609 epoch total loss 0.577462137\n",
      "Trained batch 2073 batch loss 0.486130863 epoch total loss 0.577418089\n",
      "Trained batch 2074 batch loss 0.453609616 epoch total loss 0.577358365\n",
      "Trained batch 2075 batch loss 0.595889628 epoch total loss 0.577367365\n",
      "Trained batch 2076 batch loss 0.586754203 epoch total loss 0.577371895\n",
      "Trained batch 2077 batch loss 0.535078406 epoch total loss 0.577351511\n",
      "Trained batch 2078 batch loss 0.514465213 epoch total loss 0.577321231\n",
      "Trained batch 2079 batch loss 0.461034715 epoch total loss 0.577265263\n",
      "Trained batch 2080 batch loss 0.480144918 epoch total loss 0.577218592\n",
      "Trained batch 2081 batch loss 0.490591109 epoch total loss 0.577176929\n",
      "Trained batch 2082 batch loss 0.511246324 epoch total loss 0.577145278\n",
      "Trained batch 2083 batch loss 0.618667185 epoch total loss 0.577165186\n",
      "Trained batch 2084 batch loss 0.603564739 epoch total loss 0.577177823\n",
      "Trained batch 2085 batch loss 0.593522906 epoch total loss 0.57718569\n",
      "Trained batch 2086 batch loss 0.597765267 epoch total loss 0.577195525\n",
      "Trained batch 2087 batch loss 0.574039936 epoch total loss 0.577194035\n",
      "Trained batch 2088 batch loss 0.667058 epoch total loss 0.577237129\n",
      "Trained batch 2089 batch loss 0.611268 epoch total loss 0.577253461\n",
      "Trained batch 2090 batch loss 0.58880347 epoch total loss 0.577258945\n",
      "Trained batch 2091 batch loss 0.551599681 epoch total loss 0.577246666\n",
      "Trained batch 2092 batch loss 0.587742746 epoch total loss 0.577251732\n",
      "Trained batch 2093 batch loss 0.623946249 epoch total loss 0.577274\n",
      "Trained batch 2094 batch loss 0.584708571 epoch total loss 0.577277541\n",
      "Trained batch 2095 batch loss 0.551401 epoch total loss 0.577265203\n",
      "Trained batch 2096 batch loss 0.584239841 epoch total loss 0.577268541\n",
      "Trained batch 2097 batch loss 0.523658 epoch total loss 0.577243\n",
      "Trained batch 2098 batch loss 0.582209527 epoch total loss 0.577245295\n",
      "Trained batch 2099 batch loss 0.493966788 epoch total loss 0.577205658\n",
      "Trained batch 2100 batch loss 0.51802212 epoch total loss 0.577177525\n",
      "Trained batch 2101 batch loss 0.574171901 epoch total loss 0.577176094\n",
      "Trained batch 2102 batch loss 0.537130356 epoch total loss 0.577157\n",
      "Trained batch 2103 batch loss 0.640074 epoch total loss 0.577186942\n",
      "Trained batch 2104 batch loss 0.572279572 epoch total loss 0.577184558\n",
      "Trained batch 2105 batch loss 0.596994102 epoch total loss 0.577194035\n",
      "Trained batch 2106 batch loss 0.696618676 epoch total loss 0.577250719\n",
      "Trained batch 2107 batch loss 0.613866091 epoch total loss 0.577268124\n",
      "Trained batch 2108 batch loss 0.581262708 epoch total loss 0.577270031\n",
      "Trained batch 2109 batch loss 0.6234 epoch total loss 0.577291906\n",
      "Trained batch 2110 batch loss 0.529219508 epoch total loss 0.577269137\n",
      "Trained batch 2111 batch loss 0.580426812 epoch total loss 0.577270627\n",
      "Trained batch 2112 batch loss 0.60745573 epoch total loss 0.577284873\n",
      "Trained batch 2113 batch loss 0.597086191 epoch total loss 0.57729423\n",
      "Trained batch 2114 batch loss 0.529558539 epoch total loss 0.57727164\n",
      "Trained batch 2115 batch loss 0.602904916 epoch total loss 0.5772838\n",
      "Trained batch 2116 batch loss 0.500886083 epoch total loss 0.577247679\n",
      "Trained batch 2117 batch loss 0.477317154 epoch total loss 0.577200472\n",
      "Trained batch 2118 batch loss 0.473960191 epoch total loss 0.577151716\n",
      "Trained batch 2119 batch loss 0.594229817 epoch total loss 0.577159762\n",
      "Trained batch 2120 batch loss 0.602430582 epoch total loss 0.577171683\n",
      "Trained batch 2121 batch loss 0.634822667 epoch total loss 0.577198863\n",
      "Trained batch 2122 batch loss 0.641924262 epoch total loss 0.577229381\n",
      "Trained batch 2123 batch loss 0.61942 epoch total loss 0.577249229\n",
      "Trained batch 2124 batch loss 0.626943529 epoch total loss 0.577272654\n",
      "Trained batch 2125 batch loss 0.554567933 epoch total loss 0.577261925\n",
      "Trained batch 2126 batch loss 0.537615538 epoch total loss 0.577243268\n",
      "Trained batch 2127 batch loss 0.516307831 epoch total loss 0.577214658\n",
      "Trained batch 2128 batch loss 0.563090086 epoch total loss 0.577208042\n",
      "Trained batch 2129 batch loss 0.52951 epoch total loss 0.577185631\n",
      "Trained batch 2130 batch loss 0.488629192 epoch total loss 0.577144086\n",
      "Trained batch 2131 batch loss 0.525727332 epoch total loss 0.577119946\n",
      "Trained batch 2132 batch loss 0.539382517 epoch total loss 0.577102304\n",
      "Trained batch 2133 batch loss 0.523978233 epoch total loss 0.577077329\n",
      "Trained batch 2134 batch loss 0.589527249 epoch total loss 0.57708317\n",
      "Trained batch 2135 batch loss 0.47840935 epoch total loss 0.577036917\n",
      "Trained batch 2136 batch loss 0.538139164 epoch total loss 0.577018738\n",
      "Trained batch 2137 batch loss 0.641478419 epoch total loss 0.577048898\n",
      "Trained batch 2138 batch loss 0.560348511 epoch total loss 0.57704103\n",
      "Trained batch 2139 batch loss 0.593206525 epoch total loss 0.5770486\n",
      "Trained batch 2140 batch loss 0.694015 epoch total loss 0.577103257\n",
      "Trained batch 2141 batch loss 0.598869205 epoch total loss 0.57711345\n",
      "Trained batch 2142 batch loss 0.677990377 epoch total loss 0.577160537\n",
      "Trained batch 2143 batch loss 0.64825511 epoch total loss 0.577193737\n",
      "Trained batch 2144 batch loss 0.580596745 epoch total loss 0.577195287\n",
      "Trained batch 2145 batch loss 0.590454042 epoch total loss 0.577201486\n",
      "Trained batch 2146 batch loss 0.581124187 epoch total loss 0.577203333\n",
      "Trained batch 2147 batch loss 0.535831451 epoch total loss 0.577184081\n",
      "Trained batch 2148 batch loss 0.576628327 epoch total loss 0.577183843\n",
      "Trained batch 2149 batch loss 0.6086483 epoch total loss 0.577198505\n",
      "Trained batch 2150 batch loss 0.46399045 epoch total loss 0.577145815\n",
      "Trained batch 2151 batch loss 0.455810934 epoch total loss 0.577089429\n",
      "Trained batch 2152 batch loss 0.485022038 epoch total loss 0.577046633\n",
      "Trained batch 2153 batch loss 0.580542207 epoch total loss 0.577048242\n",
      "Trained batch 2154 batch loss 0.567598164 epoch total loss 0.577043891\n",
      "Trained batch 2155 batch loss 0.57408613 epoch total loss 0.57704252\n",
      "Trained batch 2156 batch loss 0.673233032 epoch total loss 0.577087104\n",
      "Trained batch 2157 batch loss 0.650706112 epoch total loss 0.577121258\n",
      "Trained batch 2158 batch loss 0.605255365 epoch total loss 0.577134311\n",
      "Trained batch 2159 batch loss 0.592839777 epoch total loss 0.577141583\n",
      "Trained batch 2160 batch loss 0.593967617 epoch total loss 0.577149391\n",
      "Trained batch 2161 batch loss 0.531396 epoch total loss 0.577128232\n",
      "Trained batch 2162 batch loss 0.566921473 epoch total loss 0.577123463\n",
      "Trained batch 2163 batch loss 0.550797164 epoch total loss 0.577111304\n",
      "Trained batch 2164 batch loss 0.550958872 epoch total loss 0.577099204\n",
      "Trained batch 2165 batch loss 0.623225808 epoch total loss 0.577120483\n",
      "Trained batch 2166 batch loss 0.625530481 epoch total loss 0.577142775\n",
      "Trained batch 2167 batch loss 0.539818287 epoch total loss 0.577125549\n",
      "Trained batch 2168 batch loss 0.387304246 epoch total loss 0.57703805\n",
      "Trained batch 2169 batch loss 0.499927938 epoch total loss 0.577002466\n",
      "Trained batch 2170 batch loss 0.566257894 epoch total loss 0.576997519\n",
      "Trained batch 2171 batch loss 0.580968499 epoch total loss 0.576999307\n",
      "Trained batch 2172 batch loss 0.62609154 epoch total loss 0.577021956\n",
      "Trained batch 2173 batch loss 0.620483518 epoch total loss 0.577041924\n",
      "Trained batch 2174 batch loss 0.646117 epoch total loss 0.577073693\n",
      "Trained batch 2175 batch loss 0.601502836 epoch total loss 0.577084959\n",
      "Trained batch 2176 batch loss 0.513543129 epoch total loss 0.577055752\n",
      "Trained batch 2177 batch loss 0.571672559 epoch total loss 0.577053308\n",
      "Trained batch 2178 batch loss 0.584073663 epoch total loss 0.577056527\n",
      "Trained batch 2179 batch loss 0.560255527 epoch total loss 0.577048838\n",
      "Trained batch 2180 batch loss 0.555405 epoch total loss 0.577038884\n",
      "Trained batch 2181 batch loss 0.548600674 epoch total loss 0.57702589\n",
      "Trained batch 2182 batch loss 0.499943733 epoch total loss 0.576990545\n",
      "Trained batch 2183 batch loss 0.491468668 epoch total loss 0.576951385\n",
      "Trained batch 2184 batch loss 0.567875862 epoch total loss 0.576947212\n",
      "Trained batch 2185 batch loss 0.593540668 epoch total loss 0.576954782\n",
      "Trained batch 2186 batch loss 0.592817366 epoch total loss 0.576962054\n",
      "Trained batch 2187 batch loss 0.530629277 epoch total loss 0.576940835\n",
      "Trained batch 2188 batch loss 0.533683 epoch total loss 0.576921105\n",
      "Trained batch 2189 batch loss 0.51007843 epoch total loss 0.576890588\n",
      "Trained batch 2190 batch loss 0.550351 epoch total loss 0.576878428\n",
      "Trained batch 2191 batch loss 0.617545247 epoch total loss 0.576897\n",
      "Trained batch 2192 batch loss 0.532805085 epoch total loss 0.576876879\n",
      "Trained batch 2193 batch loss 0.54074645 epoch total loss 0.576860428\n",
      "Trained batch 2194 batch loss 0.505864501 epoch total loss 0.576828063\n",
      "Trained batch 2195 batch loss 0.543535233 epoch total loss 0.576812923\n",
      "Trained batch 2196 batch loss 0.523090184 epoch total loss 0.576788425\n",
      "Trained batch 2197 batch loss 0.594029903 epoch total loss 0.576796293\n",
      "Trained batch 2198 batch loss 0.53716 epoch total loss 0.576778233\n",
      "Trained batch 2199 batch loss 0.571816266 epoch total loss 0.576775968\n",
      "Trained batch 2200 batch loss 0.530430913 epoch total loss 0.576754868\n",
      "Trained batch 2201 batch loss 0.626371741 epoch total loss 0.576777399\n",
      "Trained batch 2202 batch loss 0.573726237 epoch total loss 0.576776\n",
      "Trained batch 2203 batch loss 0.486272812 epoch total loss 0.57673496\n",
      "Trained batch 2204 batch loss 0.524344 epoch total loss 0.576711178\n",
      "Trained batch 2205 batch loss 0.511987865 epoch total loss 0.576681793\n",
      "Trained batch 2206 batch loss 0.619619727 epoch total loss 0.576701283\n",
      "Trained batch 2207 batch loss 0.57092607 epoch total loss 0.576698661\n",
      "Trained batch 2208 batch loss 0.466341227 epoch total loss 0.576648653\n",
      "Trained batch 2209 batch loss 0.58473736 epoch total loss 0.576652288\n",
      "Trained batch 2210 batch loss 0.632730305 epoch total loss 0.57667768\n",
      "Trained batch 2211 batch loss 0.559792757 epoch total loss 0.576670051\n",
      "Trained batch 2212 batch loss 0.693574369 epoch total loss 0.57672292\n",
      "Trained batch 2213 batch loss 0.691058934 epoch total loss 0.576774538\n",
      "Trained batch 2214 batch loss 0.58939451 epoch total loss 0.57678026\n",
      "Trained batch 2215 batch loss 0.585744202 epoch total loss 0.576784253\n",
      "Trained batch 2216 batch loss 0.606333196 epoch total loss 0.576797605\n",
      "Trained batch 2217 batch loss 0.596800685 epoch total loss 0.576806605\n",
      "Trained batch 2218 batch loss 0.549895167 epoch total loss 0.576794505\n",
      "Trained batch 2219 batch loss 0.585415363 epoch total loss 0.576798379\n",
      "Trained batch 2220 batch loss 0.54011482 epoch total loss 0.576781869\n",
      "Trained batch 2221 batch loss 0.728535473 epoch total loss 0.576850235\n",
      "Trained batch 2222 batch loss 0.584970355 epoch total loss 0.576853871\n",
      "Trained batch 2223 batch loss 0.602710068 epoch total loss 0.576865494\n",
      "Trained batch 2224 batch loss 0.541340888 epoch total loss 0.57684952\n",
      "Trained batch 2225 batch loss 0.631871164 epoch total loss 0.576874256\n",
      "Trained batch 2226 batch loss 0.644409359 epoch total loss 0.576904595\n",
      "Trained batch 2227 batch loss 0.529994309 epoch total loss 0.576883495\n",
      "Trained batch 2228 batch loss 0.651595652 epoch total loss 0.576917052\n",
      "Trained batch 2229 batch loss 0.658977091 epoch total loss 0.576953888\n",
      "Trained batch 2230 batch loss 0.596048892 epoch total loss 0.576962411\n",
      "Trained batch 2231 batch loss 0.549826086 epoch total loss 0.576950252\n",
      "Trained batch 2232 batch loss 0.578431904 epoch total loss 0.576950967\n",
      "Trained batch 2233 batch loss 0.54818356 epoch total loss 0.576938093\n",
      "Trained batch 2234 batch loss 0.624362 epoch total loss 0.576959312\n",
      "Trained batch 2235 batch loss 0.621128321 epoch total loss 0.576979041\n",
      "Trained batch 2236 batch loss 0.631702185 epoch total loss 0.577003539\n",
      "Trained batch 2237 batch loss 0.571379244 epoch total loss 0.577001035\n",
      "Trained batch 2238 batch loss 0.630701184 epoch total loss 0.577025056\n",
      "Trained batch 2239 batch loss 0.555940509 epoch total loss 0.577015638\n",
      "Trained batch 2240 batch loss 0.493619829 epoch total loss 0.576978385\n",
      "Trained batch 2241 batch loss 0.593348682 epoch total loss 0.576985717\n",
      "Trained batch 2242 batch loss 0.530222535 epoch total loss 0.576964915\n",
      "Trained batch 2243 batch loss 0.510638416 epoch total loss 0.576935291\n",
      "Trained batch 2244 batch loss 0.471883029 epoch total loss 0.576888502\n",
      "Trained batch 2245 batch loss 0.435477585 epoch total loss 0.5768255\n",
      "Trained batch 2246 batch loss 0.483159065 epoch total loss 0.576783776\n",
      "Trained batch 2247 batch loss 0.510225058 epoch total loss 0.576754212\n",
      "Trained batch 2248 batch loss 0.519703209 epoch total loss 0.576728821\n",
      "Trained batch 2249 batch loss 0.534829557 epoch total loss 0.576710165\n",
      "Trained batch 2250 batch loss 0.577239633 epoch total loss 0.576710403\n",
      "Trained batch 2251 batch loss 0.555244684 epoch total loss 0.576700866\n",
      "Trained batch 2252 batch loss 0.542288542 epoch total loss 0.576685607\n",
      "Trained batch 2253 batch loss 0.47765 epoch total loss 0.576641619\n",
      "Trained batch 2254 batch loss 0.461718321 epoch total loss 0.576590598\n",
      "Trained batch 2255 batch loss 0.525986552 epoch total loss 0.576568186\n",
      "Trained batch 2256 batch loss 0.492976695 epoch total loss 0.576531112\n",
      "Trained batch 2257 batch loss 0.469707131 epoch total loss 0.576483786\n",
      "Trained batch 2258 batch loss 0.503169119 epoch total loss 0.576451302\n",
      "Trained batch 2259 batch loss 0.519929767 epoch total loss 0.576426268\n",
      "Trained batch 2260 batch loss 0.445916176 epoch total loss 0.576368511\n",
      "Trained batch 2261 batch loss 0.517989516 epoch total loss 0.576342702\n",
      "Trained batch 2262 batch loss 0.487788558 epoch total loss 0.576303542\n",
      "Trained batch 2263 batch loss 0.583398 epoch total loss 0.576306701\n",
      "Trained batch 2264 batch loss 0.57060051 epoch total loss 0.576304138\n",
      "Trained batch 2265 batch loss 0.574101627 epoch total loss 0.576303184\n",
      "Trained batch 2266 batch loss 0.601616 epoch total loss 0.57631433\n",
      "Trained batch 2267 batch loss 0.586228 epoch total loss 0.576318681\n",
      "Trained batch 2268 batch loss 0.562085211 epoch total loss 0.576312423\n",
      "Trained batch 2269 batch loss 0.676654 epoch total loss 0.576356649\n",
      "Trained batch 2270 batch loss 0.584804833 epoch total loss 0.576360345\n",
      "Trained batch 2271 batch loss 0.574182928 epoch total loss 0.576359391\n",
      "Trained batch 2272 batch loss 0.67532444 epoch total loss 0.576402962\n",
      "Trained batch 2273 batch loss 0.604266644 epoch total loss 0.576415181\n",
      "Trained batch 2274 batch loss 0.512787342 epoch total loss 0.576387227\n",
      "Trained batch 2275 batch loss 0.603639543 epoch total loss 0.576399207\n",
      "Trained batch 2276 batch loss 0.605618119 epoch total loss 0.576412\n",
      "Trained batch 2277 batch loss 0.605226517 epoch total loss 0.576424718\n",
      "Trained batch 2278 batch loss 0.623429537 epoch total loss 0.576445341\n",
      "Trained batch 2279 batch loss 0.500628054 epoch total loss 0.576412082\n",
      "Trained batch 2280 batch loss 0.570687532 epoch total loss 0.576409519\n",
      "Trained batch 2281 batch loss 0.585498631 epoch total loss 0.576413512\n",
      "Trained batch 2282 batch loss 0.673587799 epoch total loss 0.57645607\n",
      "Trained batch 2283 batch loss 0.586931944 epoch total loss 0.57646066\n",
      "Trained batch 2284 batch loss 0.644246697 epoch total loss 0.576490343\n",
      "Trained batch 2285 batch loss 0.615630269 epoch total loss 0.576507449\n",
      "Trained batch 2286 batch loss 0.673083305 epoch total loss 0.576549709\n",
      "Trained batch 2287 batch loss 0.570683539 epoch total loss 0.576547146\n",
      "Trained batch 2288 batch loss 0.507785738 epoch total loss 0.576517105\n",
      "Trained batch 2289 batch loss 0.588551044 epoch total loss 0.57652235\n",
      "Trained batch 2290 batch loss 0.626984477 epoch total loss 0.576544404\n",
      "Trained batch 2291 batch loss 0.661941528 epoch total loss 0.576581657\n",
      "Trained batch 2292 batch loss 0.637355804 epoch total loss 0.576608181\n",
      "Trained batch 2293 batch loss 0.670084178 epoch total loss 0.576648951\n",
      "Trained batch 2294 batch loss 0.690516949 epoch total loss 0.576698601\n",
      "Trained batch 2295 batch loss 0.681888044 epoch total loss 0.576744437\n",
      "Trained batch 2296 batch loss 0.54128319 epoch total loss 0.57672894\n",
      "Trained batch 2297 batch loss 0.584693432 epoch total loss 0.576732457\n",
      "Trained batch 2298 batch loss 0.528334379 epoch total loss 0.576711357\n",
      "Trained batch 2299 batch loss 0.612095475 epoch total loss 0.576726735\n",
      "Trained batch 2300 batch loss 0.619683921 epoch total loss 0.576745391\n",
      "Trained batch 2301 batch loss 0.62770611 epoch total loss 0.576767504\n",
      "Trained batch 2302 batch loss 0.579116821 epoch total loss 0.576768517\n",
      "Trained batch 2303 batch loss 0.631581247 epoch total loss 0.576792359\n",
      "Trained batch 2304 batch loss 0.575123 epoch total loss 0.576791584\n",
      "Trained batch 2305 batch loss 0.549832821 epoch total loss 0.576779902\n",
      "Trained batch 2306 batch loss 0.362964153 epoch total loss 0.576687157\n",
      "Trained batch 2307 batch loss 0.457873195 epoch total loss 0.576635659\n",
      "Trained batch 2308 batch loss 0.597719431 epoch total loss 0.576644838\n",
      "Trained batch 2309 batch loss 0.525478661 epoch total loss 0.576622665\n",
      "Trained batch 2310 batch loss 0.46106413 epoch total loss 0.576572657\n",
      "Trained batch 2311 batch loss 0.436611772 epoch total loss 0.576512098\n",
      "Trained batch 2312 batch loss 0.550325513 epoch total loss 0.576500773\n",
      "Trained batch 2313 batch loss 0.511712909 epoch total loss 0.576472759\n",
      "Trained batch 2314 batch loss 0.494921267 epoch total loss 0.576437473\n",
      "Trained batch 2315 batch loss 0.541224837 epoch total loss 0.576422274\n",
      "Trained batch 2316 batch loss 0.59327054 epoch total loss 0.576429546\n",
      "Trained batch 2317 batch loss 0.650827229 epoch total loss 0.576461673\n",
      "Trained batch 2318 batch loss 0.570389807 epoch total loss 0.57645911\n",
      "Trained batch 2319 batch loss 0.493747741 epoch total loss 0.576423466\n",
      "Trained batch 2320 batch loss 0.494195849 epoch total loss 0.576387942\n",
      "Trained batch 2321 batch loss 0.566682816 epoch total loss 0.57638377\n",
      "Trained batch 2322 batch loss 0.610737443 epoch total loss 0.576398551\n",
      "Trained batch 2323 batch loss 0.606495082 epoch total loss 0.576411486\n",
      "Trained batch 2324 batch loss 0.57151705 epoch total loss 0.5764094\n",
      "Trained batch 2325 batch loss 0.623488665 epoch total loss 0.576429665\n",
      "Trained batch 2326 batch loss 0.662866712 epoch total loss 0.576466799\n",
      "Trained batch 2327 batch loss 0.74644804 epoch total loss 0.576539874\n",
      "Trained batch 2328 batch loss 0.67885232 epoch total loss 0.576583803\n",
      "Trained batch 2329 batch loss 0.683024228 epoch total loss 0.576629519\n",
      "Trained batch 2330 batch loss 0.704140127 epoch total loss 0.576684177\n",
      "Trained batch 2331 batch loss 0.647665858 epoch total loss 0.576714694\n",
      "Trained batch 2332 batch loss 0.578065872 epoch total loss 0.576715291\n",
      "Trained batch 2333 batch loss 0.534939706 epoch total loss 0.57669735\n",
      "Trained batch 2334 batch loss 0.512203217 epoch total loss 0.576669753\n",
      "Trained batch 2335 batch loss 0.572858036 epoch total loss 0.576668084\n",
      "Trained batch 2336 batch loss 0.582497716 epoch total loss 0.576670587\n",
      "Trained batch 2337 batch loss 0.628742695 epoch total loss 0.576692879\n",
      "Trained batch 2338 batch loss 0.630241513 epoch total loss 0.576715827\n",
      "Trained batch 2339 batch loss 0.6700809 epoch total loss 0.576755702\n",
      "Trained batch 2340 batch loss 0.667252779 epoch total loss 0.576794386\n",
      "Trained batch 2341 batch loss 0.631597042 epoch total loss 0.576817811\n",
      "Trained batch 2342 batch loss 0.667362034 epoch total loss 0.576856434\n",
      "Trained batch 2343 batch loss 0.657821953 epoch total loss 0.576891\n",
      "Trained batch 2344 batch loss 0.647078753 epoch total loss 0.576921\n",
      "Trained batch 2345 batch loss 0.657784343 epoch total loss 0.576955497\n",
      "Trained batch 2346 batch loss 0.647875547 epoch total loss 0.576985657\n",
      "Trained batch 2347 batch loss 0.587101579 epoch total loss 0.57699\n",
      "Trained batch 2348 batch loss 0.629357398 epoch total loss 0.5770123\n",
      "Trained batch 2349 batch loss 0.538489044 epoch total loss 0.576995909\n",
      "Trained batch 2350 batch loss 0.537320793 epoch total loss 0.576979041\n",
      "Trained batch 2351 batch loss 0.567470431 epoch total loss 0.576975\n",
      "Trained batch 2352 batch loss 0.599921882 epoch total loss 0.576984823\n",
      "Trained batch 2353 batch loss 0.599383414 epoch total loss 0.5769943\n",
      "Trained batch 2354 batch loss 0.606654406 epoch total loss 0.577006936\n",
      "Trained batch 2355 batch loss 0.631731153 epoch total loss 0.577030122\n",
      "Trained batch 2356 batch loss 0.550407469 epoch total loss 0.577018857\n",
      "Trained batch 2357 batch loss 0.536171257 epoch total loss 0.577001512\n",
      "Trained batch 2358 batch loss 0.519474864 epoch total loss 0.576977134\n",
      "Trained batch 2359 batch loss 0.475444674 epoch total loss 0.576934099\n",
      "Trained batch 2360 batch loss 0.480154604 epoch total loss 0.576893091\n",
      "Trained batch 2361 batch loss 0.458817035 epoch total loss 0.576843083\n",
      "Trained batch 2362 batch loss 0.560065627 epoch total loss 0.576836\n",
      "Trained batch 2363 batch loss 0.536878884 epoch total loss 0.576819062\n",
      "Trained batch 2364 batch loss 0.546697915 epoch total loss 0.576806366\n",
      "Trained batch 2365 batch loss 0.556465507 epoch total loss 0.576797783\n",
      "Trained batch 2366 batch loss 0.488282621 epoch total loss 0.576760352\n",
      "Trained batch 2367 batch loss 0.455284178 epoch total loss 0.576709032\n",
      "Trained batch 2368 batch loss 0.501650155 epoch total loss 0.576677382\n",
      "Trained batch 2369 batch loss 0.61476028 epoch total loss 0.576693416\n",
      "Trained batch 2370 batch loss 0.60953325 epoch total loss 0.576707304\n",
      "Trained batch 2371 batch loss 0.525141895 epoch total loss 0.576685548\n",
      "Trained batch 2372 batch loss 0.562240481 epoch total loss 0.576679468\n",
      "Trained batch 2373 batch loss 0.592936039 epoch total loss 0.576686263\n",
      "Trained batch 2374 batch loss 0.664432108 epoch total loss 0.576723278\n",
      "Trained batch 2375 batch loss 0.545637 epoch total loss 0.576710165\n",
      "Trained batch 2376 batch loss 0.481730521 epoch total loss 0.57667017\n",
      "Trained batch 2377 batch loss 0.543989122 epoch total loss 0.576656401\n",
      "Trained batch 2378 batch loss 0.472327143 epoch total loss 0.576612532\n",
      "Trained batch 2379 batch loss 0.595903456 epoch total loss 0.576620638\n",
      "Trained batch 2380 batch loss 0.640058458 epoch total loss 0.576647282\n",
      "Trained batch 2381 batch loss 0.672076166 epoch total loss 0.576687396\n",
      "Trained batch 2382 batch loss 0.688238323 epoch total loss 0.576734185\n",
      "Trained batch 2383 batch loss 0.548573434 epoch total loss 0.576722383\n",
      "Trained batch 2384 batch loss 0.594881356 epoch total loss 0.57673\n",
      "Trained batch 2385 batch loss 0.550996 epoch total loss 0.576719224\n",
      "Trained batch 2386 batch loss 0.582784653 epoch total loss 0.576721728\n",
      "Trained batch 2387 batch loss 0.635492921 epoch total loss 0.576746404\n",
      "Trained batch 2388 batch loss 0.570639193 epoch total loss 0.576743841\n",
      "Trained batch 2389 batch loss 0.575868785 epoch total loss 0.576743484\n",
      "Trained batch 2390 batch loss 0.58453697 epoch total loss 0.576746762\n",
      "Trained batch 2391 batch loss 0.66324991 epoch total loss 0.576782942\n",
      "Trained batch 2392 batch loss 0.614386559 epoch total loss 0.576798677\n",
      "Trained batch 2393 batch loss 0.646412611 epoch total loss 0.576827705\n",
      "Trained batch 2394 batch loss 0.59036541 epoch total loss 0.576833367\n",
      "Trained batch 2395 batch loss 0.534925759 epoch total loss 0.576815844\n",
      "Trained batch 2396 batch loss 0.64590925 epoch total loss 0.576844692\n",
      "Trained batch 2397 batch loss 0.704834342 epoch total loss 0.576898098\n",
      "Trained batch 2398 batch loss 0.638358295 epoch total loss 0.576923668\n",
      "Trained batch 2399 batch loss 0.593534 epoch total loss 0.576930583\n",
      "Trained batch 2400 batch loss 0.579105794 epoch total loss 0.576931536\n",
      "Trained batch 2401 batch loss 0.509869933 epoch total loss 0.576903582\n",
      "Trained batch 2402 batch loss 0.492077529 epoch total loss 0.576868236\n",
      "Trained batch 2403 batch loss 0.539636135 epoch total loss 0.576852798\n",
      "Trained batch 2404 batch loss 0.57352227 epoch total loss 0.576851368\n",
      "Trained batch 2405 batch loss 0.703071356 epoch total loss 0.57690388\n",
      "Trained batch 2406 batch loss 0.658047616 epoch total loss 0.576937616\n",
      "Trained batch 2407 batch loss 0.697518468 epoch total loss 0.576987743\n",
      "Trained batch 2408 batch loss 0.628709674 epoch total loss 0.577009201\n",
      "Trained batch 2409 batch loss 0.596180141 epoch total loss 0.577017128\n",
      "Trained batch 2410 batch loss 0.60087347 epoch total loss 0.577027\n",
      "Trained batch 2411 batch loss 0.551835299 epoch total loss 0.577016592\n",
      "Trained batch 2412 batch loss 0.50357312 epoch total loss 0.576986134\n",
      "Trained batch 2413 batch loss 0.593406677 epoch total loss 0.576992929\n",
      "Trained batch 2414 batch loss 0.580951273 epoch total loss 0.576994538\n",
      "Trained batch 2415 batch loss 0.595705 epoch total loss 0.577002287\n",
      "Trained batch 2416 batch loss 0.485051274 epoch total loss 0.576964259\n",
      "Trained batch 2417 batch loss 0.603622198 epoch total loss 0.576975286\n",
      "Trained batch 2418 batch loss 0.610910535 epoch total loss 0.576989353\n",
      "Trained batch 2419 batch loss 0.671706796 epoch total loss 0.577028513\n",
      "Trained batch 2420 batch loss 0.675115824 epoch total loss 0.577069104\n",
      "Trained batch 2421 batch loss 0.641997635 epoch total loss 0.577095926\n",
      "Trained batch 2422 batch loss 0.708856702 epoch total loss 0.577150285\n",
      "Trained batch 2423 batch loss 0.570202 epoch total loss 0.577147424\n",
      "Trained batch 2424 batch loss 0.602976143 epoch total loss 0.577158093\n",
      "Trained batch 2425 batch loss 0.529734492 epoch total loss 0.577138543\n",
      "Trained batch 2426 batch loss 0.494729608 epoch total loss 0.577104628\n",
      "Trained batch 2427 batch loss 0.547845542 epoch total loss 0.577092528\n",
      "Trained batch 2428 batch loss 0.452610195 epoch total loss 0.577041328\n",
      "Trained batch 2429 batch loss 0.575203836 epoch total loss 0.577040553\n",
      "Trained batch 2430 batch loss 0.568285167 epoch total loss 0.577036917\n",
      "Trained batch 2431 batch loss 0.623972178 epoch total loss 0.577056229\n",
      "Trained batch 2432 batch loss 0.575723767 epoch total loss 0.577055693\n",
      "Trained batch 2433 batch loss 0.630654693 epoch total loss 0.577077687\n",
      "Trained batch 2434 batch loss 0.666424274 epoch total loss 0.577114403\n",
      "Trained batch 2435 batch loss 0.71304667 epoch total loss 0.577170193\n",
      "Trained batch 2436 batch loss 0.578337312 epoch total loss 0.57717067\n",
      "Trained batch 2437 batch loss 0.565598845 epoch total loss 0.577165902\n",
      "Trained batch 2438 batch loss 0.657174945 epoch total loss 0.577198744\n",
      "Trained batch 2439 batch loss 0.67921108 epoch total loss 0.577240586\n",
      "Trained batch 2440 batch loss 0.68302989 epoch total loss 0.577283919\n",
      "Trained batch 2441 batch loss 0.702768564 epoch total loss 0.577335298\n",
      "Trained batch 2442 batch loss 0.545211673 epoch total loss 0.577322125\n",
      "Trained batch 2443 batch loss 0.555623233 epoch total loss 0.577313304\n",
      "Trained batch 2444 batch loss 0.682581842 epoch total loss 0.577356339\n",
      "Trained batch 2445 batch loss 0.736178041 epoch total loss 0.577421308\n",
      "Trained batch 2446 batch loss 0.646158 epoch total loss 0.577449441\n",
      "Trained batch 2447 batch loss 0.676863432 epoch total loss 0.577490032\n",
      "Trained batch 2448 batch loss 0.603868127 epoch total loss 0.57750082\n",
      "Trained batch 2449 batch loss 0.610382318 epoch total loss 0.577514231\n",
      "Trained batch 2450 batch loss 0.603760362 epoch total loss 0.57752496\n",
      "Trained batch 2451 batch loss 0.569133401 epoch total loss 0.577521503\n",
      "Trained batch 2452 batch loss 0.685976863 epoch total loss 0.577565789\n",
      "Trained batch 2453 batch loss 0.585827768 epoch total loss 0.577569127\n",
      "Trained batch 2454 batch loss 0.513011336 epoch total loss 0.577542841\n",
      "Trained batch 2455 batch loss 0.546768069 epoch total loss 0.577530324\n",
      "Trained batch 2456 batch loss 0.488755256 epoch total loss 0.577494144\n",
      "Trained batch 2457 batch loss 0.615910172 epoch total loss 0.57750982\n",
      "Trained batch 2458 batch loss 0.583381772 epoch total loss 0.577512205\n",
      "Trained batch 2459 batch loss 0.646673203 epoch total loss 0.577540338\n",
      "Trained batch 2460 batch loss 0.575377 epoch total loss 0.577539444\n",
      "Trained batch 2461 batch loss 0.596789122 epoch total loss 0.577547312\n",
      "Trained batch 2462 batch loss 0.593647 epoch total loss 0.577553809\n",
      "Trained batch 2463 batch loss 0.660529554 epoch total loss 0.577587485\n",
      "Trained batch 2464 batch loss 0.523759961 epoch total loss 0.57756567\n",
      "Trained batch 2465 batch loss 0.566977441 epoch total loss 0.577561378\n",
      "Trained batch 2466 batch loss 0.55560267 epoch total loss 0.577552438\n",
      "Trained batch 2467 batch loss 0.575938463 epoch total loss 0.577551782\n",
      "Trained batch 2468 batch loss 0.587505221 epoch total loss 0.577555835\n",
      "Trained batch 2469 batch loss 0.599968553 epoch total loss 0.577564955\n",
      "Trained batch 2470 batch loss 0.551471591 epoch total loss 0.577554405\n",
      "Trained batch 2471 batch loss 0.571339846 epoch total loss 0.577551842\n",
      "Trained batch 2472 batch loss 0.559648514 epoch total loss 0.57754463\n",
      "Trained batch 2473 batch loss 0.619403839 epoch total loss 0.577561557\n",
      "Trained batch 2474 batch loss 0.594726384 epoch total loss 0.577568471\n",
      "Trained batch 2475 batch loss 0.623535156 epoch total loss 0.577587068\n",
      "Trained batch 2476 batch loss 0.638168931 epoch total loss 0.577611506\n",
      "Trained batch 2477 batch loss 0.660886347 epoch total loss 0.577645123\n",
      "Trained batch 2478 batch loss 0.570468187 epoch total loss 0.577642262\n",
      "Trained batch 2479 batch loss 0.640764 epoch total loss 0.577667713\n",
      "Trained batch 2480 batch loss 0.649660707 epoch total loss 0.577696741\n",
      "Trained batch 2481 batch loss 0.587536812 epoch total loss 0.577700675\n",
      "Trained batch 2482 batch loss 0.633470416 epoch total loss 0.577723145\n",
      "Trained batch 2483 batch loss 0.637019873 epoch total loss 0.577747\n",
      "Trained batch 2484 batch loss 0.557785511 epoch total loss 0.577738941\n",
      "Trained batch 2485 batch loss 0.637170911 epoch total loss 0.577762842\n",
      "Trained batch 2486 batch loss 0.554201901 epoch total loss 0.577753365\n",
      "Trained batch 2487 batch loss 0.58424753 epoch total loss 0.577756\n",
      "Trained batch 2488 batch loss 0.624617696 epoch total loss 0.577774823\n",
      "Trained batch 2489 batch loss 0.59008193 epoch total loss 0.57777977\n",
      "Trained batch 2490 batch loss 0.550302 epoch total loss 0.577768743\n",
      "Trained batch 2491 batch loss 0.560194969 epoch total loss 0.57776171\n",
      "Trained batch 2492 batch loss 0.590295851 epoch total loss 0.577766716\n",
      "Trained batch 2493 batch loss 0.537743568 epoch total loss 0.577750683\n",
      "Trained batch 2494 batch loss 0.53855896 epoch total loss 0.577734947\n",
      "Trained batch 2495 batch loss 0.472769 epoch total loss 0.577692866\n",
      "Trained batch 2496 batch loss 0.622611403 epoch total loss 0.577710867\n",
      "Trained batch 2497 batch loss 0.611858785 epoch total loss 0.577724516\n",
      "Trained batch 2498 batch loss 0.486289769 epoch total loss 0.577687919\n",
      "Trained batch 2499 batch loss 0.562755942 epoch total loss 0.577681959\n",
      "Trained batch 2500 batch loss 0.523558199 epoch total loss 0.577660322\n",
      "Trained batch 2501 batch loss 0.527889431 epoch total loss 0.577640355\n",
      "Trained batch 2502 batch loss 0.483041465 epoch total loss 0.577602565\n",
      "Trained batch 2503 batch loss 0.512071967 epoch total loss 0.577576399\n",
      "Trained batch 2504 batch loss 0.46050477 epoch total loss 0.577529609\n",
      "Trained batch 2505 batch loss 0.489782751 epoch total loss 0.577494562\n",
      "Trained batch 2506 batch loss 0.45997107 epoch total loss 0.577447653\n",
      "Trained batch 2507 batch loss 0.558487713 epoch total loss 0.577440083\n",
      "Trained batch 2508 batch loss 0.594341815 epoch total loss 0.577446818\n",
      "Trained batch 2509 batch loss 0.582646847 epoch total loss 0.577448905\n",
      "Trained batch 2510 batch loss 0.564856231 epoch total loss 0.577443898\n",
      "Trained batch 2511 batch loss 0.538548648 epoch total loss 0.577428401\n",
      "Trained batch 2512 batch loss 0.642819285 epoch total loss 0.577454448\n",
      "Trained batch 2513 batch loss 0.606707871 epoch total loss 0.577466071\n",
      "Trained batch 2514 batch loss 0.596882761 epoch total loss 0.577473819\n",
      "Trained batch 2515 batch loss 0.638808131 epoch total loss 0.577498198\n",
      "Trained batch 2516 batch loss 0.614675 epoch total loss 0.577513\n",
      "Trained batch 2517 batch loss 0.533774793 epoch total loss 0.577495575\n",
      "Trained batch 2518 batch loss 0.482643634 epoch total loss 0.577457905\n",
      "Trained batch 2519 batch loss 0.485668063 epoch total loss 0.577421486\n",
      "Trained batch 2520 batch loss 0.49876529 epoch total loss 0.577390313\n",
      "Trained batch 2521 batch loss 0.53028357 epoch total loss 0.577371597\n",
      "Trained batch 2522 batch loss 0.509158075 epoch total loss 0.577344537\n",
      "Trained batch 2523 batch loss 0.560164332 epoch total loss 0.577337742\n",
      "Trained batch 2524 batch loss 0.610303104 epoch total loss 0.577350855\n",
      "Trained batch 2525 batch loss 0.632981777 epoch total loss 0.577372849\n",
      "Trained batch 2526 batch loss 0.573092759 epoch total loss 0.57737118\n",
      "Trained batch 2527 batch loss 0.667369127 epoch total loss 0.577406764\n",
      "Trained batch 2528 batch loss 0.657022 epoch total loss 0.577438235\n",
      "Trained batch 2529 batch loss 0.557488739 epoch total loss 0.577430367\n",
      "Trained batch 2530 batch loss 0.565362871 epoch total loss 0.577425599\n",
      "Trained batch 2531 batch loss 0.601957798 epoch total loss 0.577435255\n",
      "Trained batch 2532 batch loss 0.533428371 epoch total loss 0.57741791\n",
      "Trained batch 2533 batch loss 0.484415472 epoch total loss 0.577381134\n",
      "Trained batch 2534 batch loss 0.523780763 epoch total loss 0.577360034\n",
      "Trained batch 2535 batch loss 0.562175155 epoch total loss 0.577354\n",
      "Trained batch 2536 batch loss 0.494816482 epoch total loss 0.57732147\n",
      "Trained batch 2537 batch loss 0.536499739 epoch total loss 0.577305377\n",
      "Trained batch 2538 batch loss 0.494864196 epoch total loss 0.577272892\n",
      "Trained batch 2539 batch loss 0.560167491 epoch total loss 0.577266157\n",
      "Trained batch 2540 batch loss 0.574262857 epoch total loss 0.577264965\n",
      "Trained batch 2541 batch loss 0.631207764 epoch total loss 0.577286243\n",
      "Trained batch 2542 batch loss 0.658282 epoch total loss 0.577318072\n",
      "Trained batch 2543 batch loss 0.595330954 epoch total loss 0.577325165\n",
      "Trained batch 2544 batch loss 0.593989193 epoch total loss 0.577331722\n",
      "Trained batch 2545 batch loss 0.614788651 epoch total loss 0.577346444\n",
      "Trained batch 2546 batch loss 0.57481122 epoch total loss 0.577345431\n",
      "Trained batch 2547 batch loss 0.570603848 epoch total loss 0.577342808\n",
      "Trained batch 2548 batch loss 0.55180949 epoch total loss 0.577332735\n",
      "Trained batch 2549 batch loss 0.57407254 epoch total loss 0.577331483\n",
      "Trained batch 2550 batch loss 0.557581663 epoch total loss 0.577323735\n",
      "Trained batch 2551 batch loss 0.618379772 epoch total loss 0.577339828\n",
      "Trained batch 2552 batch loss 0.567094743 epoch total loss 0.577335835\n",
      "Trained batch 2553 batch loss 0.546365142 epoch total loss 0.577323735\n",
      "Trained batch 2554 batch loss 0.601146698 epoch total loss 0.577333093\n",
      "Trained batch 2555 batch loss 0.533833742 epoch total loss 0.577316046\n",
      "Trained batch 2556 batch loss 0.598170161 epoch total loss 0.577324212\n",
      "Trained batch 2557 batch loss 0.558914185 epoch total loss 0.577317\n",
      "Trained batch 2558 batch loss 0.681106865 epoch total loss 0.57735759\n",
      "Trained batch 2559 batch loss 0.538366914 epoch total loss 0.577342331\n",
      "Trained batch 2560 batch loss 0.579678178 epoch total loss 0.577343285\n",
      "Trained batch 2561 batch loss 0.615988 epoch total loss 0.577358365\n",
      "Trained batch 2562 batch loss 0.623289585 epoch total loss 0.577376306\n",
      "Trained batch 2563 batch loss 0.590445459 epoch total loss 0.577381372\n",
      "Trained batch 2564 batch loss 0.556575119 epoch total loss 0.577373266\n",
      "Trained batch 2565 batch loss 0.597511411 epoch total loss 0.577381134\n",
      "Trained batch 2566 batch loss 0.579751134 epoch total loss 0.577382\n",
      "Trained batch 2567 batch loss 0.560135424 epoch total loss 0.577375293\n",
      "Trained batch 2568 batch loss 0.628269911 epoch total loss 0.577395141\n",
      "Trained batch 2569 batch loss 0.63751018 epoch total loss 0.577418506\n",
      "Trained batch 2570 batch loss 0.600281715 epoch total loss 0.577427447\n",
      "Trained batch 2571 batch loss 0.507747114 epoch total loss 0.577400327\n",
      "Trained batch 2572 batch loss 0.532989144 epoch total loss 0.577383041\n",
      "Trained batch 2573 batch loss 0.589856744 epoch total loss 0.577387869\n",
      "Trained batch 2574 batch loss 0.510196686 epoch total loss 0.577361822\n",
      "Trained batch 2575 batch loss 0.620884776 epoch total loss 0.57737869\n",
      "Trained batch 2576 batch loss 0.55436641 epoch total loss 0.57736975\n",
      "Trained batch 2577 batch loss 0.552967072 epoch total loss 0.577360272\n",
      "Trained batch 2578 batch loss 0.561487 epoch total loss 0.577354133\n",
      "Trained batch 2579 batch loss 0.459901929 epoch total loss 0.577308595\n",
      "Trained batch 2580 batch loss 0.473956078 epoch total loss 0.577268541\n",
      "Trained batch 2581 batch loss 0.598626554 epoch total loss 0.577276826\n",
      "Trained batch 2582 batch loss 0.544229388 epoch total loss 0.577264\n",
      "Trained batch 2583 batch loss 0.532213867 epoch total loss 0.577246606\n",
      "Trained batch 2584 batch loss 0.63402 epoch total loss 0.5772686\n",
      "Trained batch 2585 batch loss 0.615882635 epoch total loss 0.577283502\n",
      "Trained batch 2586 batch loss 0.653365731 epoch total loss 0.577312887\n",
      "Trained batch 2587 batch loss 0.657079935 epoch total loss 0.577343762\n",
      "Trained batch 2588 batch loss 0.791698 epoch total loss 0.577426612\n",
      "Trained batch 2589 batch loss 0.653833151 epoch total loss 0.577456117\n",
      "Trained batch 2590 batch loss 0.654838681 epoch total loss 0.577485919\n",
      "Trained batch 2591 batch loss 0.697244585 epoch total loss 0.577532172\n",
      "Trained batch 2592 batch loss 0.602480352 epoch total loss 0.577541828\n",
      "Trained batch 2593 batch loss 0.620779455 epoch total loss 0.577558458\n",
      "Trained batch 2594 batch loss 0.563863337 epoch total loss 0.577553213\n",
      "Trained batch 2595 batch loss 0.50725174 epoch total loss 0.577526093\n",
      "Trained batch 2596 batch loss 0.523049712 epoch total loss 0.577505112\n",
      "Trained batch 2597 batch loss 0.519647241 epoch total loss 0.57748282\n",
      "Trained batch 2598 batch loss 0.458727181 epoch total loss 0.577437103\n",
      "Trained batch 2599 batch loss 0.419618309 epoch total loss 0.577376425\n",
      "Trained batch 2600 batch loss 0.451416314 epoch total loss 0.577327967\n",
      "Trained batch 2601 batch loss 0.444530517 epoch total loss 0.577276945\n",
      "Trained batch 2602 batch loss 0.459731907 epoch total loss 0.577231765\n",
      "Trained batch 2603 batch loss 0.605983734 epoch total loss 0.577242792\n",
      "Trained batch 2604 batch loss 0.590504467 epoch total loss 0.577247858\n",
      "Trained batch 2605 batch loss 0.534070969 epoch total loss 0.577231288\n",
      "Trained batch 2606 batch loss 0.615689218 epoch total loss 0.57724607\n",
      "Trained batch 2607 batch loss 0.512388647 epoch total loss 0.577221155\n",
      "Trained batch 2608 batch loss 0.540254891 epoch total loss 0.577206969\n",
      "Trained batch 2609 batch loss 0.623035312 epoch total loss 0.577224553\n",
      "Trained batch 2610 batch loss 0.58350867 epoch total loss 0.577226937\n",
      "Trained batch 2611 batch loss 0.533413708 epoch total loss 0.577210188\n",
      "Trained batch 2612 batch loss 0.626945138 epoch total loss 0.577229261\n",
      "Trained batch 2613 batch loss 0.580248713 epoch total loss 0.577230394\n",
      "Trained batch 2614 batch loss 0.602956176 epoch total loss 0.577240229\n",
      "Trained batch 2615 batch loss 0.467767715 epoch total loss 0.577198327\n",
      "Trained batch 2616 batch loss 0.595291257 epoch total loss 0.5772053\n",
      "Trained batch 2617 batch loss 0.511057556 epoch total loss 0.57718\n",
      "Trained batch 2618 batch loss 0.511171162 epoch total loss 0.577154815\n",
      "Trained batch 2619 batch loss 0.521830142 epoch total loss 0.577133715\n",
      "Trained batch 2620 batch loss 0.525648892 epoch total loss 0.577114046\n",
      "Trained batch 2621 batch loss 0.49056 epoch total loss 0.577081\n",
      "Trained batch 2622 batch loss 0.554903507 epoch total loss 0.57707262\n",
      "Trained batch 2623 batch loss 0.558500648 epoch total loss 0.577065527\n",
      "Trained batch 2624 batch loss 0.609667122 epoch total loss 0.577077925\n",
      "Trained batch 2625 batch loss 0.545332849 epoch total loss 0.577065825\n",
      "Trained batch 2626 batch loss 0.634888172 epoch total loss 0.57708782\n",
      "Trained batch 2627 batch loss 0.707933664 epoch total loss 0.577137589\n",
      "Trained batch 2628 batch loss 0.566560507 epoch total loss 0.577133596\n",
      "Trained batch 2629 batch loss 0.555115223 epoch total loss 0.577125192\n",
      "Trained batch 2630 batch loss 0.552180469 epoch total loss 0.577115715\n",
      "Trained batch 2631 batch loss 0.559446752 epoch total loss 0.577109\n",
      "Trained batch 2632 batch loss 0.532070637 epoch total loss 0.577091873\n",
      "Trained batch 2633 batch loss 0.626624942 epoch total loss 0.577110708\n",
      "Trained batch 2634 batch loss 0.574562907 epoch total loss 0.577109754\n",
      "Trained batch 2635 batch loss 0.566827595 epoch total loss 0.57710582\n",
      "Trained batch 2636 batch loss 0.584555507 epoch total loss 0.577108681\n",
      "Trained batch 2637 batch loss 0.545734406 epoch total loss 0.57709676\n",
      "Trained batch 2638 batch loss 0.534995914 epoch total loss 0.577080846\n",
      "Trained batch 2639 batch loss 0.481983662 epoch total loss 0.577044785\n",
      "Trained batch 2640 batch loss 0.582583904 epoch total loss 0.577046871\n",
      "Trained batch 2641 batch loss 0.599187493 epoch total loss 0.577055275\n",
      "Trained batch 2642 batch loss 0.539709747 epoch total loss 0.577041149\n",
      "Trained batch 2643 batch loss 0.507822931 epoch total loss 0.577015\n",
      "Trained batch 2644 batch loss 0.495336831 epoch total loss 0.576984048\n",
      "Trained batch 2645 batch loss 0.556971312 epoch total loss 0.576976538\n",
      "Trained batch 2646 batch loss 0.69581908 epoch total loss 0.57702142\n",
      "Trained batch 2647 batch loss 0.61647433 epoch total loss 0.577036321\n",
      "Trained batch 2648 batch loss 0.621806622 epoch total loss 0.577053249\n",
      "Trained batch 2649 batch loss 0.680245221 epoch total loss 0.57709223\n",
      "Trained batch 2650 batch loss 0.655647635 epoch total loss 0.577121854\n",
      "Trained batch 2651 batch loss 0.653667 epoch total loss 0.577150762\n",
      "Trained batch 2652 batch loss 0.594452322 epoch total loss 0.577157259\n",
      "Trained batch 2653 batch loss 0.619671822 epoch total loss 0.577173293\n",
      "Trained batch 2654 batch loss 0.556769669 epoch total loss 0.577165604\n",
      "Trained batch 2655 batch loss 0.575101852 epoch total loss 0.577164829\n",
      "Trained batch 2656 batch loss 0.598902941 epoch total loss 0.577173\n",
      "Trained batch 2657 batch loss 0.576940894 epoch total loss 0.577172875\n",
      "Trained batch 2658 batch loss 0.564003587 epoch total loss 0.577167928\n",
      "Trained batch 2659 batch loss 0.592190623 epoch total loss 0.577173531\n",
      "Trained batch 2660 batch loss 0.571889877 epoch total loss 0.577171564\n",
      "Trained batch 2661 batch loss 0.580045342 epoch total loss 0.577172637\n",
      "Trained batch 2662 batch loss 0.548313916 epoch total loss 0.577161849\n",
      "Trained batch 2663 batch loss 0.520762384 epoch total loss 0.577140629\n",
      "Trained batch 2664 batch loss 0.599562883 epoch total loss 0.577149093\n",
      "Trained batch 2665 batch loss 0.646831214 epoch total loss 0.57717526\n",
      "Trained batch 2666 batch loss 0.634807408 epoch total loss 0.577196836\n",
      "Trained batch 2667 batch loss 0.598510742 epoch total loss 0.577204823\n",
      "Trained batch 2668 batch loss 0.529206514 epoch total loss 0.577186823\n",
      "Trained batch 2669 batch loss 0.597650349 epoch total loss 0.577194512\n",
      "Trained batch 2670 batch loss 0.588567257 epoch total loss 0.577198803\n",
      "Trained batch 2671 batch loss 0.613818347 epoch total loss 0.577212453\n",
      "Trained batch 2672 batch loss 0.565223277 epoch total loss 0.577208\n",
      "Trained batch 2673 batch loss 0.584510803 epoch total loss 0.577210665\n",
      "Trained batch 2674 batch loss 0.547554314 epoch total loss 0.577199638\n",
      "Trained batch 2675 batch loss 0.460580021 epoch total loss 0.577156\n",
      "Trained batch 2676 batch loss 0.550577879 epoch total loss 0.577146053\n",
      "Trained batch 2677 batch loss 0.549830675 epoch total loss 0.577135861\n",
      "Trained batch 2678 batch loss 0.569924712 epoch total loss 0.577133179\n",
      "Trained batch 2679 batch loss 0.57149446 epoch total loss 0.577131093\n",
      "Trained batch 2680 batch loss 0.557231903 epoch total loss 0.577123642\n",
      "Trained batch 2681 batch loss 0.517030954 epoch total loss 0.57710129\n",
      "Trained batch 2682 batch loss 0.491572738 epoch total loss 0.577069402\n",
      "Trained batch 2683 batch loss 0.514963508 epoch total loss 0.577046275\n",
      "Trained batch 2684 batch loss 0.490044445 epoch total loss 0.577013791\n",
      "Trained batch 2685 batch loss 0.504026055 epoch total loss 0.576986611\n",
      "Trained batch 2686 batch loss 0.578896463 epoch total loss 0.576987326\n",
      "Trained batch 2687 batch loss 0.521855652 epoch total loss 0.576966822\n",
      "Trained batch 2688 batch loss 0.624683559 epoch total loss 0.576984525\n",
      "Trained batch 2689 batch loss 0.625481 epoch total loss 0.577002585\n",
      "Trained batch 2690 batch loss 0.548811197 epoch total loss 0.576992095\n",
      "Trained batch 2691 batch loss 0.597308457 epoch total loss 0.576999664\n",
      "Trained batch 2692 batch loss 0.599249303 epoch total loss 0.57700789\n",
      "Trained batch 2693 batch loss 0.590299547 epoch total loss 0.577012837\n",
      "Trained batch 2694 batch loss 0.567839205 epoch total loss 0.577009439\n",
      "Trained batch 2695 batch loss 0.589800954 epoch total loss 0.577014208\n",
      "Trained batch 2696 batch loss 0.507405221 epoch total loss 0.576988399\n",
      "Trained batch 2697 batch loss 0.500711203 epoch total loss 0.576960146\n",
      "Trained batch 2698 batch loss 0.486616731 epoch total loss 0.576926649\n",
      "Trained batch 2699 batch loss 0.562078953 epoch total loss 0.576921165\n",
      "Trained batch 2700 batch loss 0.551142097 epoch total loss 0.576911628\n",
      "Trained batch 2701 batch loss 0.586650789 epoch total loss 0.576915205\n",
      "Trained batch 2702 batch loss 0.570486665 epoch total loss 0.57691282\n",
      "Trained batch 2703 batch loss 0.57384032 epoch total loss 0.576911688\n",
      "Trained batch 2704 batch loss 0.648654521 epoch total loss 0.576938272\n",
      "Trained batch 2705 batch loss 0.628040671 epoch total loss 0.576957166\n",
      "Trained batch 2706 batch loss 0.595731139 epoch total loss 0.57696408\n",
      "Trained batch 2707 batch loss 0.533083081 epoch total loss 0.576947868\n",
      "Trained batch 2708 batch loss 0.634918272 epoch total loss 0.576969266\n",
      "Trained batch 2709 batch loss 0.53462106 epoch total loss 0.57695365\n",
      "Trained batch 2710 batch loss 0.523375869 epoch total loss 0.576933861\n",
      "Trained batch 2711 batch loss 0.503134 epoch total loss 0.576906621\n",
      "Trained batch 2712 batch loss 0.639021635 epoch total loss 0.576929569\n",
      "Trained batch 2713 batch loss 0.616665184 epoch total loss 0.576944232\n",
      "Trained batch 2714 batch loss 0.619429946 epoch total loss 0.576959848\n",
      "Trained batch 2715 batch loss 0.579977632 epoch total loss 0.576960921\n",
      "Trained batch 2716 batch loss 0.634737372 epoch total loss 0.57698226\n",
      "Trained batch 2717 batch loss 0.567438304 epoch total loss 0.576978683\n",
      "Trained batch 2718 batch loss 0.603962362 epoch total loss 0.576988637\n",
      "Trained batch 2719 batch loss 0.594751894 epoch total loss 0.576995194\n",
      "Trained batch 2720 batch loss 0.589864254 epoch total loss 0.576999903\n",
      "Trained batch 2721 batch loss 0.569184542 epoch total loss 0.576997042\n",
      "Trained batch 2722 batch loss 0.53833133 epoch total loss 0.576982796\n",
      "Trained batch 2723 batch loss 0.558021069 epoch total loss 0.576975822\n",
      "Trained batch 2724 batch loss 0.54518491 epoch total loss 0.57696414\n",
      "Trained batch 2725 batch loss 0.511117 epoch total loss 0.57694\n",
      "Trained batch 2726 batch loss 0.565781355 epoch total loss 0.576935887\n",
      "Trained batch 2727 batch loss 0.577076435 epoch total loss 0.576935947\n",
      "Trained batch 2728 batch loss 0.630412877 epoch total loss 0.576955557\n",
      "Trained batch 2729 batch loss 0.604191959 epoch total loss 0.576965511\n",
      "Trained batch 2730 batch loss 0.558838308 epoch total loss 0.576958895\n",
      "Trained batch 2731 batch loss 0.578016102 epoch total loss 0.576959252\n",
      "Trained batch 2732 batch loss 0.619740844 epoch total loss 0.576974928\n",
      "Trained batch 2733 batch loss 0.491719365 epoch total loss 0.576943755\n",
      "Trained batch 2734 batch loss 0.538054109 epoch total loss 0.57692951\n",
      "Trained batch 2735 batch loss 0.498567939 epoch total loss 0.57690084\n",
      "Trained batch 2736 batch loss 0.572024524 epoch total loss 0.576899052\n",
      "Trained batch 2737 batch loss 0.613123298 epoch total loss 0.576912344\n",
      "Trained batch 2738 batch loss 0.533843875 epoch total loss 0.576896608\n",
      "Trained batch 2739 batch loss 0.631876528 epoch total loss 0.576916635\n",
      "Trained batch 2740 batch loss 0.63244909 epoch total loss 0.576936901\n",
      "Trained batch 2741 batch loss 0.541107178 epoch total loss 0.576923847\n",
      "Trained batch 2742 batch loss 0.687926292 epoch total loss 0.576964319\n",
      "Trained batch 2743 batch loss 0.604714036 epoch total loss 0.576974452\n",
      "Trained batch 2744 batch loss 0.518654048 epoch total loss 0.576953173\n",
      "Trained batch 2745 batch loss 0.606251121 epoch total loss 0.576963842\n",
      "Trained batch 2746 batch loss 0.589755893 epoch total loss 0.576968491\n",
      "Trained batch 2747 batch loss 0.59608382 epoch total loss 0.576975465\n",
      "Trained batch 2748 batch loss 0.639759421 epoch total loss 0.576998293\n",
      "Trained batch 2749 batch loss 0.643417716 epoch total loss 0.577022433\n",
      "Trained batch 2750 batch loss 0.574401677 epoch total loss 0.57702148\n",
      "Trained batch 2751 batch loss 0.607755 epoch total loss 0.577032685\n",
      "Trained batch 2752 batch loss 0.660319149 epoch total loss 0.577062905\n",
      "Trained batch 2753 batch loss 0.627440631 epoch total loss 0.577081203\n",
      "Trained batch 2754 batch loss 0.581885159 epoch total loss 0.577083\n",
      "Trained batch 2755 batch loss 0.604864717 epoch total loss 0.577093065\n",
      "Trained batch 2756 batch loss 0.58616966 epoch total loss 0.577096343\n",
      "Trained batch 2757 batch loss 0.614586115 epoch total loss 0.577109933\n",
      "Trained batch 2758 batch loss 0.60832876 epoch total loss 0.577121258\n",
      "Trained batch 2759 batch loss 0.596987247 epoch total loss 0.57712847\n",
      "Trained batch 2760 batch loss 0.631492734 epoch total loss 0.577148199\n",
      "Trained batch 2761 batch loss 0.549426198 epoch total loss 0.577138126\n",
      "Trained batch 2762 batch loss 0.559286058 epoch total loss 0.577131689\n",
      "Trained batch 2763 batch loss 0.608118892 epoch total loss 0.577142894\n",
      "Trained batch 2764 batch loss 0.600549817 epoch total loss 0.577151418\n",
      "Trained batch 2765 batch loss 0.56907022 epoch total loss 0.577148497\n",
      "Trained batch 2766 batch loss 0.483608454 epoch total loss 0.577114701\n",
      "Trained batch 2767 batch loss 0.605789 epoch total loss 0.577125072\n",
      "Trained batch 2768 batch loss 0.593309104 epoch total loss 0.577130914\n",
      "Trained batch 2769 batch loss 0.617724419 epoch total loss 0.577145517\n",
      "Trained batch 2770 batch loss 0.585409462 epoch total loss 0.577148497\n",
      "Trained batch 2771 batch loss 0.534763277 epoch total loss 0.577133238\n",
      "Trained batch 2772 batch loss 0.587764263 epoch total loss 0.577137053\n",
      "Trained batch 2773 batch loss 0.527457237 epoch total loss 0.577119172\n",
      "Trained batch 2774 batch loss 0.502961457 epoch total loss 0.577092409\n",
      "Trained batch 2775 batch loss 0.54703331 epoch total loss 0.577081561\n",
      "Trained batch 2776 batch loss 0.496981591 epoch total loss 0.577052712\n",
      "Epoch 5 train loss 0.5770527124404907\n",
      "Validated batch 1 batch loss 0.66204989\n",
      "Validated batch 2 batch loss 0.604739368\n",
      "Validated batch 3 batch loss 0.543343306\n",
      "Validated batch 4 batch loss 0.668387055\n",
      "Validated batch 5 batch loss 0.540629447\n",
      "Validated batch 6 batch loss 0.547172666\n",
      "Validated batch 7 batch loss 0.551791549\n",
      "Validated batch 8 batch loss 0.583682954\n",
      "Validated batch 9 batch loss 0.473287463\n",
      "Validated batch 10 batch loss 0.565695107\n",
      "Validated batch 11 batch loss 0.639666438\n",
      "Validated batch 12 batch loss 0.548545182\n",
      "Validated batch 13 batch loss 0.596728683\n",
      "Validated batch 14 batch loss 0.608653784\n",
      "Validated batch 15 batch loss 0.505605459\n",
      "Validated batch 16 batch loss 0.523181081\n",
      "Validated batch 17 batch loss 0.538372636\n",
      "Validated batch 18 batch loss 0.661220193\n",
      "Validated batch 19 batch loss 0.555037439\n",
      "Validated batch 20 batch loss 0.542455375\n",
      "Validated batch 21 batch loss 0.57996\n",
      "Validated batch 22 batch loss 0.641336799\n",
      "Validated batch 23 batch loss 0.599757969\n",
      "Validated batch 24 batch loss 0.554320157\n",
      "Validated batch 25 batch loss 0.673673093\n",
      "Validated batch 26 batch loss 0.530915856\n",
      "Validated batch 27 batch loss 0.538297\n",
      "Validated batch 28 batch loss 0.588302612\n",
      "Validated batch 29 batch loss 0.542712331\n",
      "Validated batch 30 batch loss 0.513726294\n",
      "Validated batch 31 batch loss 0.603586793\n",
      "Validated batch 32 batch loss 0.555254817\n",
      "Validated batch 33 batch loss 0.560395241\n",
      "Validated batch 34 batch loss 0.547502279\n",
      "Validated batch 35 batch loss 0.634608805\n",
      "Validated batch 36 batch loss 0.574449778\n",
      "Validated batch 37 batch loss 0.562937\n",
      "Validated batch 38 batch loss 0.634809315\n",
      "Validated batch 39 batch loss 0.656907797\n",
      "Validated batch 40 batch loss 0.754249394\n",
      "Validated batch 41 batch loss 0.622805357\n",
      "Validated batch 42 batch loss 0.574882627\n",
      "Validated batch 43 batch loss 0.556417227\n",
      "Validated batch 44 batch loss 0.626273274\n",
      "Validated batch 45 batch loss 0.466575861\n",
      "Validated batch 46 batch loss 0.525870264\n",
      "Validated batch 47 batch loss 0.50390619\n",
      "Validated batch 48 batch loss 0.580347121\n",
      "Validated batch 49 batch loss 0.511376441\n",
      "Validated batch 50 batch loss 0.536540031\n",
      "Validated batch 51 batch loss 0.577766895\n",
      "Validated batch 52 batch loss 0.541755557\n",
      "Validated batch 53 batch loss 0.595581651\n",
      "Validated batch 54 batch loss 0.500384629\n",
      "Validated batch 55 batch loss 0.553364933\n",
      "Validated batch 56 batch loss 0.578443646\n",
      "Validated batch 57 batch loss 0.543528318\n",
      "Validated batch 58 batch loss 0.605971694\n",
      "Validated batch 59 batch loss 0.516494751\n",
      "Validated batch 60 batch loss 0.665959418\n",
      "Validated batch 61 batch loss 0.542132139\n",
      "Validated batch 62 batch loss 0.566407621\n",
      "Validated batch 63 batch loss 0.648400426\n",
      "Validated batch 64 batch loss 0.495087415\n",
      "Validated batch 65 batch loss 0.51592648\n",
      "Validated batch 66 batch loss 0.625744104\n",
      "Validated batch 67 batch loss 0.598920882\n",
      "Validated batch 68 batch loss 0.534860849\n",
      "Validated batch 69 batch loss 0.611187339\n",
      "Validated batch 70 batch loss 0.528250456\n",
      "Validated batch 71 batch loss 0.506245315\n",
      "Validated batch 72 batch loss 0.602567434\n",
      "Validated batch 73 batch loss 0.569696307\n",
      "Validated batch 74 batch loss 0.579479456\n",
      "Validated batch 75 batch loss 0.585425496\n",
      "Validated batch 76 batch loss 0.570787311\n",
      "Validated batch 77 batch loss 0.50905782\n",
      "Validated batch 78 batch loss 0.584174275\n",
      "Validated batch 79 batch loss 0.617180467\n",
      "Validated batch 80 batch loss 0.63118428\n",
      "Validated batch 81 batch loss 0.508147895\n",
      "Validated batch 82 batch loss 0.67848444\n",
      "Validated batch 83 batch loss 0.573559761\n",
      "Validated batch 84 batch loss 0.454276443\n",
      "Validated batch 85 batch loss 0.579791903\n",
      "Validated batch 86 batch loss 0.660381854\n",
      "Validated batch 87 batch loss 0.545301318\n",
      "Validated batch 88 batch loss 0.564513624\n",
      "Validated batch 89 batch loss 0.630690217\n",
      "Validated batch 90 batch loss 0.455711305\n",
      "Validated batch 91 batch loss 0.59657824\n",
      "Validated batch 92 batch loss 0.581930757\n",
      "Validated batch 93 batch loss 0.549209952\n",
      "Validated batch 94 batch loss 0.570257187\n",
      "Validated batch 95 batch loss 0.589699686\n",
      "Validated batch 96 batch loss 0.579093\n",
      "Validated batch 97 batch loss 0.569670498\n",
      "Validated batch 98 batch loss 0.507556856\n",
      "Validated batch 99 batch loss 0.544738472\n",
      "Validated batch 100 batch loss 0.607115269\n",
      "Validated batch 101 batch loss 0.516550601\n",
      "Validated batch 102 batch loss 0.602276742\n",
      "Validated batch 103 batch loss 0.551017523\n",
      "Validated batch 104 batch loss 0.632994235\n",
      "Validated batch 105 batch loss 0.583478808\n",
      "Validated batch 106 batch loss 0.644209504\n",
      "Validated batch 107 batch loss 0.622083545\n",
      "Validated batch 108 batch loss 0.600300968\n",
      "Validated batch 109 batch loss 0.625761688\n",
      "Validated batch 110 batch loss 0.558534086\n",
      "Validated batch 111 batch loss 0.537979126\n",
      "Validated batch 112 batch loss 0.566346407\n",
      "Validated batch 113 batch loss 0.574829876\n",
      "Validated batch 114 batch loss 0.594768822\n",
      "Validated batch 115 batch loss 0.583773315\n",
      "Validated batch 116 batch loss 0.570269704\n",
      "Validated batch 117 batch loss 0.554928601\n",
      "Validated batch 118 batch loss 0.621532559\n",
      "Validated batch 119 batch loss 0.635573089\n",
      "Validated batch 120 batch loss 0.638853729\n",
      "Validated batch 121 batch loss 0.64612174\n",
      "Validated batch 122 batch loss 0.606332898\n",
      "Validated batch 123 batch loss 0.571724296\n",
      "Validated batch 124 batch loss 0.563163877\n",
      "Validated batch 125 batch loss 0.613785863\n",
      "Validated batch 126 batch loss 0.725699902\n",
      "Validated batch 127 batch loss 0.475122809\n",
      "Validated batch 128 batch loss 0.52236408\n",
      "Validated batch 129 batch loss 0.588337123\n",
      "Validated batch 130 batch loss 0.671396375\n",
      "Validated batch 131 batch loss 0.494166255\n",
      "Validated batch 132 batch loss 0.46572578\n",
      "Validated batch 133 batch loss 0.502632797\n",
      "Validated batch 134 batch loss 0.60207355\n",
      "Validated batch 135 batch loss 0.587070525\n",
      "Validated batch 136 batch loss 0.66288805\n",
      "Validated batch 137 batch loss 0.477191359\n",
      "Validated batch 138 batch loss 0.561660647\n",
      "Validated batch 139 batch loss 0.582113802\n",
      "Validated batch 140 batch loss 0.562775254\n",
      "Validated batch 141 batch loss 0.544351816\n",
      "Validated batch 142 batch loss 0.540516198\n",
      "Validated batch 143 batch loss 0.526197612\n",
      "Validated batch 144 batch loss 0.609793067\n",
      "Validated batch 145 batch loss 0.50479728\n",
      "Validated batch 146 batch loss 0.534499526\n",
      "Validated batch 147 batch loss 0.598761678\n",
      "Validated batch 148 batch loss 0.534125924\n",
      "Validated batch 149 batch loss 0.551234365\n",
      "Validated batch 150 batch loss 0.649289787\n",
      "Validated batch 151 batch loss 0.518446207\n",
      "Validated batch 152 batch loss 0.650091648\n",
      "Validated batch 153 batch loss 0.568843305\n",
      "Validated batch 154 batch loss 0.706883907\n",
      "Validated batch 155 batch loss 0.582345724\n",
      "Validated batch 156 batch loss 0.60705477\n",
      "Validated batch 157 batch loss 0.624480546\n",
      "Validated batch 158 batch loss 0.537982047\n",
      "Validated batch 159 batch loss 0.579720259\n",
      "Validated batch 160 batch loss 0.595198929\n",
      "Validated batch 161 batch loss 0.673649\n",
      "Validated batch 162 batch loss 0.646547496\n",
      "Validated batch 163 batch loss 0.615940154\n",
      "Validated batch 164 batch loss 0.58208704\n",
      "Validated batch 165 batch loss 0.645585656\n",
      "Validated batch 166 batch loss 0.619269\n",
      "Validated batch 167 batch loss 0.600238919\n",
      "Validated batch 168 batch loss 0.624269545\n",
      "Validated batch 169 batch loss 0.629841685\n",
      "Validated batch 170 batch loss 0.609045\n",
      "Validated batch 171 batch loss 0.675451159\n",
      "Validated batch 172 batch loss 0.562300503\n",
      "Validated batch 173 batch loss 0.605856121\n",
      "Validated batch 174 batch loss 0.464811325\n",
      "Validated batch 175 batch loss 0.560836136\n",
      "Validated batch 176 batch loss 0.598632276\n",
      "Validated batch 177 batch loss 0.651898563\n",
      "Validated batch 178 batch loss 0.592673302\n",
      "Validated batch 179 batch loss 0.656725526\n",
      "Validated batch 180 batch loss 0.555517197\n",
      "Validated batch 181 batch loss 0.517426193\n",
      "Validated batch 182 batch loss 0.578174531\n",
      "Validated batch 183 batch loss 0.640562892\n",
      "Validated batch 184 batch loss 0.518120468\n",
      "Validated batch 185 batch loss 0.549089789\n",
      "Validated batch 186 batch loss 0.575668275\n",
      "Validated batch 187 batch loss 0.635498047\n",
      "Validated batch 188 batch loss 0.572286785\n",
      "Validated batch 189 batch loss 0.578651309\n",
      "Validated batch 190 batch loss 0.556932807\n",
      "Validated batch 191 batch loss 0.479766726\n",
      "Validated batch 192 batch loss 0.550013661\n",
      "Validated batch 193 batch loss 0.582435429\n",
      "Validated batch 194 batch loss 0.485599548\n",
      "Validated batch 195 batch loss 0.636395872\n",
      "Validated batch 196 batch loss 0.617926359\n",
      "Validated batch 197 batch loss 0.540549755\n",
      "Validated batch 198 batch loss 0.505226851\n",
      "Validated batch 199 batch loss 0.589273334\n",
      "Validated batch 200 batch loss 0.417640507\n",
      "Validated batch 201 batch loss 0.515277863\n",
      "Validated batch 202 batch loss 0.556708634\n",
      "Validated batch 203 batch loss 0.551773131\n",
      "Validated batch 204 batch loss 0.530466378\n",
      "Validated batch 205 batch loss 0.539248943\n",
      "Validated batch 206 batch loss 0.466171682\n",
      "Validated batch 207 batch loss 0.609217346\n",
      "Validated batch 208 batch loss 0.537044525\n",
      "Validated batch 209 batch loss 0.532537\n",
      "Validated batch 210 batch loss 0.584297717\n",
      "Validated batch 211 batch loss 0.528830767\n",
      "Validated batch 212 batch loss 0.539031446\n",
      "Validated batch 213 batch loss 0.599296212\n",
      "Validated batch 214 batch loss 0.600059569\n",
      "Validated batch 215 batch loss 0.660618\n",
      "Validated batch 216 batch loss 0.618561208\n",
      "Validated batch 217 batch loss 0.532166839\n",
      "Validated batch 218 batch loss 0.497781694\n",
      "Validated batch 219 batch loss 0.660637498\n",
      "Validated batch 220 batch loss 0.573266268\n",
      "Validated batch 221 batch loss 0.525235\n",
      "Validated batch 222 batch loss 0.439654827\n",
      "Validated batch 223 batch loss 0.532807946\n",
      "Validated batch 224 batch loss 0.548113644\n",
      "Validated batch 225 batch loss 0.578304\n",
      "Validated batch 226 batch loss 0.481360972\n",
      "Validated batch 227 batch loss 0.528439343\n",
      "Validated batch 228 batch loss 0.567461848\n",
      "Validated batch 229 batch loss 0.649766803\n",
      "Validated batch 230 batch loss 0.630234778\n",
      "Validated batch 231 batch loss 0.59699446\n",
      "Validated batch 232 batch loss 0.47878778\n",
      "Validated batch 233 batch loss 0.540966153\n",
      "Validated batch 234 batch loss 0.584094346\n",
      "Validated batch 235 batch loss 0.622316718\n",
      "Validated batch 236 batch loss 0.578142405\n",
      "Validated batch 237 batch loss 0.533460736\n",
      "Validated batch 238 batch loss 0.514405847\n",
      "Validated batch 239 batch loss 0.581860542\n",
      "Validated batch 240 batch loss 0.608528197\n",
      "Validated batch 241 batch loss 0.702989578\n",
      "Validated batch 242 batch loss 0.65343231\n",
      "Validated batch 243 batch loss 0.532904\n",
      "Validated batch 244 batch loss 0.484275758\n",
      "Validated batch 245 batch loss 0.579779744\n",
      "Validated batch 246 batch loss 0.581928253\n",
      "Validated batch 247 batch loss 0.603292942\n",
      "Validated batch 248 batch loss 0.541377723\n",
      "Validated batch 249 batch loss 0.575785\n",
      "Validated batch 250 batch loss 0.608820438\n",
      "Validated batch 251 batch loss 0.611731708\n",
      "Validated batch 252 batch loss 0.573457778\n",
      "Validated batch 253 batch loss 0.532519937\n",
      "Validated batch 254 batch loss 0.557781935\n",
      "Validated batch 255 batch loss 0.382093608\n",
      "Validated batch 256 batch loss 0.523403466\n",
      "Validated batch 257 batch loss 0.601015449\n",
      "Validated batch 258 batch loss 0.553004861\n",
      "Validated batch 259 batch loss 0.552751601\n",
      "Validated batch 260 batch loss 0.509536922\n",
      "Validated batch 261 batch loss 0.564888716\n",
      "Validated batch 262 batch loss 0.560243905\n",
      "Validated batch 263 batch loss 0.651542425\n",
      "Validated batch 264 batch loss 0.585439563\n",
      "Validated batch 265 batch loss 0.540674269\n",
      "Validated batch 266 batch loss 0.474308372\n",
      "Validated batch 267 batch loss 0.598754168\n",
      "Validated batch 268 batch loss 0.572638631\n",
      "Validated batch 269 batch loss 0.625613332\n",
      "Validated batch 270 batch loss 0.619998276\n",
      "Validated batch 271 batch loss 0.572508216\n",
      "Validated batch 272 batch loss 0.59850651\n",
      "Validated batch 273 batch loss 0.580730736\n",
      "Validated batch 274 batch loss 0.571822941\n",
      "Validated batch 275 batch loss 0.514294565\n",
      "Validated batch 276 batch loss 0.507361233\n",
      "Validated batch 277 batch loss 0.58127892\n",
      "Validated batch 278 batch loss 0.600775898\n",
      "Validated batch 279 batch loss 0.639593\n",
      "Validated batch 280 batch loss 0.517722905\n",
      "Validated batch 281 batch loss 0.52357614\n",
      "Validated batch 282 batch loss 0.583815455\n",
      "Validated batch 283 batch loss 0.603210568\n",
      "Validated batch 284 batch loss 0.506077945\n",
      "Validated batch 285 batch loss 0.554463446\n",
      "Validated batch 286 batch loss 0.596478045\n",
      "Validated batch 287 batch loss 0.514345169\n",
      "Validated batch 288 batch loss 0.643255711\n",
      "Validated batch 289 batch loss 0.581199467\n",
      "Validated batch 290 batch loss 0.604711056\n",
      "Validated batch 291 batch loss 0.52203846\n",
      "Validated batch 292 batch loss 0.613921046\n",
      "Validated batch 293 batch loss 0.460619628\n",
      "Validated batch 294 batch loss 0.598360837\n",
      "Validated batch 295 batch loss 0.714175344\n",
      "Validated batch 296 batch loss 0.596378803\n",
      "Validated batch 297 batch loss 0.575204074\n",
      "Validated batch 298 batch loss 0.505879104\n",
      "Validated batch 299 batch loss 0.498879433\n",
      "Validated batch 300 batch loss 0.489399463\n",
      "Validated batch 301 batch loss 0.576246142\n",
      "Validated batch 302 batch loss 0.579091728\n",
      "Validated batch 303 batch loss 0.621674061\n",
      "Validated batch 304 batch loss 0.679986835\n",
      "Validated batch 305 batch loss 0.537211299\n",
      "Validated batch 306 batch loss 0.706984699\n",
      "Validated batch 307 batch loss 0.669157088\n",
      "Validated batch 308 batch loss 0.626087368\n",
      "Validated batch 309 batch loss 0.594309\n",
      "Validated batch 310 batch loss 0.508862138\n",
      "Validated batch 311 batch loss 0.668636382\n",
      "Validated batch 312 batch loss 0.638123453\n",
      "Validated batch 313 batch loss 0.592507422\n",
      "Validated batch 314 batch loss 0.571593285\n",
      "Validated batch 315 batch loss 0.705962777\n",
      "Validated batch 316 batch loss 0.552286327\n",
      "Validated batch 317 batch loss 0.583881199\n",
      "Validated batch 318 batch loss 0.609511733\n",
      "Validated batch 319 batch loss 0.486795068\n",
      "Validated batch 320 batch loss 0.420330584\n",
      "Validated batch 321 batch loss 0.511875808\n",
      "Validated batch 322 batch loss 0.566406488\n",
      "Validated batch 323 batch loss 0.629955411\n",
      "Validated batch 324 batch loss 0.578994334\n",
      "Validated batch 325 batch loss 0.597556114\n",
      "Validated batch 326 batch loss 0.541262567\n",
      "Validated batch 327 batch loss 0.607524753\n",
      "Validated batch 328 batch loss 0.562866867\n",
      "Validated batch 329 batch loss 0.562877595\n",
      "Validated batch 330 batch loss 0.56723696\n",
      "Validated batch 331 batch loss 0.589813411\n",
      "Validated batch 332 batch loss 0.63193208\n",
      "Validated batch 333 batch loss 0.66234237\n",
      "Validated batch 334 batch loss 0.703660786\n",
      "Validated batch 335 batch loss 0.575795352\n",
      "Validated batch 336 batch loss 0.53015542\n",
      "Validated batch 337 batch loss 0.572133422\n",
      "Validated batch 338 batch loss 0.607668638\n",
      "Validated batch 339 batch loss 0.581250906\n",
      "Validated batch 340 batch loss 0.556966841\n",
      "Validated batch 341 batch loss 0.659261882\n",
      "Validated batch 342 batch loss 0.576930821\n",
      "Validated batch 343 batch loss 0.593173802\n",
      "Validated batch 344 batch loss 0.571074545\n",
      "Validated batch 345 batch loss 0.607025\n",
      "Validated batch 346 batch loss 0.550040603\n",
      "Validated batch 347 batch loss 0.554614544\n",
      "Validated batch 348 batch loss 0.642878115\n",
      "Validated batch 349 batch loss 0.720525146\n",
      "Validated batch 350 batch loss 0.510256708\n",
      "Validated batch 351 batch loss 0.562230825\n",
      "Validated batch 352 batch loss 0.631734\n",
      "Validated batch 353 batch loss 0.575494289\n",
      "Validated batch 354 batch loss 0.619699836\n",
      "Validated batch 355 batch loss 0.605309129\n",
      "Validated batch 356 batch loss 0.611757755\n",
      "Validated batch 357 batch loss 0.632804751\n",
      "Validated batch 358 batch loss 0.503295839\n",
      "Validated batch 359 batch loss 0.49218604\n",
      "Validated batch 360 batch loss 0.607782125\n",
      "Validated batch 361 batch loss 0.599796116\n",
      "Validated batch 362 batch loss 0.601383924\n",
      "Validated batch 363 batch loss 0.523356915\n",
      "Validated batch 364 batch loss 0.576597333\n",
      "Validated batch 365 batch loss 0.627165496\n",
      "Validated batch 366 batch loss 0.52665\n",
      "Validated batch 367 batch loss 0.605931938\n",
      "Validated batch 368 batch loss 0.55130887\n",
      "Validated batch 369 batch loss 0.62942493\n",
      "Epoch 5 val loss 0.5759909749031067\n",
      "Model /aiffel/mpii/weights/shg_model-epoch-5-loss-0.5760.h5 saved.\n",
      "Start epoch 6 with learning rate 0.0007\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 0.578617215 epoch total loss 0.578617215\n",
      "Trained batch 2 batch loss 0.538636565 epoch total loss 0.55862689\n",
      "Trained batch 3 batch loss 0.577291489 epoch total loss 0.564848423\n",
      "Trained batch 4 batch loss 0.596089423 epoch total loss 0.572658658\n",
      "Trained batch 5 batch loss 0.630372941 epoch total loss 0.584201515\n",
      "Trained batch 6 batch loss 0.681683719 epoch total loss 0.600448549\n",
      "Trained batch 7 batch loss 0.490284711 epoch total loss 0.584710896\n",
      "Trained batch 8 batch loss 0.545467913 epoch total loss 0.579805493\n",
      "Trained batch 9 batch loss 0.648782253 epoch total loss 0.587469578\n",
      "Trained batch 10 batch loss 0.555500507 epoch total loss 0.584272683\n",
      "Trained batch 11 batch loss 0.627851367 epoch total loss 0.588234365\n",
      "Trained batch 12 batch loss 0.665174961 epoch total loss 0.594646096\n",
      "Trained batch 13 batch loss 0.505831242 epoch total loss 0.587814212\n",
      "Trained batch 14 batch loss 0.61089927 epoch total loss 0.589463115\n",
      "Trained batch 15 batch loss 0.505743265 epoch total loss 0.583881736\n",
      "Trained batch 16 batch loss 0.525521398 epoch total loss 0.58023423\n",
      "Trained batch 17 batch loss 0.472171396 epoch total loss 0.573877633\n",
      "Trained batch 18 batch loss 0.575912595 epoch total loss 0.573990643\n",
      "Trained batch 19 batch loss 0.635342062 epoch total loss 0.577219665\n",
      "Trained batch 20 batch loss 0.650409162 epoch total loss 0.580879092\n",
      "Trained batch 21 batch loss 0.683201373 epoch total loss 0.585751653\n",
      "Trained batch 22 batch loss 0.635760427 epoch total loss 0.588024735\n",
      "Trained batch 23 batch loss 0.609005332 epoch total loss 0.588936925\n",
      "Trained batch 24 batch loss 0.611522138 epoch total loss 0.589877963\n",
      "Trained batch 25 batch loss 0.519351482 epoch total loss 0.587056875\n",
      "Trained batch 26 batch loss 0.549099565 epoch total loss 0.585597\n",
      "Trained batch 27 batch loss 0.513641 epoch total loss 0.582932\n",
      "Trained batch 28 batch loss 0.525829494 epoch total loss 0.580892622\n",
      "Trained batch 29 batch loss 0.584663033 epoch total loss 0.58102268\n",
      "Trained batch 30 batch loss 0.591628969 epoch total loss 0.581376195\n",
      "Trained batch 31 batch loss 0.540846467 epoch total loss 0.580068767\n",
      "Trained batch 32 batch loss 0.548574209 epoch total loss 0.579084575\n",
      "Trained batch 33 batch loss 0.609800875 epoch total loss 0.580015361\n",
      "Trained batch 34 batch loss 0.662707 epoch total loss 0.582447469\n",
      "Trained batch 35 batch loss 0.620563 epoch total loss 0.583536446\n",
      "Trained batch 36 batch loss 0.643180549 epoch total loss 0.585193276\n",
      "Trained batch 37 batch loss 0.598638833 epoch total loss 0.585556626\n",
      "Trained batch 38 batch loss 0.52673012 epoch total loss 0.584008574\n",
      "Trained batch 39 batch loss 0.586928725 epoch total loss 0.584083438\n",
      "Trained batch 40 batch loss 0.441199213 epoch total loss 0.580511332\n",
      "Trained batch 41 batch loss 0.499252647 epoch total loss 0.578529418\n",
      "Trained batch 42 batch loss 0.470556617 epoch total loss 0.57595861\n",
      "Trained batch 43 batch loss 0.490272373 epoch total loss 0.573965907\n",
      "Trained batch 44 batch loss 0.581515491 epoch total loss 0.574137509\n",
      "Trained batch 45 batch loss 0.569878042 epoch total loss 0.574042857\n",
      "Trained batch 46 batch loss 0.524166167 epoch total loss 0.572958589\n",
      "Trained batch 47 batch loss 0.593464613 epoch total loss 0.573394835\n",
      "Trained batch 48 batch loss 0.532829225 epoch total loss 0.57254976\n",
      "Trained batch 49 batch loss 0.665092647 epoch total loss 0.574438393\n",
      "Trained batch 50 batch loss 0.682227135 epoch total loss 0.576594174\n",
      "Trained batch 51 batch loss 0.568827391 epoch total loss 0.576441884\n",
      "Trained batch 52 batch loss 0.522193491 epoch total loss 0.575398624\n",
      "Trained batch 53 batch loss 0.527443 epoch total loss 0.574493825\n",
      "Trained batch 54 batch loss 0.597362 epoch total loss 0.574917316\n",
      "Trained batch 55 batch loss 0.569195509 epoch total loss 0.574813247\n",
      "Trained batch 56 batch loss 0.509020209 epoch total loss 0.57363838\n",
      "Trained batch 57 batch loss 0.571636081 epoch total loss 0.573603272\n",
      "Trained batch 58 batch loss 0.570122063 epoch total loss 0.573543191\n",
      "Trained batch 59 batch loss 0.674248695 epoch total loss 0.575250089\n",
      "Trained batch 60 batch loss 0.6323663 epoch total loss 0.576202035\n",
      "Trained batch 61 batch loss 0.66953671 epoch total loss 0.577732086\n",
      "Trained batch 62 batch loss 0.489827126 epoch total loss 0.57631427\n",
      "Trained batch 63 batch loss 0.481234699 epoch total loss 0.574805081\n",
      "Trained batch 64 batch loss 0.444777131 epoch total loss 0.572773397\n",
      "Trained batch 65 batch loss 0.44186604 epoch total loss 0.570759475\n",
      "Trained batch 66 batch loss 0.492258966 epoch total loss 0.569570065\n",
      "Trained batch 67 batch loss 0.588688135 epoch total loss 0.569855392\n",
      "Trained batch 68 batch loss 0.579153955 epoch total loss 0.569992185\n",
      "Trained batch 69 batch loss 0.565853953 epoch total loss 0.569932163\n",
      "Trained batch 70 batch loss 0.528511524 epoch total loss 0.569340467\n",
      "Trained batch 71 batch loss 0.541407526 epoch total loss 0.568947077\n",
      "Trained batch 72 batch loss 0.516894698 epoch total loss 0.568224132\n",
      "Trained batch 73 batch loss 0.553997159 epoch total loss 0.568029225\n",
      "Trained batch 74 batch loss 0.548752427 epoch total loss 0.567768693\n",
      "Trained batch 75 batch loss 0.586109102 epoch total loss 0.568013251\n",
      "Trained batch 76 batch loss 0.607810915 epoch total loss 0.568536937\n",
      "Trained batch 77 batch loss 0.594883323 epoch total loss 0.568879068\n",
      "Trained batch 78 batch loss 0.678175509 epoch total loss 0.570280313\n",
      "Trained batch 79 batch loss 0.647332728 epoch total loss 0.571255624\n",
      "Trained batch 80 batch loss 0.557620168 epoch total loss 0.571085215\n",
      "Trained batch 81 batch loss 0.549093 epoch total loss 0.570813656\n",
      "Trained batch 82 batch loss 0.571106851 epoch total loss 0.570817232\n",
      "Trained batch 83 batch loss 0.704233289 epoch total loss 0.57242471\n",
      "Trained batch 84 batch loss 0.703622937 epoch total loss 0.57398659\n",
      "Trained batch 85 batch loss 0.714674413 epoch total loss 0.575641751\n",
      "Trained batch 86 batch loss 0.738109112 epoch total loss 0.577530921\n",
      "Trained batch 87 batch loss 0.671612859 epoch total loss 0.578612328\n",
      "Trained batch 88 batch loss 0.680093527 epoch total loss 0.579765499\n",
      "Trained batch 89 batch loss 0.727130294 epoch total loss 0.581421256\n",
      "Trained batch 90 batch loss 0.674902201 epoch total loss 0.58246\n",
      "Trained batch 91 batch loss 0.5389328 epoch total loss 0.581981659\n",
      "Trained batch 92 batch loss 0.563187122 epoch total loss 0.581777394\n",
      "Trained batch 93 batch loss 0.522392273 epoch total loss 0.58113879\n",
      "Trained batch 94 batch loss 0.475887179 epoch total loss 0.580019116\n",
      "Trained batch 95 batch loss 0.53355366 epoch total loss 0.57953\n",
      "Trained batch 96 batch loss 0.598637581 epoch total loss 0.579729\n",
      "Trained batch 97 batch loss 0.575931251 epoch total loss 0.57968992\n",
      "Trained batch 98 batch loss 0.681061447 epoch total loss 0.580724299\n",
      "Trained batch 99 batch loss 0.67690289 epoch total loss 0.581695795\n",
      "Trained batch 100 batch loss 0.637258649 epoch total loss 0.58225143\n",
      "Trained batch 101 batch loss 0.620210767 epoch total loss 0.582627296\n",
      "Trained batch 102 batch loss 0.668649256 epoch total loss 0.583470643\n",
      "Trained batch 103 batch loss 0.607462347 epoch total loss 0.583703578\n",
      "Trained batch 104 batch loss 0.615231693 epoch total loss 0.584006727\n",
      "Trained batch 105 batch loss 0.580826 epoch total loss 0.583976448\n",
      "Trained batch 106 batch loss 0.599014819 epoch total loss 0.584118307\n",
      "Trained batch 107 batch loss 0.596301854 epoch total loss 0.584232152\n",
      "Trained batch 108 batch loss 0.580657959 epoch total loss 0.584199071\n",
      "Trained batch 109 batch loss 0.583364248 epoch total loss 0.584191382\n",
      "Trained batch 110 batch loss 0.603085756 epoch total loss 0.584363163\n",
      "Trained batch 111 batch loss 0.584896624 epoch total loss 0.584368\n",
      "Trained batch 112 batch loss 0.606381536 epoch total loss 0.584564567\n",
      "Trained batch 113 batch loss 0.566528797 epoch total loss 0.584404945\n",
      "Trained batch 114 batch loss 0.555924296 epoch total loss 0.584155083\n",
      "Trained batch 115 batch loss 0.560698628 epoch total loss 0.583951116\n",
      "Trained batch 116 batch loss 0.589016616 epoch total loss 0.583994806\n",
      "Trained batch 117 batch loss 0.588937342 epoch total loss 0.584037066\n",
      "Trained batch 118 batch loss 0.550261378 epoch total loss 0.583750844\n",
      "Trained batch 119 batch loss 0.595556617 epoch total loss 0.583850086\n",
      "Trained batch 120 batch loss 0.626508951 epoch total loss 0.584205568\n",
      "Trained batch 121 batch loss 0.532679319 epoch total loss 0.583779693\n",
      "Trained batch 122 batch loss 0.614424 epoch total loss 0.584030926\n",
      "Trained batch 123 batch loss 0.612133622 epoch total loss 0.584259391\n",
      "Trained batch 124 batch loss 0.590664268 epoch total loss 0.584311068\n",
      "Trained batch 125 batch loss 0.59985435 epoch total loss 0.584435403\n",
      "Trained batch 126 batch loss 0.614809036 epoch total loss 0.584676445\n",
      "Trained batch 127 batch loss 0.634777188 epoch total loss 0.585070968\n",
      "Trained batch 128 batch loss 0.566458106 epoch total loss 0.584925592\n",
      "Trained batch 129 batch loss 0.562206686 epoch total loss 0.58474952\n",
      "Trained batch 130 batch loss 0.688491046 epoch total loss 0.585547507\n",
      "Trained batch 131 batch loss 0.51156646 epoch total loss 0.584982753\n",
      "Trained batch 132 batch loss 0.496328115 epoch total loss 0.584311187\n",
      "Trained batch 133 batch loss 0.572170794 epoch total loss 0.584219933\n",
      "Trained batch 134 batch loss 0.624493062 epoch total loss 0.584520459\n",
      "Trained batch 135 batch loss 0.810145 epoch total loss 0.586191773\n",
      "Trained batch 136 batch loss 0.644765854 epoch total loss 0.586622477\n",
      "Trained batch 137 batch loss 0.564252 epoch total loss 0.586459219\n",
      "Trained batch 138 batch loss 0.532130957 epoch total loss 0.586065471\n",
      "Trained batch 139 batch loss 0.54279 epoch total loss 0.585754156\n",
      "Trained batch 140 batch loss 0.509468794 epoch total loss 0.58520925\n",
      "Trained batch 141 batch loss 0.456987679 epoch total loss 0.584299862\n",
      "Trained batch 142 batch loss 0.45207867 epoch total loss 0.583368778\n",
      "Trained batch 143 batch loss 0.478470862 epoch total loss 0.582635224\n",
      "Trained batch 144 batch loss 0.558936656 epoch total loss 0.582470655\n",
      "Trained batch 145 batch loss 0.555367231 epoch total loss 0.582283676\n",
      "Trained batch 146 batch loss 0.550441742 epoch total loss 0.582065642\n",
      "Trained batch 147 batch loss 0.606402516 epoch total loss 0.582231164\n",
      "Trained batch 148 batch loss 0.587983549 epoch total loss 0.58227\n",
      "Trained batch 149 batch loss 0.546609521 epoch total loss 0.582030714\n",
      "Trained batch 150 batch loss 0.612282515 epoch total loss 0.582232356\n",
      "Trained batch 151 batch loss 0.551952958 epoch total loss 0.582031846\n",
      "Trained batch 152 batch loss 0.667023957 epoch total loss 0.582591\n",
      "Trained batch 153 batch loss 0.575554967 epoch total loss 0.582545042\n",
      "Trained batch 154 batch loss 0.586316586 epoch total loss 0.58256954\n",
      "Trained batch 155 batch loss 0.462355614 epoch total loss 0.581793964\n",
      "Trained batch 156 batch loss 0.554705143 epoch total loss 0.581620276\n",
      "Trained batch 157 batch loss 0.566809714 epoch total loss 0.581526\n",
      "Trained batch 158 batch loss 0.514295816 epoch total loss 0.581100464\n",
      "Trained batch 159 batch loss 0.602799177 epoch total loss 0.581236959\n",
      "Trained batch 160 batch loss 0.518268108 epoch total loss 0.580843329\n",
      "Trained batch 161 batch loss 0.597088516 epoch total loss 0.5809443\n",
      "Trained batch 162 batch loss 0.644703031 epoch total loss 0.581337869\n",
      "Trained batch 163 batch loss 0.646529615 epoch total loss 0.581737816\n",
      "Trained batch 164 batch loss 0.63113749 epoch total loss 0.582039\n",
      "Trained batch 165 batch loss 0.574127257 epoch total loss 0.581991076\n",
      "Trained batch 166 batch loss 0.536378086 epoch total loss 0.581716299\n",
      "Trained batch 167 batch loss 0.452147931 epoch total loss 0.580940425\n",
      "Trained batch 168 batch loss 0.477985799 epoch total loss 0.58032763\n",
      "Trained batch 169 batch loss 0.511282 epoch total loss 0.5799191\n",
      "Trained batch 170 batch loss 0.494956166 epoch total loss 0.579419315\n",
      "Trained batch 171 batch loss 0.49360013 epoch total loss 0.578917444\n",
      "Trained batch 172 batch loss 0.467767894 epoch total loss 0.57827121\n",
      "Trained batch 173 batch loss 0.655336261 epoch total loss 0.578716636\n",
      "Trained batch 174 batch loss 0.573317766 epoch total loss 0.578685641\n",
      "Trained batch 175 batch loss 0.595716 epoch total loss 0.578783\n",
      "Trained batch 176 batch loss 0.639042199 epoch total loss 0.579125345\n",
      "Trained batch 177 batch loss 0.697790384 epoch total loss 0.579795778\n",
      "Trained batch 178 batch loss 0.734463692 epoch total loss 0.580664754\n",
      "Trained batch 179 batch loss 0.681755126 epoch total loss 0.581229508\n",
      "Trained batch 180 batch loss 0.669426858 epoch total loss 0.581719458\n",
      "Trained batch 181 batch loss 0.626461089 epoch total loss 0.581966698\n",
      "Trained batch 182 batch loss 0.611570537 epoch total loss 0.582129359\n",
      "Trained batch 183 batch loss 0.689677715 epoch total loss 0.582717\n",
      "Trained batch 184 batch loss 0.646053195 epoch total loss 0.583061218\n",
      "Trained batch 185 batch loss 0.56172049 epoch total loss 0.582945883\n",
      "Trained batch 186 batch loss 0.646530151 epoch total loss 0.583287716\n",
      "Trained batch 187 batch loss 0.543334782 epoch total loss 0.583074093\n",
      "Trained batch 188 batch loss 0.629751384 epoch total loss 0.583322346\n",
      "Trained batch 189 batch loss 0.562487066 epoch total loss 0.583212137\n",
      "Trained batch 190 batch loss 0.616293192 epoch total loss 0.583386242\n",
      "Trained batch 191 batch loss 0.622548223 epoch total loss 0.583591282\n",
      "Trained batch 192 batch loss 0.598773777 epoch total loss 0.583670318\n",
      "Trained batch 193 batch loss 0.580283582 epoch total loss 0.583652794\n",
      "Trained batch 194 batch loss 0.595713377 epoch total loss 0.583714962\n",
      "Trained batch 195 batch loss 0.583665192 epoch total loss 0.583714664\n",
      "Trained batch 196 batch loss 0.583448648 epoch total loss 0.583713353\n",
      "Trained batch 197 batch loss 0.525975764 epoch total loss 0.583420277\n",
      "Trained batch 198 batch loss 0.592554152 epoch total loss 0.583466411\n",
      "Trained batch 199 batch loss 0.620433927 epoch total loss 0.583652198\n",
      "Trained batch 200 batch loss 0.535351872 epoch total loss 0.58341068\n",
      "Trained batch 201 batch loss 0.50064075 epoch total loss 0.582998872\n",
      "Trained batch 202 batch loss 0.590707898 epoch total loss 0.583037\n",
      "Trained batch 203 batch loss 0.493980974 epoch total loss 0.582598329\n",
      "Trained batch 204 batch loss 0.546152711 epoch total loss 0.582419693\n",
      "Trained batch 205 batch loss 0.482567191 epoch total loss 0.581932604\n",
      "Trained batch 206 batch loss 0.426228791 epoch total loss 0.581176758\n",
      "Trained batch 207 batch loss 0.435970336 epoch total loss 0.580475271\n",
      "Trained batch 208 batch loss 0.47202003 epoch total loss 0.579953909\n",
      "Trained batch 209 batch loss 0.609986365 epoch total loss 0.580097556\n",
      "Trained batch 210 batch loss 0.536995709 epoch total loss 0.579892337\n",
      "Trained batch 211 batch loss 0.679143071 epoch total loss 0.580362737\n",
      "Trained batch 212 batch loss 0.581281662 epoch total loss 0.580367088\n",
      "Trained batch 213 batch loss 0.560634196 epoch total loss 0.580274403\n",
      "Trained batch 214 batch loss 0.694422543 epoch total loss 0.580807805\n",
      "Trained batch 215 batch loss 0.663801432 epoch total loss 0.581193805\n",
      "Trained batch 216 batch loss 0.559334 epoch total loss 0.581092596\n",
      "Trained batch 217 batch loss 0.619436 epoch total loss 0.581269324\n",
      "Trained batch 218 batch loss 0.590365887 epoch total loss 0.581311047\n",
      "Trained batch 219 batch loss 0.57394886 epoch total loss 0.58127743\n",
      "Trained batch 220 batch loss 0.640427947 epoch total loss 0.581546307\n",
      "Trained batch 221 batch loss 0.523519039 epoch total loss 0.581283689\n",
      "Trained batch 222 batch loss 0.624875546 epoch total loss 0.581480086\n",
      "Trained batch 223 batch loss 0.59725 epoch total loss 0.581550777\n",
      "Trained batch 224 batch loss 0.589745581 epoch total loss 0.581587374\n",
      "Trained batch 225 batch loss 0.596987784 epoch total loss 0.5816558\n",
      "Trained batch 226 batch loss 0.651905477 epoch total loss 0.581966639\n",
      "Trained batch 227 batch loss 0.576708555 epoch total loss 0.581943452\n",
      "Trained batch 228 batch loss 0.651459 epoch total loss 0.58224833\n",
      "Trained batch 229 batch loss 0.576795757 epoch total loss 0.582224548\n",
      "Trained batch 230 batch loss 0.504507601 epoch total loss 0.581886649\n",
      "Trained batch 231 batch loss 0.502520144 epoch total loss 0.581543\n",
      "Trained batch 232 batch loss 0.601663232 epoch total loss 0.581629813\n",
      "Trained batch 233 batch loss 0.572825074 epoch total loss 0.581592\n",
      "Trained batch 234 batch loss 0.521765709 epoch total loss 0.581336319\n",
      "Trained batch 235 batch loss 0.465821147 epoch total loss 0.58084476\n",
      "Trained batch 236 batch loss 0.451570392 epoch total loss 0.580297\n",
      "Trained batch 237 batch loss 0.479934633 epoch total loss 0.579873502\n",
      "Trained batch 238 batch loss 0.48156 epoch total loss 0.579460442\n",
      "Trained batch 239 batch loss 0.527077258 epoch total loss 0.579241335\n",
      "Trained batch 240 batch loss 0.524987 epoch total loss 0.579015315\n",
      "Trained batch 241 batch loss 0.489577472 epoch total loss 0.578644156\n",
      "Trained batch 242 batch loss 0.527321577 epoch total loss 0.578432143\n",
      "Trained batch 243 batch loss 0.594182789 epoch total loss 0.578496933\n",
      "Trained batch 244 batch loss 0.55262959 epoch total loss 0.578390896\n",
      "Trained batch 245 batch loss 0.516900539 epoch total loss 0.578139961\n",
      "Trained batch 246 batch loss 0.564089894 epoch total loss 0.5780828\n",
      "Trained batch 247 batch loss 0.516339 epoch total loss 0.577832878\n",
      "Trained batch 248 batch loss 0.516438961 epoch total loss 0.57758528\n",
      "Trained batch 249 batch loss 0.595540822 epoch total loss 0.577657342\n",
      "Trained batch 250 batch loss 0.452106714 epoch total loss 0.577155173\n",
      "Trained batch 251 batch loss 0.511927128 epoch total loss 0.576895297\n",
      "Trained batch 252 batch loss 0.549332559 epoch total loss 0.576785922\n",
      "Trained batch 253 batch loss 0.584328 epoch total loss 0.576815784\n",
      "Trained batch 254 batch loss 0.66337347 epoch total loss 0.577156544\n",
      "Trained batch 255 batch loss 0.587147355 epoch total loss 0.577195704\n",
      "Trained batch 256 batch loss 0.614914596 epoch total loss 0.577343047\n",
      "Trained batch 257 batch loss 0.627687037 epoch total loss 0.577538908\n",
      "Trained batch 258 batch loss 0.571882665 epoch total loss 0.577517033\n",
      "Trained batch 259 batch loss 0.569317102 epoch total loss 0.577485383\n",
      "Trained batch 260 batch loss 0.540059 epoch total loss 0.577341378\n",
      "Trained batch 261 batch loss 0.437864661 epoch total loss 0.576807\n",
      "Trained batch 262 batch loss 0.449334383 epoch total loss 0.576320529\n",
      "Trained batch 263 batch loss 0.538293719 epoch total loss 0.576175928\n",
      "Trained batch 264 batch loss 0.413264304 epoch total loss 0.575558841\n",
      "Trained batch 265 batch loss 0.57175833 epoch total loss 0.575544536\n",
      "Trained batch 266 batch loss 0.530917466 epoch total loss 0.575376749\n",
      "Trained batch 267 batch loss 0.506915689 epoch total loss 0.57512033\n",
      "Trained batch 268 batch loss 0.539854467 epoch total loss 0.574988723\n",
      "Trained batch 269 batch loss 0.537133217 epoch total loss 0.574848056\n",
      "Trained batch 270 batch loss 0.495230824 epoch total loss 0.574553132\n",
      "Trained batch 271 batch loss 0.504793584 epoch total loss 0.5742957\n",
      "Trained batch 272 batch loss 0.456611216 epoch total loss 0.573863\n",
      "Trained batch 273 batch loss 0.477955937 epoch total loss 0.57351172\n",
      "Trained batch 274 batch loss 0.48428303 epoch total loss 0.57318604\n",
      "Trained batch 275 batch loss 0.568246 epoch total loss 0.573168099\n",
      "Trained batch 276 batch loss 0.543009639 epoch total loss 0.573058844\n",
      "Trained batch 277 batch loss 0.579934239 epoch total loss 0.573083699\n",
      "Trained batch 278 batch loss 0.592584 epoch total loss 0.573153853\n",
      "Trained batch 279 batch loss 0.544633925 epoch total loss 0.573051631\n",
      "Trained batch 280 batch loss 0.429042161 epoch total loss 0.572537363\n",
      "Trained batch 281 batch loss 0.611626506 epoch total loss 0.57267648\n",
      "Trained batch 282 batch loss 0.505440176 epoch total loss 0.572438061\n",
      "Trained batch 283 batch loss 0.59259814 epoch total loss 0.572509348\n",
      "Trained batch 284 batch loss 0.451654196 epoch total loss 0.572083831\n",
      "Trained batch 285 batch loss 0.51154381 epoch total loss 0.5718714\n",
      "Trained batch 286 batch loss 0.557913244 epoch total loss 0.571822584\n",
      "Trained batch 287 batch loss 0.525272667 epoch total loss 0.571660399\n",
      "Trained batch 288 batch loss 0.454961777 epoch total loss 0.571255147\n",
      "Trained batch 289 batch loss 0.521526754 epoch total loss 0.571083069\n",
      "Trained batch 290 batch loss 0.586071372 epoch total loss 0.571134806\n",
      "Trained batch 291 batch loss 0.565077841 epoch total loss 0.571114\n",
      "Trained batch 292 batch loss 0.597223163 epoch total loss 0.571203411\n",
      "Trained batch 293 batch loss 0.663870692 epoch total loss 0.571519673\n",
      "Trained batch 294 batch loss 0.660021603 epoch total loss 0.571820676\n",
      "Trained batch 295 batch loss 0.695802093 epoch total loss 0.572240949\n",
      "Trained batch 296 batch loss 0.66918546 epoch total loss 0.572568476\n",
      "Trained batch 297 batch loss 0.637604117 epoch total loss 0.572787464\n",
      "Trained batch 298 batch loss 0.618237793 epoch total loss 0.57294\n",
      "Trained batch 299 batch loss 0.514930248 epoch total loss 0.572745919\n",
      "Trained batch 300 batch loss 0.528518498 epoch total loss 0.572598517\n",
      "Trained batch 301 batch loss 0.57265 epoch total loss 0.572598696\n",
      "Trained batch 302 batch loss 0.454670131 epoch total loss 0.572208166\n",
      "Trained batch 303 batch loss 0.531009257 epoch total loss 0.572072208\n",
      "Trained batch 304 batch loss 0.64806807 epoch total loss 0.57232219\n",
      "Trained batch 305 batch loss 0.632524371 epoch total loss 0.5725196\n",
      "Trained batch 306 batch loss 0.594268084 epoch total loss 0.572590649\n",
      "Trained batch 307 batch loss 0.561209559 epoch total loss 0.572553575\n",
      "Trained batch 308 batch loss 0.523058355 epoch total loss 0.572392821\n",
      "Trained batch 309 batch loss 0.553727508 epoch total loss 0.572332442\n",
      "Trained batch 310 batch loss 0.591625154 epoch total loss 0.572394669\n",
      "Trained batch 311 batch loss 0.58333683 epoch total loss 0.572429895\n",
      "Trained batch 312 batch loss 0.575636804 epoch total loss 0.572440147\n",
      "Trained batch 313 batch loss 0.476179779 epoch total loss 0.572132647\n",
      "Trained batch 314 batch loss 0.547593415 epoch total loss 0.572054505\n",
      "Trained batch 315 batch loss 0.477246 epoch total loss 0.571753502\n",
      "Trained batch 316 batch loss 0.489080489 epoch total loss 0.571491897\n",
      "Trained batch 317 batch loss 0.563500106 epoch total loss 0.571466684\n",
      "Trained batch 318 batch loss 0.595791459 epoch total loss 0.571543157\n",
      "Trained batch 319 batch loss 0.54633832 epoch total loss 0.571464181\n",
      "Trained batch 320 batch loss 0.646659493 epoch total loss 0.571699142\n",
      "Trained batch 321 batch loss 0.560603917 epoch total loss 0.571664572\n",
      "Trained batch 322 batch loss 0.575646639 epoch total loss 0.57167697\n",
      "Trained batch 323 batch loss 0.473265767 epoch total loss 0.57137233\n",
      "Trained batch 324 batch loss 0.59432292 epoch total loss 0.571443141\n",
      "Trained batch 325 batch loss 0.530211449 epoch total loss 0.571316302\n",
      "Trained batch 326 batch loss 0.606452584 epoch total loss 0.571424067\n",
      "Trained batch 327 batch loss 0.639657378 epoch total loss 0.571632743\n",
      "Trained batch 328 batch loss 0.573042393 epoch total loss 0.571637034\n",
      "Trained batch 329 batch loss 0.601124823 epoch total loss 0.57172668\n",
      "Trained batch 330 batch loss 0.426318854 epoch total loss 0.571286\n",
      "Trained batch 331 batch loss 0.530361414 epoch total loss 0.571162403\n",
      "Trained batch 332 batch loss 0.50373286 epoch total loss 0.57095927\n",
      "Trained batch 333 batch loss 0.590668678 epoch total loss 0.571018457\n",
      "Trained batch 334 batch loss 0.6528669 epoch total loss 0.571263552\n",
      "Trained batch 335 batch loss 0.568391681 epoch total loss 0.571254969\n",
      "Trained batch 336 batch loss 0.525624931 epoch total loss 0.57111913\n",
      "Trained batch 337 batch loss 0.570448816 epoch total loss 0.571117163\n",
      "Trained batch 338 batch loss 0.647426665 epoch total loss 0.571342945\n",
      "Trained batch 339 batch loss 0.529689133 epoch total loss 0.57122004\n",
      "Trained batch 340 batch loss 0.640710473 epoch total loss 0.571424484\n",
      "Trained batch 341 batch loss 0.537002087 epoch total loss 0.571323514\n",
      "Trained batch 342 batch loss 0.437032938 epoch total loss 0.570930839\n",
      "Trained batch 343 batch loss 0.469403386 epoch total loss 0.570634842\n",
      "Trained batch 344 batch loss 0.543155193 epoch total loss 0.570555\n",
      "Trained batch 345 batch loss 0.595416605 epoch total loss 0.570627034\n",
      "Trained batch 346 batch loss 0.61251533 epoch total loss 0.570748091\n",
      "Trained batch 347 batch loss 0.622783065 epoch total loss 0.570898056\n",
      "Trained batch 348 batch loss 0.633364379 epoch total loss 0.571077526\n",
      "Trained batch 349 batch loss 0.675827742 epoch total loss 0.571377695\n",
      "Trained batch 350 batch loss 0.632620275 epoch total loss 0.571552634\n",
      "Trained batch 351 batch loss 0.540087521 epoch total loss 0.571463\n",
      "Trained batch 352 batch loss 0.587947905 epoch total loss 0.571509838\n",
      "Trained batch 353 batch loss 0.56340605 epoch total loss 0.57148689\n",
      "Trained batch 354 batch loss 0.591457844 epoch total loss 0.571543276\n",
      "Trained batch 355 batch loss 0.558506906 epoch total loss 0.57150656\n",
      "Trained batch 356 batch loss 0.617229104 epoch total loss 0.571635\n",
      "Trained batch 357 batch loss 0.563969493 epoch total loss 0.571613491\n",
      "Trained batch 358 batch loss 0.6037 epoch total loss 0.571703136\n",
      "Trained batch 359 batch loss 0.541260362 epoch total loss 0.571618319\n",
      "Trained batch 360 batch loss 0.545961201 epoch total loss 0.571547091\n",
      "Trained batch 361 batch loss 0.602924228 epoch total loss 0.571634\n",
      "Trained batch 362 batch loss 0.586800396 epoch total loss 0.571675897\n",
      "Trained batch 363 batch loss 0.59643358 epoch total loss 0.571744084\n",
      "Trained batch 364 batch loss 0.634107828 epoch total loss 0.571915448\n",
      "Trained batch 365 batch loss 0.61825043 epoch total loss 0.572042406\n",
      "Trained batch 366 batch loss 0.62532264 epoch total loss 0.57218796\n",
      "Trained batch 367 batch loss 0.636097074 epoch total loss 0.572362065\n",
      "Trained batch 368 batch loss 0.680384696 epoch total loss 0.572655618\n",
      "Trained batch 369 batch loss 0.613480031 epoch total loss 0.572766244\n",
      "Trained batch 370 batch loss 0.602655053 epoch total loss 0.572847068\n",
      "Trained batch 371 batch loss 0.572487354 epoch total loss 0.572846115\n",
      "Trained batch 372 batch loss 0.618506253 epoch total loss 0.572968841\n",
      "Trained batch 373 batch loss 0.553745568 epoch total loss 0.572917283\n",
      "Trained batch 374 batch loss 0.537735224 epoch total loss 0.572823226\n",
      "Trained batch 375 batch loss 0.541193187 epoch total loss 0.572738886\n",
      "Trained batch 376 batch loss 0.603249848 epoch total loss 0.572820067\n",
      "Trained batch 377 batch loss 0.567671835 epoch total loss 0.572806418\n",
      "Trained batch 378 batch loss 0.51024735 epoch total loss 0.572640896\n",
      "Trained batch 379 batch loss 0.557670534 epoch total loss 0.572601378\n",
      "Trained batch 380 batch loss 0.619823813 epoch total loss 0.572725654\n",
      "Trained batch 381 batch loss 0.563246429 epoch total loss 0.572700799\n",
      "Trained batch 382 batch loss 0.572065175 epoch total loss 0.57269913\n",
      "Trained batch 383 batch loss 0.628074765 epoch total loss 0.57284373\n",
      "Trained batch 384 batch loss 0.545620501 epoch total loss 0.572772861\n",
      "Trained batch 385 batch loss 0.565125465 epoch total loss 0.572753\n",
      "Trained batch 386 batch loss 0.593228042 epoch total loss 0.57280606\n",
      "Trained batch 387 batch loss 0.588865697 epoch total loss 0.572847545\n",
      "Trained batch 388 batch loss 0.547428846 epoch total loss 0.57278204\n",
      "Trained batch 389 batch loss 0.559541523 epoch total loss 0.572748\n",
      "Trained batch 390 batch loss 0.631126046 epoch total loss 0.572897673\n",
      "Trained batch 391 batch loss 0.657362 epoch total loss 0.57311368\n",
      "Trained batch 392 batch loss 0.59282589 epoch total loss 0.573163927\n",
      "Trained batch 393 batch loss 0.635541 epoch total loss 0.573322654\n",
      "Trained batch 394 batch loss 0.591343641 epoch total loss 0.57336843\n",
      "Trained batch 395 batch loss 0.585672259 epoch total loss 0.573399544\n",
      "Trained batch 396 batch loss 0.534399211 epoch total loss 0.573301077\n",
      "Trained batch 397 batch loss 0.573563159 epoch total loss 0.573301733\n",
      "Trained batch 398 batch loss 0.587762237 epoch total loss 0.573338091\n",
      "Trained batch 399 batch loss 0.639965475 epoch total loss 0.573505044\n",
      "Trained batch 400 batch loss 0.569243371 epoch total loss 0.573494434\n",
      "Trained batch 401 batch loss 0.543557823 epoch total loss 0.57341975\n",
      "Trained batch 402 batch loss 0.604834259 epoch total loss 0.573497891\n",
      "Trained batch 403 batch loss 0.634438634 epoch total loss 0.573649168\n",
      "Trained batch 404 batch loss 0.553870678 epoch total loss 0.573600173\n",
      "Trained batch 405 batch loss 0.598671079 epoch total loss 0.573662102\n",
      "Trained batch 406 batch loss 0.626651466 epoch total loss 0.573792577\n",
      "Trained batch 407 batch loss 0.604730964 epoch total loss 0.573868632\n",
      "Trained batch 408 batch loss 0.574657321 epoch total loss 0.57387054\n",
      "Trained batch 409 batch loss 0.572672486 epoch total loss 0.573867619\n",
      "Trained batch 410 batch loss 0.508644462 epoch total loss 0.573708594\n",
      "Trained batch 411 batch loss 0.558222532 epoch total loss 0.573670924\n",
      "Trained batch 412 batch loss 0.608811855 epoch total loss 0.573756218\n",
      "Trained batch 413 batch loss 0.617183 epoch total loss 0.573861361\n",
      "Trained batch 414 batch loss 0.601510584 epoch total loss 0.573928177\n",
      "Trained batch 415 batch loss 0.576495826 epoch total loss 0.573934376\n",
      "Trained batch 416 batch loss 0.647087812 epoch total loss 0.57411021\n",
      "Trained batch 417 batch loss 0.522357821 epoch total loss 0.573986113\n",
      "Trained batch 418 batch loss 0.493778467 epoch total loss 0.573794186\n",
      "Trained batch 419 batch loss 0.570045531 epoch total loss 0.573785305\n",
      "Trained batch 420 batch loss 0.475027829 epoch total loss 0.573550105\n",
      "Trained batch 421 batch loss 0.501673341 epoch total loss 0.573379397\n",
      "Trained batch 422 batch loss 0.446291268 epoch total loss 0.573078275\n",
      "Trained batch 423 batch loss 0.474614829 epoch total loss 0.572845459\n",
      "Trained batch 424 batch loss 0.502483904 epoch total loss 0.57267952\n",
      "Trained batch 425 batch loss 0.47694093 epoch total loss 0.572454274\n",
      "Trained batch 426 batch loss 0.495346338 epoch total loss 0.572273254\n",
      "Trained batch 427 batch loss 0.458671749 epoch total loss 0.572007239\n",
      "Trained batch 428 batch loss 0.638124526 epoch total loss 0.572161674\n",
      "Trained batch 429 batch loss 0.509632111 epoch total loss 0.572015941\n",
      "Trained batch 430 batch loss 0.519887924 epoch total loss 0.571894705\n",
      "Trained batch 431 batch loss 0.437866151 epoch total loss 0.571583748\n",
      "Trained batch 432 batch loss 0.476906955 epoch total loss 0.571364582\n",
      "Trained batch 433 batch loss 0.555247366 epoch total loss 0.571327388\n",
      "Trained batch 434 batch loss 0.50557071 epoch total loss 0.571175873\n",
      "Trained batch 435 batch loss 0.526860416 epoch total loss 0.571073949\n",
      "Trained batch 436 batch loss 0.533117294 epoch total loss 0.570986867\n",
      "Trained batch 437 batch loss 0.652974486 epoch total loss 0.571174502\n",
      "Trained batch 438 batch loss 0.580795169 epoch total loss 0.571196437\n",
      "Trained batch 439 batch loss 0.5408566 epoch total loss 0.571127355\n",
      "Trained batch 440 batch loss 0.416265488 epoch total loss 0.57077539\n",
      "Trained batch 441 batch loss 0.492993087 epoch total loss 0.570599\n",
      "Trained batch 442 batch loss 0.52521652 epoch total loss 0.57049638\n",
      "Trained batch 443 batch loss 0.522831857 epoch total loss 0.570388734\n",
      "Trained batch 444 batch loss 0.460719824 epoch total loss 0.570141792\n",
      "Trained batch 445 batch loss 0.484968066 epoch total loss 0.569950342\n",
      "Trained batch 446 batch loss 0.545062542 epoch total loss 0.569894552\n",
      "Trained batch 447 batch loss 0.52151221 epoch total loss 0.56978631\n",
      "Trained batch 448 batch loss 0.469508618 epoch total loss 0.569562495\n",
      "Trained batch 449 batch loss 0.435787052 epoch total loss 0.569264591\n",
      "Trained batch 450 batch loss 0.511652827 epoch total loss 0.56913656\n",
      "Trained batch 451 batch loss 0.525927842 epoch total loss 0.569040775\n",
      "Trained batch 452 batch loss 0.474148601 epoch total loss 0.568830848\n",
      "Trained batch 453 batch loss 0.528162062 epoch total loss 0.568741083\n",
      "Trained batch 454 batch loss 0.548344493 epoch total loss 0.568696141\n",
      "Trained batch 455 batch loss 0.56461072 epoch total loss 0.568687141\n",
      "Trained batch 456 batch loss 0.502238095 epoch total loss 0.568541408\n",
      "Trained batch 457 batch loss 0.509827077 epoch total loss 0.56841296\n",
      "Trained batch 458 batch loss 0.470156968 epoch total loss 0.568198383\n",
      "Trained batch 459 batch loss 0.508539379 epoch total loss 0.568068445\n",
      "Trained batch 460 batch loss 0.58369863 epoch total loss 0.568102419\n",
      "Trained batch 461 batch loss 0.563378692 epoch total loss 0.568092227\n",
      "Trained batch 462 batch loss 0.558210313 epoch total loss 0.568070769\n",
      "Trained batch 463 batch loss 0.58450973 epoch total loss 0.568106294\n",
      "Trained batch 464 batch loss 0.585481882 epoch total loss 0.568143725\n",
      "Trained batch 465 batch loss 0.593390048 epoch total loss 0.568197966\n",
      "Trained batch 466 batch loss 0.592299938 epoch total loss 0.568249702\n",
      "Trained batch 467 batch loss 0.636446536 epoch total loss 0.568395734\n",
      "Trained batch 468 batch loss 0.596608 epoch total loss 0.568456\n",
      "Trained batch 469 batch loss 0.472184747 epoch total loss 0.568250775\n",
      "Trained batch 470 batch loss 0.576072752 epoch total loss 0.568267405\n",
      "Trained batch 471 batch loss 0.555746734 epoch total loss 0.568240881\n",
      "Trained batch 472 batch loss 0.523811042 epoch total loss 0.568146706\n",
      "Trained batch 473 batch loss 0.581297338 epoch total loss 0.568174541\n",
      "Trained batch 474 batch loss 0.535740614 epoch total loss 0.568106115\n",
      "Trained batch 475 batch loss 0.514861345 epoch total loss 0.567994\n",
      "Trained batch 476 batch loss 0.482581288 epoch total loss 0.567814529\n",
      "Trained batch 477 batch loss 0.484102339 epoch total loss 0.567639053\n",
      "Trained batch 478 batch loss 0.677518129 epoch total loss 0.567868948\n",
      "Trained batch 479 batch loss 0.742151678 epoch total loss 0.568232775\n",
      "Trained batch 480 batch loss 0.709591 epoch total loss 0.568527281\n",
      "Trained batch 481 batch loss 0.731193662 epoch total loss 0.568865478\n",
      "Trained batch 482 batch loss 0.671214342 epoch total loss 0.56907779\n",
      "Trained batch 483 batch loss 0.559785545 epoch total loss 0.569058537\n",
      "Trained batch 484 batch loss 0.584963202 epoch total loss 0.569091439\n",
      "Trained batch 485 batch loss 0.464798 epoch total loss 0.568876386\n",
      "Trained batch 486 batch loss 0.581434309 epoch total loss 0.568902194\n",
      "Trained batch 487 batch loss 0.547333956 epoch total loss 0.568857908\n",
      "Trained batch 488 batch loss 0.497793287 epoch total loss 0.568712354\n",
      "Trained batch 489 batch loss 0.56561929 epoch total loss 0.568706\n",
      "Trained batch 490 batch loss 0.664103627 epoch total loss 0.568900645\n",
      "Trained batch 491 batch loss 0.614539146 epoch total loss 0.568993568\n",
      "Trained batch 492 batch loss 0.591616035 epoch total loss 0.569039583\n",
      "Trained batch 493 batch loss 0.597596 epoch total loss 0.569097519\n",
      "Trained batch 494 batch loss 0.59718591 epoch total loss 0.569154382\n",
      "Trained batch 495 batch loss 0.660471618 epoch total loss 0.569338858\n",
      "Trained batch 496 batch loss 0.576894581 epoch total loss 0.569354117\n",
      "Trained batch 497 batch loss 0.68405652 epoch total loss 0.569584846\n",
      "Trained batch 498 batch loss 0.648195 epoch total loss 0.569742739\n",
      "Trained batch 499 batch loss 0.677594185 epoch total loss 0.569958806\n",
      "Trained batch 500 batch loss 0.647392213 epoch total loss 0.570113719\n",
      "Trained batch 501 batch loss 0.653217673 epoch total loss 0.570279598\n",
      "Trained batch 502 batch loss 0.613098204 epoch total loss 0.570364892\n",
      "Trained batch 503 batch loss 0.58338058 epoch total loss 0.570390761\n",
      "Trained batch 504 batch loss 0.646179855 epoch total loss 0.570541143\n",
      "Trained batch 505 batch loss 0.567088366 epoch total loss 0.570534289\n",
      "Trained batch 506 batch loss 0.520163059 epoch total loss 0.570434749\n",
      "Trained batch 507 batch loss 0.513630509 epoch total loss 0.570322752\n",
      "Trained batch 508 batch loss 0.606659532 epoch total loss 0.570394278\n",
      "Trained batch 509 batch loss 0.581563592 epoch total loss 0.570416212\n",
      "Trained batch 510 batch loss 0.602897584 epoch total loss 0.570479929\n",
      "Trained batch 511 batch loss 0.595762968 epoch total loss 0.570529401\n",
      "Trained batch 512 batch loss 0.543327 epoch total loss 0.570476294\n",
      "Trained batch 513 batch loss 0.561891675 epoch total loss 0.570459545\n",
      "Trained batch 514 batch loss 0.606565356 epoch total loss 0.570529819\n",
      "Trained batch 515 batch loss 0.523429215 epoch total loss 0.570438385\n",
      "Trained batch 516 batch loss 0.499124676 epoch total loss 0.570300162\n",
      "Trained batch 517 batch loss 0.591060042 epoch total loss 0.570340276\n",
      "Trained batch 518 batch loss 0.550641537 epoch total loss 0.570302248\n",
      "Trained batch 519 batch loss 0.563798666 epoch total loss 0.570289731\n",
      "Trained batch 520 batch loss 0.540096819 epoch total loss 0.570231676\n",
      "Trained batch 521 batch loss 0.504342318 epoch total loss 0.570105195\n",
      "Trained batch 522 batch loss 0.54511857 epoch total loss 0.570057333\n",
      "Trained batch 523 batch loss 0.50355792 epoch total loss 0.569930196\n",
      "Trained batch 524 batch loss 0.514088869 epoch total loss 0.569823623\n",
      "Trained batch 525 batch loss 0.500404358 epoch total loss 0.56969142\n",
      "Trained batch 526 batch loss 0.546216667 epoch total loss 0.569646716\n",
      "Trained batch 527 batch loss 0.552125812 epoch total loss 0.569613516\n",
      "Trained batch 528 batch loss 0.599114776 epoch total loss 0.569669366\n",
      "Trained batch 529 batch loss 0.572895229 epoch total loss 0.569675505\n",
      "Trained batch 530 batch loss 0.614170909 epoch total loss 0.569759429\n",
      "Trained batch 531 batch loss 0.530231774 epoch total loss 0.569685042\n",
      "Trained batch 532 batch loss 0.6218642 epoch total loss 0.569783092\n",
      "Trained batch 533 batch loss 0.567029715 epoch total loss 0.569777906\n",
      "Trained batch 534 batch loss 0.606462657 epoch total loss 0.56984663\n",
      "Trained batch 535 batch loss 0.615368664 epoch total loss 0.569931686\n",
      "Trained batch 536 batch loss 0.657021523 epoch total loss 0.570094168\n",
      "Trained batch 537 batch loss 0.674198627 epoch total loss 0.570288\n",
      "Trained batch 538 batch loss 0.633986712 epoch total loss 0.570406377\n",
      "Trained batch 539 batch loss 0.650837541 epoch total loss 0.570555627\n",
      "Trained batch 540 batch loss 0.587022364 epoch total loss 0.570586145\n",
      "Trained batch 541 batch loss 0.660953045 epoch total loss 0.570753157\n",
      "Trained batch 542 batch loss 0.554448187 epoch total loss 0.570723057\n",
      "Trained batch 543 batch loss 0.537700415 epoch total loss 0.57066226\n",
      "Trained batch 544 batch loss 0.529948652 epoch total loss 0.570587397\n",
      "Trained batch 545 batch loss 0.490367472 epoch total loss 0.570440173\n",
      "Trained batch 546 batch loss 0.620880723 epoch total loss 0.57053256\n",
      "Trained batch 547 batch loss 0.593264401 epoch total loss 0.570574105\n",
      "Trained batch 548 batch loss 0.663370252 epoch total loss 0.570743442\n",
      "Trained batch 549 batch loss 0.689340472 epoch total loss 0.570959449\n",
      "Trained batch 550 batch loss 0.509443581 epoch total loss 0.570847571\n",
      "Trained batch 551 batch loss 0.582317114 epoch total loss 0.570868373\n",
      "Trained batch 552 batch loss 0.54262352 epoch total loss 0.570817173\n",
      "Trained batch 553 batch loss 0.482758522 epoch total loss 0.570657969\n",
      "Trained batch 554 batch loss 0.482201219 epoch total loss 0.570498288\n",
      "Trained batch 555 batch loss 0.557980716 epoch total loss 0.570475757\n",
      "Trained batch 556 batch loss 0.575772107 epoch total loss 0.570485294\n",
      "Trained batch 557 batch loss 0.498757184 epoch total loss 0.570356488\n",
      "Trained batch 558 batch loss 0.514884055 epoch total loss 0.570257127\n",
      "Trained batch 559 batch loss 0.484549046 epoch total loss 0.570103765\n",
      "Trained batch 560 batch loss 0.492927223 epoch total loss 0.569965959\n",
      "Trained batch 561 batch loss 0.576427221 epoch total loss 0.569977462\n",
      "Trained batch 562 batch loss 0.588689 epoch total loss 0.570010722\n",
      "Trained batch 563 batch loss 0.515637636 epoch total loss 0.569914162\n",
      "Trained batch 564 batch loss 0.632702291 epoch total loss 0.570025444\n",
      "Trained batch 565 batch loss 0.660747647 epoch total loss 0.570186\n",
      "Trained batch 566 batch loss 0.603352189 epoch total loss 0.57024461\n",
      "Trained batch 567 batch loss 0.560899138 epoch total loss 0.570228159\n",
      "Trained batch 568 batch loss 0.591816962 epoch total loss 0.570266187\n",
      "Trained batch 569 batch loss 0.573554873 epoch total loss 0.570271969\n",
      "Trained batch 570 batch loss 0.711771369 epoch total loss 0.570520163\n",
      "Trained batch 571 batch loss 0.529908 epoch total loss 0.570449054\n",
      "Trained batch 572 batch loss 0.513942897 epoch total loss 0.570350289\n",
      "Trained batch 573 batch loss 0.528892457 epoch total loss 0.570277929\n",
      "Trained batch 574 batch loss 0.476028889 epoch total loss 0.570113778\n",
      "Trained batch 575 batch loss 0.545967102 epoch total loss 0.570071757\n",
      "Trained batch 576 batch loss 0.531623721 epoch total loss 0.570005\n",
      "Trained batch 577 batch loss 0.552948177 epoch total loss 0.569975436\n",
      "Trained batch 578 batch loss 0.54079479 epoch total loss 0.569924951\n",
      "Trained batch 579 batch loss 0.628464103 epoch total loss 0.5700261\n",
      "Trained batch 580 batch loss 0.597951174 epoch total loss 0.57007426\n",
      "Trained batch 581 batch loss 0.555578649 epoch total loss 0.570049286\n",
      "Trained batch 582 batch loss 0.56844914 epoch total loss 0.570046544\n",
      "Trained batch 583 batch loss 0.548619866 epoch total loss 0.570009768\n",
      "Trained batch 584 batch loss 0.512865782 epoch total loss 0.569911957\n",
      "Trained batch 585 batch loss 0.554122806 epoch total loss 0.569884956\n",
      "Trained batch 586 batch loss 0.57530725 epoch total loss 0.569894195\n",
      "Trained batch 587 batch loss 0.513826311 epoch total loss 0.569798708\n",
      "Trained batch 588 batch loss 0.495602906 epoch total loss 0.569672525\n",
      "Trained batch 589 batch loss 0.553223372 epoch total loss 0.56964457\n",
      "Trained batch 590 batch loss 0.498330414 epoch total loss 0.569523692\n",
      "Trained batch 591 batch loss 0.576108217 epoch total loss 0.569534838\n",
      "Trained batch 592 batch loss 0.558770895 epoch total loss 0.569516659\n",
      "Trained batch 593 batch loss 0.550389886 epoch total loss 0.569484413\n",
      "Trained batch 594 batch loss 0.542369783 epoch total loss 0.569438756\n",
      "Trained batch 595 batch loss 0.498243809 epoch total loss 0.569319069\n",
      "Trained batch 596 batch loss 0.591194034 epoch total loss 0.569355726\n",
      "Trained batch 597 batch loss 0.614660919 epoch total loss 0.569431603\n",
      "Trained batch 598 batch loss 0.508003771 epoch total loss 0.569328904\n",
      "Trained batch 599 batch loss 0.578138232 epoch total loss 0.569343567\n",
      "Trained batch 600 batch loss 0.579994559 epoch total loss 0.569361329\n",
      "Trained batch 601 batch loss 0.550026417 epoch total loss 0.569329143\n",
      "Trained batch 602 batch loss 0.57059294 epoch total loss 0.569331229\n",
      "Trained batch 603 batch loss 0.574374557 epoch total loss 0.569339573\n",
      "Trained batch 604 batch loss 0.520280361 epoch total loss 0.569258392\n",
      "Trained batch 605 batch loss 0.509180784 epoch total loss 0.569159091\n",
      "Trained batch 606 batch loss 0.492775381 epoch total loss 0.569033\n",
      "Trained batch 607 batch loss 0.496878803 epoch total loss 0.568914175\n",
      "Trained batch 608 batch loss 0.51362294 epoch total loss 0.568823218\n",
      "Trained batch 609 batch loss 0.530302 epoch total loss 0.56876\n",
      "Trained batch 610 batch loss 0.539628267 epoch total loss 0.568712234\n",
      "Trained batch 611 batch loss 0.509433568 epoch total loss 0.568615198\n",
      "Trained batch 612 batch loss 0.549079835 epoch total loss 0.56858325\n",
      "Trained batch 613 batch loss 0.611078799 epoch total loss 0.56865263\n",
      "Trained batch 614 batch loss 0.634186149 epoch total loss 0.568759322\n",
      "Trained batch 615 batch loss 0.526830196 epoch total loss 0.568691134\n",
      "Trained batch 616 batch loss 0.47774148 epoch total loss 0.568543494\n",
      "Trained batch 617 batch loss 0.485994071 epoch total loss 0.568409741\n",
      "Trained batch 618 batch loss 0.578442752 epoch total loss 0.568425953\n",
      "Trained batch 619 batch loss 0.520745 epoch total loss 0.568348944\n",
      "Trained batch 620 batch loss 0.517406166 epoch total loss 0.568266749\n",
      "Trained batch 621 batch loss 0.530207336 epoch total loss 0.568205476\n",
      "Trained batch 622 batch loss 0.52368468 epoch total loss 0.568133891\n",
      "Trained batch 623 batch loss 0.557816386 epoch total loss 0.568117321\n",
      "Trained batch 624 batch loss 0.537890553 epoch total loss 0.568068922\n",
      "Trained batch 625 batch loss 0.560106099 epoch total loss 0.568056226\n",
      "Trained batch 626 batch loss 0.522624195 epoch total loss 0.567983627\n",
      "Trained batch 627 batch loss 0.530683815 epoch total loss 0.567924082\n",
      "Trained batch 628 batch loss 0.56738615 epoch total loss 0.567923248\n",
      "Trained batch 629 batch loss 0.582583129 epoch total loss 0.567946553\n",
      "Trained batch 630 batch loss 0.575612545 epoch total loss 0.567958713\n",
      "Trained batch 631 batch loss 0.45381096 epoch total loss 0.567777812\n",
      "Trained batch 632 batch loss 0.552062273 epoch total loss 0.567752957\n",
      "Trained batch 633 batch loss 0.614047408 epoch total loss 0.567826092\n",
      "Trained batch 634 batch loss 0.556203961 epoch total loss 0.567807734\n",
      "Trained batch 635 batch loss 0.55312264 epoch total loss 0.567784607\n",
      "Trained batch 636 batch loss 0.466208369 epoch total loss 0.567624927\n",
      "Trained batch 637 batch loss 0.581781864 epoch total loss 0.567647159\n",
      "Trained batch 638 batch loss 0.556099355 epoch total loss 0.567629039\n",
      "Trained batch 639 batch loss 0.586260438 epoch total loss 0.567658246\n",
      "Trained batch 640 batch loss 0.652372718 epoch total loss 0.567790627\n",
      "Trained batch 641 batch loss 0.614104271 epoch total loss 0.567862868\n",
      "Trained batch 642 batch loss 0.579889655 epoch total loss 0.567881584\n",
      "Trained batch 643 batch loss 0.503244758 epoch total loss 0.567781031\n",
      "Trained batch 644 batch loss 0.580573738 epoch total loss 0.567800939\n",
      "Trained batch 645 batch loss 0.641721427 epoch total loss 0.567915499\n",
      "Trained batch 646 batch loss 0.544691682 epoch total loss 0.567879558\n",
      "Trained batch 647 batch loss 0.482755423 epoch total loss 0.567748\n",
      "Trained batch 648 batch loss 0.519282162 epoch total loss 0.567673206\n",
      "Trained batch 649 batch loss 0.547303736 epoch total loss 0.567641795\n",
      "Trained batch 650 batch loss 0.570981443 epoch total loss 0.567647\n",
      "Trained batch 651 batch loss 0.558065236 epoch total loss 0.567632258\n",
      "Trained batch 652 batch loss 0.544561863 epoch total loss 0.567596853\n",
      "Trained batch 653 batch loss 0.595348358 epoch total loss 0.567639351\n",
      "Trained batch 654 batch loss 0.623947442 epoch total loss 0.56772548\n",
      "Trained batch 655 batch loss 0.633890331 epoch total loss 0.56782645\n",
      "Trained batch 656 batch loss 0.623894036 epoch total loss 0.567911923\n",
      "Trained batch 657 batch loss 0.569116116 epoch total loss 0.567913771\n",
      "Trained batch 658 batch loss 0.558792949 epoch total loss 0.567899942\n",
      "Trained batch 659 batch loss 0.53235352 epoch total loss 0.567846\n",
      "Trained batch 660 batch loss 0.678264201 epoch total loss 0.568013251\n",
      "Trained batch 661 batch loss 0.610784888 epoch total loss 0.568078\n",
      "Trained batch 662 batch loss 0.570049644 epoch total loss 0.568080962\n",
      "Trained batch 663 batch loss 0.593673766 epoch total loss 0.568119586\n",
      "Trained batch 664 batch loss 0.528383493 epoch total loss 0.568059742\n",
      "Trained batch 665 batch loss 0.564534962 epoch total loss 0.568054438\n",
      "Trained batch 666 batch loss 0.635021806 epoch total loss 0.568155\n",
      "Trained batch 667 batch loss 0.63206017 epoch total loss 0.568250775\n",
      "Trained batch 668 batch loss 0.63876915 epoch total loss 0.568356335\n",
      "Trained batch 669 batch loss 0.6889503 epoch total loss 0.56853658\n",
      "Trained batch 670 batch loss 0.681025684 epoch total loss 0.568704486\n",
      "Trained batch 671 batch loss 0.673961878 epoch total loss 0.568861365\n",
      "Trained batch 672 batch loss 0.523818374 epoch total loss 0.56879431\n",
      "Trained batch 673 batch loss 0.518129885 epoch total loss 0.568719\n",
      "Trained batch 674 batch loss 0.584561467 epoch total loss 0.568742514\n",
      "Trained batch 675 batch loss 0.596047878 epoch total loss 0.568783\n",
      "Trained batch 676 batch loss 0.635297418 epoch total loss 0.568881333\n",
      "Trained batch 677 batch loss 0.544897437 epoch total loss 0.568845928\n",
      "Trained batch 678 batch loss 0.607017398 epoch total loss 0.568902194\n",
      "Trained batch 679 batch loss 0.66535604 epoch total loss 0.569044232\n",
      "Trained batch 680 batch loss 0.574224293 epoch total loss 0.569051862\n",
      "Trained batch 681 batch loss 0.521509409 epoch total loss 0.568982065\n",
      "Trained batch 682 batch loss 0.594608545 epoch total loss 0.569019616\n",
      "Trained batch 683 batch loss 0.558552265 epoch total loss 0.569004297\n",
      "Trained batch 684 batch loss 0.585850298 epoch total loss 0.569028914\n",
      "Trained batch 685 batch loss 0.596617281 epoch total loss 0.569069207\n",
      "Trained batch 686 batch loss 0.584707081 epoch total loss 0.569092035\n",
      "Trained batch 687 batch loss 0.614897728 epoch total loss 0.569158673\n",
      "Trained batch 688 batch loss 0.485854417 epoch total loss 0.569037616\n",
      "Trained batch 689 batch loss 0.624762356 epoch total loss 0.56911844\n",
      "Trained batch 690 batch loss 0.598558128 epoch total loss 0.569161177\n",
      "Trained batch 691 batch loss 0.611187935 epoch total loss 0.569222\n",
      "Trained batch 692 batch loss 0.599622965 epoch total loss 0.569265842\n",
      "Trained batch 693 batch loss 0.595731378 epoch total loss 0.569304049\n",
      "Trained batch 694 batch loss 0.451122403 epoch total loss 0.569133759\n",
      "Trained batch 695 batch loss 0.507471204 epoch total loss 0.569045\n",
      "Trained batch 696 batch loss 0.554228663 epoch total loss 0.569023728\n",
      "Trained batch 697 batch loss 0.455524027 epoch total loss 0.568860948\n",
      "Trained batch 698 batch loss 0.591162384 epoch total loss 0.568892837\n",
      "Trained batch 699 batch loss 0.557192 epoch total loss 0.568876147\n",
      "Trained batch 700 batch loss 0.577368617 epoch total loss 0.568888247\n",
      "Trained batch 701 batch loss 0.664883614 epoch total loss 0.569025218\n",
      "Trained batch 702 batch loss 0.589329123 epoch total loss 0.569054127\n",
      "Trained batch 703 batch loss 0.554560959 epoch total loss 0.569033504\n",
      "Trained batch 704 batch loss 0.531480312 epoch total loss 0.568980157\n",
      "Trained batch 705 batch loss 0.647507787 epoch total loss 0.569091558\n",
      "Trained batch 706 batch loss 0.57408154 epoch total loss 0.569098651\n",
      "Trained batch 707 batch loss 0.596033633 epoch total loss 0.569136798\n",
      "Trained batch 708 batch loss 0.483446836 epoch total loss 0.569015741\n",
      "Trained batch 709 batch loss 0.529491842 epoch total loss 0.56896\n",
      "Trained batch 710 batch loss 0.550970316 epoch total loss 0.568934679\n",
      "Trained batch 711 batch loss 0.607245445 epoch total loss 0.568988502\n",
      "Trained batch 712 batch loss 0.588831663 epoch total loss 0.569016397\n",
      "Trained batch 713 batch loss 0.552242339 epoch total loss 0.568992853\n",
      "Trained batch 714 batch loss 0.600294173 epoch total loss 0.569036722\n",
      "Trained batch 715 batch loss 0.481334955 epoch total loss 0.568914056\n",
      "Trained batch 716 batch loss 0.537326217 epoch total loss 0.568869889\n",
      "Trained batch 717 batch loss 0.488812298 epoch total loss 0.568758249\n",
      "Trained batch 718 batch loss 0.490801901 epoch total loss 0.56864965\n",
      "Trained batch 719 batch loss 0.539689302 epoch total loss 0.568609416\n",
      "Trained batch 720 batch loss 0.578568935 epoch total loss 0.568623245\n",
      "Trained batch 721 batch loss 0.617920756 epoch total loss 0.568691611\n",
      "Trained batch 722 batch loss 0.472376406 epoch total loss 0.568558276\n",
      "Trained batch 723 batch loss 0.613489211 epoch total loss 0.568620384\n",
      "Trained batch 724 batch loss 0.565040171 epoch total loss 0.568615437\n",
      "Trained batch 725 batch loss 0.632192194 epoch total loss 0.568703175\n",
      "Trained batch 726 batch loss 0.560032189 epoch total loss 0.568691194\n",
      "Trained batch 727 batch loss 0.551730752 epoch total loss 0.568667889\n",
      "Trained batch 728 batch loss 0.515697896 epoch total loss 0.568595111\n",
      "Trained batch 729 batch loss 0.454618871 epoch total loss 0.568438768\n",
      "Trained batch 730 batch loss 0.447277367 epoch total loss 0.568272769\n",
      "Trained batch 731 batch loss 0.44708997 epoch total loss 0.568106949\n",
      "Trained batch 732 batch loss 0.437383682 epoch total loss 0.567928374\n",
      "Trained batch 733 batch loss 0.422924727 epoch total loss 0.567730546\n",
      "Trained batch 734 batch loss 0.403874487 epoch total loss 0.567507267\n",
      "Trained batch 735 batch loss 0.468592137 epoch total loss 0.567372739\n",
      "Trained batch 736 batch loss 0.487359345 epoch total loss 0.567264\n",
      "Trained batch 737 batch loss 0.463009268 epoch total loss 0.567122579\n",
      "Trained batch 738 batch loss 0.728792131 epoch total loss 0.567341626\n",
      "Trained batch 739 batch loss 0.726254284 epoch total loss 0.567556679\n",
      "Trained batch 740 batch loss 0.665664494 epoch total loss 0.56768924\n",
      "Trained batch 741 batch loss 0.536576211 epoch total loss 0.567647278\n",
      "Trained batch 742 batch loss 0.603360832 epoch total loss 0.567695379\n",
      "Trained batch 743 batch loss 0.68818295 epoch total loss 0.567857563\n",
      "Trained batch 744 batch loss 0.538675368 epoch total loss 0.567818284\n",
      "Trained batch 745 batch loss 0.550406 epoch total loss 0.567794919\n",
      "Trained batch 746 batch loss 0.485515296 epoch total loss 0.56768465\n",
      "Trained batch 747 batch loss 0.58617413 epoch total loss 0.567709386\n",
      "Trained batch 748 batch loss 0.575661659 epoch total loss 0.56772\n",
      "Trained batch 749 batch loss 0.637902081 epoch total loss 0.567813754\n",
      "Trained batch 750 batch loss 0.577367842 epoch total loss 0.56782645\n",
      "Trained batch 751 batch loss 0.602376 epoch total loss 0.567872465\n",
      "Trained batch 752 batch loss 0.484007925 epoch total loss 0.567760944\n",
      "Trained batch 753 batch loss 0.501961 epoch total loss 0.567673564\n",
      "Trained batch 754 batch loss 0.462806672 epoch total loss 0.567534447\n",
      "Trained batch 755 batch loss 0.456548572 epoch total loss 0.567387462\n",
      "Trained batch 756 batch loss 0.451579213 epoch total loss 0.567234278\n",
      "Trained batch 757 batch loss 0.509889901 epoch total loss 0.56715852\n",
      "Trained batch 758 batch loss 0.541299 epoch total loss 0.567124367\n",
      "Trained batch 759 batch loss 0.526838303 epoch total loss 0.567071259\n",
      "Trained batch 760 batch loss 0.594489217 epoch total loss 0.567107379\n",
      "Trained batch 761 batch loss 0.464909583 epoch total loss 0.566973031\n",
      "Trained batch 762 batch loss 0.450789779 epoch total loss 0.566820562\n",
      "Trained batch 763 batch loss 0.518889248 epoch total loss 0.566757739\n",
      "Trained batch 764 batch loss 0.531358063 epoch total loss 0.566711426\n",
      "Trained batch 765 batch loss 0.551305056 epoch total loss 0.566691279\n",
      "Trained batch 766 batch loss 0.539255857 epoch total loss 0.566655457\n",
      "Trained batch 767 batch loss 0.557828069 epoch total loss 0.566643953\n",
      "Trained batch 768 batch loss 0.583797634 epoch total loss 0.566666305\n",
      "Trained batch 769 batch loss 0.638141036 epoch total loss 0.566759229\n",
      "Trained batch 770 batch loss 0.511121273 epoch total loss 0.566687\n",
      "Trained batch 771 batch loss 0.544994 epoch total loss 0.566658795\n",
      "Trained batch 772 batch loss 0.54992 epoch total loss 0.566637158\n",
      "Trained batch 773 batch loss 0.491499931 epoch total loss 0.566539943\n",
      "Trained batch 774 batch loss 0.559323549 epoch total loss 0.566530585\n",
      "Trained batch 775 batch loss 0.606075406 epoch total loss 0.566581666\n",
      "Trained batch 776 batch loss 0.626545072 epoch total loss 0.566658914\n",
      "Trained batch 777 batch loss 0.69781816 epoch total loss 0.566827714\n",
      "Trained batch 778 batch loss 0.573864222 epoch total loss 0.566836774\n",
      "Trained batch 779 batch loss 0.569757462 epoch total loss 0.566840529\n",
      "Trained batch 780 batch loss 0.567211509 epoch total loss 0.566840947\n",
      "Trained batch 781 batch loss 0.603234529 epoch total loss 0.566887558\n",
      "Trained batch 782 batch loss 0.652263165 epoch total loss 0.566996753\n",
      "Trained batch 783 batch loss 0.706217408 epoch total loss 0.567174554\n",
      "Trained batch 784 batch loss 0.593479276 epoch total loss 0.567208052\n",
      "Trained batch 785 batch loss 0.637717783 epoch total loss 0.567297935\n",
      "Trained batch 786 batch loss 0.63588649 epoch total loss 0.567385197\n",
      "Trained batch 787 batch loss 0.533988 epoch total loss 0.567342758\n",
      "Trained batch 788 batch loss 0.597633958 epoch total loss 0.567381203\n",
      "Trained batch 789 batch loss 0.606404245 epoch total loss 0.567430675\n",
      "Trained batch 790 batch loss 0.574043155 epoch total loss 0.567439\n",
      "Trained batch 791 batch loss 0.651462436 epoch total loss 0.567545235\n",
      "Trained batch 792 batch loss 0.629456639 epoch total loss 0.567623436\n",
      "Trained batch 793 batch loss 0.564931393 epoch total loss 0.567620039\n",
      "Trained batch 794 batch loss 0.558320165 epoch total loss 0.567608297\n",
      "Trained batch 795 batch loss 0.465988159 epoch total loss 0.567480445\n",
      "Trained batch 796 batch loss 0.573641896 epoch total loss 0.567488194\n",
      "Trained batch 797 batch loss 0.583508193 epoch total loss 0.56750828\n",
      "Trained batch 798 batch loss 0.496521831 epoch total loss 0.56741935\n",
      "Trained batch 799 batch loss 0.478435308 epoch total loss 0.567307949\n",
      "Trained batch 800 batch loss 0.403879 epoch total loss 0.567103624\n",
      "Trained batch 801 batch loss 0.560993254 epoch total loss 0.567096055\n",
      "Trained batch 802 batch loss 0.431635857 epoch total loss 0.566927135\n",
      "Trained batch 803 batch loss 0.479570597 epoch total loss 0.566818357\n",
      "Trained batch 804 batch loss 0.507974 epoch total loss 0.566745162\n",
      "Trained batch 805 batch loss 0.532693923 epoch total loss 0.566702843\n",
      "Trained batch 806 batch loss 0.450741112 epoch total loss 0.566559\n",
      "Trained batch 807 batch loss 0.456235439 epoch total loss 0.566422284\n",
      "Trained batch 808 batch loss 0.378322959 epoch total loss 0.566189468\n",
      "Trained batch 809 batch loss 0.384447277 epoch total loss 0.565964878\n",
      "Trained batch 810 batch loss 0.427323461 epoch total loss 0.565793693\n",
      "Trained batch 811 batch loss 0.444949865 epoch total loss 0.565644681\n",
      "Trained batch 812 batch loss 0.415820539 epoch total loss 0.565460205\n",
      "Trained batch 813 batch loss 0.387790561 epoch total loss 0.565241694\n",
      "Trained batch 814 batch loss 0.473801851 epoch total loss 0.56512934\n",
      "Trained batch 815 batch loss 0.552906513 epoch total loss 0.565114379\n",
      "Trained batch 816 batch loss 0.58595413 epoch total loss 0.565139949\n",
      "Trained batch 817 batch loss 0.592111 epoch total loss 0.565172911\n",
      "Trained batch 818 batch loss 0.523969173 epoch total loss 0.565122545\n",
      "Trained batch 819 batch loss 0.588090599 epoch total loss 0.565150619\n",
      "Trained batch 820 batch loss 0.467299432 epoch total loss 0.56503123\n",
      "Trained batch 821 batch loss 0.511716 epoch total loss 0.564966321\n",
      "Trained batch 822 batch loss 0.577031 epoch total loss 0.564981\n",
      "Trained batch 823 batch loss 0.498465538 epoch total loss 0.56490016\n",
      "Trained batch 824 batch loss 0.588636041 epoch total loss 0.564928949\n",
      "Trained batch 825 batch loss 0.605908334 epoch total loss 0.5649786\n",
      "Trained batch 826 batch loss 0.577010751 epoch total loss 0.564993143\n",
      "Trained batch 827 batch loss 0.592754424 epoch total loss 0.5650267\n",
      "Trained batch 828 batch loss 0.610241592 epoch total loss 0.565081298\n",
      "Trained batch 829 batch loss 0.531527698 epoch total loss 0.565040827\n",
      "Trained batch 830 batch loss 0.606824815 epoch total loss 0.565091193\n",
      "Trained batch 831 batch loss 0.497057974 epoch total loss 0.565009296\n",
      "Trained batch 832 batch loss 0.612400711 epoch total loss 0.565066278\n",
      "Trained batch 833 batch loss 0.646916568 epoch total loss 0.565164506\n",
      "Trained batch 834 batch loss 0.683840752 epoch total loss 0.565306783\n",
      "Trained batch 835 batch loss 0.622280478 epoch total loss 0.56537503\n",
      "Trained batch 836 batch loss 0.684265733 epoch total loss 0.565517247\n",
      "Trained batch 837 batch loss 0.650117 epoch total loss 0.565618336\n",
      "Trained batch 838 batch loss 0.593476236 epoch total loss 0.565651596\n",
      "Trained batch 839 batch loss 0.530561805 epoch total loss 0.565609753\n",
      "Trained batch 840 batch loss 0.549835742 epoch total loss 0.565591\n",
      "Trained batch 841 batch loss 0.547832489 epoch total loss 0.565569818\n",
      "Trained batch 842 batch loss 0.547090173 epoch total loss 0.565547884\n",
      "Trained batch 843 batch loss 0.528329372 epoch total loss 0.565503716\n",
      "Trained batch 844 batch loss 0.508540332 epoch total loss 0.565436244\n",
      "Trained batch 845 batch loss 0.55747205 epoch total loss 0.565426826\n",
      "Trained batch 846 batch loss 0.654611826 epoch total loss 0.565532207\n",
      "Trained batch 847 batch loss 0.626382768 epoch total loss 0.565604031\n",
      "Trained batch 848 batch loss 0.602296948 epoch total loss 0.565647304\n",
      "Trained batch 849 batch loss 0.642037928 epoch total loss 0.565737247\n",
      "Trained batch 850 batch loss 0.650385797 epoch total loss 0.565836847\n",
      "Trained batch 851 batch loss 0.744574368 epoch total loss 0.566046894\n",
      "Trained batch 852 batch loss 0.757392287 epoch total loss 0.566271484\n",
      "Trained batch 853 batch loss 0.550455809 epoch total loss 0.566252887\n",
      "Trained batch 854 batch loss 0.570974708 epoch total loss 0.56625843\n",
      "Trained batch 855 batch loss 0.495340437 epoch total loss 0.56617552\n",
      "Trained batch 856 batch loss 0.595402241 epoch total loss 0.566209614\n",
      "Trained batch 857 batch loss 0.650256038 epoch total loss 0.566307724\n",
      "Trained batch 858 batch loss 0.582286119 epoch total loss 0.56632632\n",
      "Trained batch 859 batch loss 0.59869951 epoch total loss 0.566364\n",
      "Trained batch 860 batch loss 0.551659942 epoch total loss 0.566346943\n",
      "Trained batch 861 batch loss 0.635299146 epoch total loss 0.566427\n",
      "Trained batch 862 batch loss 0.662387908 epoch total loss 0.566538274\n",
      "Trained batch 863 batch loss 0.628412843 epoch total loss 0.56661\n",
      "Trained batch 864 batch loss 0.637524307 epoch total loss 0.566692054\n",
      "Trained batch 865 batch loss 0.544005513 epoch total loss 0.566665828\n",
      "Trained batch 866 batch loss 0.553392 epoch total loss 0.56665051\n",
      "Trained batch 867 batch loss 0.441982269 epoch total loss 0.566506743\n",
      "Trained batch 868 batch loss 0.477009654 epoch total loss 0.566403627\n",
      "Trained batch 869 batch loss 0.53025347 epoch total loss 0.566362\n",
      "Trained batch 870 batch loss 0.571606517 epoch total loss 0.566368043\n",
      "Trained batch 871 batch loss 0.498552382 epoch total loss 0.5662902\n",
      "Trained batch 872 batch loss 0.54189086 epoch total loss 0.566262245\n",
      "Trained batch 873 batch loss 0.570120931 epoch total loss 0.566266656\n",
      "Trained batch 874 batch loss 0.589362442 epoch total loss 0.566293061\n",
      "Trained batch 875 batch loss 0.614594579 epoch total loss 0.566348255\n",
      "Trained batch 876 batch loss 0.593029737 epoch total loss 0.566378713\n",
      "Trained batch 877 batch loss 0.646256804 epoch total loss 0.566469848\n",
      "Trained batch 878 batch loss 0.566649914 epoch total loss 0.56647\n",
      "Trained batch 879 batch loss 0.595333874 epoch total loss 0.566502869\n",
      "Trained batch 880 batch loss 0.642210722 epoch total loss 0.566588879\n",
      "Trained batch 881 batch loss 0.54223454 epoch total loss 0.566561282\n",
      "Trained batch 882 batch loss 0.577991903 epoch total loss 0.566574216\n",
      "Trained batch 883 batch loss 0.65111959 epoch total loss 0.56667\n",
      "Trained batch 884 batch loss 0.597688138 epoch total loss 0.566705048\n",
      "Trained batch 885 batch loss 0.615813494 epoch total loss 0.56676054\n",
      "Trained batch 886 batch loss 0.535011351 epoch total loss 0.566724718\n",
      "Trained batch 887 batch loss 0.635369956 epoch total loss 0.566802144\n",
      "Trained batch 888 batch loss 0.588595629 epoch total loss 0.566826642\n",
      "Trained batch 889 batch loss 0.647860587 epoch total loss 0.566917777\n",
      "Trained batch 890 batch loss 0.598537207 epoch total loss 0.566953361\n",
      "Trained batch 891 batch loss 0.51948303 epoch total loss 0.566900074\n",
      "Trained batch 892 batch loss 0.574422657 epoch total loss 0.566908479\n",
      "Trained batch 893 batch loss 0.581355214 epoch total loss 0.566924691\n",
      "Trained batch 894 batch loss 0.615751743 epoch total loss 0.566979289\n",
      "Trained batch 895 batch loss 0.641021371 epoch total loss 0.567062\n",
      "Trained batch 896 batch loss 0.496171743 epoch total loss 0.566982925\n",
      "Trained batch 897 batch loss 0.539958954 epoch total loss 0.566952765\n",
      "Trained batch 898 batch loss 0.603467345 epoch total loss 0.566993415\n",
      "Trained batch 899 batch loss 0.572836697 epoch total loss 0.566999912\n",
      "Trained batch 900 batch loss 0.579443932 epoch total loss 0.567013741\n",
      "Trained batch 901 batch loss 0.538099408 epoch total loss 0.566981673\n",
      "Trained batch 902 batch loss 0.617683232 epoch total loss 0.567037821\n",
      "Trained batch 903 batch loss 0.657440364 epoch total loss 0.567138\n",
      "Trained batch 904 batch loss 0.572127402 epoch total loss 0.567143559\n",
      "Trained batch 905 batch loss 0.462308556 epoch total loss 0.567027688\n",
      "Trained batch 906 batch loss 0.575701714 epoch total loss 0.567037225\n",
      "Trained batch 907 batch loss 0.560092807 epoch total loss 0.567029595\n",
      "Trained batch 908 batch loss 0.550733328 epoch total loss 0.567011654\n",
      "Trained batch 909 batch loss 0.473172903 epoch total loss 0.56690836\n",
      "Trained batch 910 batch loss 0.50922823 epoch total loss 0.56684494\n",
      "Trained batch 911 batch loss 0.576532066 epoch total loss 0.566855609\n",
      "Trained batch 912 batch loss 0.602363288 epoch total loss 0.566894531\n",
      "Trained batch 913 batch loss 0.556626081 epoch total loss 0.566883326\n",
      "Trained batch 914 batch loss 0.594367623 epoch total loss 0.566913366\n",
      "Trained batch 915 batch loss 0.575749397 epoch total loss 0.566923\n",
      "Trained batch 916 batch loss 0.571449041 epoch total loss 0.566927969\n",
      "Trained batch 917 batch loss 0.610584617 epoch total loss 0.566975594\n",
      "Trained batch 918 batch loss 0.523479581 epoch total loss 0.566928267\n",
      "Trained batch 919 batch loss 0.529535592 epoch total loss 0.566887558\n",
      "Trained batch 920 batch loss 0.571180522 epoch total loss 0.566892207\n",
      "Trained batch 921 batch loss 0.74278307 epoch total loss 0.56708318\n",
      "Trained batch 922 batch loss 0.702179611 epoch total loss 0.567229748\n",
      "Trained batch 923 batch loss 0.638423443 epoch total loss 0.567306876\n",
      "Trained batch 924 batch loss 0.544942558 epoch total loss 0.567282677\n",
      "Trained batch 925 batch loss 0.529733539 epoch total loss 0.567242086\n",
      "Trained batch 926 batch loss 0.581098258 epoch total loss 0.567257047\n",
      "Trained batch 927 batch loss 0.717175 epoch total loss 0.567418754\n",
      "Trained batch 928 batch loss 0.637545943 epoch total loss 0.567494333\n",
      "Trained batch 929 batch loss 0.658125758 epoch total loss 0.567591906\n",
      "Trained batch 930 batch loss 0.608285367 epoch total loss 0.567635655\n",
      "Trained batch 931 batch loss 0.594240248 epoch total loss 0.567664266\n",
      "Trained batch 932 batch loss 0.537639499 epoch total loss 0.567632079\n",
      "Trained batch 933 batch loss 0.544998944 epoch total loss 0.56760776\n",
      "Trained batch 934 batch loss 0.62584883 epoch total loss 0.567670166\n",
      "Trained batch 935 batch loss 0.546916664 epoch total loss 0.567648\n",
      "Trained batch 936 batch loss 0.568899 epoch total loss 0.567649305\n",
      "Trained batch 937 batch loss 0.526319623 epoch total loss 0.567605197\n",
      "Trained batch 938 batch loss 0.515699863 epoch total loss 0.567549825\n",
      "Trained batch 939 batch loss 0.59644711 epoch total loss 0.567580581\n",
      "Trained batch 940 batch loss 0.548763633 epoch total loss 0.567560613\n",
      "Trained batch 941 batch loss 0.625821948 epoch total loss 0.567622483\n",
      "Trained batch 942 batch loss 0.533032775 epoch total loss 0.567585766\n",
      "Trained batch 943 batch loss 0.569626093 epoch total loss 0.567587912\n",
      "Trained batch 944 batch loss 0.622721612 epoch total loss 0.567646325\n",
      "Trained batch 945 batch loss 0.547940612 epoch total loss 0.567625463\n",
      "Trained batch 946 batch loss 0.585177064 epoch total loss 0.56764406\n",
      "Trained batch 947 batch loss 0.473647028 epoch total loss 0.567544758\n",
      "Trained batch 948 batch loss 0.556103945 epoch total loss 0.567532718\n",
      "Trained batch 949 batch loss 0.549295902 epoch total loss 0.567513525\n",
      "Trained batch 950 batch loss 0.546314836 epoch total loss 0.567491174\n",
      "Trained batch 951 batch loss 0.566304445 epoch total loss 0.567489922\n",
      "Trained batch 952 batch loss 0.588121235 epoch total loss 0.567511618\n",
      "Trained batch 953 batch loss 0.539411843 epoch total loss 0.567482173\n",
      "Trained batch 954 batch loss 0.517440498 epoch total loss 0.567429721\n",
      "Trained batch 955 batch loss 0.558187723 epoch total loss 0.56742\n",
      "Trained batch 956 batch loss 0.595537603 epoch total loss 0.567449391\n",
      "Trained batch 957 batch loss 0.55588758 epoch total loss 0.567437351\n",
      "Trained batch 958 batch loss 0.618707 epoch total loss 0.567490876\n",
      "Trained batch 959 batch loss 0.622104406 epoch total loss 0.567547858\n",
      "Trained batch 960 batch loss 0.641861379 epoch total loss 0.567625225\n",
      "Trained batch 961 batch loss 0.621197343 epoch total loss 0.567681\n",
      "Trained batch 962 batch loss 0.631189764 epoch total loss 0.567747\n",
      "Trained batch 963 batch loss 0.600500226 epoch total loss 0.567781031\n",
      "Trained batch 964 batch loss 0.588804126 epoch total loss 0.567802846\n",
      "Trained batch 965 batch loss 0.546824813 epoch total loss 0.567781091\n",
      "Trained batch 966 batch loss 0.608924747 epoch total loss 0.567823708\n",
      "Trained batch 967 batch loss 0.681000471 epoch total loss 0.567940772\n",
      "Trained batch 968 batch loss 0.559357762 epoch total loss 0.56793195\n",
      "Trained batch 969 batch loss 0.625751 epoch total loss 0.567991614\n",
      "Trained batch 970 batch loss 0.581751585 epoch total loss 0.568005741\n",
      "Trained batch 971 batch loss 0.586979806 epoch total loss 0.568025291\n",
      "Trained batch 972 batch loss 0.624849677 epoch total loss 0.568083763\n",
      "Trained batch 973 batch loss 0.563187838 epoch total loss 0.568078697\n",
      "Trained batch 974 batch loss 0.603947699 epoch total loss 0.568115532\n",
      "Trained batch 975 batch loss 0.539712191 epoch total loss 0.568086445\n",
      "Trained batch 976 batch loss 0.613203347 epoch total loss 0.568132699\n",
      "Trained batch 977 batch loss 0.589191 epoch total loss 0.568154216\n",
      "Trained batch 978 batch loss 0.662718534 epoch total loss 0.568250895\n",
      "Trained batch 979 batch loss 0.673405766 epoch total loss 0.568358302\n",
      "Trained batch 980 batch loss 0.62049371 epoch total loss 0.568411529\n",
      "Trained batch 981 batch loss 0.571649969 epoch total loss 0.568414807\n",
      "Trained batch 982 batch loss 0.590157509 epoch total loss 0.568436921\n",
      "Trained batch 983 batch loss 0.575797617 epoch total loss 0.568444431\n",
      "Trained batch 984 batch loss 0.593420267 epoch total loss 0.568469822\n",
      "Trained batch 985 batch loss 0.547214925 epoch total loss 0.568448305\n",
      "Trained batch 986 batch loss 0.512821794 epoch total loss 0.56839186\n",
      "Trained batch 987 batch loss 0.450727612 epoch total loss 0.56827265\n",
      "Trained batch 988 batch loss 0.421649456 epoch total loss 0.568124235\n",
      "Trained batch 989 batch loss 0.506276309 epoch total loss 0.568061709\n",
      "Trained batch 990 batch loss 0.524027467 epoch total loss 0.568017244\n",
      "Trained batch 991 batch loss 0.468209594 epoch total loss 0.567916572\n",
      "Trained batch 992 batch loss 0.557922959 epoch total loss 0.567906499\n",
      "Trained batch 993 batch loss 0.525407553 epoch total loss 0.567863643\n",
      "Trained batch 994 batch loss 0.628926158 epoch total loss 0.567925096\n",
      "Trained batch 995 batch loss 0.647820413 epoch total loss 0.568005383\n",
      "Trained batch 996 batch loss 0.56156081 epoch total loss 0.567998946\n",
      "Trained batch 997 batch loss 0.659656405 epoch total loss 0.568090856\n",
      "Trained batch 998 batch loss 0.586188 epoch total loss 0.568109\n",
      "Trained batch 999 batch loss 0.589832783 epoch total loss 0.568130732\n",
      "Trained batch 1000 batch loss 0.609717607 epoch total loss 0.568172336\n",
      "Trained batch 1001 batch loss 0.617528677 epoch total loss 0.568221688\n",
      "Trained batch 1002 batch loss 0.660565197 epoch total loss 0.568313897\n",
      "Trained batch 1003 batch loss 0.605130374 epoch total loss 0.568350554\n",
      "Trained batch 1004 batch loss 0.618384 epoch total loss 0.568400383\n",
      "Trained batch 1005 batch loss 0.598761499 epoch total loss 0.568430603\n",
      "Trained batch 1006 batch loss 0.527529716 epoch total loss 0.568389952\n",
      "Trained batch 1007 batch loss 0.529695153 epoch total loss 0.568351567\n",
      "Trained batch 1008 batch loss 0.526016653 epoch total loss 0.568309546\n",
      "Trained batch 1009 batch loss 0.573214471 epoch total loss 0.568314433\n",
      "Trained batch 1010 batch loss 0.567469358 epoch total loss 0.568313539\n",
      "Trained batch 1011 batch loss 0.510115206 epoch total loss 0.568256\n",
      "Trained batch 1012 batch loss 0.49725163 epoch total loss 0.568185866\n",
      "Trained batch 1013 batch loss 0.580411494 epoch total loss 0.568197906\n",
      "Trained batch 1014 batch loss 0.516120195 epoch total loss 0.568146527\n",
      "Trained batch 1015 batch loss 0.631949663 epoch total loss 0.56820941\n",
      "Trained batch 1016 batch loss 0.542344928 epoch total loss 0.568183959\n",
      "Trained batch 1017 batch loss 0.550285 epoch total loss 0.568166375\n",
      "Trained batch 1018 batch loss 0.561708331 epoch total loss 0.56816\n",
      "Trained batch 1019 batch loss 0.450750828 epoch total loss 0.568044782\n",
      "Trained batch 1020 batch loss 0.572640181 epoch total loss 0.568049312\n",
      "Trained batch 1021 batch loss 0.534316182 epoch total loss 0.568016231\n",
      "Trained batch 1022 batch loss 0.564691722 epoch total loss 0.568013\n",
      "Trained batch 1023 batch loss 0.484949499 epoch total loss 0.567931771\n",
      "Trained batch 1024 batch loss 0.550479352 epoch total loss 0.567914724\n",
      "Trained batch 1025 batch loss 0.485460788 epoch total loss 0.567834318\n",
      "Trained batch 1026 batch loss 0.571526647 epoch total loss 0.567837894\n",
      "Trained batch 1027 batch loss 0.572764874 epoch total loss 0.567842662\n",
      "Trained batch 1028 batch loss 0.534108102 epoch total loss 0.56780988\n",
      "Trained batch 1029 batch loss 0.548381627 epoch total loss 0.567791045\n",
      "Trained batch 1030 batch loss 0.511835933 epoch total loss 0.567736685\n",
      "Trained batch 1031 batch loss 0.527147889 epoch total loss 0.567697346\n",
      "Trained batch 1032 batch loss 0.560096443 epoch total loss 0.56769\n",
      "Trained batch 1033 batch loss 0.571714044 epoch total loss 0.567693889\n",
      "Trained batch 1034 batch loss 0.510196507 epoch total loss 0.567638278\n",
      "Trained batch 1035 batch loss 0.552405477 epoch total loss 0.567623615\n",
      "Trained batch 1036 batch loss 0.492211759 epoch total loss 0.567550778\n",
      "Trained batch 1037 batch loss 0.554442763 epoch total loss 0.567538142\n",
      "Trained batch 1038 batch loss 0.565417409 epoch total loss 0.567536116\n",
      "Trained batch 1039 batch loss 0.591216207 epoch total loss 0.567558885\n",
      "Trained batch 1040 batch loss 0.592580795 epoch total loss 0.567582965\n",
      "Trained batch 1041 batch loss 0.484623879 epoch total loss 0.567503214\n",
      "Trained batch 1042 batch loss 0.465496063 epoch total loss 0.567405343\n",
      "Trained batch 1043 batch loss 0.405760944 epoch total loss 0.567250371\n",
      "Trained batch 1044 batch loss 0.477668166 epoch total loss 0.5671646\n",
      "Trained batch 1045 batch loss 0.562253475 epoch total loss 0.567159891\n",
      "Trained batch 1046 batch loss 0.663324773 epoch total loss 0.567251801\n",
      "Trained batch 1047 batch loss 0.606382966 epoch total loss 0.567289174\n",
      "Trained batch 1048 batch loss 0.623122215 epoch total loss 0.56734246\n",
      "Trained batch 1049 batch loss 0.592927754 epoch total loss 0.567366898\n",
      "Trained batch 1050 batch loss 0.518589854 epoch total loss 0.567320466\n",
      "Trained batch 1051 batch loss 0.59417218 epoch total loss 0.567346\n",
      "Trained batch 1052 batch loss 0.562802613 epoch total loss 0.567341685\n",
      "Trained batch 1053 batch loss 0.620389521 epoch total loss 0.567392051\n",
      "Trained batch 1054 batch loss 0.624403298 epoch total loss 0.567446113\n",
      "Trained batch 1055 batch loss 0.538833 epoch total loss 0.567419\n",
      "Trained batch 1056 batch loss 0.545592546 epoch total loss 0.56739831\n",
      "Trained batch 1057 batch loss 0.503242493 epoch total loss 0.567337573\n",
      "Trained batch 1058 batch loss 0.457859397 epoch total loss 0.567234159\n",
      "Trained batch 1059 batch loss 0.512282431 epoch total loss 0.567182243\n",
      "Trained batch 1060 batch loss 0.47958529 epoch total loss 0.567099631\n",
      "Trained batch 1061 batch loss 0.505637288 epoch total loss 0.567041695\n",
      "Trained batch 1062 batch loss 0.577971101 epoch total loss 0.567051947\n",
      "Trained batch 1063 batch loss 0.613188684 epoch total loss 0.567095339\n",
      "Trained batch 1064 batch loss 0.519314408 epoch total loss 0.567050397\n",
      "Trained batch 1065 batch loss 0.615367651 epoch total loss 0.567095757\n",
      "Trained batch 1066 batch loss 0.521595538 epoch total loss 0.56705308\n",
      "Trained batch 1067 batch loss 0.578362942 epoch total loss 0.567063689\n",
      "Trained batch 1068 batch loss 0.695607662 epoch total loss 0.567184031\n",
      "Trained batch 1069 batch loss 0.640296578 epoch total loss 0.567252457\n",
      "Trained batch 1070 batch loss 0.57477057 epoch total loss 0.56725949\n",
      "Trained batch 1071 batch loss 0.519105196 epoch total loss 0.567214549\n",
      "Trained batch 1072 batch loss 0.611794 epoch total loss 0.567256153\n",
      "Trained batch 1073 batch loss 0.644054294 epoch total loss 0.567327678\n",
      "Trained batch 1074 batch loss 0.64489305 epoch total loss 0.567399919\n",
      "Trained batch 1075 batch loss 0.572372675 epoch total loss 0.567404568\n",
      "Trained batch 1076 batch loss 0.634821177 epoch total loss 0.567467213\n",
      "Trained batch 1077 batch loss 0.673490882 epoch total loss 0.56756562\n",
      "Trained batch 1078 batch loss 0.612998486 epoch total loss 0.56760776\n",
      "Trained batch 1079 batch loss 0.590568 epoch total loss 0.567629039\n",
      "Trained batch 1080 batch loss 0.638395786 epoch total loss 0.567694545\n",
      "Trained batch 1081 batch loss 0.681081414 epoch total loss 0.567799449\n",
      "Trained batch 1082 batch loss 0.678469837 epoch total loss 0.567901731\n",
      "Trained batch 1083 batch loss 0.540270805 epoch total loss 0.56787622\n",
      "Trained batch 1084 batch loss 0.569071651 epoch total loss 0.567877352\n",
      "Trained batch 1085 batch loss 0.661358178 epoch total loss 0.567963541\n",
      "Trained batch 1086 batch loss 0.534451902 epoch total loss 0.567932606\n",
      "Trained batch 1087 batch loss 0.456697166 epoch total loss 0.567830324\n",
      "Trained batch 1088 batch loss 0.632037759 epoch total loss 0.567889333\n",
      "Trained batch 1089 batch loss 0.639437914 epoch total loss 0.567955077\n",
      "Trained batch 1090 batch loss 0.641749144 epoch total loss 0.568022728\n",
      "Trained batch 1091 batch loss 0.588935852 epoch total loss 0.568041921\n",
      "Trained batch 1092 batch loss 0.539147854 epoch total loss 0.568015397\n",
      "Trained batch 1093 batch loss 0.682122767 epoch total loss 0.568119824\n",
      "Trained batch 1094 batch loss 0.673174679 epoch total loss 0.568215847\n",
      "Trained batch 1095 batch loss 0.708211541 epoch total loss 0.568343639\n",
      "Trained batch 1096 batch loss 0.682270348 epoch total loss 0.56844759\n",
      "Trained batch 1097 batch loss 0.682164669 epoch total loss 0.568551242\n",
      "Trained batch 1098 batch loss 0.579981923 epoch total loss 0.568561673\n",
      "Trained batch 1099 batch loss 0.555087507 epoch total loss 0.568549395\n",
      "Trained batch 1100 batch loss 0.52989608 epoch total loss 0.568514287\n",
      "Trained batch 1101 batch loss 0.580755472 epoch total loss 0.568525434\n",
      "Trained batch 1102 batch loss 0.637734056 epoch total loss 0.568588257\n",
      "Trained batch 1103 batch loss 0.541456401 epoch total loss 0.56856364\n",
      "Trained batch 1104 batch loss 0.52704227 epoch total loss 0.568526\n",
      "Trained batch 1105 batch loss 0.686371326 epoch total loss 0.568632662\n",
      "Trained batch 1106 batch loss 0.559447408 epoch total loss 0.568624377\n",
      "Trained batch 1107 batch loss 0.634067774 epoch total loss 0.568683505\n",
      "Trained batch 1108 batch loss 0.55443126 epoch total loss 0.56867069\n",
      "Trained batch 1109 batch loss 0.663041413 epoch total loss 0.568755746\n",
      "Trained batch 1110 batch loss 0.622778594 epoch total loss 0.568804443\n",
      "Trained batch 1111 batch loss 0.639004827 epoch total loss 0.568867624\n",
      "Trained batch 1112 batch loss 0.649266124 epoch total loss 0.568939924\n",
      "Trained batch 1113 batch loss 0.685332954 epoch total loss 0.569044471\n",
      "Trained batch 1114 batch loss 0.646613657 epoch total loss 0.569114089\n",
      "Trained batch 1115 batch loss 0.627965093 epoch total loss 0.569166899\n",
      "Trained batch 1116 batch loss 0.632186472 epoch total loss 0.569223404\n",
      "Trained batch 1117 batch loss 0.70965451 epoch total loss 0.56934911\n",
      "Trained batch 1118 batch loss 0.702238858 epoch total loss 0.569467962\n",
      "Trained batch 1119 batch loss 0.658196568 epoch total loss 0.569547236\n",
      "Trained batch 1120 batch loss 0.646769226 epoch total loss 0.569616199\n",
      "Trained batch 1121 batch loss 0.662701249 epoch total loss 0.569699287\n",
      "Trained batch 1122 batch loss 0.652104676 epoch total loss 0.56977272\n",
      "Trained batch 1123 batch loss 0.615801215 epoch total loss 0.569813669\n",
      "Trained batch 1124 batch loss 0.595930457 epoch total loss 0.569836915\n",
      "Trained batch 1125 batch loss 0.640678465 epoch total loss 0.569899917\n",
      "Trained batch 1126 batch loss 0.59330374 epoch total loss 0.569920719\n",
      "Trained batch 1127 batch loss 0.616946042 epoch total loss 0.569962442\n",
      "Trained batch 1128 batch loss 0.634583592 epoch total loss 0.570019722\n",
      "Trained batch 1129 batch loss 0.618492484 epoch total loss 0.570062637\n",
      "Trained batch 1130 batch loss 0.665637076 epoch total loss 0.570147216\n",
      "Trained batch 1131 batch loss 0.664538 epoch total loss 0.570230663\n",
      "Trained batch 1132 batch loss 0.611736238 epoch total loss 0.570267379\n",
      "Trained batch 1133 batch loss 0.602281272 epoch total loss 0.570295632\n",
      "Trained batch 1134 batch loss 0.646282852 epoch total loss 0.570362687\n",
      "Trained batch 1135 batch loss 0.592663288 epoch total loss 0.570382297\n",
      "Trained batch 1136 batch loss 0.539202094 epoch total loss 0.570354819\n",
      "Trained batch 1137 batch loss 0.575892746 epoch total loss 0.570359707\n",
      "Trained batch 1138 batch loss 0.533372581 epoch total loss 0.570327222\n",
      "Trained batch 1139 batch loss 0.553807795 epoch total loss 0.570312738\n",
      "Trained batch 1140 batch loss 0.579014599 epoch total loss 0.570320368\n",
      "Trained batch 1141 batch loss 0.575024247 epoch total loss 0.570324481\n",
      "Trained batch 1142 batch loss 0.494140327 epoch total loss 0.570257783\n",
      "Trained batch 1143 batch loss 0.566787481 epoch total loss 0.570254743\n",
      "Trained batch 1144 batch loss 0.582179964 epoch total loss 0.570265114\n",
      "Trained batch 1145 batch loss 0.558092535 epoch total loss 0.570254505\n",
      "Trained batch 1146 batch loss 0.590371907 epoch total loss 0.570272088\n",
      "Trained batch 1147 batch loss 0.645649 epoch total loss 0.570337772\n",
      "Trained batch 1148 batch loss 0.606065929 epoch total loss 0.570368886\n",
      "Trained batch 1149 batch loss 0.516433597 epoch total loss 0.570322\n",
      "Trained batch 1150 batch loss 0.605513573 epoch total loss 0.570352554\n",
      "Trained batch 1151 batch loss 0.51609093 epoch total loss 0.570305467\n",
      "Trained batch 1152 batch loss 0.5577209 epoch total loss 0.570294559\n",
      "Trained batch 1153 batch loss 0.624447286 epoch total loss 0.570341527\n",
      "Trained batch 1154 batch loss 0.601497412 epoch total loss 0.570368528\n",
      "Trained batch 1155 batch loss 0.591070652 epoch total loss 0.57038641\n",
      "Trained batch 1156 batch loss 0.443472207 epoch total loss 0.570276678\n",
      "Trained batch 1157 batch loss 0.506081164 epoch total loss 0.570221186\n",
      "Trained batch 1158 batch loss 0.465229779 epoch total loss 0.570130527\n",
      "Trained batch 1159 batch loss 0.585533857 epoch total loss 0.570143759\n",
      "Trained batch 1160 batch loss 0.55601877 epoch total loss 0.5701316\n",
      "Trained batch 1161 batch loss 0.544345081 epoch total loss 0.570109427\n",
      "Trained batch 1162 batch loss 0.53254813 epoch total loss 0.570077062\n",
      "Trained batch 1163 batch loss 0.507110357 epoch total loss 0.570022941\n",
      "Trained batch 1164 batch loss 0.485421896 epoch total loss 0.569950223\n",
      "Trained batch 1165 batch loss 0.481152683 epoch total loss 0.569874\n",
      "Trained batch 1166 batch loss 0.534564614 epoch total loss 0.569843709\n",
      "Trained batch 1167 batch loss 0.466880679 epoch total loss 0.569755435\n",
      "Trained batch 1168 batch loss 0.537482917 epoch total loss 0.569727778\n",
      "Trained batch 1169 batch loss 0.52838093 epoch total loss 0.569692433\n",
      "Trained batch 1170 batch loss 0.610595345 epoch total loss 0.569727421\n",
      "Trained batch 1171 batch loss 0.581666589 epoch total loss 0.569737613\n",
      "Trained batch 1172 batch loss 0.549562275 epoch total loss 0.569720387\n",
      "Trained batch 1173 batch loss 0.579842567 epoch total loss 0.569729\n",
      "Trained batch 1174 batch loss 0.580964327 epoch total loss 0.569738567\n",
      "Trained batch 1175 batch loss 0.543060243 epoch total loss 0.569715858\n",
      "Trained batch 1176 batch loss 0.575422645 epoch total loss 0.569720745\n",
      "Trained batch 1177 batch loss 0.558829904 epoch total loss 0.569711506\n",
      "Trained batch 1178 batch loss 0.480851263 epoch total loss 0.569636047\n",
      "Trained batch 1179 batch loss 0.52049315 epoch total loss 0.569594383\n",
      "Trained batch 1180 batch loss 0.494703054 epoch total loss 0.569530904\n",
      "Trained batch 1181 batch loss 0.481908232 epoch total loss 0.569456697\n",
      "Trained batch 1182 batch loss 0.557682216 epoch total loss 0.569446743\n",
      "Trained batch 1183 batch loss 0.536274076 epoch total loss 0.569418669\n",
      "Trained batch 1184 batch loss 0.553840339 epoch total loss 0.569405556\n",
      "Trained batch 1185 batch loss 0.535186052 epoch total loss 0.569376647\n",
      "Trained batch 1186 batch loss 0.674239039 epoch total loss 0.569465041\n",
      "Trained batch 1187 batch loss 0.633903086 epoch total loss 0.569519341\n",
      "Trained batch 1188 batch loss 0.538150251 epoch total loss 0.569492936\n",
      "Trained batch 1189 batch loss 0.566369593 epoch total loss 0.569490314\n",
      "Trained batch 1190 batch loss 0.592250288 epoch total loss 0.569509387\n",
      "Trained batch 1191 batch loss 0.515396357 epoch total loss 0.569463968\n",
      "Trained batch 1192 batch loss 0.494681776 epoch total loss 0.569401205\n",
      "Trained batch 1193 batch loss 0.501163185 epoch total loss 0.569344044\n",
      "Trained batch 1194 batch loss 0.531472087 epoch total loss 0.569312334\n",
      "Trained batch 1195 batch loss 0.625250816 epoch total loss 0.569359124\n",
      "Trained batch 1196 batch loss 0.609289646 epoch total loss 0.569392562\n",
      "Trained batch 1197 batch loss 0.593510866 epoch total loss 0.569412649\n",
      "Trained batch 1198 batch loss 0.596696079 epoch total loss 0.569435418\n",
      "Trained batch 1199 batch loss 0.58429873 epoch total loss 0.569447815\n",
      "Trained batch 1200 batch loss 0.525484324 epoch total loss 0.569411218\n",
      "Trained batch 1201 batch loss 0.579584599 epoch total loss 0.569419682\n",
      "Trained batch 1202 batch loss 0.574398458 epoch total loss 0.569423854\n",
      "Trained batch 1203 batch loss 0.60821414 epoch total loss 0.5694561\n",
      "Trained batch 1204 batch loss 0.500261247 epoch total loss 0.569398582\n",
      "Trained batch 1205 batch loss 0.576107383 epoch total loss 0.569404185\n",
      "Trained batch 1206 batch loss 0.530242085 epoch total loss 0.569371641\n",
      "Trained batch 1207 batch loss 0.435496271 epoch total loss 0.569260716\n",
      "Trained batch 1208 batch loss 0.558272898 epoch total loss 0.569251657\n",
      "Trained batch 1209 batch loss 0.585816205 epoch total loss 0.569265366\n",
      "Trained batch 1210 batch loss 0.584001 epoch total loss 0.569277525\n",
      "Trained batch 1211 batch loss 0.646655142 epoch total loss 0.569341421\n",
      "Trained batch 1212 batch loss 0.51260066 epoch total loss 0.569294572\n",
      "Trained batch 1213 batch loss 0.503741324 epoch total loss 0.56924051\n",
      "Trained batch 1214 batch loss 0.598511398 epoch total loss 0.56926465\n",
      "Trained batch 1215 batch loss 0.531081 epoch total loss 0.569233179\n",
      "Trained batch 1216 batch loss 0.598167062 epoch total loss 0.569256961\n",
      "Trained batch 1217 batch loss 0.548980236 epoch total loss 0.569240272\n",
      "Trained batch 1218 batch loss 0.518870354 epoch total loss 0.569198906\n",
      "Trained batch 1219 batch loss 0.549863517 epoch total loss 0.569183052\n",
      "Trained batch 1220 batch loss 0.547099 epoch total loss 0.569165\n",
      "Trained batch 1221 batch loss 0.483717978 epoch total loss 0.569095\n",
      "Trained batch 1222 batch loss 0.482161522 epoch total loss 0.569023848\n",
      "Trained batch 1223 batch loss 0.414465487 epoch total loss 0.568897486\n",
      "Trained batch 1224 batch loss 0.476666629 epoch total loss 0.568822145\n",
      "Trained batch 1225 batch loss 0.447715044 epoch total loss 0.568723321\n",
      "Trained batch 1226 batch loss 0.448201507 epoch total loss 0.568625\n",
      "Trained batch 1227 batch loss 0.550061524 epoch total loss 0.568609834\n",
      "Trained batch 1228 batch loss 0.627695322 epoch total loss 0.568657935\n",
      "Trained batch 1229 batch loss 0.540240526 epoch total loss 0.568634808\n",
      "Trained batch 1230 batch loss 0.562253833 epoch total loss 0.568629622\n",
      "Trained batch 1231 batch loss 0.619320273 epoch total loss 0.568670809\n",
      "Trained batch 1232 batch loss 0.619430244 epoch total loss 0.568712\n",
      "Trained batch 1233 batch loss 0.734491169 epoch total loss 0.568846464\n",
      "Trained batch 1234 batch loss 0.663764179 epoch total loss 0.568923354\n",
      "Trained batch 1235 batch loss 0.706168592 epoch total loss 0.569034517\n",
      "Trained batch 1236 batch loss 0.563546658 epoch total loss 0.569030046\n",
      "Trained batch 1237 batch loss 0.66518712 epoch total loss 0.569107771\n",
      "Trained batch 1238 batch loss 0.550018847 epoch total loss 0.569092393\n",
      "Trained batch 1239 batch loss 0.572126508 epoch total loss 0.569094837\n",
      "Trained batch 1240 batch loss 0.604952633 epoch total loss 0.569123805\n",
      "Trained batch 1241 batch loss 0.499519378 epoch total loss 0.569067717\n",
      "Trained batch 1242 batch loss 0.548463821 epoch total loss 0.569051087\n",
      "Trained batch 1243 batch loss 0.513460517 epoch total loss 0.569006383\n",
      "Trained batch 1244 batch loss 0.478886694 epoch total loss 0.568933964\n",
      "Trained batch 1245 batch loss 0.537415564 epoch total loss 0.568908632\n",
      "Trained batch 1246 batch loss 0.593093097 epoch total loss 0.568928063\n",
      "Trained batch 1247 batch loss 0.6242522 epoch total loss 0.568972409\n",
      "Trained batch 1248 batch loss 0.533760369 epoch total loss 0.568944216\n",
      "Trained batch 1249 batch loss 0.574514031 epoch total loss 0.568948686\n",
      "Trained batch 1250 batch loss 0.569781 epoch total loss 0.568949342\n",
      "Trained batch 1251 batch loss 0.570188642 epoch total loss 0.568950295\n",
      "Trained batch 1252 batch loss 0.591465056 epoch total loss 0.568968296\n",
      "Trained batch 1253 batch loss 0.556824565 epoch total loss 0.56895864\n",
      "Trained batch 1254 batch loss 0.564312339 epoch total loss 0.568954945\n",
      "Trained batch 1255 batch loss 0.623460472 epoch total loss 0.568998396\n",
      "Trained batch 1256 batch loss 0.545051634 epoch total loss 0.568979323\n",
      "Trained batch 1257 batch loss 0.525733829 epoch total loss 0.568944931\n",
      "Trained batch 1258 batch loss 0.60201329 epoch total loss 0.568971157\n",
      "Trained batch 1259 batch loss 0.556815 epoch total loss 0.568961561\n",
      "Trained batch 1260 batch loss 0.496663332 epoch total loss 0.568904161\n",
      "Trained batch 1261 batch loss 0.562260568 epoch total loss 0.568898857\n",
      "Trained batch 1262 batch loss 0.700695753 epoch total loss 0.569003284\n",
      "Trained batch 1263 batch loss 0.58698684 epoch total loss 0.569017529\n",
      "Trained batch 1264 batch loss 0.617990196 epoch total loss 0.569056273\n",
      "Trained batch 1265 batch loss 0.511289299 epoch total loss 0.569010615\n",
      "Trained batch 1266 batch loss 0.608891189 epoch total loss 0.569042087\n",
      "Trained batch 1267 batch loss 0.679830432 epoch total loss 0.569129527\n",
      "Trained batch 1268 batch loss 0.682756 epoch total loss 0.569219112\n",
      "Trained batch 1269 batch loss 0.638846636 epoch total loss 0.569274\n",
      "Trained batch 1270 batch loss 0.643890679 epoch total loss 0.569332778\n",
      "Trained batch 1271 batch loss 0.727160573 epoch total loss 0.569456935\n",
      "Trained batch 1272 batch loss 0.699269772 epoch total loss 0.569559038\n",
      "Trained batch 1273 batch loss 0.569066 epoch total loss 0.56955862\n",
      "Trained batch 1274 batch loss 0.525834262 epoch total loss 0.569524288\n",
      "Trained batch 1275 batch loss 0.514874518 epoch total loss 0.569481492\n",
      "Trained batch 1276 batch loss 0.44263792 epoch total loss 0.569382071\n",
      "Trained batch 1277 batch loss 0.548406601 epoch total loss 0.569365621\n",
      "Trained batch 1278 batch loss 0.612846076 epoch total loss 0.569399655\n",
      "Trained batch 1279 batch loss 0.534813106 epoch total loss 0.569372594\n",
      "Trained batch 1280 batch loss 0.5456568 epoch total loss 0.569354057\n",
      "Trained batch 1281 batch loss 0.421426028 epoch total loss 0.569238603\n",
      "Trained batch 1282 batch loss 0.56045109 epoch total loss 0.569231749\n",
      "Trained batch 1283 batch loss 0.468269616 epoch total loss 0.569153\n",
      "Trained batch 1284 batch loss 0.473135293 epoch total loss 0.569078267\n",
      "Trained batch 1285 batch loss 0.523462236 epoch total loss 0.569042742\n",
      "Trained batch 1286 batch loss 0.492634416 epoch total loss 0.568983316\n",
      "Trained batch 1287 batch loss 0.512971163 epoch total loss 0.568939805\n",
      "Trained batch 1288 batch loss 0.490569532 epoch total loss 0.568878949\n",
      "Trained batch 1289 batch loss 0.463056296 epoch total loss 0.568796873\n",
      "Trained batch 1290 batch loss 0.452174067 epoch total loss 0.568706453\n",
      "Trained batch 1291 batch loss 0.468593627 epoch total loss 0.568628848\n",
      "Trained batch 1292 batch loss 0.41630739 epoch total loss 0.56851095\n",
      "Trained batch 1293 batch loss 0.551112175 epoch total loss 0.568497479\n",
      "Trained batch 1294 batch loss 0.611701369 epoch total loss 0.568530858\n",
      "Trained batch 1295 batch loss 0.554660141 epoch total loss 0.568520188\n",
      "Trained batch 1296 batch loss 0.627287388 epoch total loss 0.568565488\n",
      "Trained batch 1297 batch loss 0.443509549 epoch total loss 0.568469048\n",
      "Trained batch 1298 batch loss 0.480897278 epoch total loss 0.568401575\n",
      "Trained batch 1299 batch loss 0.435709596 epoch total loss 0.568299472\n",
      "Trained batch 1300 batch loss 0.438044786 epoch total loss 0.568199277\n",
      "Trained batch 1301 batch loss 0.525876641 epoch total loss 0.568166733\n",
      "Trained batch 1302 batch loss 0.608781159 epoch total loss 0.568197906\n",
      "Trained batch 1303 batch loss 0.570577502 epoch total loss 0.568199754\n",
      "Trained batch 1304 batch loss 0.547943354 epoch total loss 0.568184197\n",
      "Trained batch 1305 batch loss 0.559779 epoch total loss 0.56817776\n",
      "Trained batch 1306 batch loss 0.45340094 epoch total loss 0.568089902\n",
      "Trained batch 1307 batch loss 0.434932649 epoch total loss 0.567988038\n",
      "Trained batch 1308 batch loss 0.535086513 epoch total loss 0.567962885\n",
      "Trained batch 1309 batch loss 0.568935096 epoch total loss 0.5679636\n",
      "Trained batch 1310 batch loss 0.621340334 epoch total loss 0.56800437\n",
      "Trained batch 1311 batch loss 0.59913826 epoch total loss 0.568028092\n",
      "Trained batch 1312 batch loss 0.622174561 epoch total loss 0.568069339\n",
      "Trained batch 1313 batch loss 0.591816366 epoch total loss 0.568087459\n",
      "Trained batch 1314 batch loss 0.61449039 epoch total loss 0.568122745\n",
      "Trained batch 1315 batch loss 0.571158171 epoch total loss 0.568125069\n",
      "Trained batch 1316 batch loss 0.589699268 epoch total loss 0.56814146\n",
      "Trained batch 1317 batch loss 0.593012273 epoch total loss 0.568160355\n",
      "Trained batch 1318 batch loss 0.56695354 epoch total loss 0.568159461\n",
      "Trained batch 1319 batch loss 0.509920955 epoch total loss 0.568115354\n",
      "Trained batch 1320 batch loss 0.619995832 epoch total loss 0.568154633\n",
      "Trained batch 1321 batch loss 0.644977 epoch total loss 0.568212748\n",
      "Trained batch 1322 batch loss 0.612756968 epoch total loss 0.568246424\n",
      "Trained batch 1323 batch loss 0.627006769 epoch total loss 0.568290889\n",
      "Trained batch 1324 batch loss 0.569671392 epoch total loss 0.568291903\n",
      "Trained batch 1325 batch loss 0.493884236 epoch total loss 0.568235755\n",
      "Trained batch 1326 batch loss 0.525306 epoch total loss 0.56820339\n",
      "Trained batch 1327 batch loss 0.538932323 epoch total loss 0.568181336\n",
      "Trained batch 1328 batch loss 0.566614926 epoch total loss 0.568180144\n",
      "Trained batch 1329 batch loss 0.509039104 epoch total loss 0.568135619\n",
      "Trained batch 1330 batch loss 0.575839281 epoch total loss 0.56814146\n",
      "Trained batch 1331 batch loss 0.51088655 epoch total loss 0.568098426\n",
      "Trained batch 1332 batch loss 0.4877823 epoch total loss 0.568038106\n",
      "Trained batch 1333 batch loss 0.450131744 epoch total loss 0.567949653\n",
      "Trained batch 1334 batch loss 0.586620152 epoch total loss 0.56796366\n",
      "Trained batch 1335 batch loss 0.56303829 epoch total loss 0.567959964\n",
      "Trained batch 1336 batch loss 0.571248233 epoch total loss 0.567962408\n",
      "Trained batch 1337 batch loss 0.5709939 epoch total loss 0.567964673\n",
      "Trained batch 1338 batch loss 0.444553226 epoch total loss 0.567872465\n",
      "Trained batch 1339 batch loss 0.567886829 epoch total loss 0.567872465\n",
      "Trained batch 1340 batch loss 0.482323 epoch total loss 0.567808628\n",
      "Trained batch 1341 batch loss 0.463518888 epoch total loss 0.567730844\n",
      "Trained batch 1342 batch loss 0.462401956 epoch total loss 0.567652345\n",
      "Trained batch 1343 batch loss 0.565866351 epoch total loss 0.567651\n",
      "Trained batch 1344 batch loss 0.534169197 epoch total loss 0.567626119\n",
      "Trained batch 1345 batch loss 0.483308911 epoch total loss 0.567563415\n",
      "Trained batch 1346 batch loss 0.53572 epoch total loss 0.567539752\n",
      "Trained batch 1347 batch loss 0.494620979 epoch total loss 0.567485631\n",
      "Trained batch 1348 batch loss 0.594011128 epoch total loss 0.5675053\n",
      "Trained batch 1349 batch loss 0.638675332 epoch total loss 0.56755805\n",
      "Trained batch 1350 batch loss 0.669420958 epoch total loss 0.56763351\n",
      "Trained batch 1351 batch loss 0.590702593 epoch total loss 0.567650557\n",
      "Trained batch 1352 batch loss 0.625113606 epoch total loss 0.567693114\n",
      "Trained batch 1353 batch loss 0.678556502 epoch total loss 0.567775\n",
      "Trained batch 1354 batch loss 0.652750731 epoch total loss 0.567837775\n",
      "Trained batch 1355 batch loss 0.598632216 epoch total loss 0.567860484\n",
      "Trained batch 1356 batch loss 0.585731328 epoch total loss 0.567873716\n",
      "Trained batch 1357 batch loss 0.624416351 epoch total loss 0.56791538\n",
      "Trained batch 1358 batch loss 0.564378679 epoch total loss 0.567912757\n",
      "Trained batch 1359 batch loss 0.626721501 epoch total loss 0.56795603\n",
      "Trained batch 1360 batch loss 0.595172763 epoch total loss 0.567976\n",
      "Trained batch 1361 batch loss 0.591051579 epoch total loss 0.567993\n",
      "Trained batch 1362 batch loss 0.480475366 epoch total loss 0.567928731\n",
      "Trained batch 1363 batch loss 0.551359951 epoch total loss 0.567916572\n",
      "Trained batch 1364 batch loss 0.494689703 epoch total loss 0.567862868\n",
      "Trained batch 1365 batch loss 0.578830957 epoch total loss 0.567870915\n",
      "Trained batch 1366 batch loss 0.530329 epoch total loss 0.567843437\n",
      "Trained batch 1367 batch loss 0.595269442 epoch total loss 0.567863524\n",
      "Trained batch 1368 batch loss 0.636975884 epoch total loss 0.567914\n",
      "Trained batch 1369 batch loss 0.614421427 epoch total loss 0.567948\n",
      "Trained batch 1370 batch loss 0.598491788 epoch total loss 0.567970335\n",
      "Trained batch 1371 batch loss 0.591813564 epoch total loss 0.56798768\n",
      "Trained batch 1372 batch loss 0.572730601 epoch total loss 0.567991138\n",
      "Trained batch 1373 batch loss 0.547055483 epoch total loss 0.567975938\n",
      "Trained batch 1374 batch loss 0.488222301 epoch total loss 0.567917883\n",
      "Trained batch 1375 batch loss 0.539412439 epoch total loss 0.567897141\n",
      "Trained batch 1376 batch loss 0.491119742 epoch total loss 0.567841351\n",
      "Trained batch 1377 batch loss 0.480478913 epoch total loss 0.567777932\n",
      "Trained batch 1378 batch loss 0.475180894 epoch total loss 0.567710698\n",
      "Trained batch 1379 batch loss 0.431668162 epoch total loss 0.567612052\n",
      "Trained batch 1380 batch loss 0.448513985 epoch total loss 0.567525685\n",
      "Trained batch 1381 batch loss 0.544854403 epoch total loss 0.567509294\n",
      "Trained batch 1382 batch loss 0.527398348 epoch total loss 0.567480266\n",
      "Trained batch 1383 batch loss 0.546994448 epoch total loss 0.567465484\n",
      "Trained batch 1384 batch loss 0.532887816 epoch total loss 0.56744051\n",
      "Trained batch 1385 batch loss 0.445509553 epoch total loss 0.567352474\n",
      "Trained batch 1386 batch loss 0.555585802 epoch total loss 0.56734395\n",
      "Trained batch 1387 batch loss 0.514004588 epoch total loss 0.567305505\n",
      "Trained batch 1388 batch loss 0.490611613 epoch total loss 0.567250252\n",
      "Trained batch 1389 batch loss 0.55680716 epoch total loss 0.567242742\n",
      "Trained batch 1390 batch loss 0.567394853 epoch total loss 0.567242801\n",
      "Trained batch 1391 batch loss 0.597300529 epoch total loss 0.567264438\n",
      "Trained batch 1392 batch loss 0.596702099 epoch total loss 0.567285538\n",
      "Trained batch 1393 batch loss 0.724502146 epoch total loss 0.567398429\n",
      "Trained batch 1394 batch loss 0.733319461 epoch total loss 0.567517459\n",
      "Trained batch 1395 batch loss 0.730822861 epoch total loss 0.567634523\n",
      "Trained batch 1396 batch loss 0.500530303 epoch total loss 0.567586482\n",
      "Trained batch 1397 batch loss 0.6576823 epoch total loss 0.567650914\n",
      "Trained batch 1398 batch loss 0.624472 epoch total loss 0.567691565\n",
      "Trained batch 1399 batch loss 0.522367537 epoch total loss 0.56765914\n",
      "Trained batch 1400 batch loss 0.560743213 epoch total loss 0.567654192\n",
      "Trained batch 1401 batch loss 0.530609906 epoch total loss 0.567627788\n",
      "Trained batch 1402 batch loss 0.612786353 epoch total loss 0.56766\n",
      "Trained batch 1403 batch loss 0.611847281 epoch total loss 0.567691505\n",
      "Trained batch 1404 batch loss 0.575253 epoch total loss 0.567696929\n",
      "Trained batch 1405 batch loss 0.437876493 epoch total loss 0.567604482\n",
      "Trained batch 1406 batch loss 0.563046217 epoch total loss 0.567601264\n",
      "Trained batch 1407 batch loss 0.585344374 epoch total loss 0.56761384\n",
      "Trained batch 1408 batch loss 0.537862539 epoch total loss 0.56759268\n",
      "Trained batch 1409 batch loss 0.597313106 epoch total loss 0.56761378\n",
      "Trained batch 1410 batch loss 0.51188314 epoch total loss 0.567574263\n",
      "Trained batch 1411 batch loss 0.50538063 epoch total loss 0.567530215\n",
      "Trained batch 1412 batch loss 0.517016172 epoch total loss 0.567494392\n",
      "Trained batch 1413 batch loss 0.511209071 epoch total loss 0.567454576\n",
      "Trained batch 1414 batch loss 0.395700216 epoch total loss 0.567333102\n",
      "Trained batch 1415 batch loss 0.480563104 epoch total loss 0.567271829\n",
      "Trained batch 1416 batch loss 0.446689248 epoch total loss 0.567186713\n",
      "Trained batch 1417 batch loss 0.454181492 epoch total loss 0.567106903\n",
      "Trained batch 1418 batch loss 0.496060073 epoch total loss 0.567056775\n",
      "Trained batch 1419 batch loss 0.581262887 epoch total loss 0.567066789\n",
      "Trained batch 1420 batch loss 0.634283602 epoch total loss 0.567114115\n",
      "Trained batch 1421 batch loss 0.686282694 epoch total loss 0.567198\n",
      "Trained batch 1422 batch loss 0.752681 epoch total loss 0.567328453\n",
      "Trained batch 1423 batch loss 0.63745296 epoch total loss 0.567377687\n",
      "Trained batch 1424 batch loss 0.684217036 epoch total loss 0.567459762\n",
      "Trained batch 1425 batch loss 0.617523611 epoch total loss 0.567494929\n",
      "Trained batch 1426 batch loss 0.607534528 epoch total loss 0.567523\n",
      "Trained batch 1427 batch loss 0.687395751 epoch total loss 0.567607\n",
      "Trained batch 1428 batch loss 0.658532083 epoch total loss 0.567670643\n",
      "Trained batch 1429 batch loss 0.630209625 epoch total loss 0.567714393\n",
      "Trained batch 1430 batch loss 0.622720182 epoch total loss 0.567752838\n",
      "Trained batch 1431 batch loss 0.659715176 epoch total loss 0.567817152\n",
      "Trained batch 1432 batch loss 0.610793531 epoch total loss 0.567847133\n",
      "Trained batch 1433 batch loss 0.579436302 epoch total loss 0.567855179\n",
      "Trained batch 1434 batch loss 0.592173278 epoch total loss 0.567872167\n",
      "Trained batch 1435 batch loss 0.582395852 epoch total loss 0.567882299\n",
      "Trained batch 1436 batch loss 0.606186271 epoch total loss 0.567908943\n",
      "Trained batch 1437 batch loss 0.577386 epoch total loss 0.567915559\n",
      "Trained batch 1438 batch loss 0.580448627 epoch total loss 0.567924261\n",
      "Trained batch 1439 batch loss 0.585875452 epoch total loss 0.567936778\n",
      "Trained batch 1440 batch loss 0.501243591 epoch total loss 0.567890406\n",
      "Trained batch 1441 batch loss 0.50183785 epoch total loss 0.56784457\n",
      "Trained batch 1442 batch loss 0.581287265 epoch total loss 0.567853928\n",
      "Trained batch 1443 batch loss 0.57470578 epoch total loss 0.567858636\n",
      "Trained batch 1444 batch loss 0.55297786 epoch total loss 0.567848325\n",
      "Trained batch 1445 batch loss 0.609055042 epoch total loss 0.567876875\n",
      "Trained batch 1446 batch loss 0.622852 epoch total loss 0.567914903\n",
      "Trained batch 1447 batch loss 0.505083859 epoch total loss 0.567871451\n",
      "Trained batch 1448 batch loss 0.657679498 epoch total loss 0.5679335\n",
      "Trained batch 1449 batch loss 0.597937822 epoch total loss 0.567954183\n",
      "Trained batch 1450 batch loss 0.587794781 epoch total loss 0.567967892\n",
      "Trained batch 1451 batch loss 0.623041928 epoch total loss 0.5680058\n",
      "Trained batch 1452 batch loss 0.567760468 epoch total loss 0.568005621\n",
      "Trained batch 1453 batch loss 0.694068789 epoch total loss 0.568092406\n",
      "Trained batch 1454 batch loss 0.597846031 epoch total loss 0.56811291\n",
      "Trained batch 1455 batch loss 0.598341346 epoch total loss 0.568133652\n",
      "Trained batch 1456 batch loss 0.557794511 epoch total loss 0.568126559\n",
      "Trained batch 1457 batch loss 0.679593086 epoch total loss 0.568203032\n",
      "Trained batch 1458 batch loss 0.612232804 epoch total loss 0.568233252\n",
      "Trained batch 1459 batch loss 0.644621372 epoch total loss 0.568285584\n",
      "Trained batch 1460 batch loss 0.643308163 epoch total loss 0.568336964\n",
      "Trained batch 1461 batch loss 0.65781939 epoch total loss 0.568398237\n",
      "Trained batch 1462 batch loss 0.542157292 epoch total loss 0.568380296\n",
      "Trained batch 1463 batch loss 0.502869666 epoch total loss 0.568335533\n",
      "Trained batch 1464 batch loss 0.514365 epoch total loss 0.568298638\n",
      "Trained batch 1465 batch loss 0.580928683 epoch total loss 0.568307281\n",
      "Trained batch 1466 batch loss 0.561097622 epoch total loss 0.568302333\n",
      "Trained batch 1467 batch loss 0.567549765 epoch total loss 0.568301857\n",
      "Trained batch 1468 batch loss 0.511969328 epoch total loss 0.568263471\n",
      "Trained batch 1469 batch loss 0.501282871 epoch total loss 0.568217874\n",
      "Trained batch 1470 batch loss 0.462356985 epoch total loss 0.568145812\n",
      "Trained batch 1471 batch loss 0.471740097 epoch total loss 0.568080306\n",
      "Trained batch 1472 batch loss 0.605294704 epoch total loss 0.568105578\n",
      "Trained batch 1473 batch loss 0.610011101 epoch total loss 0.568134\n",
      "Trained batch 1474 batch loss 0.521105051 epoch total loss 0.568102121\n",
      "Trained batch 1475 batch loss 0.578476489 epoch total loss 0.568109155\n",
      "Trained batch 1476 batch loss 0.663929582 epoch total loss 0.568174064\n",
      "Trained batch 1477 batch loss 0.547569156 epoch total loss 0.568160117\n",
      "Trained batch 1478 batch loss 0.556146324 epoch total loss 0.568152\n",
      "Trained batch 1479 batch loss 0.53392309 epoch total loss 0.568128824\n",
      "Trained batch 1480 batch loss 0.603874505 epoch total loss 0.568153\n",
      "Trained batch 1481 batch loss 0.530658 epoch total loss 0.568127692\n",
      "Trained batch 1482 batch loss 0.655008316 epoch total loss 0.568186343\n",
      "Trained batch 1483 batch loss 0.552062631 epoch total loss 0.568175435\n",
      "Trained batch 1484 batch loss 0.491017342 epoch total loss 0.56812346\n",
      "Trained batch 1485 batch loss 0.511832774 epoch total loss 0.568085551\n",
      "Trained batch 1486 batch loss 0.479415745 epoch total loss 0.568025887\n",
      "Trained batch 1487 batch loss 0.620042622 epoch total loss 0.568060875\n",
      "Trained batch 1488 batch loss 0.509473264 epoch total loss 0.568021536\n",
      "Trained batch 1489 batch loss 0.495144695 epoch total loss 0.567972541\n",
      "Trained batch 1490 batch loss 0.470502496 epoch total loss 0.567907155\n",
      "Trained batch 1491 batch loss 0.492420137 epoch total loss 0.56785655\n",
      "Trained batch 1492 batch loss 0.492458224 epoch total loss 0.567805946\n",
      "Trained batch 1493 batch loss 0.46778354 epoch total loss 0.56773895\n",
      "Trained batch 1494 batch loss 0.486853302 epoch total loss 0.567684829\n",
      "Trained batch 1495 batch loss 0.432894111 epoch total loss 0.567594707\n",
      "Trained batch 1496 batch loss 0.469848424 epoch total loss 0.56752938\n",
      "Trained batch 1497 batch loss 0.437431633 epoch total loss 0.567442477\n",
      "Trained batch 1498 batch loss 0.501421213 epoch total loss 0.567398369\n",
      "Trained batch 1499 batch loss 0.588604629 epoch total loss 0.567412555\n",
      "Trained batch 1500 batch loss 0.523889959 epoch total loss 0.567383528\n",
      "Trained batch 1501 batch loss 0.516388535 epoch total loss 0.567349553\n",
      "Trained batch 1502 batch loss 0.564256847 epoch total loss 0.567347527\n",
      "Trained batch 1503 batch loss 0.478290826 epoch total loss 0.56728822\n",
      "Trained batch 1504 batch loss 0.518699765 epoch total loss 0.567255914\n",
      "Trained batch 1505 batch loss 0.500355124 epoch total loss 0.567211449\n",
      "Trained batch 1506 batch loss 0.536345 epoch total loss 0.567190945\n",
      "Trained batch 1507 batch loss 0.511878908 epoch total loss 0.567154288\n",
      "Trained batch 1508 batch loss 0.506271422 epoch total loss 0.567113876\n",
      "Trained batch 1509 batch loss 0.549171627 epoch total loss 0.567102\n",
      "Trained batch 1510 batch loss 0.632863462 epoch total loss 0.567145586\n",
      "Trained batch 1511 batch loss 0.511057556 epoch total loss 0.567108452\n",
      "Trained batch 1512 batch loss 0.633491933 epoch total loss 0.567152381\n",
      "Trained batch 1513 batch loss 0.587976217 epoch total loss 0.56716609\n",
      "Trained batch 1514 batch loss 0.659555435 epoch total loss 0.567227125\n",
      "Trained batch 1515 batch loss 0.535099745 epoch total loss 0.567205906\n",
      "Trained batch 1516 batch loss 0.620283067 epoch total loss 0.567240953\n",
      "Trained batch 1517 batch loss 0.570707142 epoch total loss 0.567243218\n",
      "Trained batch 1518 batch loss 0.551361084 epoch total loss 0.567232728\n",
      "Trained batch 1519 batch loss 0.523463845 epoch total loss 0.567203939\n",
      "Trained batch 1520 batch loss 0.562857747 epoch total loss 0.567201078\n",
      "Trained batch 1521 batch loss 0.5349226 epoch total loss 0.567179859\n",
      "Trained batch 1522 batch loss 0.466063499 epoch total loss 0.5671134\n",
      "Trained batch 1523 batch loss 0.499257982 epoch total loss 0.567068875\n",
      "Trained batch 1524 batch loss 0.517398596 epoch total loss 0.567036271\n",
      "Trained batch 1525 batch loss 0.504305363 epoch total loss 0.566995144\n",
      "Trained batch 1526 batch loss 0.519058645 epoch total loss 0.566963732\n",
      "Trained batch 1527 batch loss 0.613323689 epoch total loss 0.566994071\n",
      "Trained batch 1528 batch loss 0.575665832 epoch total loss 0.566999793\n",
      "Trained batch 1529 batch loss 0.549503922 epoch total loss 0.566988349\n",
      "Trained batch 1530 batch loss 0.575829089 epoch total loss 0.566994131\n",
      "Trained batch 1531 batch loss 0.620302737 epoch total loss 0.56702894\n",
      "Trained batch 1532 batch loss 0.535140514 epoch total loss 0.567008138\n",
      "Trained batch 1533 batch loss 0.568251133 epoch total loss 0.567008913\n",
      "Trained batch 1534 batch loss 0.577378631 epoch total loss 0.567015707\n",
      "Trained batch 1535 batch loss 0.412540883 epoch total loss 0.566915035\n",
      "Trained batch 1536 batch loss 0.557663143 epoch total loss 0.566909\n",
      "Trained batch 1537 batch loss 0.559395432 epoch total loss 0.566904128\n",
      "Trained batch 1538 batch loss 0.437035471 epoch total loss 0.566819668\n",
      "Trained batch 1539 batch loss 0.554686069 epoch total loss 0.5668118\n",
      "Trained batch 1540 batch loss 0.518249333 epoch total loss 0.566780269\n",
      "Trained batch 1541 batch loss 0.520578265 epoch total loss 0.566750288\n",
      "Trained batch 1542 batch loss 0.572233081 epoch total loss 0.566753805\n",
      "Trained batch 1543 batch loss 0.577742159 epoch total loss 0.566760957\n",
      "Trained batch 1544 batch loss 0.669395328 epoch total loss 0.566827416\n",
      "Trained batch 1545 batch loss 0.582657099 epoch total loss 0.566837668\n",
      "Trained batch 1546 batch loss 0.59521395 epoch total loss 0.566856\n",
      "Trained batch 1547 batch loss 0.552122355 epoch total loss 0.56684649\n",
      "Trained batch 1548 batch loss 0.572033763 epoch total loss 0.566849828\n",
      "Trained batch 1549 batch loss 0.549555182 epoch total loss 0.566838682\n",
      "Trained batch 1550 batch loss 0.525336146 epoch total loss 0.56681186\n",
      "Trained batch 1551 batch loss 0.517178059 epoch total loss 0.566779852\n",
      "Trained batch 1552 batch loss 0.636505306 epoch total loss 0.566824794\n",
      "Trained batch 1553 batch loss 0.494943619 epoch total loss 0.566778541\n",
      "Trained batch 1554 batch loss 0.598689258 epoch total loss 0.566799045\n",
      "Trained batch 1555 batch loss 0.573365152 epoch total loss 0.566803277\n",
      "Trained batch 1556 batch loss 0.579462707 epoch total loss 0.566811383\n",
      "Trained batch 1557 batch loss 0.519184709 epoch total loss 0.566780806\n",
      "Trained batch 1558 batch loss 0.537737191 epoch total loss 0.566762149\n",
      "Trained batch 1559 batch loss 0.5297997 epoch total loss 0.566738427\n",
      "Trained batch 1560 batch loss 0.580382705 epoch total loss 0.566747189\n",
      "Trained batch 1561 batch loss 0.561787128 epoch total loss 0.56674397\n",
      "Trained batch 1562 batch loss 0.554513812 epoch total loss 0.566736162\n",
      "Trained batch 1563 batch loss 0.498254538 epoch total loss 0.566692352\n",
      "Trained batch 1564 batch loss 0.422807097 epoch total loss 0.566600323\n",
      "Trained batch 1565 batch loss 0.425700456 epoch total loss 0.56651032\n",
      "Trained batch 1566 batch loss 0.561531961 epoch total loss 0.566507101\n",
      "Trained batch 1567 batch loss 0.618382454 epoch total loss 0.566540241\n",
      "Trained batch 1568 batch loss 0.652601242 epoch total loss 0.566595137\n",
      "Trained batch 1569 batch loss 0.633521199 epoch total loss 0.566637814\n",
      "Trained batch 1570 batch loss 0.567864 epoch total loss 0.566638589\n",
      "Trained batch 1571 batch loss 0.612630606 epoch total loss 0.566667855\n",
      "Trained batch 1572 batch loss 0.606577575 epoch total loss 0.566693187\n",
      "Trained batch 1573 batch loss 0.614274144 epoch total loss 0.566723466\n",
      "Trained batch 1574 batch loss 0.592520595 epoch total loss 0.566739857\n",
      "Trained batch 1575 batch loss 0.647779524 epoch total loss 0.566791296\n",
      "Trained batch 1576 batch loss 0.669298053 epoch total loss 0.566856325\n",
      "Trained batch 1577 batch loss 0.591592789 epoch total loss 0.56687206\n",
      "Trained batch 1578 batch loss 0.576378167 epoch total loss 0.56687808\n",
      "Trained batch 1579 batch loss 0.538734853 epoch total loss 0.566860259\n",
      "Trained batch 1580 batch loss 0.556451797 epoch total loss 0.566853642\n",
      "Trained batch 1581 batch loss 0.645876169 epoch total loss 0.566903651\n",
      "Trained batch 1582 batch loss 0.620198846 epoch total loss 0.566937327\n",
      "Trained batch 1583 batch loss 0.694762409 epoch total loss 0.567018092\n",
      "Trained batch 1584 batch loss 0.590452671 epoch total loss 0.567032874\n",
      "Trained batch 1585 batch loss 0.582553625 epoch total loss 0.567042649\n",
      "Trained batch 1586 batch loss 0.492832363 epoch total loss 0.566995919\n",
      "Trained batch 1587 batch loss 0.487938583 epoch total loss 0.566946089\n",
      "Trained batch 1588 batch loss 0.52623564 epoch total loss 0.566920459\n",
      "Trained batch 1589 batch loss 0.529198885 epoch total loss 0.566896677\n",
      "Trained batch 1590 batch loss 0.437811136 epoch total loss 0.566815495\n",
      "Trained batch 1591 batch loss 0.434862852 epoch total loss 0.566732585\n",
      "Trained batch 1592 batch loss 0.42076689 epoch total loss 0.566640854\n",
      "Trained batch 1593 batch loss 0.530558586 epoch total loss 0.566618264\n",
      "Trained batch 1594 batch loss 0.554126561 epoch total loss 0.566610396\n",
      "Trained batch 1595 batch loss 0.579523325 epoch total loss 0.566618502\n",
      "Trained batch 1596 batch loss 0.579118669 epoch total loss 0.56662631\n",
      "Trained batch 1597 batch loss 0.605407059 epoch total loss 0.566650629\n",
      "Trained batch 1598 batch loss 0.584013343 epoch total loss 0.566661477\n",
      "Trained batch 1599 batch loss 0.529889822 epoch total loss 0.56663847\n",
      "Trained batch 1600 batch loss 0.598073721 epoch total loss 0.566658139\n",
      "Trained batch 1601 batch loss 0.625656366 epoch total loss 0.566695\n",
      "Trained batch 1602 batch loss 0.66833514 epoch total loss 0.566758454\n",
      "Trained batch 1603 batch loss 0.565531373 epoch total loss 0.566757679\n",
      "Trained batch 1604 batch loss 0.580857635 epoch total loss 0.5667665\n",
      "Trained batch 1605 batch loss 0.495448977 epoch total loss 0.566722035\n",
      "Trained batch 1606 batch loss 0.533377171 epoch total loss 0.566701293\n",
      "Trained batch 1607 batch loss 0.593063116 epoch total loss 0.566717684\n",
      "Trained batch 1608 batch loss 0.54896754 epoch total loss 0.566706657\n",
      "Trained batch 1609 batch loss 0.559710622 epoch total loss 0.566702306\n",
      "Trained batch 1610 batch loss 0.579638422 epoch total loss 0.566710353\n",
      "Trained batch 1611 batch loss 0.594146073 epoch total loss 0.56672734\n",
      "Trained batch 1612 batch loss 0.575216353 epoch total loss 0.566732585\n",
      "Trained batch 1613 batch loss 0.502206 epoch total loss 0.566692591\n",
      "Trained batch 1614 batch loss 0.655378103 epoch total loss 0.566747546\n",
      "Trained batch 1615 batch loss 0.742592335 epoch total loss 0.566856444\n",
      "Trained batch 1616 batch loss 0.641906857 epoch total loss 0.566902876\n",
      "Trained batch 1617 batch loss 0.645110607 epoch total loss 0.566951215\n",
      "Trained batch 1618 batch loss 0.627000809 epoch total loss 0.566988349\n",
      "Trained batch 1619 batch loss 0.588098466 epoch total loss 0.567001343\n",
      "Trained batch 1620 batch loss 0.580075741 epoch total loss 0.567009449\n",
      "Trained batch 1621 batch loss 0.570419908 epoch total loss 0.567011535\n",
      "Trained batch 1622 batch loss 0.48547402 epoch total loss 0.566961288\n",
      "Trained batch 1623 batch loss 0.443048 epoch total loss 0.566884935\n",
      "Trained batch 1624 batch loss 0.43042028 epoch total loss 0.566800892\n",
      "Trained batch 1625 batch loss 0.368181407 epoch total loss 0.566678703\n",
      "Trained batch 1626 batch loss 0.519804418 epoch total loss 0.566649854\n",
      "Trained batch 1627 batch loss 0.547863483 epoch total loss 0.566638291\n",
      "Trained batch 1628 batch loss 0.523376465 epoch total loss 0.566611707\n",
      "Trained batch 1629 batch loss 0.56070739 epoch total loss 0.566608071\n",
      "Trained batch 1630 batch loss 0.415395439 epoch total loss 0.566515326\n",
      "Trained batch 1631 batch loss 0.36319 epoch total loss 0.566390693\n",
      "Trained batch 1632 batch loss 0.415924489 epoch total loss 0.566298485\n",
      "Trained batch 1633 batch loss 0.377776951 epoch total loss 0.566183031\n",
      "Trained batch 1634 batch loss 0.464989901 epoch total loss 0.566121101\n",
      "Trained batch 1635 batch loss 0.454016656 epoch total loss 0.566052556\n",
      "Trained batch 1636 batch loss 0.513803542 epoch total loss 0.566020608\n",
      "Trained batch 1637 batch loss 0.471104652 epoch total loss 0.565962613\n",
      "Trained batch 1638 batch loss 0.535550714 epoch total loss 0.565944076\n",
      "Trained batch 1639 batch loss 0.532206774 epoch total loss 0.565923452\n",
      "Trained batch 1640 batch loss 0.529810667 epoch total loss 0.565901458\n",
      "Trained batch 1641 batch loss 0.53214246 epoch total loss 0.565880895\n",
      "Trained batch 1642 batch loss 0.593659699 epoch total loss 0.565897822\n",
      "Trained batch 1643 batch loss 0.583934426 epoch total loss 0.56590879\n",
      "Trained batch 1644 batch loss 0.516032398 epoch total loss 0.565878451\n",
      "Trained batch 1645 batch loss 0.517761767 epoch total loss 0.565849245\n",
      "Trained batch 1646 batch loss 0.558993161 epoch total loss 0.565845072\n",
      "Trained batch 1647 batch loss 0.621269524 epoch total loss 0.565878749\n",
      "Trained batch 1648 batch loss 0.619211376 epoch total loss 0.565911055\n",
      "Trained batch 1649 batch loss 0.539497793 epoch total loss 0.565895081\n",
      "Trained batch 1650 batch loss 0.541489124 epoch total loss 0.565880299\n",
      "Trained batch 1651 batch loss 0.460525066 epoch total loss 0.565816462\n",
      "Trained batch 1652 batch loss 0.554686844 epoch total loss 0.565809727\n",
      "Trained batch 1653 batch loss 0.562893689 epoch total loss 0.565807939\n",
      "Trained batch 1654 batch loss 0.574329555 epoch total loss 0.565813124\n",
      "Trained batch 1655 batch loss 0.438759923 epoch total loss 0.565736353\n",
      "Trained batch 1656 batch loss 0.543581545 epoch total loss 0.565722942\n",
      "Trained batch 1657 batch loss 0.613146067 epoch total loss 0.565751612\n",
      "Trained batch 1658 batch loss 0.591739714 epoch total loss 0.565767229\n",
      "Trained batch 1659 batch loss 0.624042273 epoch total loss 0.565802395\n",
      "Trained batch 1660 batch loss 0.577443063 epoch total loss 0.565809369\n",
      "Trained batch 1661 batch loss 0.605355084 epoch total loss 0.565833211\n",
      "Trained batch 1662 batch loss 0.571746647 epoch total loss 0.565836728\n",
      "Trained batch 1663 batch loss 0.552685142 epoch total loss 0.5658288\n",
      "Trained batch 1664 batch loss 0.546421945 epoch total loss 0.565817177\n",
      "Trained batch 1665 batch loss 0.682556629 epoch total loss 0.565887272\n",
      "Trained batch 1666 batch loss 0.63901341 epoch total loss 0.565931201\n",
      "Trained batch 1667 batch loss 0.549873 epoch total loss 0.565921545\n",
      "Trained batch 1668 batch loss 0.581535518 epoch total loss 0.565930903\n",
      "Trained batch 1669 batch loss 0.59296608 epoch total loss 0.565947115\n",
      "Trained batch 1670 batch loss 0.631490588 epoch total loss 0.565986335\n",
      "Trained batch 1671 batch loss 0.655294895 epoch total loss 0.566039801\n",
      "Trained batch 1672 batch loss 0.551741064 epoch total loss 0.566031218\n",
      "Trained batch 1673 batch loss 0.57302916 epoch total loss 0.56603545\n",
      "Trained batch 1674 batch loss 0.518504679 epoch total loss 0.566007\n",
      "Trained batch 1675 batch loss 0.497422397 epoch total loss 0.565966129\n",
      "Trained batch 1676 batch loss 0.563655853 epoch total loss 0.565964699\n",
      "Trained batch 1677 batch loss 0.548873901 epoch total loss 0.565954566\n",
      "Trained batch 1678 batch loss 0.607480705 epoch total loss 0.565979302\n",
      "Trained batch 1679 batch loss 0.622766495 epoch total loss 0.566013098\n",
      "Trained batch 1680 batch loss 0.552017152 epoch total loss 0.566004753\n",
      "Trained batch 1681 batch loss 0.550945818 epoch total loss 0.565995812\n",
      "Trained batch 1682 batch loss 0.730286181 epoch total loss 0.566093504\n",
      "Trained batch 1683 batch loss 0.588075519 epoch total loss 0.566106558\n",
      "Trained batch 1684 batch loss 0.623530149 epoch total loss 0.566140652\n",
      "Trained batch 1685 batch loss 0.599468172 epoch total loss 0.56616044\n",
      "Trained batch 1686 batch loss 0.651901126 epoch total loss 0.566211283\n",
      "Trained batch 1687 batch loss 0.605006 epoch total loss 0.566234291\n",
      "Trained batch 1688 batch loss 0.439357758 epoch total loss 0.566159129\n",
      "Trained batch 1689 batch loss 0.484007448 epoch total loss 0.566110492\n",
      "Trained batch 1690 batch loss 0.607658148 epoch total loss 0.566135049\n",
      "Trained batch 1691 batch loss 0.584446788 epoch total loss 0.566145897\n",
      "Trained batch 1692 batch loss 0.669377387 epoch total loss 0.566206932\n",
      "Trained batch 1693 batch loss 0.598705053 epoch total loss 0.566226125\n",
      "Trained batch 1694 batch loss 0.505519271 epoch total loss 0.566190243\n",
      "Trained batch 1695 batch loss 0.65378356 epoch total loss 0.56624192\n",
      "Trained batch 1696 batch loss 0.595480323 epoch total loss 0.566259146\n",
      "Trained batch 1697 batch loss 0.54417032 epoch total loss 0.566246152\n",
      "Trained batch 1698 batch loss 0.555234849 epoch total loss 0.566239655\n",
      "Trained batch 1699 batch loss 0.500075459 epoch total loss 0.566200733\n",
      "Trained batch 1700 batch loss 0.541799128 epoch total loss 0.566186368\n",
      "Trained batch 1701 batch loss 0.559823275 epoch total loss 0.566182613\n",
      "Trained batch 1702 batch loss 0.507861137 epoch total loss 0.5661484\n",
      "Trained batch 1703 batch loss 0.501591384 epoch total loss 0.566110492\n",
      "Trained batch 1704 batch loss 0.489429295 epoch total loss 0.56606549\n",
      "Trained batch 1705 batch loss 0.503493488 epoch total loss 0.566028774\n",
      "Trained batch 1706 batch loss 0.550419033 epoch total loss 0.566019595\n",
      "Trained batch 1707 batch loss 0.443112731 epoch total loss 0.565947592\n",
      "Trained batch 1708 batch loss 0.532687545 epoch total loss 0.565928161\n",
      "Trained batch 1709 batch loss 0.48498553 epoch total loss 0.565880775\n",
      "Trained batch 1710 batch loss 0.493734539 epoch total loss 0.565838575\n",
      "Trained batch 1711 batch loss 0.478678584 epoch total loss 0.565787673\n",
      "Trained batch 1712 batch loss 0.468905896 epoch total loss 0.565731108\n",
      "Trained batch 1713 batch loss 0.44587031 epoch total loss 0.565661132\n",
      "Trained batch 1714 batch loss 0.570135951 epoch total loss 0.565663695\n",
      "Trained batch 1715 batch loss 0.454225719 epoch total loss 0.565598726\n",
      "Trained batch 1716 batch loss 0.49039337 epoch total loss 0.565554917\n",
      "Trained batch 1717 batch loss 0.510983765 epoch total loss 0.565523148\n",
      "Trained batch 1718 batch loss 0.49675858 epoch total loss 0.565483093\n",
      "Trained batch 1719 batch loss 0.726611376 epoch total loss 0.565576851\n",
      "Trained batch 1720 batch loss 0.710905552 epoch total loss 0.565661311\n",
      "Trained batch 1721 batch loss 0.584099889 epoch total loss 0.56567204\n",
      "Trained batch 1722 batch loss 0.525649965 epoch total loss 0.565648794\n",
      "Trained batch 1723 batch loss 0.533201218 epoch total loss 0.565629959\n",
      "Trained batch 1724 batch loss 0.546363354 epoch total loss 0.565618813\n",
      "Trained batch 1725 batch loss 0.607278407 epoch total loss 0.565642953\n",
      "Trained batch 1726 batch loss 0.466488153 epoch total loss 0.565585554\n",
      "Trained batch 1727 batch loss 0.597933769 epoch total loss 0.56560427\n",
      "Trained batch 1728 batch loss 0.578434467 epoch total loss 0.56561172\n",
      "Trained batch 1729 batch loss 0.507026196 epoch total loss 0.565577805\n",
      "Trained batch 1730 batch loss 0.475337863 epoch total loss 0.565525651\n",
      "Trained batch 1731 batch loss 0.527458072 epoch total loss 0.565503657\n",
      "Trained batch 1732 batch loss 0.623616576 epoch total loss 0.565537214\n",
      "Trained batch 1733 batch loss 0.562600732 epoch total loss 0.565535545\n",
      "Trained batch 1734 batch loss 0.573365033 epoch total loss 0.56554\n",
      "Trained batch 1735 batch loss 0.61274 epoch total loss 0.565567255\n",
      "Trained batch 1736 batch loss 0.546120882 epoch total loss 0.565556049\n",
      "Trained batch 1737 batch loss 0.566863239 epoch total loss 0.565556765\n",
      "Trained batch 1738 batch loss 0.540968776 epoch total loss 0.565542638\n",
      "Trained batch 1739 batch loss 0.58354795 epoch total loss 0.565553\n",
      "Trained batch 1740 batch loss 0.592054963 epoch total loss 0.565568209\n",
      "Trained batch 1741 batch loss 0.641149 epoch total loss 0.56561166\n",
      "Trained batch 1742 batch loss 0.671022534 epoch total loss 0.565672159\n",
      "Trained batch 1743 batch loss 0.618124604 epoch total loss 0.5657022\n",
      "Trained batch 1744 batch loss 0.536834 epoch total loss 0.56568563\n",
      "Trained batch 1745 batch loss 0.647891283 epoch total loss 0.565732777\n",
      "Trained batch 1746 batch loss 0.536290228 epoch total loss 0.565715909\n",
      "Trained batch 1747 batch loss 0.490857065 epoch total loss 0.565673053\n",
      "Trained batch 1748 batch loss 0.505986094 epoch total loss 0.5656389\n",
      "Trained batch 1749 batch loss 0.488815159 epoch total loss 0.565595\n",
      "Trained batch 1750 batch loss 0.497479379 epoch total loss 0.565556109\n",
      "Trained batch 1751 batch loss 0.510340214 epoch total loss 0.565524518\n",
      "Trained batch 1752 batch loss 0.563962698 epoch total loss 0.565523624\n",
      "Trained batch 1753 batch loss 0.525191903 epoch total loss 0.565500617\n",
      "Trained batch 1754 batch loss 0.550127804 epoch total loss 0.565491855\n",
      "Trained batch 1755 batch loss 0.611646831 epoch total loss 0.565518141\n",
      "Trained batch 1756 batch loss 0.542192876 epoch total loss 0.565504849\n",
      "Trained batch 1757 batch loss 0.602468312 epoch total loss 0.565525889\n",
      "Trained batch 1758 batch loss 0.51742667 epoch total loss 0.56549859\n",
      "Trained batch 1759 batch loss 0.486431181 epoch total loss 0.565453649\n",
      "Trained batch 1760 batch loss 0.496555686 epoch total loss 0.565414488\n",
      "Trained batch 1761 batch loss 0.516238689 epoch total loss 0.565386593\n",
      "Trained batch 1762 batch loss 0.490338922 epoch total loss 0.565344\n",
      "Trained batch 1763 batch loss 0.495606899 epoch total loss 0.565304458\n",
      "Trained batch 1764 batch loss 0.606967032 epoch total loss 0.565328062\n",
      "Trained batch 1765 batch loss 0.543650508 epoch total loss 0.565315783\n",
      "Trained batch 1766 batch loss 0.70095706 epoch total loss 0.565392554\n",
      "Trained batch 1767 batch loss 0.657691 epoch total loss 0.565444827\n",
      "Trained batch 1768 batch loss 0.59002769 epoch total loss 0.565458715\n",
      "Trained batch 1769 batch loss 0.520860434 epoch total loss 0.565433502\n",
      "Trained batch 1770 batch loss 0.556994915 epoch total loss 0.565428734\n",
      "Trained batch 1771 batch loss 0.555971861 epoch total loss 0.565423429\n",
      "Trained batch 1772 batch loss 0.433640629 epoch total loss 0.565349042\n",
      "Trained batch 1773 batch loss 0.390630364 epoch total loss 0.565250516\n",
      "Trained batch 1774 batch loss 0.387588948 epoch total loss 0.565150321\n",
      "Trained batch 1775 batch loss 0.317727417 epoch total loss 0.565010965\n",
      "Trained batch 1776 batch loss 0.372654438 epoch total loss 0.564902663\n",
      "Trained batch 1777 batch loss 0.370746225 epoch total loss 0.564793408\n",
      "Trained batch 1778 batch loss 0.444577426 epoch total loss 0.564725816\n",
      "Trained batch 1779 batch loss 0.440919638 epoch total loss 0.564656198\n",
      "Trained batch 1780 batch loss 0.608873725 epoch total loss 0.564681053\n",
      "Trained batch 1781 batch loss 0.701281726 epoch total loss 0.564757764\n",
      "Trained batch 1782 batch loss 0.549743 epoch total loss 0.5647493\n",
      "Trained batch 1783 batch loss 0.44595781 epoch total loss 0.564682722\n",
      "Trained batch 1784 batch loss 0.524753094 epoch total loss 0.56466037\n",
      "Trained batch 1785 batch loss 0.59067136 epoch total loss 0.564674914\n",
      "Trained batch 1786 batch loss 0.581141472 epoch total loss 0.564684153\n",
      "Trained batch 1787 batch loss 0.698181152 epoch total loss 0.564758837\n",
      "Trained batch 1788 batch loss 0.516003788 epoch total loss 0.564731598\n",
      "Trained batch 1789 batch loss 0.487651527 epoch total loss 0.564688504\n",
      "Trained batch 1790 batch loss 0.475557834 epoch total loss 0.564638734\n",
      "Trained batch 1791 batch loss 0.501677275 epoch total loss 0.564603567\n",
      "Trained batch 1792 batch loss 0.676318109 epoch total loss 0.564665914\n",
      "Trained batch 1793 batch loss 0.641199589 epoch total loss 0.564708591\n",
      "Trained batch 1794 batch loss 0.711319923 epoch total loss 0.564790308\n",
      "Trained batch 1795 batch loss 0.648690522 epoch total loss 0.564837039\n",
      "Trained batch 1796 batch loss 0.554262042 epoch total loss 0.564831138\n",
      "Trained batch 1797 batch loss 0.561551869 epoch total loss 0.56482929\n",
      "Trained batch 1798 batch loss 0.449683547 epoch total loss 0.564765275\n",
      "Trained batch 1799 batch loss 0.444499314 epoch total loss 0.564698398\n",
      "Trained batch 1800 batch loss 0.508434057 epoch total loss 0.564667165\n",
      "Trained batch 1801 batch loss 0.40064469 epoch total loss 0.564576089\n",
      "Trained batch 1802 batch loss 0.535481 epoch total loss 0.564559937\n",
      "Trained batch 1803 batch loss 0.475476742 epoch total loss 0.564510524\n",
      "Trained batch 1804 batch loss 0.46761325 epoch total loss 0.564456761\n",
      "Trained batch 1805 batch loss 0.485551536 epoch total loss 0.564413071\n",
      "Trained batch 1806 batch loss 0.443690717 epoch total loss 0.564346194\n",
      "Trained batch 1807 batch loss 0.456102729 epoch total loss 0.564286292\n",
      "Trained batch 1808 batch loss 0.506773293 epoch total loss 0.564254463\n",
      "Trained batch 1809 batch loss 0.52068305 epoch total loss 0.564230382\n",
      "Trained batch 1810 batch loss 0.514734268 epoch total loss 0.564203\n",
      "Trained batch 1811 batch loss 0.500393391 epoch total loss 0.564167798\n",
      "Trained batch 1812 batch loss 0.492393732 epoch total loss 0.56412816\n",
      "Trained batch 1813 batch loss 0.558571696 epoch total loss 0.564125121\n",
      "Trained batch 1814 batch loss 0.47823143 epoch total loss 0.564077735\n",
      "Trained batch 1815 batch loss 0.525860906 epoch total loss 0.564056695\n",
      "Trained batch 1816 batch loss 0.619409204 epoch total loss 0.564087212\n",
      "Trained batch 1817 batch loss 0.608074069 epoch total loss 0.564111352\n",
      "Trained batch 1818 batch loss 0.636052489 epoch total loss 0.564151\n",
      "Trained batch 1819 batch loss 0.567884266 epoch total loss 0.564153\n",
      "Trained batch 1820 batch loss 0.598773062 epoch total loss 0.564172\n",
      "Trained batch 1821 batch loss 0.559886336 epoch total loss 0.564169705\n",
      "Trained batch 1822 batch loss 0.564588785 epoch total loss 0.564169943\n",
      "Trained batch 1823 batch loss 0.562413812 epoch total loss 0.56416893\n",
      "Trained batch 1824 batch loss 0.559550405 epoch total loss 0.564166427\n",
      "Trained batch 1825 batch loss 0.512130737 epoch total loss 0.564137876\n",
      "Trained batch 1826 batch loss 0.572865605 epoch total loss 0.564142644\n",
      "Trained batch 1827 batch loss 0.596971631 epoch total loss 0.564160585\n",
      "Trained batch 1828 batch loss 0.59915787 epoch total loss 0.564179718\n",
      "Trained batch 1829 batch loss 0.531764567 epoch total loss 0.564162\n",
      "Trained batch 1830 batch loss 0.585645437 epoch total loss 0.564173758\n",
      "Trained batch 1831 batch loss 0.551327229 epoch total loss 0.564166725\n",
      "Trained batch 1832 batch loss 0.559963226 epoch total loss 0.5641644\n",
      "Trained batch 1833 batch loss 0.561063 epoch total loss 0.564162672\n",
      "Trained batch 1834 batch loss 0.569239914 epoch total loss 0.564165473\n",
      "Trained batch 1835 batch loss 0.522720218 epoch total loss 0.564142883\n",
      "Trained batch 1836 batch loss 0.555341244 epoch total loss 0.564138055\n",
      "Trained batch 1837 batch loss 0.563418329 epoch total loss 0.564137697\n",
      "Trained batch 1838 batch loss 0.578261912 epoch total loss 0.564145386\n",
      "Trained batch 1839 batch loss 0.613224745 epoch total loss 0.564172089\n",
      "Trained batch 1840 batch loss 0.618642032 epoch total loss 0.564201713\n",
      "Trained batch 1841 batch loss 0.597301 epoch total loss 0.564219654\n",
      "Trained batch 1842 batch loss 0.609785259 epoch total loss 0.56424439\n",
      "Trained batch 1843 batch loss 0.619609475 epoch total loss 0.56427443\n",
      "Trained batch 1844 batch loss 0.524193287 epoch total loss 0.564252675\n",
      "Trained batch 1845 batch loss 0.552234352 epoch total loss 0.564246178\n",
      "Trained batch 1846 batch loss 0.44771418 epoch total loss 0.564183056\n",
      "Trained batch 1847 batch loss 0.540069818 epoch total loss 0.56417\n",
      "Trained batch 1848 batch loss 0.497053951 epoch total loss 0.564133704\n",
      "Trained batch 1849 batch loss 0.534459233 epoch total loss 0.56411761\n",
      "Trained batch 1850 batch loss 0.613627851 epoch total loss 0.564144373\n",
      "Trained batch 1851 batch loss 0.564450562 epoch total loss 0.564144552\n",
      "Trained batch 1852 batch loss 0.616340935 epoch total loss 0.564172745\n",
      "Trained batch 1853 batch loss 0.61413753 epoch total loss 0.564199686\n",
      "Trained batch 1854 batch loss 0.605133772 epoch total loss 0.56422174\n",
      "Trained batch 1855 batch loss 0.662352264 epoch total loss 0.564274669\n",
      "Trained batch 1856 batch loss 0.644488811 epoch total loss 0.564317882\n",
      "Trained batch 1857 batch loss 0.647643209 epoch total loss 0.564362764\n",
      "Trained batch 1858 batch loss 0.606271565 epoch total loss 0.564385295\n",
      "Trained batch 1859 batch loss 0.56572336 epoch total loss 0.564386\n",
      "Trained batch 1860 batch loss 0.665396214 epoch total loss 0.56444031\n",
      "Trained batch 1861 batch loss 0.547793865 epoch total loss 0.564431429\n",
      "Trained batch 1862 batch loss 0.604917049 epoch total loss 0.564453125\n",
      "Trained batch 1863 batch loss 0.584942222 epoch total loss 0.564464152\n",
      "Trained batch 1864 batch loss 0.546463132 epoch total loss 0.564454496\n",
      "Trained batch 1865 batch loss 0.46521309 epoch total loss 0.564401269\n",
      "Trained batch 1866 batch loss 0.48825106 epoch total loss 0.564360499\n",
      "Trained batch 1867 batch loss 0.451355755 epoch total loss 0.5643\n",
      "Trained batch 1868 batch loss 0.480013 epoch total loss 0.56425488\n",
      "Trained batch 1869 batch loss 0.482566118 epoch total loss 0.56421113\n",
      "Trained batch 1870 batch loss 0.469980776 epoch total loss 0.564160764\n",
      "Trained batch 1871 batch loss 0.541303 epoch total loss 0.564148486\n",
      "Trained batch 1872 batch loss 0.518664658 epoch total loss 0.564124227\n",
      "Trained batch 1873 batch loss 0.524666786 epoch total loss 0.564103127\n",
      "Trained batch 1874 batch loss 0.586865366 epoch total loss 0.564115286\n",
      "Trained batch 1875 batch loss 0.630363286 epoch total loss 0.564150631\n",
      "Trained batch 1876 batch loss 0.557172298 epoch total loss 0.564146936\n",
      "Trained batch 1877 batch loss 0.517489493 epoch total loss 0.564122\n",
      "Trained batch 1878 batch loss 0.592809379 epoch total loss 0.56413728\n",
      "Trained batch 1879 batch loss 0.597561717 epoch total loss 0.564155042\n",
      "Trained batch 1880 batch loss 0.599158525 epoch total loss 0.564173639\n",
      "Trained batch 1881 batch loss 0.693836868 epoch total loss 0.564242601\n",
      "Trained batch 1882 batch loss 0.610114455 epoch total loss 0.564267\n",
      "Trained batch 1883 batch loss 0.487540662 epoch total loss 0.56422621\n",
      "Trained batch 1884 batch loss 0.528579831 epoch total loss 0.564207315\n",
      "Trained batch 1885 batch loss 0.6206761 epoch total loss 0.564237297\n",
      "Trained batch 1886 batch loss 0.612781882 epoch total loss 0.564263046\n",
      "Trained batch 1887 batch loss 0.560638368 epoch total loss 0.564261138\n",
      "Trained batch 1888 batch loss 0.567867279 epoch total loss 0.564263046\n",
      "Trained batch 1889 batch loss 0.493809342 epoch total loss 0.564225733\n",
      "Trained batch 1890 batch loss 0.512405932 epoch total loss 0.564198315\n",
      "Trained batch 1891 batch loss 0.625294566 epoch total loss 0.564230621\n",
      "Trained batch 1892 batch loss 0.612916 epoch total loss 0.56425637\n",
      "Trained batch 1893 batch loss 0.623850703 epoch total loss 0.564287841\n",
      "Trained batch 1894 batch loss 0.629551589 epoch total loss 0.564322293\n",
      "Trained batch 1895 batch loss 0.62002027 epoch total loss 0.564351678\n",
      "Trained batch 1896 batch loss 0.593452752 epoch total loss 0.564367056\n",
      "Trained batch 1897 batch loss 0.442305624 epoch total loss 0.564302683\n",
      "Trained batch 1898 batch loss 0.592057824 epoch total loss 0.564317286\n",
      "Trained batch 1899 batch loss 0.645844102 epoch total loss 0.564360261\n",
      "Trained batch 1900 batch loss 0.541113317 epoch total loss 0.564348042\n",
      "Trained batch 1901 batch loss 0.603434 epoch total loss 0.564368546\n",
      "Trained batch 1902 batch loss 0.676125824 epoch total loss 0.564427316\n",
      "Trained batch 1903 batch loss 0.627040446 epoch total loss 0.564460218\n",
      "Trained batch 1904 batch loss 0.647370517 epoch total loss 0.564503789\n",
      "Trained batch 1905 batch loss 0.619267344 epoch total loss 0.564532518\n",
      "Trained batch 1906 batch loss 0.456997663 epoch total loss 0.564476132\n",
      "Trained batch 1907 batch loss 0.52672565 epoch total loss 0.564456344\n",
      "Trained batch 1908 batch loss 0.526572406 epoch total loss 0.564436495\n",
      "Trained batch 1909 batch loss 0.567806661 epoch total loss 0.564438224\n",
      "Trained batch 1910 batch loss 0.648788 epoch total loss 0.564482391\n",
      "Trained batch 1911 batch loss 0.641706944 epoch total loss 0.564522803\n",
      "Trained batch 1912 batch loss 0.596056104 epoch total loss 0.564539313\n",
      "Trained batch 1913 batch loss 0.50701946 epoch total loss 0.564509273\n",
      "Trained batch 1914 batch loss 0.589881599 epoch total loss 0.564522505\n",
      "Trained batch 1915 batch loss 0.583913445 epoch total loss 0.564532638\n",
      "Trained batch 1916 batch loss 0.61501193 epoch total loss 0.564558923\n",
      "Trained batch 1917 batch loss 0.581111908 epoch total loss 0.564567566\n",
      "Trained batch 1918 batch loss 0.557808459 epoch total loss 0.564564049\n",
      "Trained batch 1919 batch loss 0.649974048 epoch total loss 0.564608574\n",
      "Trained batch 1920 batch loss 0.533754289 epoch total loss 0.56459254\n",
      "Trained batch 1921 batch loss 0.600195289 epoch total loss 0.564611077\n",
      "Trained batch 1922 batch loss 0.537923574 epoch total loss 0.564597249\n",
      "Trained batch 1923 batch loss 0.57777065 epoch total loss 0.564604104\n",
      "Trained batch 1924 batch loss 0.537451088 epoch total loss 0.56459\n",
      "Trained batch 1925 batch loss 0.547148883 epoch total loss 0.564580917\n",
      "Trained batch 1926 batch loss 0.54118681 epoch total loss 0.564568758\n",
      "Trained batch 1927 batch loss 0.564018667 epoch total loss 0.5645684\n",
      "Trained batch 1928 batch loss 0.52324146 epoch total loss 0.564546943\n",
      "Trained batch 1929 batch loss 0.540227175 epoch total loss 0.564534366\n",
      "Trained batch 1930 batch loss 0.49340114 epoch total loss 0.56449753\n",
      "Trained batch 1931 batch loss 0.564849138 epoch total loss 0.564497709\n",
      "Trained batch 1932 batch loss 0.550399184 epoch total loss 0.564490378\n",
      "Trained batch 1933 batch loss 0.5485906 epoch total loss 0.564482152\n",
      "Trained batch 1934 batch loss 0.499418676 epoch total loss 0.564448535\n",
      "Trained batch 1935 batch loss 0.561007679 epoch total loss 0.564446747\n",
      "Trained batch 1936 batch loss 0.515973926 epoch total loss 0.564421713\n",
      "Trained batch 1937 batch loss 0.509756 epoch total loss 0.56439352\n",
      "Trained batch 1938 batch loss 0.493368506 epoch total loss 0.564356863\n",
      "Trained batch 1939 batch loss 0.458249032 epoch total loss 0.564302146\n",
      "Trained batch 1940 batch loss 0.464479029 epoch total loss 0.564250708\n",
      "Trained batch 1941 batch loss 0.482906491 epoch total loss 0.564208806\n",
      "Trained batch 1942 batch loss 0.449272394 epoch total loss 0.564149559\n",
      "Trained batch 1943 batch loss 0.619221628 epoch total loss 0.56417793\n",
      "Trained batch 1944 batch loss 0.54837805 epoch total loss 0.564169824\n",
      "Trained batch 1945 batch loss 0.56413579 epoch total loss 0.564169765\n",
      "Trained batch 1946 batch loss 0.606237 epoch total loss 0.564191341\n",
      "Trained batch 1947 batch loss 0.540270686 epoch total loss 0.564179063\n",
      "Trained batch 1948 batch loss 0.51778692 epoch total loss 0.564155281\n",
      "Trained batch 1949 batch loss 0.649609625 epoch total loss 0.56419915\n",
      "Trained batch 1950 batch loss 0.559656143 epoch total loss 0.564196825\n",
      "Trained batch 1951 batch loss 0.566708684 epoch total loss 0.564198077\n",
      "Trained batch 1952 batch loss 0.660153806 epoch total loss 0.564247251\n",
      "Trained batch 1953 batch loss 0.551514626 epoch total loss 0.564240754\n",
      "Trained batch 1954 batch loss 0.528690875 epoch total loss 0.564222515\n",
      "Trained batch 1955 batch loss 0.478644639 epoch total loss 0.564178765\n",
      "Trained batch 1956 batch loss 0.578411579 epoch total loss 0.564186037\n",
      "Trained batch 1957 batch loss 0.582342684 epoch total loss 0.564195335\n",
      "Trained batch 1958 batch loss 0.486281544 epoch total loss 0.564155579\n",
      "Trained batch 1959 batch loss 0.497650653 epoch total loss 0.564121604\n",
      "Trained batch 1960 batch loss 0.522585273 epoch total loss 0.564100444\n",
      "Trained batch 1961 batch loss 0.579690874 epoch total loss 0.564108372\n",
      "Trained batch 1962 batch loss 0.486000597 epoch total loss 0.564068556\n",
      "Trained batch 1963 batch loss 0.539659381 epoch total loss 0.564056158\n",
      "Trained batch 1964 batch loss 0.591970444 epoch total loss 0.564070344\n",
      "Trained batch 1965 batch loss 0.625012 epoch total loss 0.564101338\n",
      "Trained batch 1966 batch loss 0.538707137 epoch total loss 0.564088404\n",
      "Trained batch 1967 batch loss 0.554611266 epoch total loss 0.564083576\n",
      "Trained batch 1968 batch loss 0.519719481 epoch total loss 0.564061046\n",
      "Trained batch 1969 batch loss 0.511375368 epoch total loss 0.564034283\n",
      "Trained batch 1970 batch loss 0.511788845 epoch total loss 0.564007759\n",
      "Trained batch 1971 batch loss 0.428922713 epoch total loss 0.563939273\n",
      "Trained batch 1972 batch loss 0.344773471 epoch total loss 0.563828111\n",
      "Trained batch 1973 batch loss 0.459423184 epoch total loss 0.563775182\n",
      "Trained batch 1974 batch loss 0.523696959 epoch total loss 0.563754916\n",
      "Trained batch 1975 batch loss 0.566697478 epoch total loss 0.563756347\n",
      "Trained batch 1976 batch loss 0.679041862 epoch total loss 0.5638147\n",
      "Trained batch 1977 batch loss 0.619470417 epoch total loss 0.563842893\n",
      "Trained batch 1978 batch loss 0.541724145 epoch total loss 0.563831747\n",
      "Trained batch 1979 batch loss 0.605503 epoch total loss 0.563852787\n",
      "Trained batch 1980 batch loss 0.623891592 epoch total loss 0.563883066\n",
      "Trained batch 1981 batch loss 0.541471839 epoch total loss 0.563871801\n",
      "Trained batch 1982 batch loss 0.558860898 epoch total loss 0.563869238\n",
      "Trained batch 1983 batch loss 0.537097394 epoch total loss 0.563855767\n",
      "Trained batch 1984 batch loss 0.519491315 epoch total loss 0.563833416\n",
      "Trained batch 1985 batch loss 0.448401242 epoch total loss 0.563775241\n",
      "Trained batch 1986 batch loss 0.509577215 epoch total loss 0.563747942\n",
      "Trained batch 1987 batch loss 0.500611544 epoch total loss 0.563716173\n",
      "Trained batch 1988 batch loss 0.506635845 epoch total loss 0.563687444\n",
      "Trained batch 1989 batch loss 0.532262802 epoch total loss 0.563671589\n",
      "Trained batch 1990 batch loss 0.574490428 epoch total loss 0.563677\n",
      "Trained batch 1991 batch loss 0.555606544 epoch total loss 0.563673\n",
      "Trained batch 1992 batch loss 0.562263191 epoch total loss 0.563672304\n",
      "Trained batch 1993 batch loss 0.580298364 epoch total loss 0.563680649\n",
      "Trained batch 1994 batch loss 0.583872378 epoch total loss 0.563690782\n",
      "Trained batch 1995 batch loss 0.530130208 epoch total loss 0.563674\n",
      "Trained batch 1996 batch loss 0.557535648 epoch total loss 0.563670874\n",
      "Trained batch 1997 batch loss 0.569659531 epoch total loss 0.563673854\n",
      "Trained batch 1998 batch loss 0.528381348 epoch total loss 0.563656211\n",
      "Trained batch 1999 batch loss 0.496455669 epoch total loss 0.563622594\n",
      "Trained batch 2000 batch loss 0.55161804 epoch total loss 0.563616574\n",
      "Trained batch 2001 batch loss 0.446884781 epoch total loss 0.563558221\n",
      "Trained batch 2002 batch loss 0.481791973 epoch total loss 0.563517392\n",
      "Trained batch 2003 batch loss 0.487057269 epoch total loss 0.563479245\n",
      "Trained batch 2004 batch loss 0.479844093 epoch total loss 0.563437521\n",
      "Trained batch 2005 batch loss 0.600739896 epoch total loss 0.563456118\n",
      "Trained batch 2006 batch loss 0.582113922 epoch total loss 0.563465416\n",
      "Trained batch 2007 batch loss 0.578958333 epoch total loss 0.563473165\n",
      "Trained batch 2008 batch loss 0.638170242 epoch total loss 0.563510358\n",
      "Trained batch 2009 batch loss 0.578792095 epoch total loss 0.563517928\n",
      "Trained batch 2010 batch loss 0.598305643 epoch total loss 0.563535213\n",
      "Trained batch 2011 batch loss 0.623139858 epoch total loss 0.563564897\n",
      "Trained batch 2012 batch loss 0.535696 epoch total loss 0.563551\n",
      "Trained batch 2013 batch loss 0.709397912 epoch total loss 0.563623428\n",
      "Trained batch 2014 batch loss 0.643744111 epoch total loss 0.563663244\n",
      "Trained batch 2015 batch loss 0.595253527 epoch total loss 0.56367892\n",
      "Trained batch 2016 batch loss 0.532734 epoch total loss 0.563663542\n",
      "Trained batch 2017 batch loss 0.587137341 epoch total loss 0.563675165\n",
      "Trained batch 2018 batch loss 0.596784115 epoch total loss 0.563691616\n",
      "Trained batch 2019 batch loss 0.538076043 epoch total loss 0.56367892\n",
      "Trained batch 2020 batch loss 0.571797788 epoch total loss 0.563682914\n",
      "Trained batch 2021 batch loss 0.549391687 epoch total loss 0.56367588\n",
      "Trained batch 2022 batch loss 0.563199461 epoch total loss 0.563675642\n",
      "Trained batch 2023 batch loss 0.614884436 epoch total loss 0.563701\n",
      "Trained batch 2024 batch loss 0.55687511 epoch total loss 0.563697577\n",
      "Trained batch 2025 batch loss 0.498897523 epoch total loss 0.563665628\n",
      "Trained batch 2026 batch loss 0.433872133 epoch total loss 0.563601553\n",
      "Trained batch 2027 batch loss 0.387290657 epoch total loss 0.56351459\n",
      "Trained batch 2028 batch loss 0.469772607 epoch total loss 0.563468337\n",
      "Trained batch 2029 batch loss 0.43441236 epoch total loss 0.563404739\n",
      "Trained batch 2030 batch loss 0.516367 epoch total loss 0.563381553\n",
      "Trained batch 2031 batch loss 0.58769691 epoch total loss 0.563393474\n",
      "Trained batch 2032 batch loss 0.586290479 epoch total loss 0.563404799\n",
      "Trained batch 2033 batch loss 0.662280858 epoch total loss 0.563453376\n",
      "Trained batch 2034 batch loss 0.647447288 epoch total loss 0.563494682\n",
      "Trained batch 2035 batch loss 0.628574133 epoch total loss 0.56352663\n",
      "Trained batch 2036 batch loss 0.66130513 epoch total loss 0.563574672\n",
      "Trained batch 2037 batch loss 0.630353212 epoch total loss 0.563607454\n",
      "Trained batch 2038 batch loss 0.602279842 epoch total loss 0.563626409\n",
      "Trained batch 2039 batch loss 0.504027486 epoch total loss 0.563597202\n",
      "Trained batch 2040 batch loss 0.533026636 epoch total loss 0.563582242\n",
      "Trained batch 2041 batch loss 0.597853839 epoch total loss 0.56359905\n",
      "Trained batch 2042 batch loss 0.54977262 epoch total loss 0.563592315\n",
      "Trained batch 2043 batch loss 0.489388943 epoch total loss 0.563555956\n",
      "Trained batch 2044 batch loss 0.474363 epoch total loss 0.563512325\n",
      "Trained batch 2045 batch loss 0.478433132 epoch total loss 0.563470721\n",
      "Trained batch 2046 batch loss 0.512698412 epoch total loss 0.563445866\n",
      "Trained batch 2047 batch loss 0.464278281 epoch total loss 0.563397408\n",
      "Trained batch 2048 batch loss 0.496065617 epoch total loss 0.563364565\n",
      "Trained batch 2049 batch loss 0.498543799 epoch total loss 0.563332915\n",
      "Trained batch 2050 batch loss 0.491272569 epoch total loss 0.563297808\n",
      "Trained batch 2051 batch loss 0.435893536 epoch total loss 0.5632357\n",
      "Trained batch 2052 batch loss 0.497840315 epoch total loss 0.563203812\n",
      "Trained batch 2053 batch loss 0.478902787 epoch total loss 0.563162744\n",
      "Trained batch 2054 batch loss 0.530423224 epoch total loss 0.56314677\n",
      "Trained batch 2055 batch loss 0.514069855 epoch total loss 0.563122869\n",
      "Trained batch 2056 batch loss 0.539913356 epoch total loss 0.563111603\n",
      "Trained batch 2057 batch loss 0.473957419 epoch total loss 0.563068271\n",
      "Trained batch 2058 batch loss 0.491383135 epoch total loss 0.563033402\n",
      "Trained batch 2059 batch loss 0.427884549 epoch total loss 0.562967777\n",
      "Trained batch 2060 batch loss 0.551158309 epoch total loss 0.562962055\n",
      "Trained batch 2061 batch loss 0.508563578 epoch total loss 0.56293565\n",
      "Trained batch 2062 batch loss 0.515559614 epoch total loss 0.562912643\n",
      "Trained batch 2063 batch loss 0.52034229 epoch total loss 0.562892\n",
      "Trained batch 2064 batch loss 0.506555617 epoch total loss 0.562864721\n",
      "Trained batch 2065 batch loss 0.581476629 epoch total loss 0.562873721\n",
      "Trained batch 2066 batch loss 0.642689407 epoch total loss 0.562912345\n",
      "Trained batch 2067 batch loss 0.671033323 epoch total loss 0.562964678\n",
      "Trained batch 2068 batch loss 0.706716657 epoch total loss 0.563034117\n",
      "Trained batch 2069 batch loss 0.530494213 epoch total loss 0.563018441\n",
      "Trained batch 2070 batch loss 0.572921515 epoch total loss 0.56302321\n",
      "Trained batch 2071 batch loss 0.593025088 epoch total loss 0.563037694\n",
      "Trained batch 2072 batch loss 0.565130949 epoch total loss 0.563038707\n",
      "Trained batch 2073 batch loss 0.62704 epoch total loss 0.563069582\n",
      "Trained batch 2074 batch loss 0.6490435 epoch total loss 0.563111067\n",
      "Trained batch 2075 batch loss 0.601662457 epoch total loss 0.563129663\n",
      "Trained batch 2076 batch loss 0.516056836 epoch total loss 0.563107\n",
      "Trained batch 2077 batch loss 0.547944725 epoch total loss 0.563099742\n",
      "Trained batch 2078 batch loss 0.600221694 epoch total loss 0.563117564\n",
      "Trained batch 2079 batch loss 0.615678966 epoch total loss 0.563142896\n",
      "Trained batch 2080 batch loss 0.519048631 epoch total loss 0.563121676\n",
      "Trained batch 2081 batch loss 0.476966262 epoch total loss 0.563080251\n",
      "Trained batch 2082 batch loss 0.487549543 epoch total loss 0.563043952\n",
      "Trained batch 2083 batch loss 0.583914638 epoch total loss 0.563053966\n",
      "Trained batch 2084 batch loss 0.567222416 epoch total loss 0.563056\n",
      "Trained batch 2085 batch loss 0.52763772 epoch total loss 0.563039\n",
      "Trained batch 2086 batch loss 0.56689024 epoch total loss 0.563040853\n",
      "Trained batch 2087 batch loss 0.533989608 epoch total loss 0.563026905\n",
      "Trained batch 2088 batch loss 0.42344752 epoch total loss 0.56296\n",
      "Trained batch 2089 batch loss 0.420354694 epoch total loss 0.562891781\n",
      "Trained batch 2090 batch loss 0.378099024 epoch total loss 0.562803388\n",
      "Trained batch 2091 batch loss 0.448636413 epoch total loss 0.56274873\n",
      "Trained batch 2092 batch loss 0.464839548 epoch total loss 0.562701941\n",
      "Trained batch 2093 batch loss 0.506380677 epoch total loss 0.562675\n",
      "Trained batch 2094 batch loss 0.559604466 epoch total loss 0.562673569\n",
      "Trained batch 2095 batch loss 0.482576 epoch total loss 0.562635303\n",
      "Trained batch 2096 batch loss 0.60150671 epoch total loss 0.56265384\n",
      "Trained batch 2097 batch loss 0.533162236 epoch total loss 0.562639832\n",
      "Trained batch 2098 batch loss 0.572552 epoch total loss 0.562644541\n",
      "Trained batch 2099 batch loss 0.526569545 epoch total loss 0.562627375\n",
      "Trained batch 2100 batch loss 0.524230838 epoch total loss 0.562609076\n",
      "Trained batch 2101 batch loss 0.582425356 epoch total loss 0.562618494\n",
      "Trained batch 2102 batch loss 0.57400161 epoch total loss 0.562623858\n",
      "Trained batch 2103 batch loss 0.656059623 epoch total loss 0.562668264\n",
      "Trained batch 2104 batch loss 0.612888217 epoch total loss 0.562692165\n",
      "Trained batch 2105 batch loss 0.634070635 epoch total loss 0.562726\n",
      "Trained batch 2106 batch loss 0.680396795 epoch total loss 0.56278193\n",
      "Trained batch 2107 batch loss 0.584973395 epoch total loss 0.56279248\n",
      "Trained batch 2108 batch loss 0.665354311 epoch total loss 0.562841117\n",
      "Trained batch 2109 batch loss 0.499308288 epoch total loss 0.562811\n",
      "Trained batch 2110 batch loss 0.49944973 epoch total loss 0.562780917\n",
      "Trained batch 2111 batch loss 0.533154845 epoch total loss 0.56276691\n",
      "Trained batch 2112 batch loss 0.538060725 epoch total loss 0.562755227\n",
      "Trained batch 2113 batch loss 0.502633214 epoch total loss 0.562726796\n",
      "Trained batch 2114 batch loss 0.588157237 epoch total loss 0.562738836\n",
      "Trained batch 2115 batch loss 0.639711082 epoch total loss 0.562775254\n",
      "Trained batch 2116 batch loss 0.606939793 epoch total loss 0.562796116\n",
      "Trained batch 2117 batch loss 0.642597198 epoch total loss 0.562833786\n",
      "Trained batch 2118 batch loss 0.629649222 epoch total loss 0.562865317\n",
      "Trained batch 2119 batch loss 0.673169911 epoch total loss 0.562917411\n",
      "Trained batch 2120 batch loss 0.556053162 epoch total loss 0.562914193\n",
      "Trained batch 2121 batch loss 0.588875353 epoch total loss 0.562926412\n",
      "Trained batch 2122 batch loss 0.575488 epoch total loss 0.562932312\n",
      "Trained batch 2123 batch loss 0.526710629 epoch total loss 0.562915266\n",
      "Trained batch 2124 batch loss 0.58272475 epoch total loss 0.562924623\n",
      "Trained batch 2125 batch loss 0.524919748 epoch total loss 0.562906682\n",
      "Trained batch 2126 batch loss 0.523736894 epoch total loss 0.562888265\n",
      "Trained batch 2127 batch loss 0.613167 epoch total loss 0.562911868\n",
      "Trained batch 2128 batch loss 0.564846456 epoch total loss 0.562912762\n",
      "Trained batch 2129 batch loss 0.53834933 epoch total loss 0.562901258\n",
      "Trained batch 2130 batch loss 0.548361123 epoch total loss 0.562894404\n",
      "Trained batch 2131 batch loss 0.561223686 epoch total loss 0.562893629\n",
      "Trained batch 2132 batch loss 0.54288137 epoch total loss 0.562884271\n",
      "Trained batch 2133 batch loss 0.462666273 epoch total loss 0.562837243\n",
      "Trained batch 2134 batch loss 0.518131137 epoch total loss 0.562816322\n",
      "Trained batch 2135 batch loss 0.629580379 epoch total loss 0.562847614\n",
      "Trained batch 2136 batch loss 0.607129097 epoch total loss 0.562868357\n",
      "Trained batch 2137 batch loss 0.4967632 epoch total loss 0.562837422\n",
      "Trained batch 2138 batch loss 0.709251404 epoch total loss 0.562905908\n",
      "Trained batch 2139 batch loss 0.774540961 epoch total loss 0.563004851\n",
      "Trained batch 2140 batch loss 0.569563329 epoch total loss 0.563007891\n",
      "Trained batch 2141 batch loss 0.645094395 epoch total loss 0.563046277\n",
      "Trained batch 2142 batch loss 0.537888408 epoch total loss 0.563034475\n",
      "Trained batch 2143 batch loss 0.680526495 epoch total loss 0.563089311\n",
      "Trained batch 2144 batch loss 0.557472825 epoch total loss 0.563086748\n",
      "Trained batch 2145 batch loss 0.608526707 epoch total loss 0.563107908\n",
      "Trained batch 2146 batch loss 0.539051473 epoch total loss 0.563096702\n",
      "Trained batch 2147 batch loss 0.62639147 epoch total loss 0.563126147\n",
      "Trained batch 2148 batch loss 0.590399 epoch total loss 0.563138902\n",
      "Trained batch 2149 batch loss 0.57761395 epoch total loss 0.563145638\n",
      "Trained batch 2150 batch loss 0.551151216 epoch total loss 0.563140035\n",
      "Trained batch 2151 batch loss 0.577196658 epoch total loss 0.563146532\n",
      "Trained batch 2152 batch loss 0.5302068 epoch total loss 0.563131213\n",
      "Trained batch 2153 batch loss 0.618438363 epoch total loss 0.563156903\n",
      "Trained batch 2154 batch loss 0.602957845 epoch total loss 0.56317538\n",
      "Trained batch 2155 batch loss 0.546262 epoch total loss 0.563167512\n",
      "Trained batch 2156 batch loss 0.529562116 epoch total loss 0.563151896\n",
      "Trained batch 2157 batch loss 0.511083782 epoch total loss 0.563127756\n",
      "Trained batch 2158 batch loss 0.587945938 epoch total loss 0.56313926\n",
      "Trained batch 2159 batch loss 0.600365162 epoch total loss 0.563156486\n",
      "Trained batch 2160 batch loss 0.573304653 epoch total loss 0.563161194\n",
      "Trained batch 2161 batch loss 0.569957554 epoch total loss 0.563164353\n",
      "Trained batch 2162 batch loss 0.540887833 epoch total loss 0.563154042\n",
      "Trained batch 2163 batch loss 0.539503932 epoch total loss 0.563143134\n",
      "Trained batch 2164 batch loss 0.524168134 epoch total loss 0.563125134\n",
      "Trained batch 2165 batch loss 0.486894518 epoch total loss 0.563089967\n",
      "Trained batch 2166 batch loss 0.531948924 epoch total loss 0.563075602\n",
      "Trained batch 2167 batch loss 0.593408167 epoch total loss 0.56308955\n",
      "Trained batch 2168 batch loss 0.535290778 epoch total loss 0.563076735\n",
      "Trained batch 2169 batch loss 0.639768183 epoch total loss 0.56311208\n",
      "Trained batch 2170 batch loss 0.54510808 epoch total loss 0.563103855\n",
      "Trained batch 2171 batch loss 0.600740492 epoch total loss 0.56312114\n",
      "Trained batch 2172 batch loss 0.554338753 epoch total loss 0.563117087\n",
      "Trained batch 2173 batch loss 0.591405332 epoch total loss 0.56313014\n",
      "Trained batch 2174 batch loss 0.458683968 epoch total loss 0.563082099\n",
      "Trained batch 2175 batch loss 0.522292793 epoch total loss 0.563063383\n",
      "Trained batch 2176 batch loss 0.431794494 epoch total loss 0.563003063\n",
      "Trained batch 2177 batch loss 0.453709185 epoch total loss 0.562952816\n",
      "Trained batch 2178 batch loss 0.461601466 epoch total loss 0.562906265\n",
      "Trained batch 2179 batch loss 0.481183887 epoch total loss 0.562868774\n",
      "Trained batch 2180 batch loss 0.459438115 epoch total loss 0.562821388\n",
      "Trained batch 2181 batch loss 0.667059958 epoch total loss 0.562869191\n",
      "Trained batch 2182 batch loss 0.801319838 epoch total loss 0.562978446\n",
      "Trained batch 2183 batch loss 0.617094398 epoch total loss 0.563003182\n",
      "Trained batch 2184 batch loss 0.594233632 epoch total loss 0.563017488\n",
      "Trained batch 2185 batch loss 0.595662355 epoch total loss 0.563032448\n",
      "Trained batch 2186 batch loss 0.501116693 epoch total loss 0.563004136\n",
      "Trained batch 2187 batch loss 0.531006753 epoch total loss 0.562989533\n",
      "Trained batch 2188 batch loss 0.484839439 epoch total loss 0.56295383\n",
      "Trained batch 2189 batch loss 0.50042218 epoch total loss 0.56292522\n",
      "Trained batch 2190 batch loss 0.561345041 epoch total loss 0.562924504\n",
      "Trained batch 2191 batch loss 0.516597152 epoch total loss 0.562903345\n",
      "Trained batch 2192 batch loss 0.542075 epoch total loss 0.562893867\n",
      "Trained batch 2193 batch loss 0.550142765 epoch total loss 0.562888086\n",
      "Trained batch 2194 batch loss 0.468019664 epoch total loss 0.562844872\n",
      "Trained batch 2195 batch loss 0.549289107 epoch total loss 0.562838674\n",
      "Trained batch 2196 batch loss 0.544159 epoch total loss 0.56283021\n",
      "Trained batch 2197 batch loss 0.546865 epoch total loss 0.562822938\n",
      "Trained batch 2198 batch loss 0.565742075 epoch total loss 0.562824309\n",
      "Trained batch 2199 batch loss 0.56622076 epoch total loss 0.562825799\n",
      "Trained batch 2200 batch loss 0.574148595 epoch total loss 0.562830925\n",
      "Trained batch 2201 batch loss 0.59141165 epoch total loss 0.562843919\n",
      "Trained batch 2202 batch loss 0.59694618 epoch total loss 0.562859416\n",
      "Trained batch 2203 batch loss 0.555605114 epoch total loss 0.562856138\n",
      "Trained batch 2204 batch loss 0.587174535 epoch total loss 0.562867165\n",
      "Trained batch 2205 batch loss 0.604615569 epoch total loss 0.562886059\n",
      "Trained batch 2206 batch loss 0.49066782 epoch total loss 0.562853396\n",
      "Trained batch 2207 batch loss 0.462279558 epoch total loss 0.562807798\n",
      "Trained batch 2208 batch loss 0.481209576 epoch total loss 0.562770844\n",
      "Trained batch 2209 batch loss 0.555586457 epoch total loss 0.562767565\n",
      "Trained batch 2210 batch loss 0.522296906 epoch total loss 0.562749267\n",
      "Trained batch 2211 batch loss 0.506074369 epoch total loss 0.562723637\n",
      "Trained batch 2212 batch loss 0.600927055 epoch total loss 0.562740922\n",
      "Trained batch 2213 batch loss 0.646042407 epoch total loss 0.562778533\n",
      "Trained batch 2214 batch loss 0.587059319 epoch total loss 0.5627895\n",
      "Trained batch 2215 batch loss 0.570937693 epoch total loss 0.562793195\n",
      "Trained batch 2216 batch loss 0.561013401 epoch total loss 0.56279242\n",
      "Trained batch 2217 batch loss 0.558549702 epoch total loss 0.562790513\n",
      "Trained batch 2218 batch loss 0.553313 epoch total loss 0.562786222\n",
      "Trained batch 2219 batch loss 0.598941326 epoch total loss 0.562802553\n",
      "Trained batch 2220 batch loss 0.602435231 epoch total loss 0.562820435\n",
      "Trained batch 2221 batch loss 0.541989684 epoch total loss 0.562811\n",
      "Trained batch 2222 batch loss 0.594273865 epoch total loss 0.562825143\n",
      "Trained batch 2223 batch loss 0.548458457 epoch total loss 0.562818706\n",
      "Trained batch 2224 batch loss 0.635650277 epoch total loss 0.562851429\n",
      "Trained batch 2225 batch loss 0.735791445 epoch total loss 0.562929213\n",
      "Trained batch 2226 batch loss 0.682847559 epoch total loss 0.562983096\n",
      "Trained batch 2227 batch loss 0.617705226 epoch total loss 0.563007653\n",
      "Trained batch 2228 batch loss 0.588617682 epoch total loss 0.563019156\n",
      "Trained batch 2229 batch loss 0.496285617 epoch total loss 0.562989235\n",
      "Trained batch 2230 batch loss 0.580013871 epoch total loss 0.562996805\n",
      "Trained batch 2231 batch loss 0.58283639 epoch total loss 0.563005745\n",
      "Trained batch 2232 batch loss 0.561507702 epoch total loss 0.56300509\n",
      "Trained batch 2233 batch loss 0.662616968 epoch total loss 0.563049674\n",
      "Trained batch 2234 batch loss 0.606803715 epoch total loss 0.563069284\n",
      "Trained batch 2235 batch loss 0.54238981 epoch total loss 0.56306\n",
      "Trained batch 2236 batch loss 0.582325101 epoch total loss 0.563068569\n",
      "Trained batch 2237 batch loss 0.63646245 epoch total loss 0.563101411\n",
      "Trained batch 2238 batch loss 0.546494067 epoch total loss 0.56309396\n",
      "Trained batch 2239 batch loss 0.582632124 epoch total loss 0.563102722\n",
      "Trained batch 2240 batch loss 0.515168905 epoch total loss 0.563081324\n",
      "Trained batch 2241 batch loss 0.564724 epoch total loss 0.563082039\n",
      "Trained batch 2242 batch loss 0.636150241 epoch total loss 0.563114583\n",
      "Trained batch 2243 batch loss 0.648769677 epoch total loss 0.56315279\n",
      "Trained batch 2244 batch loss 0.636221349 epoch total loss 0.563185394\n",
      "Trained batch 2245 batch loss 0.59391 epoch total loss 0.563199043\n",
      "Trained batch 2246 batch loss 0.630972147 epoch total loss 0.563229203\n",
      "Trained batch 2247 batch loss 0.615778446 epoch total loss 0.563252568\n",
      "Trained batch 2248 batch loss 0.601168454 epoch total loss 0.563269436\n",
      "Trained batch 2249 batch loss 0.563344598 epoch total loss 0.563269496\n",
      "Trained batch 2250 batch loss 0.627595782 epoch total loss 0.563298047\n",
      "Trained batch 2251 batch loss 0.602288485 epoch total loss 0.563315392\n",
      "Trained batch 2252 batch loss 0.624646604 epoch total loss 0.563342631\n",
      "Trained batch 2253 batch loss 0.6369977 epoch total loss 0.563375294\n",
      "Trained batch 2254 batch loss 0.563254952 epoch total loss 0.563375235\n",
      "Trained batch 2255 batch loss 0.56268549 epoch total loss 0.563374937\n",
      "Trained batch 2256 batch loss 0.560257733 epoch total loss 0.563373566\n",
      "Trained batch 2257 batch loss 0.611472189 epoch total loss 0.563394904\n",
      "Trained batch 2258 batch loss 0.626884043 epoch total loss 0.563423\n",
      "Trained batch 2259 batch loss 0.551512539 epoch total loss 0.563417733\n",
      "Trained batch 2260 batch loss 0.49742496 epoch total loss 0.563388526\n",
      "Trained batch 2261 batch loss 0.562841594 epoch total loss 0.563388288\n",
      "Trained batch 2262 batch loss 0.627866566 epoch total loss 0.563416779\n",
      "Trained batch 2263 batch loss 0.551757395 epoch total loss 0.563411593\n",
      "Trained batch 2264 batch loss 0.556929111 epoch total loss 0.563408732\n",
      "Trained batch 2265 batch loss 0.577012 epoch total loss 0.563414752\n",
      "Trained batch 2266 batch loss 0.567301333 epoch total loss 0.563416421\n",
      "Trained batch 2267 batch loss 0.615080655 epoch total loss 0.56343925\n",
      "Trained batch 2268 batch loss 0.578566074 epoch total loss 0.563445926\n",
      "Trained batch 2269 batch loss 0.572246671 epoch total loss 0.5634498\n",
      "Trained batch 2270 batch loss 0.587623298 epoch total loss 0.563460469\n",
      "Trained batch 2271 batch loss 0.535616815 epoch total loss 0.56344825\n",
      "Trained batch 2272 batch loss 0.481261849 epoch total loss 0.563412\n",
      "Trained batch 2273 batch loss 0.488136232 epoch total loss 0.56337893\n",
      "Trained batch 2274 batch loss 0.550005436 epoch total loss 0.563373089\n",
      "Trained batch 2275 batch loss 0.512221932 epoch total loss 0.563350558\n",
      "Trained batch 2276 batch loss 0.544225097 epoch total loss 0.563342154\n",
      "Trained batch 2277 batch loss 0.650685549 epoch total loss 0.56338048\n",
      "Trained batch 2278 batch loss 0.551600933 epoch total loss 0.563375354\n",
      "Trained batch 2279 batch loss 0.630627155 epoch total loss 0.563404858\n",
      "Trained batch 2280 batch loss 0.631039619 epoch total loss 0.563434482\n",
      "Trained batch 2281 batch loss 0.536040783 epoch total loss 0.563422441\n",
      "Trained batch 2282 batch loss 0.560729802 epoch total loss 0.563421249\n",
      "Trained batch 2283 batch loss 0.560558081 epoch total loss 0.56342\n",
      "Trained batch 2284 batch loss 0.550481319 epoch total loss 0.563414335\n",
      "Trained batch 2285 batch loss 0.554730773 epoch total loss 0.563410521\n",
      "Trained batch 2286 batch loss 0.583462596 epoch total loss 0.563419342\n",
      "Trained batch 2287 batch loss 0.516009033 epoch total loss 0.5633986\n",
      "Trained batch 2288 batch loss 0.610334933 epoch total loss 0.563419104\n",
      "Trained batch 2289 batch loss 0.484378636 epoch total loss 0.563384593\n",
      "Trained batch 2290 batch loss 0.52623564 epoch total loss 0.56336838\n",
      "Trained batch 2291 batch loss 0.424444944 epoch total loss 0.563307703\n",
      "Trained batch 2292 batch loss 0.54319191 epoch total loss 0.563298941\n",
      "Trained batch 2293 batch loss 0.627263904 epoch total loss 0.563326836\n",
      "Trained batch 2294 batch loss 0.619884968 epoch total loss 0.563351512\n",
      "Trained batch 2295 batch loss 0.61859858 epoch total loss 0.563375592\n",
      "Trained batch 2296 batch loss 0.598135 epoch total loss 0.563390732\n",
      "Trained batch 2297 batch loss 0.532766342 epoch total loss 0.56337738\n",
      "Trained batch 2298 batch loss 0.60830164 epoch total loss 0.563396931\n",
      "Trained batch 2299 batch loss 0.514405489 epoch total loss 0.563375652\n",
      "Trained batch 2300 batch loss 0.576988876 epoch total loss 0.563381553\n",
      "Trained batch 2301 batch loss 0.418662131 epoch total loss 0.56331867\n",
      "Trained batch 2302 batch loss 0.552235603 epoch total loss 0.563313901\n",
      "Trained batch 2303 batch loss 0.509360731 epoch total loss 0.563290477\n",
      "Trained batch 2304 batch loss 0.515002549 epoch total loss 0.563269496\n",
      "Trained batch 2305 batch loss 0.511337638 epoch total loss 0.563246965\n",
      "Trained batch 2306 batch loss 0.497597814 epoch total loss 0.563218474\n",
      "Trained batch 2307 batch loss 0.555924952 epoch total loss 0.563215315\n",
      "Trained batch 2308 batch loss 0.553152561 epoch total loss 0.563210964\n",
      "Trained batch 2309 batch loss 0.473591983 epoch total loss 0.563172162\n",
      "Trained batch 2310 batch loss 0.558100462 epoch total loss 0.563169956\n",
      "Trained batch 2311 batch loss 0.568267882 epoch total loss 0.563172162\n",
      "Trained batch 2312 batch loss 0.604558289 epoch total loss 0.563190103\n",
      "Trained batch 2313 batch loss 0.647601604 epoch total loss 0.563226581\n",
      "Trained batch 2314 batch loss 0.597684085 epoch total loss 0.563241422\n",
      "Trained batch 2315 batch loss 0.626292288 epoch total loss 0.563268721\n",
      "Trained batch 2316 batch loss 0.663824379 epoch total loss 0.563312113\n",
      "Trained batch 2317 batch loss 0.561257422 epoch total loss 0.563311219\n",
      "Trained batch 2318 batch loss 0.593616545 epoch total loss 0.563324332\n",
      "Trained batch 2319 batch loss 0.558252156 epoch total loss 0.563322127\n",
      "Trained batch 2320 batch loss 0.536732197 epoch total loss 0.563310683\n",
      "Trained batch 2321 batch loss 0.557479203 epoch total loss 0.563308179\n",
      "Trained batch 2322 batch loss 0.553109109 epoch total loss 0.563303769\n",
      "Trained batch 2323 batch loss 0.432447195 epoch total loss 0.563247442\n",
      "Trained batch 2324 batch loss 0.459716976 epoch total loss 0.563202918\n",
      "Trained batch 2325 batch loss 0.571187 epoch total loss 0.563206315\n",
      "Trained batch 2326 batch loss 0.46898973 epoch total loss 0.563165843\n",
      "Trained batch 2327 batch loss 0.528901935 epoch total loss 0.563151121\n",
      "Trained batch 2328 batch loss 0.65142411 epoch total loss 0.563189\n",
      "Trained batch 2329 batch loss 0.598371685 epoch total loss 0.56320411\n",
      "Trained batch 2330 batch loss 0.663537681 epoch total loss 0.563247204\n",
      "Trained batch 2331 batch loss 0.671060145 epoch total loss 0.563293457\n",
      "Trained batch 2332 batch loss 0.605236828 epoch total loss 0.563311398\n",
      "Trained batch 2333 batch loss 0.574494779 epoch total loss 0.563316166\n",
      "Trained batch 2334 batch loss 0.536190629 epoch total loss 0.563304543\n",
      "Trained batch 2335 batch loss 0.527706504 epoch total loss 0.563289285\n",
      "Trained batch 2336 batch loss 0.516096234 epoch total loss 0.563269079\n",
      "Trained batch 2337 batch loss 0.533403695 epoch total loss 0.563256323\n",
      "Trained batch 2338 batch loss 0.525959 epoch total loss 0.563240409\n",
      "Trained batch 2339 batch loss 0.600526571 epoch total loss 0.563256383\n",
      "Trained batch 2340 batch loss 0.534964442 epoch total loss 0.563244283\n",
      "Trained batch 2341 batch loss 0.552493095 epoch total loss 0.563239694\n",
      "Trained batch 2342 batch loss 0.472290069 epoch total loss 0.563200831\n",
      "Trained batch 2343 batch loss 0.540614903 epoch total loss 0.563191235\n",
      "Trained batch 2344 batch loss 0.509129584 epoch total loss 0.563168168\n",
      "Trained batch 2345 batch loss 0.627845466 epoch total loss 0.563195705\n",
      "Trained batch 2346 batch loss 0.632365584 epoch total loss 0.56322521\n",
      "Trained batch 2347 batch loss 0.567199767 epoch total loss 0.563226938\n",
      "Trained batch 2348 batch loss 0.53143841 epoch total loss 0.563213408\n",
      "Trained batch 2349 batch loss 0.526615 epoch total loss 0.563197792\n",
      "Trained batch 2350 batch loss 0.571653068 epoch total loss 0.563201427\n",
      "Trained batch 2351 batch loss 0.570753098 epoch total loss 0.563204646\n",
      "Trained batch 2352 batch loss 0.68578124 epoch total loss 0.563256741\n",
      "Trained batch 2353 batch loss 0.58390826 epoch total loss 0.563265502\n",
      "Trained batch 2354 batch loss 0.521163821 epoch total loss 0.563247621\n",
      "Trained batch 2355 batch loss 0.567367852 epoch total loss 0.56324935\n",
      "Trained batch 2356 batch loss 0.632137775 epoch total loss 0.563278615\n",
      "Trained batch 2357 batch loss 0.546286464 epoch total loss 0.563271344\n",
      "Trained batch 2358 batch loss 0.542175353 epoch total loss 0.563262463\n",
      "Trained batch 2359 batch loss 0.45798105 epoch total loss 0.563217819\n",
      "Trained batch 2360 batch loss 0.562040269 epoch total loss 0.563217342\n",
      "Trained batch 2361 batch loss 0.575168252 epoch total loss 0.563222408\n",
      "Trained batch 2362 batch loss 0.590785086 epoch total loss 0.563234091\n",
      "Trained batch 2363 batch loss 0.491776168 epoch total loss 0.563203871\n",
      "Trained batch 2364 batch loss 0.523757398 epoch total loss 0.563187182\n",
      "Trained batch 2365 batch loss 0.540335894 epoch total loss 0.563177526\n",
      "Trained batch 2366 batch loss 0.553521514 epoch total loss 0.563173413\n",
      "Trained batch 2367 batch loss 0.575878143 epoch total loss 0.563178778\n",
      "Trained batch 2368 batch loss 0.56075877 epoch total loss 0.563177764\n",
      "Trained batch 2369 batch loss 0.637549639 epoch total loss 0.563209176\n",
      "Trained batch 2370 batch loss 0.701205611 epoch total loss 0.56326741\n",
      "Trained batch 2371 batch loss 0.657395 epoch total loss 0.563307047\n",
      "Trained batch 2372 batch loss 0.7272439 epoch total loss 0.563376188\n",
      "Trained batch 2373 batch loss 0.578903794 epoch total loss 0.563382745\n",
      "Trained batch 2374 batch loss 0.630686 epoch total loss 0.563411117\n",
      "Trained batch 2375 batch loss 0.593091369 epoch total loss 0.563423634\n",
      "Trained batch 2376 batch loss 0.646193266 epoch total loss 0.563458502\n",
      "Trained batch 2377 batch loss 0.484286755 epoch total loss 0.563425183\n",
      "Trained batch 2378 batch loss 0.585148394 epoch total loss 0.563434303\n",
      "Trained batch 2379 batch loss 0.431880295 epoch total loss 0.563379\n",
      "Trained batch 2380 batch loss 0.48652339 epoch total loss 0.563346744\n",
      "Trained batch 2381 batch loss 0.483166963 epoch total loss 0.563313067\n",
      "Trained batch 2382 batch loss 0.427824795 epoch total loss 0.563256204\n",
      "Trained batch 2383 batch loss 0.451227814 epoch total loss 0.563209176\n",
      "Trained batch 2384 batch loss 0.405495971 epoch total loss 0.563143\n",
      "Trained batch 2385 batch loss 0.47241348 epoch total loss 0.563105\n",
      "Trained batch 2386 batch loss 0.412797 epoch total loss 0.563042\n",
      "Trained batch 2387 batch loss 0.441678226 epoch total loss 0.562991142\n",
      "Trained batch 2388 batch loss 0.44069013 epoch total loss 0.562939942\n",
      "Trained batch 2389 batch loss 0.449032784 epoch total loss 0.562892199\n",
      "Trained batch 2390 batch loss 0.361014366 epoch total loss 0.562807739\n",
      "Trained batch 2391 batch loss 0.498312652 epoch total loss 0.562780738\n",
      "Trained batch 2392 batch loss 0.605792046 epoch total loss 0.562798738\n",
      "Trained batch 2393 batch loss 0.612244 epoch total loss 0.562819421\n",
      "Trained batch 2394 batch loss 0.657856703 epoch total loss 0.562859118\n",
      "Trained batch 2395 batch loss 0.719588637 epoch total loss 0.562924564\n",
      "Trained batch 2396 batch loss 0.767576933 epoch total loss 0.56301\n",
      "Trained batch 2397 batch loss 0.728215933 epoch total loss 0.56307894\n",
      "Trained batch 2398 batch loss 0.662796855 epoch total loss 0.563120544\n",
      "Trained batch 2399 batch loss 0.455847919 epoch total loss 0.563075781\n",
      "Trained batch 2400 batch loss 0.583234787 epoch total loss 0.563084185\n",
      "Trained batch 2401 batch loss 0.542384326 epoch total loss 0.563075602\n",
      "Trained batch 2402 batch loss 0.504376173 epoch total loss 0.563051164\n",
      "Trained batch 2403 batch loss 0.509126 epoch total loss 0.563028693\n",
      "Trained batch 2404 batch loss 0.633166611 epoch total loss 0.563057899\n",
      "Trained batch 2405 batch loss 0.626311779 epoch total loss 0.563084185\n",
      "Trained batch 2406 batch loss 0.592797637 epoch total loss 0.563096523\n",
      "Trained batch 2407 batch loss 0.599592626 epoch total loss 0.563111722\n",
      "Trained batch 2408 batch loss 0.517520249 epoch total loss 0.563092828\n",
      "Trained batch 2409 batch loss 0.578389049 epoch total loss 0.563099146\n",
      "Trained batch 2410 batch loss 0.627340138 epoch total loss 0.563125789\n",
      "Trained batch 2411 batch loss 0.556549788 epoch total loss 0.563123047\n",
      "Trained batch 2412 batch loss 0.618944705 epoch total loss 0.563146174\n",
      "Trained batch 2413 batch loss 0.590111 epoch total loss 0.56315732\n",
      "Trained batch 2414 batch loss 0.57629019 epoch total loss 0.563162804\n",
      "Trained batch 2415 batch loss 0.532476187 epoch total loss 0.563150108\n",
      "Trained batch 2416 batch loss 0.544925034 epoch total loss 0.563142538\n",
      "Trained batch 2417 batch loss 0.622193396 epoch total loss 0.563167\n",
      "Trained batch 2418 batch loss 0.516022861 epoch total loss 0.563147485\n",
      "Trained batch 2419 batch loss 0.524647474 epoch total loss 0.563131571\n",
      "Trained batch 2420 batch loss 0.457410634 epoch total loss 0.563087881\n",
      "Trained batch 2421 batch loss 0.517738461 epoch total loss 0.563069105\n",
      "Trained batch 2422 batch loss 0.526631594 epoch total loss 0.563054\n",
      "Trained batch 2423 batch loss 0.575580955 epoch total loss 0.563059211\n",
      "Trained batch 2424 batch loss 0.595101595 epoch total loss 0.563072443\n",
      "Trained batch 2425 batch loss 0.590431273 epoch total loss 0.563083708\n",
      "Trained batch 2426 batch loss 0.512200356 epoch total loss 0.563062727\n",
      "Trained batch 2427 batch loss 0.464226753 epoch total loss 0.563022\n",
      "Trained batch 2428 batch loss 0.45030728 epoch total loss 0.562975585\n",
      "Trained batch 2429 batch loss 0.483074963 epoch total loss 0.562942684\n",
      "Trained batch 2430 batch loss 0.571993232 epoch total loss 0.562946439\n",
      "Trained batch 2431 batch loss 0.608799756 epoch total loss 0.562965274\n",
      "Trained batch 2432 batch loss 0.548818707 epoch total loss 0.562959492\n",
      "Trained batch 2433 batch loss 0.528877258 epoch total loss 0.562945485\n",
      "Trained batch 2434 batch loss 0.513694704 epoch total loss 0.56292522\n",
      "Trained batch 2435 batch loss 0.594152689 epoch total loss 0.562938035\n",
      "Trained batch 2436 batch loss 0.591252506 epoch total loss 0.562949717\n",
      "Trained batch 2437 batch loss 0.548065782 epoch total loss 0.562943578\n",
      "Trained batch 2438 batch loss 0.593778968 epoch total loss 0.562956214\n",
      "Trained batch 2439 batch loss 0.600033104 epoch total loss 0.562971413\n",
      "Trained batch 2440 batch loss 0.484398186 epoch total loss 0.562939227\n",
      "Trained batch 2441 batch loss 0.570961595 epoch total loss 0.562942445\n",
      "Trained batch 2442 batch loss 0.498825371 epoch total loss 0.562916219\n",
      "Trained batch 2443 batch loss 0.503314316 epoch total loss 0.562891781\n",
      "Trained batch 2444 batch loss 0.492464602 epoch total loss 0.562863\n",
      "Trained batch 2445 batch loss 0.513793468 epoch total loss 0.562842906\n",
      "Trained batch 2446 batch loss 0.481290698 epoch total loss 0.562809587\n",
      "Trained batch 2447 batch loss 0.476073325 epoch total loss 0.562774122\n",
      "Trained batch 2448 batch loss 0.490345061 epoch total loss 0.562744558\n",
      "Trained batch 2449 batch loss 0.51066494 epoch total loss 0.562723279\n",
      "Trained batch 2450 batch loss 0.562323272 epoch total loss 0.5627231\n",
      "Trained batch 2451 batch loss 0.547651887 epoch total loss 0.562716961\n",
      "Trained batch 2452 batch loss 0.602188349 epoch total loss 0.562733054\n",
      "Trained batch 2453 batch loss 0.622648299 epoch total loss 0.562757492\n",
      "Trained batch 2454 batch loss 0.557214677 epoch total loss 0.562755227\n",
      "Trained batch 2455 batch loss 0.613475561 epoch total loss 0.56277591\n",
      "Trained batch 2456 batch loss 0.560938537 epoch total loss 0.562775135\n",
      "Trained batch 2457 batch loss 0.484801173 epoch total loss 0.562743366\n",
      "Trained batch 2458 batch loss 0.553136528 epoch total loss 0.562739491\n",
      "Trained batch 2459 batch loss 0.581380546 epoch total loss 0.562747061\n",
      "Trained batch 2460 batch loss 0.579774201 epoch total loss 0.562754035\n",
      "Trained batch 2461 batch loss 0.582857549 epoch total loss 0.562762201\n",
      "Trained batch 2462 batch loss 0.540589631 epoch total loss 0.562753201\n",
      "Trained batch 2463 batch loss 0.532149434 epoch total loss 0.562740743\n",
      "Trained batch 2464 batch loss 0.51968044 epoch total loss 0.562723279\n",
      "Trained batch 2465 batch loss 0.456610352 epoch total loss 0.562680244\n",
      "Trained batch 2466 batch loss 0.520272493 epoch total loss 0.562663078\n",
      "Trained batch 2467 batch loss 0.616218925 epoch total loss 0.562684774\n",
      "Trained batch 2468 batch loss 0.504741788 epoch total loss 0.56266129\n",
      "Trained batch 2469 batch loss 0.512647033 epoch total loss 0.562641084\n",
      "Trained batch 2470 batch loss 0.504111469 epoch total loss 0.562617362\n",
      "Trained batch 2471 batch loss 0.499827653 epoch total loss 0.56259197\n",
      "Trained batch 2472 batch loss 0.569888294 epoch total loss 0.56259495\n",
      "Trained batch 2473 batch loss 0.645084918 epoch total loss 0.562628329\n",
      "Trained batch 2474 batch loss 0.584438801 epoch total loss 0.56263715\n",
      "Trained batch 2475 batch loss 0.592843533 epoch total loss 0.562649369\n",
      "Trained batch 2476 batch loss 0.510876119 epoch total loss 0.562628508\n",
      "Trained batch 2477 batch loss 0.647258401 epoch total loss 0.562662601\n",
      "Trained batch 2478 batch loss 0.588444054 epoch total loss 0.562673032\n",
      "Trained batch 2479 batch loss 0.627714157 epoch total loss 0.562699258\n",
      "Trained batch 2480 batch loss 0.51526475 epoch total loss 0.562680125\n",
      "Trained batch 2481 batch loss 0.490422159 epoch total loss 0.562651038\n",
      "Trained batch 2482 batch loss 0.629293144 epoch total loss 0.56267792\n",
      "Trained batch 2483 batch loss 0.60659343 epoch total loss 0.562695563\n",
      "Trained batch 2484 batch loss 0.545687854 epoch total loss 0.562688708\n",
      "Trained batch 2485 batch loss 0.521104753 epoch total loss 0.562671959\n",
      "Trained batch 2486 batch loss 0.529991269 epoch total loss 0.562658846\n",
      "Trained batch 2487 batch loss 0.501714706 epoch total loss 0.562634349\n",
      "Trained batch 2488 batch loss 0.550798714 epoch total loss 0.56262958\n",
      "Trained batch 2489 batch loss 0.598860681 epoch total loss 0.562644124\n",
      "Trained batch 2490 batch loss 0.547191739 epoch total loss 0.562637925\n",
      "Trained batch 2491 batch loss 0.667740881 epoch total loss 0.562680125\n",
      "Trained batch 2492 batch loss 0.558822334 epoch total loss 0.562678576\n",
      "Trained batch 2493 batch loss 0.627479732 epoch total loss 0.562704563\n",
      "Trained batch 2494 batch loss 0.702474833 epoch total loss 0.562760651\n",
      "Trained batch 2495 batch loss 0.67364037 epoch total loss 0.562805057\n",
      "Trained batch 2496 batch loss 0.553309441 epoch total loss 0.562801242\n",
      "Trained batch 2497 batch loss 0.657083333 epoch total loss 0.562839031\n",
      "Trained batch 2498 batch loss 0.574247062 epoch total loss 0.562843561\n",
      "Trained batch 2499 batch loss 0.578513145 epoch total loss 0.56284982\n",
      "Trained batch 2500 batch loss 0.611362 epoch total loss 0.562869251\n",
      "Trained batch 2501 batch loss 0.589991689 epoch total loss 0.562880099\n",
      "Trained batch 2502 batch loss 0.666481793 epoch total loss 0.562921464\n",
      "Trained batch 2503 batch loss 0.616303384 epoch total loss 0.562942803\n",
      "Trained batch 2504 batch loss 0.610245049 epoch total loss 0.562961698\n",
      "Trained batch 2505 batch loss 0.524903059 epoch total loss 0.562946498\n",
      "Trained batch 2506 batch loss 0.555775762 epoch total loss 0.562943637\n",
      "Trained batch 2507 batch loss 0.610667944 epoch total loss 0.562962711\n",
      "Trained batch 2508 batch loss 0.528512061 epoch total loss 0.562949\n",
      "Trained batch 2509 batch loss 0.587051392 epoch total loss 0.562958598\n",
      "Trained batch 2510 batch loss 0.536104918 epoch total loss 0.562947929\n",
      "Trained batch 2511 batch loss 0.52167356 epoch total loss 0.562931478\n",
      "Trained batch 2512 batch loss 0.525672555 epoch total loss 0.562916636\n",
      "Trained batch 2513 batch loss 0.484453201 epoch total loss 0.562885463\n",
      "Trained batch 2514 batch loss 0.461336821 epoch total loss 0.562845051\n",
      "Trained batch 2515 batch loss 0.531706 epoch total loss 0.562832654\n",
      "Trained batch 2516 batch loss 0.541991055 epoch total loss 0.562824368\n",
      "Trained batch 2517 batch loss 0.638676286 epoch total loss 0.562854528\n",
      "Trained batch 2518 batch loss 0.624647439 epoch total loss 0.562879086\n",
      "Trained batch 2519 batch loss 0.57856828 epoch total loss 0.562885284\n",
      "Trained batch 2520 batch loss 0.571846426 epoch total loss 0.562888861\n",
      "Trained batch 2521 batch loss 0.531447291 epoch total loss 0.562876403\n",
      "Trained batch 2522 batch loss 0.466247946 epoch total loss 0.562838137\n",
      "Trained batch 2523 batch loss 0.426199794 epoch total loss 0.562783957\n",
      "Trained batch 2524 batch loss 0.370340079 epoch total loss 0.562707722\n",
      "Trained batch 2525 batch loss 0.4898642 epoch total loss 0.562678874\n",
      "Trained batch 2526 batch loss 0.554748416 epoch total loss 0.562675714\n",
      "Trained batch 2527 batch loss 0.475071669 epoch total loss 0.562641084\n",
      "Trained batch 2528 batch loss 0.526083529 epoch total loss 0.5626266\n",
      "Trained batch 2529 batch loss 0.450746953 epoch total loss 0.562582374\n",
      "Trained batch 2530 batch loss 0.502868533 epoch total loss 0.56255877\n",
      "Trained batch 2531 batch loss 0.513805032 epoch total loss 0.562539518\n",
      "Trained batch 2532 batch loss 0.535061955 epoch total loss 0.56252861\n",
      "Trained batch 2533 batch loss 0.551638186 epoch total loss 0.562524319\n",
      "Trained batch 2534 batch loss 0.588735759 epoch total loss 0.56253469\n",
      "Trained batch 2535 batch loss 0.59616518 epoch total loss 0.562548\n",
      "Trained batch 2536 batch loss 0.466021329 epoch total loss 0.562509894\n",
      "Trained batch 2537 batch loss 0.510305047 epoch total loss 0.562489331\n",
      "Trained batch 2538 batch loss 0.581569374 epoch total loss 0.562496841\n",
      "Trained batch 2539 batch loss 0.657841921 epoch total loss 0.562534392\n",
      "Trained batch 2540 batch loss 0.554640472 epoch total loss 0.562531292\n",
      "Trained batch 2541 batch loss 0.57107234 epoch total loss 0.56253463\n",
      "Trained batch 2542 batch loss 0.599102795 epoch total loss 0.562549055\n",
      "Trained batch 2543 batch loss 0.651880205 epoch total loss 0.562584162\n",
      "Trained batch 2544 batch loss 0.738411784 epoch total loss 0.562653244\n",
      "Trained batch 2545 batch loss 0.648324907 epoch total loss 0.56268692\n",
      "Trained batch 2546 batch loss 0.738888443 epoch total loss 0.562756121\n",
      "Trained batch 2547 batch loss 0.67762351 epoch total loss 0.562801242\n",
      "Trained batch 2548 batch loss 0.624445 epoch total loss 0.562825382\n",
      "Trained batch 2549 batch loss 0.511428118 epoch total loss 0.562805235\n",
      "Trained batch 2550 batch loss 0.490205348 epoch total loss 0.562776804\n",
      "Trained batch 2551 batch loss 0.507574558 epoch total loss 0.562755167\n",
      "Trained batch 2552 batch loss 0.539476693 epoch total loss 0.562746\n",
      "Trained batch 2553 batch loss 0.689610839 epoch total loss 0.562795699\n",
      "Trained batch 2554 batch loss 0.607012868 epoch total loss 0.562813044\n",
      "Trained batch 2555 batch loss 0.665680289 epoch total loss 0.562853277\n",
      "Trained batch 2556 batch loss 0.565924644 epoch total loss 0.562854469\n",
      "Trained batch 2557 batch loss 0.694672465 epoch total loss 0.562906\n",
      "Trained batch 2558 batch loss 0.579331517 epoch total loss 0.562912464\n",
      "Trained batch 2559 batch loss 0.617256343 epoch total loss 0.562933683\n",
      "Trained batch 2560 batch loss 0.711303771 epoch total loss 0.562991679\n",
      "Trained batch 2561 batch loss 0.61683619 epoch total loss 0.56301266\n",
      "Trained batch 2562 batch loss 0.654480875 epoch total loss 0.563048422\n",
      "Trained batch 2563 batch loss 0.611646354 epoch total loss 0.563067377\n",
      "Trained batch 2564 batch loss 0.641480327 epoch total loss 0.563097954\n",
      "Trained batch 2565 batch loss 0.61221081 epoch total loss 0.563117087\n",
      "Trained batch 2566 batch loss 0.547745585 epoch total loss 0.563111126\n",
      "Trained batch 2567 batch loss 0.713612914 epoch total loss 0.563169718\n",
      "Trained batch 2568 batch loss 0.638147712 epoch total loss 0.563199\n",
      "Trained batch 2569 batch loss 0.628712595 epoch total loss 0.563224435\n",
      "Trained batch 2570 batch loss 0.650982261 epoch total loss 0.563258588\n",
      "Trained batch 2571 batch loss 0.547792196 epoch total loss 0.563252628\n",
      "Trained batch 2572 batch loss 0.5548594 epoch total loss 0.56324929\n",
      "Trained batch 2573 batch loss 0.55057 epoch total loss 0.563244402\n",
      "Trained batch 2574 batch loss 0.538207769 epoch total loss 0.563234627\n",
      "Trained batch 2575 batch loss 0.548968554 epoch total loss 0.563229084\n",
      "Trained batch 2576 batch loss 0.518199265 epoch total loss 0.56321162\n",
      "Trained batch 2577 batch loss 0.536346138 epoch total loss 0.563201189\n",
      "Trained batch 2578 batch loss 0.590949655 epoch total loss 0.563212\n",
      "Trained batch 2579 batch loss 0.616282046 epoch total loss 0.563232541\n",
      "Trained batch 2580 batch loss 0.609894753 epoch total loss 0.563250661\n",
      "Trained batch 2581 batch loss 0.535640478 epoch total loss 0.563239932\n",
      "Trained batch 2582 batch loss 0.591032863 epoch total loss 0.563250721\n",
      "Trained batch 2583 batch loss 0.598310947 epoch total loss 0.563264251\n",
      "Trained batch 2584 batch loss 0.630493879 epoch total loss 0.563290298\n",
      "Trained batch 2585 batch loss 0.587273777 epoch total loss 0.563299596\n",
      "Trained batch 2586 batch loss 0.621308088 epoch total loss 0.563322\n",
      "Trained batch 2587 batch loss 0.531684339 epoch total loss 0.563309789\n",
      "Trained batch 2588 batch loss 0.609336257 epoch total loss 0.56332761\n",
      "Trained batch 2589 batch loss 0.604199 epoch total loss 0.563343406\n",
      "Trained batch 2590 batch loss 0.68198657 epoch total loss 0.563389242\n",
      "Trained batch 2591 batch loss 0.590706289 epoch total loss 0.563399792\n",
      "Trained batch 2592 batch loss 0.459593177 epoch total loss 0.563359737\n",
      "Trained batch 2593 batch loss 0.471988916 epoch total loss 0.563324511\n",
      "Trained batch 2594 batch loss 0.50894016 epoch total loss 0.56330353\n",
      "Trained batch 2595 batch loss 0.503522038 epoch total loss 0.563280523\n",
      "Trained batch 2596 batch loss 0.432327211 epoch total loss 0.563230097\n",
      "Trained batch 2597 batch loss 0.611970723 epoch total loss 0.563248813\n",
      "Trained batch 2598 batch loss 0.65334326 epoch total loss 0.563283503\n",
      "Trained batch 2599 batch loss 0.552663147 epoch total loss 0.56327939\n",
      "Trained batch 2600 batch loss 0.594621062 epoch total loss 0.56329143\n",
      "Trained batch 2601 batch loss 0.645108342 epoch total loss 0.563322902\n",
      "Trained batch 2602 batch loss 0.5288046 epoch total loss 0.563309669\n",
      "Trained batch 2603 batch loss 0.56773454 epoch total loss 0.563311338\n",
      "Trained batch 2604 batch loss 0.508107424 epoch total loss 0.563290119\n",
      "Trained batch 2605 batch loss 0.670474112 epoch total loss 0.563331306\n",
      "Trained batch 2606 batch loss 0.580433846 epoch total loss 0.563337862\n",
      "Trained batch 2607 batch loss 0.406837195 epoch total loss 0.563277841\n",
      "Trained batch 2608 batch loss 0.502229631 epoch total loss 0.563254416\n",
      "Trained batch 2609 batch loss 0.441426158 epoch total loss 0.563207746\n",
      "Trained batch 2610 batch loss 0.460922539 epoch total loss 0.563168526\n",
      "Trained batch 2611 batch loss 0.442972 epoch total loss 0.563122511\n",
      "Trained batch 2612 batch loss 0.403270781 epoch total loss 0.563061357\n",
      "Trained batch 2613 batch loss 0.449813247 epoch total loss 0.563017964\n",
      "Trained batch 2614 batch loss 0.593344092 epoch total loss 0.563029587\n",
      "Trained batch 2615 batch loss 0.60724628 epoch total loss 0.563046515\n",
      "Trained batch 2616 batch loss 0.634574711 epoch total loss 0.563073874\n",
      "Trained batch 2617 batch loss 0.616535544 epoch total loss 0.563094318\n",
      "Trained batch 2618 batch loss 0.680678189 epoch total loss 0.5631392\n",
      "Trained batch 2619 batch loss 0.618765 epoch total loss 0.563160479\n",
      "Trained batch 2620 batch loss 0.56950748 epoch total loss 0.563162863\n",
      "Trained batch 2621 batch loss 0.567880332 epoch total loss 0.563164651\n",
      "Trained batch 2622 batch loss 0.522592 epoch total loss 0.563149154\n",
      "Trained batch 2623 batch loss 0.497267902 epoch total loss 0.563124061\n",
      "Trained batch 2624 batch loss 0.485720962 epoch total loss 0.563094556\n",
      "Trained batch 2625 batch loss 0.465012103 epoch total loss 0.563057184\n",
      "Trained batch 2626 batch loss 0.512475967 epoch total loss 0.563037932\n",
      "Trained batch 2627 batch loss 0.595540404 epoch total loss 0.56305033\n",
      "Trained batch 2628 batch loss 0.63495177 epoch total loss 0.563077688\n",
      "Trained batch 2629 batch loss 0.544739962 epoch total loss 0.563070774\n",
      "Trained batch 2630 batch loss 0.520841122 epoch total loss 0.563054681\n",
      "Trained batch 2631 batch loss 0.650333643 epoch total loss 0.563087881\n",
      "Trained batch 2632 batch loss 0.549612105 epoch total loss 0.563082755\n",
      "Trained batch 2633 batch loss 0.541174352 epoch total loss 0.56307441\n",
      "Trained batch 2634 batch loss 0.544235289 epoch total loss 0.563067257\n",
      "Trained batch 2635 batch loss 0.623854339 epoch total loss 0.563090324\n",
      "Trained batch 2636 batch loss 0.692786634 epoch total loss 0.563139498\n",
      "Trained batch 2637 batch loss 0.681356072 epoch total loss 0.563184381\n",
      "Trained batch 2638 batch loss 0.594213903 epoch total loss 0.563196123\n",
      "Trained batch 2639 batch loss 0.510944247 epoch total loss 0.563176334\n",
      "Trained batch 2640 batch loss 0.592245102 epoch total loss 0.563187361\n",
      "Trained batch 2641 batch loss 0.585928082 epoch total loss 0.563196\n",
      "Trained batch 2642 batch loss 0.531871676 epoch total loss 0.563184142\n",
      "Trained batch 2643 batch loss 0.507193685 epoch total loss 0.563163\n",
      "Trained batch 2644 batch loss 0.519029617 epoch total loss 0.563146293\n",
      "Trained batch 2645 batch loss 0.463053286 epoch total loss 0.563108385\n",
      "Trained batch 2646 batch loss 0.472984612 epoch total loss 0.56307435\n",
      "Trained batch 2647 batch loss 0.590508819 epoch total loss 0.563084722\n",
      "Trained batch 2648 batch loss 0.520570874 epoch total loss 0.563068688\n",
      "Trained batch 2649 batch loss 0.624906063 epoch total loss 0.563092\n",
      "Trained batch 2650 batch loss 0.60275048 epoch total loss 0.563107\n",
      "Trained batch 2651 batch loss 0.521686852 epoch total loss 0.563091397\n",
      "Trained batch 2652 batch loss 0.584956884 epoch total loss 0.563099623\n",
      "Trained batch 2653 batch loss 0.614843309 epoch total loss 0.563119113\n",
      "Trained batch 2654 batch loss 0.610108793 epoch total loss 0.563136816\n",
      "Trained batch 2655 batch loss 0.566052616 epoch total loss 0.563137949\n",
      "Trained batch 2656 batch loss 0.462602675 epoch total loss 0.5631001\n",
      "Trained batch 2657 batch loss 0.475940019 epoch total loss 0.563067317\n",
      "Trained batch 2658 batch loss 0.454117596 epoch total loss 0.563026309\n",
      "Trained batch 2659 batch loss 0.432546526 epoch total loss 0.562977195\n",
      "Trained batch 2660 batch loss 0.508618534 epoch total loss 0.56295681\n",
      "Trained batch 2661 batch loss 0.584427297 epoch total loss 0.562964857\n",
      "Trained batch 2662 batch loss 0.648118 epoch total loss 0.562996864\n",
      "Trained batch 2663 batch loss 0.660687864 epoch total loss 0.563033521\n",
      "Trained batch 2664 batch loss 0.592226267 epoch total loss 0.563044488\n",
      "Trained batch 2665 batch loss 0.625286758 epoch total loss 0.563067853\n",
      "Trained batch 2666 batch loss 0.659570098 epoch total loss 0.563104033\n",
      "Trained batch 2667 batch loss 0.553442717 epoch total loss 0.563100398\n",
      "Trained batch 2668 batch loss 0.621101558 epoch total loss 0.563122153\n",
      "Trained batch 2669 batch loss 0.637790263 epoch total loss 0.563150167\n",
      "Trained batch 2670 batch loss 0.667132 epoch total loss 0.563189089\n",
      "Trained batch 2671 batch loss 0.571194291 epoch total loss 0.56319207\n",
      "Trained batch 2672 batch loss 0.596753955 epoch total loss 0.563204646\n",
      "Trained batch 2673 batch loss 0.53880161 epoch total loss 0.563195527\n",
      "Trained batch 2674 batch loss 0.640567541 epoch total loss 0.563224494\n",
      "Trained batch 2675 batch loss 0.635057 epoch total loss 0.563251317\n",
      "Trained batch 2676 batch loss 0.538866878 epoch total loss 0.563242197\n",
      "Trained batch 2677 batch loss 0.573026419 epoch total loss 0.563245833\n",
      "Trained batch 2678 batch loss 0.515062332 epoch total loss 0.563227832\n",
      "Trained batch 2679 batch loss 0.616960347 epoch total loss 0.563247859\n",
      "Trained batch 2680 batch loss 0.625125051 epoch total loss 0.563270926\n",
      "Trained batch 2681 batch loss 0.585746109 epoch total loss 0.563279331\n",
      "Trained batch 2682 batch loss 0.626485407 epoch total loss 0.563302875\n",
      "Trained batch 2683 batch loss 0.583437145 epoch total loss 0.563310385\n",
      "Trained batch 2684 batch loss 0.614941597 epoch total loss 0.563329637\n",
      "Trained batch 2685 batch loss 0.687439442 epoch total loss 0.56337589\n",
      "Trained batch 2686 batch loss 0.580442905 epoch total loss 0.563382268\n",
      "Trained batch 2687 batch loss 0.557560444 epoch total loss 0.563380122\n",
      "Trained batch 2688 batch loss 0.616217077 epoch total loss 0.563399792\n",
      "Trained batch 2689 batch loss 0.558668375 epoch total loss 0.563398\n",
      "Trained batch 2690 batch loss 0.589732647 epoch total loss 0.563407838\n",
      "Trained batch 2691 batch loss 0.568296552 epoch total loss 0.563409626\n",
      "Trained batch 2692 batch loss 0.567535162 epoch total loss 0.563411117\n",
      "Trained batch 2693 batch loss 0.481603265 epoch total loss 0.563380718\n",
      "Trained batch 2694 batch loss 0.474697858 epoch total loss 0.563347816\n",
      "Trained batch 2695 batch loss 0.533902586 epoch total loss 0.563336909\n",
      "Trained batch 2696 batch loss 0.600437045 epoch total loss 0.563350677\n",
      "Trained batch 2697 batch loss 0.558499038 epoch total loss 0.563348889\n",
      "Trained batch 2698 batch loss 0.563341737 epoch total loss 0.563348889\n",
      "Trained batch 2699 batch loss 0.55147332 epoch total loss 0.563344479\n",
      "Trained batch 2700 batch loss 0.584855258 epoch total loss 0.563352466\n",
      "Trained batch 2701 batch loss 0.547699034 epoch total loss 0.563346684\n",
      "Trained batch 2702 batch loss 0.502030253 epoch total loss 0.563324\n",
      "Trained batch 2703 batch loss 0.478352964 epoch total loss 0.563292563\n",
      "Trained batch 2704 batch loss 0.593295872 epoch total loss 0.563303649\n",
      "Trained batch 2705 batch loss 0.510695159 epoch total loss 0.563284218\n",
      "Trained batch 2706 batch loss 0.562389076 epoch total loss 0.56328392\n",
      "Trained batch 2707 batch loss 0.554863691 epoch total loss 0.563280761\n",
      "Trained batch 2708 batch loss 0.598033249 epoch total loss 0.563293576\n",
      "Trained batch 2709 batch loss 0.564551532 epoch total loss 0.563294053\n",
      "Trained batch 2710 batch loss 0.703493536 epoch total loss 0.56334579\n",
      "Trained batch 2711 batch loss 0.574814796 epoch total loss 0.56335\n",
      "Trained batch 2712 batch loss 0.500562906 epoch total loss 0.563326895\n",
      "Trained batch 2713 batch loss 0.518889308 epoch total loss 0.563310504\n",
      "Trained batch 2714 batch loss 0.476621449 epoch total loss 0.563278556\n",
      "Trained batch 2715 batch loss 0.496300519 epoch total loss 0.563253939\n",
      "Trained batch 2716 batch loss 0.54785192 epoch total loss 0.563248217\n",
      "Trained batch 2717 batch loss 0.501826227 epoch total loss 0.563225627\n",
      "Trained batch 2718 batch loss 0.568753719 epoch total loss 0.563227654\n",
      "Trained batch 2719 batch loss 0.577061653 epoch total loss 0.56323272\n",
      "Trained batch 2720 batch loss 0.528802931 epoch total loss 0.563220084\n",
      "Trained batch 2721 batch loss 0.54807359 epoch total loss 0.56321454\n",
      "Trained batch 2722 batch loss 0.560254633 epoch total loss 0.563213468\n",
      "Trained batch 2723 batch loss 0.626839697 epoch total loss 0.563236833\n",
      "Trained batch 2724 batch loss 0.566304862 epoch total loss 0.563237906\n",
      "Trained batch 2725 batch loss 0.582750142 epoch total loss 0.563245118\n",
      "Trained batch 2726 batch loss 0.484139204 epoch total loss 0.56321609\n",
      "Trained batch 2727 batch loss 0.448517084 epoch total loss 0.563174\n",
      "Trained batch 2728 batch loss 0.509033144 epoch total loss 0.563154161\n",
      "Trained batch 2729 batch loss 0.494736969 epoch total loss 0.563129067\n",
      "Trained batch 2730 batch loss 0.529362679 epoch total loss 0.563116729\n",
      "Trained batch 2731 batch loss 0.566478372 epoch total loss 0.563118\n",
      "Trained batch 2732 batch loss 0.466965497 epoch total loss 0.563082755\n",
      "Trained batch 2733 batch loss 0.542636752 epoch total loss 0.563075304\n",
      "Trained batch 2734 batch loss 0.55179131 epoch total loss 0.563071132\n",
      "Trained batch 2735 batch loss 0.542832911 epoch total loss 0.563063741\n",
      "Trained batch 2736 batch loss 0.517431378 epoch total loss 0.563047111\n",
      "Trained batch 2737 batch loss 0.537961602 epoch total loss 0.563037932\n",
      "Trained batch 2738 batch loss 0.627871752 epoch total loss 0.563061595\n",
      "Trained batch 2739 batch loss 0.595538378 epoch total loss 0.563073516\n",
      "Trained batch 2740 batch loss 0.403998286 epoch total loss 0.563015461\n",
      "Trained batch 2741 batch loss 0.559724689 epoch total loss 0.563014269\n",
      "Trained batch 2742 batch loss 0.525614 epoch total loss 0.563000619\n",
      "Trained batch 2743 batch loss 0.538680553 epoch total loss 0.562991738\n",
      "Trained batch 2744 batch loss 0.513495445 epoch total loss 0.562973738\n",
      "Trained batch 2745 batch loss 0.671607614 epoch total loss 0.563013315\n",
      "Trained batch 2746 batch loss 0.554762959 epoch total loss 0.563010335\n",
      "Trained batch 2747 batch loss 0.476984292 epoch total loss 0.562979\n",
      "Trained batch 2748 batch loss 0.432999104 epoch total loss 0.562931716\n",
      "Trained batch 2749 batch loss 0.566895962 epoch total loss 0.562933147\n",
      "Trained batch 2750 batch loss 0.531694055 epoch total loss 0.562921762\n",
      "Trained batch 2751 batch loss 0.557364345 epoch total loss 0.562919796\n",
      "Trained batch 2752 batch loss 0.497170031 epoch total loss 0.562895894\n",
      "Trained batch 2753 batch loss 0.580141425 epoch total loss 0.562902153\n",
      "Trained batch 2754 batch loss 0.46399188 epoch total loss 0.562866271\n",
      "Trained batch 2755 batch loss 0.522813261 epoch total loss 0.562851727\n",
      "Trained batch 2756 batch loss 0.516285181 epoch total loss 0.562834799\n",
      "Trained batch 2757 batch loss 0.534369171 epoch total loss 0.562824488\n",
      "Trained batch 2758 batch loss 0.47252059 epoch total loss 0.562791765\n",
      "Trained batch 2759 batch loss 0.538552284 epoch total loss 0.562783\n",
      "Trained batch 2760 batch loss 0.545399725 epoch total loss 0.562776685\n",
      "Trained batch 2761 batch loss 0.516926765 epoch total loss 0.562760115\n",
      "Trained batch 2762 batch loss 0.611192048 epoch total loss 0.562777638\n",
      "Trained batch 2763 batch loss 0.618288517 epoch total loss 0.562797725\n",
      "Trained batch 2764 batch loss 0.565623939 epoch total loss 0.562798798\n",
      "Trained batch 2765 batch loss 0.56110996 epoch total loss 0.562798202\n",
      "Trained batch 2766 batch loss 0.55717057 epoch total loss 0.562796116\n",
      "Trained batch 2767 batch loss 0.612194121 epoch total loss 0.562814\n",
      "Trained batch 2768 batch loss 0.628851533 epoch total loss 0.562837839\n",
      "Trained batch 2769 batch loss 0.5996508 epoch total loss 0.562851131\n",
      "Trained batch 2770 batch loss 0.648406386 epoch total loss 0.562882\n",
      "Trained batch 2771 batch loss 0.558521807 epoch total loss 0.562880456\n",
      "Trained batch 2772 batch loss 0.648718536 epoch total loss 0.562911391\n",
      "Trained batch 2773 batch loss 0.620732188 epoch total loss 0.562932253\n",
      "Trained batch 2774 batch loss 0.590723276 epoch total loss 0.562942266\n",
      "Trained batch 2775 batch loss 0.591155469 epoch total loss 0.562952459\n",
      "Trained batch 2776 batch loss 0.542705238 epoch total loss 0.562945127\n",
      "Epoch 6 train loss 0.5629451274871826\n",
      "Validated batch 1 batch loss 0.534965456\n",
      "Validated batch 2 batch loss 0.521871805\n",
      "Validated batch 3 batch loss 0.493601143\n",
      "Validated batch 4 batch loss 0.547699332\n",
      "Validated batch 5 batch loss 0.57639432\n",
      "Validated batch 6 batch loss 0.535794139\n",
      "Validated batch 7 batch loss 0.602600396\n",
      "Validated batch 8 batch loss 0.491818\n",
      "Validated batch 9 batch loss 0.537910759\n",
      "Validated batch 10 batch loss 0.556735218\n",
      "Validated batch 11 batch loss 0.586821735\n",
      "Validated batch 12 batch loss 0.586643875\n",
      "Validated batch 13 batch loss 0.503354669\n",
      "Validated batch 14 batch loss 0.617911816\n",
      "Validated batch 15 batch loss 0.555448532\n",
      "Validated batch 16 batch loss 0.602577746\n",
      "Validated batch 17 batch loss 0.611234367\n",
      "Validated batch 18 batch loss 0.507477641\n",
      "Validated batch 19 batch loss 0.498459041\n",
      "Validated batch 20 batch loss 0.649121821\n",
      "Validated batch 21 batch loss 0.5633744\n",
      "Validated batch 22 batch loss 0.551479\n",
      "Validated batch 23 batch loss 0.610772908\n",
      "Validated batch 24 batch loss 0.512655199\n",
      "Validated batch 25 batch loss 0.54135859\n",
      "Validated batch 26 batch loss 0.596143425\n",
      "Validated batch 27 batch loss 0.566642761\n",
      "Validated batch 28 batch loss 0.556665063\n",
      "Validated batch 29 batch loss 0.586764932\n",
      "Validated batch 30 batch loss 0.548215508\n",
      "Validated batch 31 batch loss 0.534848154\n",
      "Validated batch 32 batch loss 0.588373542\n",
      "Validated batch 33 batch loss 0.631774247\n",
      "Validated batch 34 batch loss 0.643072069\n",
      "Validated batch 35 batch loss 0.50581193\n",
      "Validated batch 36 batch loss 0.652833581\n",
      "Validated batch 37 batch loss 0.583652735\n",
      "Validated batch 38 batch loss 0.462273151\n",
      "Validated batch 39 batch loss 0.574602783\n",
      "Validated batch 40 batch loss 0.666848\n",
      "Validated batch 41 batch loss 0.554879189\n",
      "Validated batch 42 batch loss 0.541768134\n",
      "Validated batch 43 batch loss 0.596421123\n",
      "Validated batch 44 batch loss 0.49608165\n",
      "Validated batch 45 batch loss 0.575432479\n",
      "Validated batch 46 batch loss 0.585112393\n",
      "Validated batch 47 batch loss 0.62688905\n",
      "Validated batch 48 batch loss 0.6337834\n",
      "Validated batch 49 batch loss 0.543151259\n",
      "Validated batch 50 batch loss 0.654571414\n",
      "Validated batch 51 batch loss 0.539352059\n",
      "Validated batch 52 batch loss 0.534273684\n",
      "Validated batch 53 batch loss 0.530567288\n",
      "Validated batch 54 batch loss 0.559669793\n",
      "Validated batch 55 batch loss 0.51175642\n",
      "Validated batch 56 batch loss 0.545146286\n",
      "Validated batch 57 batch loss 0.6095801\n",
      "Validated batch 58 batch loss 0.543928\n",
      "Validated batch 59 batch loss 0.581017733\n",
      "Validated batch 60 batch loss 0.611384\n",
      "Validated batch 61 batch loss 0.507758856\n",
      "Validated batch 62 batch loss 0.476782113\n",
      "Validated batch 63 batch loss 0.511956453\n",
      "Validated batch 64 batch loss 0.639478326\n",
      "Validated batch 65 batch loss 0.582337439\n",
      "Validated batch 66 batch loss 0.522113442\n",
      "Validated batch 67 batch loss 0.566225171\n",
      "Validated batch 68 batch loss 0.610737443\n",
      "Validated batch 69 batch loss 0.613449812\n",
      "Validated batch 70 batch loss 0.557836652\n",
      "Validated batch 71 batch loss 0.660359681\n",
      "Validated batch 72 batch loss 0.510218918\n",
      "Validated batch 73 batch loss 0.533006907\n",
      "Validated batch 74 batch loss 0.561998665\n",
      "Validated batch 75 batch loss 0.55961591\n",
      "Validated batch 76 batch loss 0.501249433\n",
      "Validated batch 77 batch loss 0.618014395\n",
      "Validated batch 78 batch loss 0.539912939\n",
      "Validated batch 79 batch loss 0.613499165\n",
      "Validated batch 80 batch loss 0.536151409\n",
      "Validated batch 81 batch loss 0.59578675\n",
      "Validated batch 82 batch loss 0.586651623\n",
      "Validated batch 83 batch loss 0.593825638\n",
      "Validated batch 84 batch loss 0.5757972\n",
      "Validated batch 85 batch loss 0.659802735\n",
      "Validated batch 86 batch loss 0.718525171\n",
      "Validated batch 87 batch loss 0.638262391\n",
      "Validated batch 88 batch loss 0.53353\n",
      "Validated batch 89 batch loss 0.568622172\n",
      "Validated batch 90 batch loss 0.602429688\n",
      "Validated batch 91 batch loss 0.520313859\n",
      "Validated batch 92 batch loss 0.509617865\n",
      "Validated batch 93 batch loss 0.515672565\n",
      "Validated batch 94 batch loss 0.558059573\n",
      "Validated batch 95 batch loss 0.580891848\n",
      "Validated batch 96 batch loss 0.555745244\n",
      "Validated batch 97 batch loss 0.579149\n",
      "Validated batch 98 batch loss 0.505875647\n",
      "Validated batch 99 batch loss 0.530038655\n",
      "Validated batch 100 batch loss 0.606122\n",
      "Validated batch 101 batch loss 0.52915746\n",
      "Validated batch 102 batch loss 0.592171848\n",
      "Validated batch 103 batch loss 0.554254711\n",
      "Validated batch 104 batch loss 0.630050361\n",
      "Validated batch 105 batch loss 0.578000128\n",
      "Validated batch 106 batch loss 0.634660125\n",
      "Validated batch 107 batch loss 0.600851774\n",
      "Validated batch 108 batch loss 0.591649055\n",
      "Validated batch 109 batch loss 0.637853\n",
      "Validated batch 110 batch loss 0.562238693\n",
      "Validated batch 111 batch loss 0.541193306\n",
      "Validated batch 112 batch loss 0.569947481\n",
      "Validated batch 113 batch loss 0.571885645\n",
      "Validated batch 114 batch loss 0.598782182\n",
      "Validated batch 115 batch loss 0.58536309\n",
      "Validated batch 116 batch loss 0.553714335\n",
      "Validated batch 117 batch loss 0.541206956\n",
      "Validated batch 118 batch loss 0.608227253\n",
      "Validated batch 119 batch loss 0.65170157\n",
      "Validated batch 120 batch loss 0.64667064\n",
      "Validated batch 121 batch loss 0.646553755\n",
      "Validated batch 122 batch loss 0.605942965\n",
      "Validated batch 123 batch loss 0.564134121\n",
      "Validated batch 124 batch loss 0.57241106\n",
      "Validated batch 125 batch loss 0.600627542\n",
      "Validated batch 126 batch loss 0.726278484\n",
      "Validated batch 127 batch loss 0.492666423\n",
      "Validated batch 128 batch loss 0.51328\n",
      "Validated batch 129 batch loss 0.580184102\n",
      "Validated batch 130 batch loss 0.674258351\n",
      "Validated batch 131 batch loss 0.508409083\n",
      "Validated batch 132 batch loss 0.466056436\n",
      "Validated batch 133 batch loss 0.501803517\n",
      "Validated batch 134 batch loss 0.602616251\n",
      "Validated batch 135 batch loss 0.572878301\n",
      "Validated batch 136 batch loss 0.646360874\n",
      "Validated batch 137 batch loss 0.512888551\n",
      "Validated batch 138 batch loss 0.576113403\n",
      "Validated batch 139 batch loss 0.525450885\n",
      "Validated batch 140 batch loss 0.571598887\n",
      "Validated batch 141 batch loss 0.610110044\n",
      "Validated batch 142 batch loss 0.527658224\n",
      "Validated batch 143 batch loss 0.598097146\n",
      "Validated batch 144 batch loss 0.547671437\n",
      "Validated batch 145 batch loss 0.472070098\n",
      "Validated batch 146 batch loss 0.594835222\n",
      "Validated batch 147 batch loss 0.559909284\n",
      "Validated batch 148 batch loss 0.50344789\n",
      "Validated batch 149 batch loss 0.639813125\n",
      "Validated batch 150 batch loss 0.588458061\n",
      "Validated batch 151 batch loss 0.53346324\n",
      "Validated batch 152 batch loss 0.53203994\n",
      "Validated batch 153 batch loss 0.561399102\n",
      "Validated batch 154 batch loss 0.44804281\n",
      "Validated batch 155 batch loss 0.512185693\n",
      "Validated batch 156 batch loss 0.558969378\n",
      "Validated batch 157 batch loss 0.574948788\n",
      "Validated batch 158 batch loss 0.545206785\n",
      "Validated batch 159 batch loss 0.509891391\n",
      "Validated batch 160 batch loss 0.495543033\n",
      "Validated batch 161 batch loss 0.678507447\n",
      "Validated batch 162 batch loss 0.467556655\n",
      "Validated batch 163 batch loss 0.511964679\n",
      "Validated batch 164 batch loss 0.611273766\n",
      "Validated batch 165 batch loss 0.531276226\n",
      "Validated batch 166 batch loss 0.574444294\n",
      "Validated batch 167 batch loss 0.609602451\n",
      "Validated batch 168 batch loss 0.642083168\n",
      "Validated batch 169 batch loss 0.589384437\n",
      "Validated batch 170 batch loss 0.620730221\n",
      "Validated batch 171 batch loss 0.548375607\n",
      "Validated batch 172 batch loss 0.52361995\n",
      "Validated batch 173 batch loss 0.658185303\n",
      "Validated batch 174 batch loss 0.565062344\n",
      "Validated batch 175 batch loss 0.513077438\n",
      "Validated batch 176 batch loss 0.439931124\n",
      "Validated batch 177 batch loss 0.525421143\n",
      "Validated batch 178 batch loss 0.55642724\n",
      "Validated batch 179 batch loss 0.542366862\n",
      "Validated batch 180 batch loss 0.519544601\n",
      "Validated batch 181 batch loss 0.505774\n",
      "Validated batch 182 batch loss 0.601494491\n",
      "Validated batch 183 batch loss 0.656314135\n",
      "Validated batch 184 batch loss 0.634703159\n",
      "Validated batch 185 batch loss 0.572954714\n",
      "Validated batch 186 batch loss 0.580492616\n",
      "Validated batch 187 batch loss 0.647176087\n",
      "Validated batch 188 batch loss 0.562592149\n",
      "Validated batch 189 batch loss 0.473933935\n",
      "Validated batch 190 batch loss 0.6031968\n",
      "Validated batch 191 batch loss 0.587275863\n",
      "Validated batch 192 batch loss 0.544570684\n",
      "Validated batch 193 batch loss 0.539694369\n",
      "Validated batch 194 batch loss 0.601379693\n",
      "Validated batch 195 batch loss 0.526990116\n",
      "Validated batch 196 batch loss 0.615429223\n",
      "Validated batch 197 batch loss 0.59342742\n",
      "Validated batch 198 batch loss 0.565873921\n",
      "Validated batch 199 batch loss 0.51596725\n",
      "Validated batch 200 batch loss 0.614300191\n",
      "Validated batch 201 batch loss 0.406045675\n",
      "Validated batch 202 batch loss 0.684479296\n",
      "Validated batch 203 batch loss 0.665134966\n",
      "Validated batch 204 batch loss 0.571105242\n",
      "Validated batch 205 batch loss 0.575716615\n",
      "Validated batch 206 batch loss 0.516545296\n",
      "Validated batch 207 batch loss 0.501409173\n",
      "Validated batch 208 batch loss 0.531077683\n",
      "Validated batch 209 batch loss 0.585152388\n",
      "Validated batch 210 batch loss 0.554107368\n",
      "Validated batch 211 batch loss 0.61327\n",
      "Validated batch 212 batch loss 0.642497659\n",
      "Validated batch 213 batch loss 0.588100433\n",
      "Validated batch 214 batch loss 0.696279\n",
      "Validated batch 215 batch loss 0.654307842\n",
      "Validated batch 216 batch loss 0.66467905\n",
      "Validated batch 217 batch loss 0.540582716\n",
      "Validated batch 218 batch loss 0.571827412\n",
      "Validated batch 219 batch loss 0.67100656\n",
      "Validated batch 220 batch loss 0.615854502\n",
      "Validated batch 221 batch loss 0.591241539\n",
      "Validated batch 222 batch loss 0.576496303\n",
      "Validated batch 223 batch loss 0.73473829\n",
      "Validated batch 224 batch loss 0.5015769\n",
      "Validated batch 225 batch loss 0.585666835\n",
      "Validated batch 226 batch loss 0.625128329\n",
      "Validated batch 227 batch loss 0.460317314\n",
      "Validated batch 228 batch loss 0.456641734\n",
      "Validated batch 229 batch loss 0.526945472\n",
      "Validated batch 230 batch loss 0.601159513\n",
      "Validated batch 231 batch loss 0.587391853\n",
      "Validated batch 232 batch loss 0.548616707\n",
      "Validated batch 233 batch loss 0.548488498\n",
      "Validated batch 234 batch loss 0.499718428\n",
      "Validated batch 235 batch loss 0.524372935\n",
      "Validated batch 236 batch loss 0.582769275\n",
      "Validated batch 237 batch loss 0.561331272\n",
      "Validated batch 238 batch loss 0.471958756\n",
      "Validated batch 239 batch loss 0.591416419\n",
      "Validated batch 240 batch loss 0.49108094\n",
      "Validated batch 241 batch loss 0.595102072\n",
      "Validated batch 242 batch loss 0.643545032\n",
      "Validated batch 243 batch loss 0.52640295\n",
      "Validated batch 244 batch loss 0.622706413\n",
      "Validated batch 245 batch loss 0.597696126\n",
      "Validated batch 246 batch loss 0.645893395\n",
      "Validated batch 247 batch loss 0.630597711\n",
      "Validated batch 248 batch loss 0.581241\n",
      "Validated batch 249 batch loss 0.624166965\n",
      "Validated batch 250 batch loss 0.552004874\n",
      "Validated batch 251 batch loss 0.58904773\n",
      "Validated batch 252 batch loss 0.558688402\n",
      "Validated batch 253 batch loss 0.641638696\n",
      "Validated batch 254 batch loss 0.698210835\n",
      "Validated batch 255 batch loss 0.55116272\n",
      "Validated batch 256 batch loss 0.60107404\n",
      "Validated batch 257 batch loss 0.668377817\n",
      "Validated batch 258 batch loss 0.587376\n",
      "Validated batch 259 batch loss 0.606345236\n",
      "Validated batch 260 batch loss 0.610352039\n",
      "Validated batch 261 batch loss 0.647736\n",
      "Validated batch 262 batch loss 0.622950494\n",
      "Validated batch 263 batch loss 0.625888944\n",
      "Validated batch 264 batch loss 0.608631134\n",
      "Validated batch 265 batch loss 0.610845566\n",
      "Validated batch 266 batch loss 0.478838623\n",
      "Validated batch 267 batch loss 0.55819881\n",
      "Validated batch 268 batch loss 0.606827319\n",
      "Validated batch 269 batch loss 0.617231846\n",
      "Validated batch 270 batch loss 0.611083686\n",
      "Validated batch 271 batch loss 0.650643229\n",
      "Validated batch 272 batch loss 0.58453542\n",
      "Validated batch 273 batch loss 0.523133874\n",
      "Validated batch 274 batch loss 0.564891\n",
      "Validated batch 275 batch loss 0.603116393\n",
      "Validated batch 276 batch loss 0.564452469\n",
      "Validated batch 277 batch loss 0.547892272\n",
      "Validated batch 278 batch loss 0.514834881\n",
      "Validated batch 279 batch loss 0.547454834\n",
      "Validated batch 280 batch loss 0.562160373\n",
      "Validated batch 281 batch loss 0.578544855\n",
      "Validated batch 282 batch loss 0.581139326\n",
      "Validated batch 283 batch loss 0.53935945\n",
      "Validated batch 284 batch loss 0.511965573\n",
      "Validated batch 285 batch loss 0.572572351\n",
      "Validated batch 286 batch loss 0.60170573\n",
      "Validated batch 287 batch loss 0.672296345\n",
      "Validated batch 288 batch loss 0.652968109\n",
      "Validated batch 289 batch loss 0.543141603\n",
      "Validated batch 290 batch loss 0.457100213\n",
      "Validated batch 291 batch loss 0.552069\n",
      "Validated batch 292 batch loss 0.578144073\n",
      "Validated batch 293 batch loss 0.569128036\n",
      "Validated batch 294 batch loss 0.534102559\n",
      "Validated batch 295 batch loss 0.599754632\n",
      "Validated batch 296 batch loss 0.592422724\n",
      "Validated batch 297 batch loss 0.613287\n",
      "Validated batch 298 batch loss 0.572824\n",
      "Validated batch 299 batch loss 0.544994116\n",
      "Validated batch 300 batch loss 0.562613964\n",
      "Validated batch 301 batch loss 0.391288698\n",
      "Validated batch 302 batch loss 0.487189\n",
      "Validated batch 303 batch loss 0.592390835\n",
      "Validated batch 304 batch loss 0.517428577\n",
      "Validated batch 305 batch loss 0.535625756\n",
      "Validated batch 306 batch loss 0.524294496\n",
      "Validated batch 307 batch loss 0.553771675\n",
      "Validated batch 308 batch loss 0.599298418\n",
      "Validated batch 309 batch loss 0.629347086\n",
      "Validated batch 310 batch loss 0.585990191\n",
      "Validated batch 311 batch loss 0.518495262\n",
      "Validated batch 312 batch loss 0.497618139\n",
      "Validated batch 313 batch loss 0.59763521\n",
      "Validated batch 314 batch loss 0.569900393\n",
      "Validated batch 315 batch loss 0.604755402\n",
      "Validated batch 316 batch loss 0.634627342\n",
      "Validated batch 317 batch loss 0.55561161\n",
      "Validated batch 318 batch loss 0.637653828\n",
      "Validated batch 319 batch loss 0.55773139\n",
      "Validated batch 320 batch loss 0.551960826\n",
      "Validated batch 321 batch loss 0.555300832\n",
      "Validated batch 322 batch loss 0.482410043\n",
      "Validated batch 323 batch loss 0.59767586\n",
      "Validated batch 324 batch loss 0.573828578\n",
      "Validated batch 325 batch loss 0.597095132\n",
      "Validated batch 326 batch loss 0.539641678\n",
      "Validated batch 327 batch loss 0.59700954\n",
      "Validated batch 328 batch loss 0.57396543\n",
      "Validated batch 329 batch loss 0.570131123\n",
      "Validated batch 330 batch loss 0.572184801\n",
      "Validated batch 331 batch loss 0.58348918\n",
      "Validated batch 332 batch loss 0.648049891\n",
      "Validated batch 333 batch loss 0.649735928\n",
      "Validated batch 334 batch loss 0.701826692\n",
      "Validated batch 335 batch loss 0.577052712\n",
      "Validated batch 336 batch loss 0.539304852\n",
      "Validated batch 337 batch loss 0.564128101\n",
      "Validated batch 338 batch loss 0.596177578\n",
      "Validated batch 339 batch loss 0.567146897\n",
      "Validated batch 340 batch loss 0.564537\n",
      "Validated batch 341 batch loss 0.643717706\n",
      "Validated batch 342 batch loss 0.565634668\n",
      "Validated batch 343 batch loss 0.605100691\n",
      "Validated batch 344 batch loss 0.581189811\n",
      "Validated batch 345 batch loss 0.601213574\n",
      "Validated batch 346 batch loss 0.549939334\n",
      "Validated batch 347 batch loss 0.551228344\n",
      "Validated batch 348 batch loss 0.649302185\n",
      "Validated batch 349 batch loss 0.71878171\n",
      "Validated batch 350 batch loss 0.507020235\n",
      "Validated batch 351 batch loss 0.55174613\n",
      "Validated batch 352 batch loss 0.639501512\n",
      "Validated batch 353 batch loss 0.589261115\n",
      "Validated batch 354 batch loss 0.628510475\n",
      "Validated batch 355 batch loss 0.611823618\n",
      "Validated batch 356 batch loss 0.614972472\n",
      "Validated batch 357 batch loss 0.637292266\n",
      "Validated batch 358 batch loss 0.495635748\n",
      "Validated batch 359 batch loss 0.510551393\n",
      "Validated batch 360 batch loss 0.601218104\n",
      "Validated batch 361 batch loss 0.589355707\n",
      "Validated batch 362 batch loss 0.572813451\n",
      "Validated batch 363 batch loss 0.510411739\n",
      "Validated batch 364 batch loss 0.56451726\n",
      "Validated batch 365 batch loss 0.618970692\n",
      "Validated batch 366 batch loss 0.515426695\n",
      "Validated batch 367 batch loss 0.593279362\n",
      "Validated batch 368 batch loss 0.560854614\n",
      "Validated batch 369 batch loss 0.619917572\n",
      "Epoch 6 val loss 0.5733787417411804\n",
      "Model /aiffel/mpii/weights/shg_model-epoch-6-loss-0.5734.h5 saved.\n",
      "Start epoch 7 with learning rate 0.0007\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 0.537634194 epoch total loss 0.537634194\n",
      "Trained batch 2 batch loss 0.493459374 epoch total loss 0.515546799\n",
      "Trained batch 3 batch loss 0.589599252 epoch total loss 0.54023093\n",
      "Trained batch 4 batch loss 0.640969098 epoch total loss 0.565415502\n",
      "Trained batch 5 batch loss 0.717361867 epoch total loss 0.595804811\n",
      "Trained batch 6 batch loss 0.542027712 epoch total loss 0.586841941\n",
      "Trained batch 7 batch loss 0.566132426 epoch total loss 0.583883405\n",
      "Trained batch 8 batch loss 0.506144702 epoch total loss 0.574166059\n",
      "Trained batch 9 batch loss 0.600496173 epoch total loss 0.577091634\n",
      "Trained batch 10 batch loss 0.605424166 epoch total loss 0.579924881\n",
      "Trained batch 11 batch loss 0.641842604 epoch total loss 0.585553765\n",
      "Trained batch 12 batch loss 0.508081555 epoch total loss 0.579097748\n",
      "Trained batch 13 batch loss 0.620273173 epoch total loss 0.582265079\n",
      "Trained batch 14 batch loss 0.538575351 epoch total loss 0.579144418\n",
      "Trained batch 15 batch loss 0.563451171 epoch total loss 0.578098178\n",
      "Trained batch 16 batch loss 0.57364 epoch total loss 0.577819526\n",
      "Trained batch 17 batch loss 0.611140192 epoch total loss 0.579779565\n",
      "Trained batch 18 batch loss 0.655720055 epoch total loss 0.583998442\n",
      "Trained batch 19 batch loss 0.652469814 epoch total loss 0.587602198\n",
      "Trained batch 20 batch loss 0.667194784 epoch total loss 0.591581821\n",
      "Trained batch 21 batch loss 0.577062249 epoch total loss 0.590890408\n",
      "Trained batch 22 batch loss 0.476847589 epoch total loss 0.585706651\n",
      "Trained batch 23 batch loss 0.546368062 epoch total loss 0.583996296\n",
      "Trained batch 24 batch loss 0.648545 epoch total loss 0.586685836\n",
      "Trained batch 25 batch loss 0.617030561 epoch total loss 0.587899566\n",
      "Trained batch 26 batch loss 0.561762393 epoch total loss 0.586894333\n",
      "Trained batch 27 batch loss 0.489469409 epoch total loss 0.583286\n",
      "Trained batch 28 batch loss 0.589548767 epoch total loss 0.583509624\n",
      "Trained batch 29 batch loss 0.613471866 epoch total loss 0.584542811\n",
      "Trained batch 30 batch loss 0.552712321 epoch total loss 0.583481789\n",
      "Trained batch 31 batch loss 0.586294591 epoch total loss 0.583572507\n",
      "Trained batch 32 batch loss 0.59753257 epoch total loss 0.584008753\n",
      "Trained batch 33 batch loss 0.578257143 epoch total loss 0.583834469\n",
      "Trained batch 34 batch loss 0.508675933 epoch total loss 0.581623912\n",
      "Trained batch 35 batch loss 0.611401498 epoch total loss 0.582474709\n",
      "Trained batch 36 batch loss 0.567990959 epoch total loss 0.582072377\n",
      "Trained batch 37 batch loss 0.510541618 epoch total loss 0.580139101\n",
      "Trained batch 38 batch loss 0.591681063 epoch total loss 0.580442846\n",
      "Trained batch 39 batch loss 0.642546237 epoch total loss 0.582035184\n",
      "Trained batch 40 batch loss 0.580531776 epoch total loss 0.581997633\n",
      "Trained batch 41 batch loss 0.530741513 epoch total loss 0.580747485\n",
      "Trained batch 42 batch loss 0.637357414 epoch total loss 0.582095325\n",
      "Trained batch 43 batch loss 0.443721324 epoch total loss 0.57887733\n",
      "Trained batch 44 batch loss 0.566474259 epoch total loss 0.578595459\n",
      "Trained batch 45 batch loss 0.628424883 epoch total loss 0.579702795\n",
      "Trained batch 46 batch loss 0.58784461 epoch total loss 0.57987982\n",
      "Trained batch 47 batch loss 0.466558307 epoch total loss 0.577468693\n",
      "Trained batch 48 batch loss 0.517397642 epoch total loss 0.576217234\n",
      "Trained batch 49 batch loss 0.537571132 epoch total loss 0.575428486\n",
      "Trained batch 50 batch loss 0.553348 epoch total loss 0.574986935\n",
      "Trained batch 51 batch loss 0.586875141 epoch total loss 0.575220048\n",
      "Trained batch 52 batch loss 0.621850371 epoch total loss 0.5761168\n",
      "Trained batch 53 batch loss 0.544612885 epoch total loss 0.575522363\n",
      "Trained batch 54 batch loss 0.517903924 epoch total loss 0.57445538\n",
      "Trained batch 55 batch loss 0.497476816 epoch total loss 0.573055744\n",
      "Trained batch 56 batch loss 0.525714278 epoch total loss 0.572210371\n",
      "Trained batch 57 batch loss 0.432681292 epoch total loss 0.569762528\n",
      "Trained batch 58 batch loss 0.55670619 epoch total loss 0.569537401\n",
      "Trained batch 59 batch loss 0.497793436 epoch total loss 0.568321407\n",
      "Trained batch 60 batch loss 0.539252043 epoch total loss 0.56783694\n",
      "Trained batch 61 batch loss 0.440665692 epoch total loss 0.565752208\n",
      "Trained batch 62 batch loss 0.48364231 epoch total loss 0.564427853\n",
      "Trained batch 63 batch loss 0.462206334 epoch total loss 0.562805295\n",
      "Trained batch 64 batch loss 0.531819522 epoch total loss 0.562321126\n",
      "Trained batch 65 batch loss 0.531168401 epoch total loss 0.561841905\n",
      "Trained batch 66 batch loss 0.524247646 epoch total loss 0.561272264\n",
      "Trained batch 67 batch loss 0.577581882 epoch total loss 0.561515689\n",
      "Trained batch 68 batch loss 0.584075689 epoch total loss 0.561847448\n",
      "Trained batch 69 batch loss 0.523198426 epoch total loss 0.561287284\n",
      "Trained batch 70 batch loss 0.558345735 epoch total loss 0.561245263\n",
      "Trained batch 71 batch loss 0.535322845 epoch total loss 0.560880184\n",
      "Trained batch 72 batch loss 0.519973218 epoch total loss 0.560312033\n",
      "Trained batch 73 batch loss 0.572705328 epoch total loss 0.560481787\n",
      "Trained batch 74 batch loss 0.570566118 epoch total loss 0.560618043\n",
      "Trained batch 75 batch loss 0.504930615 epoch total loss 0.559875607\n",
      "Trained batch 76 batch loss 0.530715883 epoch total loss 0.559491932\n",
      "Trained batch 77 batch loss 0.550466478 epoch total loss 0.55937469\n",
      "Trained batch 78 batch loss 0.493478626 epoch total loss 0.558529854\n",
      "Trained batch 79 batch loss 0.500939846 epoch total loss 0.557800829\n",
      "Trained batch 80 batch loss 0.418122262 epoch total loss 0.556054831\n",
      "Trained batch 81 batch loss 0.516828775 epoch total loss 0.555570602\n",
      "Trained batch 82 batch loss 0.539943814 epoch total loss 0.55538\n",
      "Trained batch 83 batch loss 0.494909346 epoch total loss 0.554651439\n",
      "Trained batch 84 batch loss 0.436134636 epoch total loss 0.553240538\n",
      "Trained batch 85 batch loss 0.486874819 epoch total loss 0.552459776\n",
      "Trained batch 86 batch loss 0.513235867 epoch total loss 0.552003682\n",
      "Trained batch 87 batch loss 0.580762863 epoch total loss 0.552334249\n",
      "Trained batch 88 batch loss 0.536642492 epoch total loss 0.552155912\n",
      "Trained batch 89 batch loss 0.547638237 epoch total loss 0.552105188\n",
      "Trained batch 90 batch loss 0.607730448 epoch total loss 0.552723229\n",
      "Trained batch 91 batch loss 0.491957128 epoch total loss 0.552055478\n",
      "Trained batch 92 batch loss 0.614605248 epoch total loss 0.552735388\n",
      "Trained batch 93 batch loss 0.587555826 epoch total loss 0.553109765\n",
      "Trained batch 94 batch loss 0.564628482 epoch total loss 0.553232312\n",
      "Trained batch 95 batch loss 0.503579915 epoch total loss 0.552709639\n",
      "Trained batch 96 batch loss 0.647345662 epoch total loss 0.55369544\n",
      "Trained batch 97 batch loss 0.718189478 epoch total loss 0.555391252\n",
      "Trained batch 98 batch loss 0.653375387 epoch total loss 0.55639106\n",
      "Trained batch 99 batch loss 0.593706429 epoch total loss 0.556768\n",
      "Trained batch 100 batch loss 0.493881166 epoch total loss 0.556139171\n",
      "Trained batch 101 batch loss 0.550794542 epoch total loss 0.556086183\n",
      "Trained batch 102 batch loss 0.579287171 epoch total loss 0.556313694\n",
      "Trained batch 103 batch loss 0.651576698 epoch total loss 0.557238579\n",
      "Trained batch 104 batch loss 0.649005771 epoch total loss 0.558120966\n",
      "Trained batch 105 batch loss 0.647045612 epoch total loss 0.558967829\n",
      "Trained batch 106 batch loss 0.573892891 epoch total loss 0.559108675\n",
      "Trained batch 107 batch loss 0.65681231 epoch total loss 0.560021758\n",
      "Trained batch 108 batch loss 0.562493503 epoch total loss 0.560044646\n",
      "Trained batch 109 batch loss 0.502191842 epoch total loss 0.559513927\n",
      "Trained batch 110 batch loss 0.573621154 epoch total loss 0.559642136\n",
      "Trained batch 111 batch loss 0.50507772 epoch total loss 0.559150577\n",
      "Trained batch 112 batch loss 0.51658988 epoch total loss 0.558770537\n",
      "Trained batch 113 batch loss 0.513967216 epoch total loss 0.558374047\n",
      "Trained batch 114 batch loss 0.552151561 epoch total loss 0.558319449\n",
      "Trained batch 115 batch loss 0.567316294 epoch total loss 0.55839771\n",
      "Trained batch 116 batch loss 0.557041287 epoch total loss 0.558386\n",
      "Trained batch 117 batch loss 0.600716949 epoch total loss 0.558747828\n",
      "Trained batch 118 batch loss 0.544568062 epoch total loss 0.558627665\n",
      "Trained batch 119 batch loss 0.604566455 epoch total loss 0.559013724\n",
      "Trained batch 120 batch loss 0.61141789 epoch total loss 0.559450448\n",
      "Trained batch 121 batch loss 0.569403946 epoch total loss 0.559532762\n",
      "Trained batch 122 batch loss 0.526823282 epoch total loss 0.55926466\n",
      "Trained batch 123 batch loss 0.500333548 epoch total loss 0.558785558\n",
      "Trained batch 124 batch loss 0.580373466 epoch total loss 0.558959663\n",
      "Trained batch 125 batch loss 0.569584429 epoch total loss 0.559044659\n",
      "Trained batch 126 batch loss 0.525665045 epoch total loss 0.558779776\n",
      "Trained batch 127 batch loss 0.590978563 epoch total loss 0.559033334\n",
      "Trained batch 128 batch loss 0.527253032 epoch total loss 0.558785\n",
      "Trained batch 129 batch loss 0.573951721 epoch total loss 0.558902621\n",
      "Trained batch 130 batch loss 0.520699382 epoch total loss 0.558608711\n",
      "Trained batch 131 batch loss 0.550610721 epoch total loss 0.558547676\n",
      "Trained batch 132 batch loss 0.48187989 epoch total loss 0.557966888\n",
      "Trained batch 133 batch loss 0.470609754 epoch total loss 0.557310045\n",
      "Trained batch 134 batch loss 0.478355885 epoch total loss 0.556720853\n",
      "Trained batch 135 batch loss 0.458132297 epoch total loss 0.555990517\n",
      "Trained batch 136 batch loss 0.501106083 epoch total loss 0.555587\n",
      "Trained batch 137 batch loss 0.504996598 epoch total loss 0.555217743\n",
      "Trained batch 138 batch loss 0.489171654 epoch total loss 0.554739118\n",
      "Trained batch 139 batch loss 0.605029 epoch total loss 0.555100918\n",
      "Trained batch 140 batch loss 0.56701833 epoch total loss 0.555186033\n",
      "Trained batch 141 batch loss 0.611768425 epoch total loss 0.555587351\n",
      "Trained batch 142 batch loss 0.612225533 epoch total loss 0.555986226\n",
      "Trained batch 143 batch loss 0.526735723 epoch total loss 0.555781662\n",
      "Trained batch 144 batch loss 0.547888637 epoch total loss 0.555726826\n",
      "Trained batch 145 batch loss 0.562740564 epoch total loss 0.555775225\n",
      "Trained batch 146 batch loss 0.53651458 epoch total loss 0.55564332\n",
      "Trained batch 147 batch loss 0.463595122 epoch total loss 0.555017114\n",
      "Trained batch 148 batch loss 0.476155341 epoch total loss 0.554484308\n",
      "Trained batch 149 batch loss 0.473356485 epoch total loss 0.553939819\n",
      "Trained batch 150 batch loss 0.461375 epoch total loss 0.553322732\n",
      "Trained batch 151 batch loss 0.55976 epoch total loss 0.55336535\n",
      "Trained batch 152 batch loss 0.488026172 epoch total loss 0.552935481\n",
      "Trained batch 153 batch loss 0.540194 epoch total loss 0.552852213\n",
      "Trained batch 154 batch loss 0.532956779 epoch total loss 0.55272305\n",
      "Trained batch 155 batch loss 0.6012761 epoch total loss 0.553036273\n",
      "Trained batch 156 batch loss 0.621282876 epoch total loss 0.553473771\n",
      "Trained batch 157 batch loss 0.598228335 epoch total loss 0.5537588\n",
      "Trained batch 158 batch loss 0.467395037 epoch total loss 0.553212166\n",
      "Trained batch 159 batch loss 0.60669148 epoch total loss 0.553548515\n",
      "Trained batch 160 batch loss 0.618766427 epoch total loss 0.553956151\n",
      "Trained batch 161 batch loss 0.558080256 epoch total loss 0.553981781\n",
      "Trained batch 162 batch loss 0.537787855 epoch total loss 0.553881824\n",
      "Trained batch 163 batch loss 0.552087605 epoch total loss 0.553870797\n",
      "Trained batch 164 batch loss 0.49276188 epoch total loss 0.553498149\n",
      "Trained batch 165 batch loss 0.533477545 epoch total loss 0.553376794\n",
      "Trained batch 166 batch loss 0.517207861 epoch total loss 0.553158939\n",
      "Trained batch 167 batch loss 0.533811 epoch total loss 0.553043067\n",
      "Trained batch 168 batch loss 0.567690492 epoch total loss 0.553130269\n",
      "Trained batch 169 batch loss 0.514995813 epoch total loss 0.552904606\n",
      "Trained batch 170 batch loss 0.601921856 epoch total loss 0.553193\n",
      "Trained batch 171 batch loss 0.537037492 epoch total loss 0.5530985\n",
      "Trained batch 172 batch loss 0.583145142 epoch total loss 0.553273201\n",
      "Trained batch 173 batch loss 0.57684958 epoch total loss 0.553409457\n",
      "Trained batch 174 batch loss 0.599282503 epoch total loss 0.553673089\n",
      "Trained batch 175 batch loss 0.550494552 epoch total loss 0.553654909\n",
      "Trained batch 176 batch loss 0.574663 epoch total loss 0.553774297\n",
      "Trained batch 177 batch loss 0.552026212 epoch total loss 0.553764403\n",
      "Trained batch 178 batch loss 0.629638612 epoch total loss 0.554190636\n",
      "Trained batch 179 batch loss 0.675795615 epoch total loss 0.55487\n",
      "Trained batch 180 batch loss 0.577541173 epoch total loss 0.554995954\n",
      "Trained batch 181 batch loss 0.481127232 epoch total loss 0.554587841\n",
      "Trained batch 182 batch loss 0.538947284 epoch total loss 0.554501891\n",
      "Trained batch 183 batch loss 0.574091196 epoch total loss 0.554608941\n",
      "Trained batch 184 batch loss 0.552304268 epoch total loss 0.554596424\n",
      "Trained batch 185 batch loss 0.686884046 epoch total loss 0.555311441\n",
      "Trained batch 186 batch loss 0.563466191 epoch total loss 0.55535531\n",
      "Trained batch 187 batch loss 0.507714 epoch total loss 0.55510056\n",
      "Trained batch 188 batch loss 0.552806139 epoch total loss 0.555088341\n",
      "Trained batch 189 batch loss 0.461247325 epoch total loss 0.554591835\n",
      "Trained batch 190 batch loss 0.488761902 epoch total loss 0.554245353\n",
      "Trained batch 191 batch loss 0.525725 epoch total loss 0.554096043\n",
      "Trained batch 192 batch loss 0.585524738 epoch total loss 0.554259717\n",
      "Trained batch 193 batch loss 0.642840266 epoch total loss 0.554718673\n",
      "Trained batch 194 batch loss 0.628802538 epoch total loss 0.55510056\n",
      "Trained batch 195 batch loss 0.665356338 epoch total loss 0.55566597\n",
      "Trained batch 196 batch loss 0.606020629 epoch total loss 0.555922866\n",
      "Trained batch 197 batch loss 0.620201886 epoch total loss 0.556249142\n",
      "Trained batch 198 batch loss 0.47480166 epoch total loss 0.55583781\n",
      "Trained batch 199 batch loss 0.492258787 epoch total loss 0.55551827\n",
      "Trained batch 200 batch loss 0.531482399 epoch total loss 0.555398107\n",
      "Trained batch 201 batch loss 0.599716663 epoch total loss 0.555618584\n",
      "Trained batch 202 batch loss 0.602905035 epoch total loss 0.555852711\n",
      "Trained batch 203 batch loss 0.605831265 epoch total loss 0.556098878\n",
      "Trained batch 204 batch loss 0.469911188 epoch total loss 0.555676401\n",
      "Trained batch 205 batch loss 0.576597333 epoch total loss 0.555778444\n",
      "Trained batch 206 batch loss 0.597796619 epoch total loss 0.555982411\n",
      "Trained batch 207 batch loss 0.639496088 epoch total loss 0.556385875\n",
      "Trained batch 208 batch loss 0.626060545 epoch total loss 0.556720853\n",
      "Trained batch 209 batch loss 0.634256244 epoch total loss 0.557091832\n",
      "Trained batch 210 batch loss 0.640928924 epoch total loss 0.557491064\n",
      "Trained batch 211 batch loss 0.583499789 epoch total loss 0.557614267\n",
      "Trained batch 212 batch loss 0.568207145 epoch total loss 0.557664275\n",
      "Trained batch 213 batch loss 0.551967442 epoch total loss 0.557637513\n",
      "Trained batch 214 batch loss 0.448328704 epoch total loss 0.557126701\n",
      "Trained batch 215 batch loss 0.454567343 epoch total loss 0.556649685\n",
      "Trained batch 216 batch loss 0.414565742 epoch total loss 0.555991888\n",
      "Trained batch 217 batch loss 0.472282767 epoch total loss 0.555606127\n",
      "Trained batch 218 batch loss 0.491628081 epoch total loss 0.555312634\n",
      "Trained batch 219 batch loss 0.464453906 epoch total loss 0.554897785\n",
      "Trained batch 220 batch loss 0.524611831 epoch total loss 0.554760098\n",
      "Trained batch 221 batch loss 0.508205295 epoch total loss 0.554549456\n",
      "Trained batch 222 batch loss 0.51605159 epoch total loss 0.554376\n",
      "Trained batch 223 batch loss 0.508407712 epoch total loss 0.554169893\n",
      "Trained batch 224 batch loss 0.532980263 epoch total loss 0.554075301\n",
      "Trained batch 225 batch loss 0.542355895 epoch total loss 0.554023206\n",
      "Trained batch 226 batch loss 0.615611851 epoch total loss 0.554295719\n",
      "Trained batch 227 batch loss 0.508505344 epoch total loss 0.554094\n",
      "Trained batch 228 batch loss 0.609617233 epoch total loss 0.554337561\n",
      "Trained batch 229 batch loss 0.527416408 epoch total loss 0.55422\n",
      "Trained batch 230 batch loss 0.678948343 epoch total loss 0.554762304\n",
      "Trained batch 231 batch loss 0.619746268 epoch total loss 0.555043578\n",
      "Trained batch 232 batch loss 0.559086442 epoch total loss 0.555061\n",
      "Trained batch 233 batch loss 0.633463621 epoch total loss 0.555397511\n",
      "Trained batch 234 batch loss 0.587072492 epoch total loss 0.555532873\n",
      "Trained batch 235 batch loss 0.437173069 epoch total loss 0.555029213\n",
      "Trained batch 236 batch loss 0.645702541 epoch total loss 0.555413485\n",
      "Trained batch 237 batch loss 0.60819459 epoch total loss 0.555636168\n",
      "Trained batch 238 batch loss 0.505804479 epoch total loss 0.555426776\n",
      "Trained batch 239 batch loss 0.532670319 epoch total loss 0.555331588\n",
      "Trained batch 240 batch loss 0.533982635 epoch total loss 0.555242598\n",
      "Trained batch 241 batch loss 0.536304712 epoch total loss 0.555164\n",
      "Trained batch 242 batch loss 0.574167132 epoch total loss 0.555242538\n",
      "Trained batch 243 batch loss 0.524139822 epoch total loss 0.555114567\n",
      "Trained batch 244 batch loss 0.578591704 epoch total loss 0.555210829\n",
      "Trained batch 245 batch loss 0.634099782 epoch total loss 0.555532753\n",
      "Trained batch 246 batch loss 0.606661499 epoch total loss 0.555740595\n",
      "Trained batch 247 batch loss 0.639073372 epoch total loss 0.556077957\n",
      "Trained batch 248 batch loss 0.588029742 epoch total loss 0.556206822\n",
      "Trained batch 249 batch loss 0.492652237 epoch total loss 0.555951536\n",
      "Trained batch 250 batch loss 0.563006878 epoch total loss 0.555979729\n",
      "Trained batch 251 batch loss 0.565831125 epoch total loss 0.556018949\n",
      "Trained batch 252 batch loss 0.594862342 epoch total loss 0.556173086\n",
      "Trained batch 253 batch loss 0.589770675 epoch total loss 0.556305885\n",
      "Trained batch 254 batch loss 0.625013173 epoch total loss 0.556576431\n",
      "Trained batch 255 batch loss 0.647896886 epoch total loss 0.556934536\n",
      "Trained batch 256 batch loss 0.63242352 epoch total loss 0.557229459\n",
      "Trained batch 257 batch loss 0.522182822 epoch total loss 0.557093084\n",
      "Trained batch 258 batch loss 0.508028209 epoch total loss 0.556902945\n",
      "Trained batch 259 batch loss 0.495778322 epoch total loss 0.556666911\n",
      "Trained batch 260 batch loss 0.506693244 epoch total loss 0.556474686\n",
      "Trained batch 261 batch loss 0.630947053 epoch total loss 0.556760073\n",
      "Trained batch 262 batch loss 0.527329385 epoch total loss 0.556647718\n",
      "Trained batch 263 batch loss 0.550388455 epoch total loss 0.556623936\n",
      "Trained batch 264 batch loss 0.544561148 epoch total loss 0.556578219\n",
      "Trained batch 265 batch loss 0.534689546 epoch total loss 0.556495607\n",
      "Trained batch 266 batch loss 0.54775393 epoch total loss 0.556462765\n",
      "Trained batch 267 batch loss 0.58616972 epoch total loss 0.556574\n",
      "Trained batch 268 batch loss 0.505362928 epoch total loss 0.556382895\n",
      "Trained batch 269 batch loss 0.55005 epoch total loss 0.556359351\n",
      "Trained batch 270 batch loss 0.571873188 epoch total loss 0.55641675\n",
      "Trained batch 271 batch loss 0.444954485 epoch total loss 0.556005478\n",
      "Trained batch 272 batch loss 0.465289 epoch total loss 0.555672\n",
      "Trained batch 273 batch loss 0.513641298 epoch total loss 0.555518031\n",
      "Trained batch 274 batch loss 0.565622091 epoch total loss 0.555554926\n",
      "Trained batch 275 batch loss 0.449791849 epoch total loss 0.555170357\n",
      "Trained batch 276 batch loss 0.512981474 epoch total loss 0.555017471\n",
      "Trained batch 277 batch loss 0.502601147 epoch total loss 0.554828227\n",
      "Trained batch 278 batch loss 0.507016361 epoch total loss 0.554656267\n",
      "Trained batch 279 batch loss 0.49853158 epoch total loss 0.554455101\n",
      "Trained batch 280 batch loss 0.472261578 epoch total loss 0.554161549\n",
      "Trained batch 281 batch loss 0.471920133 epoch total loss 0.55386889\n",
      "Trained batch 282 batch loss 0.491211861 epoch total loss 0.553646684\n",
      "Trained batch 283 batch loss 0.49799943 epoch total loss 0.553450108\n",
      "Trained batch 284 batch loss 0.456923097 epoch total loss 0.553110182\n",
      "Trained batch 285 batch loss 0.459363103 epoch total loss 0.552781284\n",
      "Trained batch 286 batch loss 0.469182521 epoch total loss 0.552489\n",
      "Trained batch 287 batch loss 0.56925261 epoch total loss 0.552547395\n",
      "Trained batch 288 batch loss 0.623740196 epoch total loss 0.552794576\n",
      "Trained batch 289 batch loss 0.549103498 epoch total loss 0.552781761\n",
      "Trained batch 290 batch loss 0.663416564 epoch total loss 0.55316329\n",
      "Trained batch 291 batch loss 0.459926307 epoch total loss 0.552842915\n",
      "Trained batch 292 batch loss 0.525777757 epoch total loss 0.55275023\n",
      "Trained batch 293 batch loss 0.551472127 epoch total loss 0.552745819\n",
      "Trained batch 294 batch loss 0.641827 epoch total loss 0.553048849\n",
      "Trained batch 295 batch loss 0.540660381 epoch total loss 0.553006887\n",
      "Trained batch 296 batch loss 0.457692713 epoch total loss 0.552684844\n",
      "Trained batch 297 batch loss 0.514292717 epoch total loss 0.552555561\n",
      "Trained batch 298 batch loss 0.492228299 epoch total loss 0.552353144\n",
      "Trained batch 299 batch loss 0.461998075 epoch total loss 0.552051\n",
      "Trained batch 300 batch loss 0.487169504 epoch total loss 0.551834702\n",
      "Trained batch 301 batch loss 0.54382515 epoch total loss 0.551808119\n",
      "Trained batch 302 batch loss 0.705882609 epoch total loss 0.552318275\n",
      "Trained batch 303 batch loss 0.492553651 epoch total loss 0.552121043\n",
      "Trained batch 304 batch loss 0.668009102 epoch total loss 0.552502275\n",
      "Trained batch 305 batch loss 0.65251565 epoch total loss 0.55283016\n",
      "Trained batch 306 batch loss 0.620355248 epoch total loss 0.553050876\n",
      "Trained batch 307 batch loss 0.563697815 epoch total loss 0.553085566\n",
      "Trained batch 308 batch loss 0.506345272 epoch total loss 0.552933812\n",
      "Trained batch 309 batch loss 0.613124609 epoch total loss 0.55312866\n",
      "Trained batch 310 batch loss 0.530258417 epoch total loss 0.553054869\n",
      "Trained batch 311 batch loss 0.577959061 epoch total loss 0.553134918\n",
      "Trained batch 312 batch loss 0.568545938 epoch total loss 0.55318433\n",
      "Trained batch 313 batch loss 0.669796646 epoch total loss 0.553556859\n",
      "Trained batch 314 batch loss 0.514695466 epoch total loss 0.55343312\n",
      "Trained batch 315 batch loss 0.485073984 epoch total loss 0.5532161\n",
      "Trained batch 316 batch loss 0.540108 epoch total loss 0.553174675\n",
      "Trained batch 317 batch loss 0.629933774 epoch total loss 0.553416789\n",
      "Trained batch 318 batch loss 0.572290659 epoch total loss 0.553476155\n",
      "Trained batch 319 batch loss 0.62473315 epoch total loss 0.553699553\n",
      "Trained batch 320 batch loss 0.603377819 epoch total loss 0.553854823\n",
      "Trained batch 321 batch loss 0.648749948 epoch total loss 0.554150403\n",
      "Trained batch 322 batch loss 0.569175959 epoch total loss 0.554197073\n",
      "Trained batch 323 batch loss 0.636345 epoch total loss 0.554451466\n",
      "Trained batch 324 batch loss 0.692747951 epoch total loss 0.554878294\n",
      "Trained batch 325 batch loss 0.622738063 epoch total loss 0.55508709\n",
      "Trained batch 326 batch loss 0.592859864 epoch total loss 0.555202961\n",
      "Trained batch 327 batch loss 0.624890625 epoch total loss 0.555416107\n",
      "Trained batch 328 batch loss 0.581906915 epoch total loss 0.555496871\n",
      "Trained batch 329 batch loss 0.58864814 epoch total loss 0.555597663\n",
      "Trained batch 330 batch loss 0.545967758 epoch total loss 0.555568516\n",
      "Trained batch 331 batch loss 0.543697655 epoch total loss 0.555532634\n",
      "Trained batch 332 batch loss 0.514234185 epoch total loss 0.555408239\n",
      "Trained batch 333 batch loss 0.586102366 epoch total loss 0.555500448\n",
      "Trained batch 334 batch loss 0.553506255 epoch total loss 0.555494487\n",
      "Trained batch 335 batch loss 0.549512327 epoch total loss 0.555476606\n",
      "Trained batch 336 batch loss 0.53838563 epoch total loss 0.555425763\n",
      "Trained batch 337 batch loss 0.630585074 epoch total loss 0.555648804\n",
      "Trained batch 338 batch loss 0.558006704 epoch total loss 0.555655777\n",
      "Trained batch 339 batch loss 0.59057647 epoch total loss 0.555758834\n",
      "Trained batch 340 batch loss 0.531017542 epoch total loss 0.555686057\n",
      "Trained batch 341 batch loss 0.600599647 epoch total loss 0.555817783\n",
      "Trained batch 342 batch loss 0.584014952 epoch total loss 0.555900216\n",
      "Trained batch 343 batch loss 0.582999945 epoch total loss 0.555979192\n",
      "Trained batch 344 batch loss 0.560086489 epoch total loss 0.555991173\n",
      "Trained batch 345 batch loss 0.573395073 epoch total loss 0.556041598\n",
      "Trained batch 346 batch loss 0.554420054 epoch total loss 0.55603689\n",
      "Trained batch 347 batch loss 0.597236395 epoch total loss 0.556155622\n",
      "Trained batch 348 batch loss 0.559036314 epoch total loss 0.556163847\n",
      "Trained batch 349 batch loss 0.629512429 epoch total loss 0.556374073\n",
      "Trained batch 350 batch loss 0.554596186 epoch total loss 0.556368947\n",
      "Trained batch 351 batch loss 0.564505279 epoch total loss 0.556392133\n",
      "Trained batch 352 batch loss 0.551924825 epoch total loss 0.556379437\n",
      "Trained batch 353 batch loss 0.558312297 epoch total loss 0.556384921\n",
      "Trained batch 354 batch loss 0.490680546 epoch total loss 0.556199312\n",
      "Trained batch 355 batch loss 0.495862275 epoch total loss 0.556029379\n",
      "Trained batch 356 batch loss 0.451187402 epoch total loss 0.555734873\n",
      "Trained batch 357 batch loss 0.498102903 epoch total loss 0.555573463\n",
      "Trained batch 358 batch loss 0.553569794 epoch total loss 0.555567861\n",
      "Trained batch 359 batch loss 0.605428755 epoch total loss 0.555706739\n",
      "Trained batch 360 batch loss 0.503232658 epoch total loss 0.555561\n",
      "Trained batch 361 batch loss 0.569971204 epoch total loss 0.555600941\n",
      "Trained batch 362 batch loss 0.607833266 epoch total loss 0.555745184\n",
      "Trained batch 363 batch loss 0.627039671 epoch total loss 0.555941641\n",
      "Trained batch 364 batch loss 0.536065817 epoch total loss 0.555887043\n",
      "Trained batch 365 batch loss 0.594629884 epoch total loss 0.555993199\n",
      "Trained batch 366 batch loss 0.567256689 epoch total loss 0.556023955\n",
      "Trained batch 367 batch loss 0.461261034 epoch total loss 0.555765748\n",
      "Trained batch 368 batch loss 0.501801431 epoch total loss 0.555619121\n",
      "Trained batch 369 batch loss 0.520501316 epoch total loss 0.555524\n",
      "Trained batch 370 batch loss 0.592938542 epoch total loss 0.555625081\n",
      "Trained batch 371 batch loss 0.627173245 epoch total loss 0.555817902\n",
      "Trained batch 372 batch loss 0.563427567 epoch total loss 0.555838406\n",
      "Trained batch 373 batch loss 0.545645654 epoch total loss 0.555811048\n",
      "Trained batch 374 batch loss 0.586348891 epoch total loss 0.555892706\n",
      "Trained batch 375 batch loss 0.553955674 epoch total loss 0.55588752\n",
      "Trained batch 376 batch loss 0.59656173 epoch total loss 0.555995703\n",
      "Trained batch 377 batch loss 0.553122163 epoch total loss 0.555988073\n",
      "Trained batch 378 batch loss 0.603943825 epoch total loss 0.556114912\n",
      "Trained batch 379 batch loss 0.541085303 epoch total loss 0.556075275\n",
      "Trained batch 380 batch loss 0.507642269 epoch total loss 0.55594784\n",
      "Trained batch 381 batch loss 0.578331232 epoch total loss 0.55600661\n",
      "Trained batch 382 batch loss 0.468235433 epoch total loss 0.555776834\n",
      "Trained batch 383 batch loss 0.511769116 epoch total loss 0.555661917\n",
      "Trained batch 384 batch loss 0.543261826 epoch total loss 0.555629611\n",
      "Trained batch 385 batch loss 0.488577873 epoch total loss 0.555455446\n",
      "Trained batch 386 batch loss 0.608451188 epoch total loss 0.555592716\n",
      "Trained batch 387 batch loss 0.512609124 epoch total loss 0.555481613\n",
      "Trained batch 388 batch loss 0.640379488 epoch total loss 0.555700421\n",
      "Trained batch 389 batch loss 0.575633824 epoch total loss 0.555751681\n",
      "Trained batch 390 batch loss 0.546337664 epoch total loss 0.555727541\n",
      "Trained batch 391 batch loss 0.502092659 epoch total loss 0.555590391\n",
      "Trained batch 392 batch loss 0.550317764 epoch total loss 0.555576921\n",
      "Trained batch 393 batch loss 0.546278059 epoch total loss 0.555553317\n",
      "Trained batch 394 batch loss 0.445726573 epoch total loss 0.555274546\n",
      "Trained batch 395 batch loss 0.543678 epoch total loss 0.555245161\n",
      "Trained batch 396 batch loss 0.58633 epoch total loss 0.55532366\n",
      "Trained batch 397 batch loss 0.512721121 epoch total loss 0.555216372\n",
      "Trained batch 398 batch loss 0.46119523 epoch total loss 0.554980159\n",
      "Trained batch 399 batch loss 0.503224969 epoch total loss 0.554850399\n",
      "Trained batch 400 batch loss 0.489428908 epoch total loss 0.554686844\n",
      "Trained batch 401 batch loss 0.497088343 epoch total loss 0.554543197\n",
      "Trained batch 402 batch loss 0.624845386 epoch total loss 0.554718077\n",
      "Trained batch 403 batch loss 0.523843884 epoch total loss 0.554641485\n",
      "Trained batch 404 batch loss 0.484347701 epoch total loss 0.554467499\n",
      "Trained batch 405 batch loss 0.433937728 epoch total loss 0.554169893\n",
      "Trained batch 406 batch loss 0.44384706 epoch total loss 0.553898156\n",
      "Trained batch 407 batch loss 0.503733873 epoch total loss 0.553774953\n",
      "Trained batch 408 batch loss 0.547510386 epoch total loss 0.553759575\n",
      "Trained batch 409 batch loss 0.538103282 epoch total loss 0.553721309\n",
      "Trained batch 410 batch loss 0.485817671 epoch total loss 0.553555727\n",
      "Trained batch 411 batch loss 0.510472655 epoch total loss 0.553450882\n",
      "Trained batch 412 batch loss 0.532337666 epoch total loss 0.553399622\n",
      "Trained batch 413 batch loss 0.442635715 epoch total loss 0.553131461\n",
      "Trained batch 414 batch loss 0.546393752 epoch total loss 0.553115129\n",
      "Trained batch 415 batch loss 0.471306413 epoch total loss 0.552918\n",
      "Trained batch 416 batch loss 0.514249265 epoch total loss 0.552825093\n",
      "Trained batch 417 batch loss 0.581997 epoch total loss 0.552895069\n",
      "Trained batch 418 batch loss 0.470537841 epoch total loss 0.552698\n",
      "Trained batch 419 batch loss 0.56591171 epoch total loss 0.552729547\n",
      "Trained batch 420 batch loss 0.599598765 epoch total loss 0.552841127\n",
      "Trained batch 421 batch loss 0.567428887 epoch total loss 0.552875817\n",
      "Trained batch 422 batch loss 0.511573672 epoch total loss 0.552777886\n",
      "Trained batch 423 batch loss 0.562996149 epoch total loss 0.552802086\n",
      "Trained batch 424 batch loss 0.542050123 epoch total loss 0.552776754\n",
      "Trained batch 425 batch loss 0.626901448 epoch total loss 0.552951157\n",
      "Trained batch 426 batch loss 0.573889613 epoch total loss 0.553000271\n",
      "Trained batch 427 batch loss 0.549352825 epoch total loss 0.552991748\n",
      "Trained batch 428 batch loss 0.577848077 epoch total loss 0.553049803\n",
      "Trained batch 429 batch loss 0.601567626 epoch total loss 0.553162873\n",
      "Trained batch 430 batch loss 0.597852707 epoch total loss 0.553266823\n",
      "Trained batch 431 batch loss 0.554744661 epoch total loss 0.55327028\n",
      "Trained batch 432 batch loss 0.565716922 epoch total loss 0.553299069\n",
      "Trained batch 433 batch loss 0.561603606 epoch total loss 0.553318262\n",
      "Trained batch 434 batch loss 0.545650661 epoch total loss 0.553300619\n",
      "Trained batch 435 batch loss 0.584571719 epoch total loss 0.553372502\n",
      "Trained batch 436 batch loss 0.585419238 epoch total loss 0.553446\n",
      "Trained batch 437 batch loss 0.569651 epoch total loss 0.553483069\n",
      "Trained batch 438 batch loss 0.521345854 epoch total loss 0.553409696\n",
      "Trained batch 439 batch loss 0.612543762 epoch total loss 0.553544402\n",
      "Trained batch 440 batch loss 0.567112327 epoch total loss 0.553575218\n",
      "Trained batch 441 batch loss 0.550433 epoch total loss 0.553568125\n",
      "Trained batch 442 batch loss 0.59563452 epoch total loss 0.553663313\n",
      "Trained batch 443 batch loss 0.572997868 epoch total loss 0.553706944\n",
      "Trained batch 444 batch loss 0.585797906 epoch total loss 0.553779244\n",
      "Trained batch 445 batch loss 0.603957653 epoch total loss 0.553891957\n",
      "Trained batch 446 batch loss 0.568827689 epoch total loss 0.553925455\n",
      "Trained batch 447 batch loss 0.514349401 epoch total loss 0.553836942\n",
      "Trained batch 448 batch loss 0.436218321 epoch total loss 0.553574383\n",
      "Trained batch 449 batch loss 0.499223441 epoch total loss 0.553453326\n",
      "Trained batch 450 batch loss 0.596537173 epoch total loss 0.553549111\n",
      "Trained batch 451 batch loss 0.445123971 epoch total loss 0.553308666\n",
      "Trained batch 452 batch loss 0.467773497 epoch total loss 0.553119481\n",
      "Trained batch 453 batch loss 0.456219643 epoch total loss 0.55290556\n",
      "Trained batch 454 batch loss 0.491401523 epoch total loss 0.552770078\n",
      "Trained batch 455 batch loss 0.475852162 epoch total loss 0.552601\n",
      "Trained batch 456 batch loss 0.481701225 epoch total loss 0.552445531\n",
      "Trained batch 457 batch loss 0.553709507 epoch total loss 0.552448273\n",
      "Trained batch 458 batch loss 0.489618927 epoch total loss 0.552311122\n",
      "Trained batch 459 batch loss 0.503239155 epoch total loss 0.552204192\n",
      "Trained batch 460 batch loss 0.580271125 epoch total loss 0.552265227\n",
      "Trained batch 461 batch loss 0.511083186 epoch total loss 0.552175879\n",
      "Trained batch 462 batch loss 0.453585058 epoch total loss 0.551962495\n",
      "Trained batch 463 batch loss 0.581077456 epoch total loss 0.552025378\n",
      "Trained batch 464 batch loss 0.515500546 epoch total loss 0.55194664\n",
      "Trained batch 465 batch loss 0.578130364 epoch total loss 0.552002966\n",
      "Trained batch 466 batch loss 0.48864311 epoch total loss 0.551867\n",
      "Trained batch 467 batch loss 0.420285076 epoch total loss 0.551585257\n",
      "Trained batch 468 batch loss 0.481237829 epoch total loss 0.551434875\n",
      "Trained batch 469 batch loss 0.545185566 epoch total loss 0.551421583\n",
      "Trained batch 470 batch loss 0.578214228 epoch total loss 0.551478624\n",
      "Trained batch 471 batch loss 0.614300728 epoch total loss 0.55161196\n",
      "Trained batch 472 batch loss 0.608656406 epoch total loss 0.551732779\n",
      "Trained batch 473 batch loss 0.622971475 epoch total loss 0.551883459\n",
      "Trained batch 474 batch loss 0.581282616 epoch total loss 0.551945448\n",
      "Trained batch 475 batch loss 0.594992 epoch total loss 0.552036047\n",
      "Trained batch 476 batch loss 0.514084 epoch total loss 0.551956356\n",
      "Trained batch 477 batch loss 0.509932041 epoch total loss 0.55186826\n",
      "Trained batch 478 batch loss 0.53012681 epoch total loss 0.551822722\n",
      "Trained batch 479 batch loss 0.602033257 epoch total loss 0.551927567\n",
      "Trained batch 480 batch loss 0.592094362 epoch total loss 0.552011251\n",
      "Trained batch 481 batch loss 0.452476054 epoch total loss 0.551804304\n",
      "Trained batch 482 batch loss 0.585605264 epoch total loss 0.551874459\n",
      "Trained batch 483 batch loss 0.4719567 epoch total loss 0.551709\n",
      "Trained batch 484 batch loss 0.571009 epoch total loss 0.551748872\n",
      "Trained batch 485 batch loss 0.56228441 epoch total loss 0.551770568\n",
      "Trained batch 486 batch loss 0.702359498 epoch total loss 0.552080452\n",
      "Trained batch 487 batch loss 0.544123113 epoch total loss 0.552064121\n",
      "Trained batch 488 batch loss 0.428916216 epoch total loss 0.551811755\n",
      "Trained batch 489 batch loss 0.442437142 epoch total loss 0.551588118\n",
      "Trained batch 490 batch loss 0.657562554 epoch total loss 0.551804423\n",
      "Trained batch 491 batch loss 0.555071831 epoch total loss 0.551811099\n",
      "Trained batch 492 batch loss 0.591312289 epoch total loss 0.551891387\n",
      "Trained batch 493 batch loss 0.613013387 epoch total loss 0.552015305\n",
      "Trained batch 494 batch loss 0.66993922 epoch total loss 0.552254081\n",
      "Trained batch 495 batch loss 0.633704841 epoch total loss 0.55241859\n",
      "Trained batch 496 batch loss 0.430096567 epoch total loss 0.552171946\n",
      "Trained batch 497 batch loss 0.373996705 epoch total loss 0.551813424\n",
      "Trained batch 498 batch loss 0.442903876 epoch total loss 0.551594734\n",
      "Trained batch 499 batch loss 0.423976898 epoch total loss 0.55133903\n",
      "Trained batch 500 batch loss 0.439253479 epoch total loss 0.551114798\n",
      "Trained batch 501 batch loss 0.51213479 epoch total loss 0.551037\n",
      "Trained batch 502 batch loss 0.414283425 epoch total loss 0.55076462\n",
      "Trained batch 503 batch loss 0.439028054 epoch total loss 0.550542474\n",
      "Trained batch 504 batch loss 0.473529 epoch total loss 0.550389647\n",
      "Trained batch 505 batch loss 0.491803378 epoch total loss 0.550273657\n",
      "Trained batch 506 batch loss 0.475787342 epoch total loss 0.550126433\n",
      "Trained batch 507 batch loss 0.452469915 epoch total loss 0.549933851\n",
      "Trained batch 508 batch loss 0.462498486 epoch total loss 0.549761713\n",
      "Trained batch 509 batch loss 0.50288105 epoch total loss 0.549669623\n",
      "Trained batch 510 batch loss 0.482846737 epoch total loss 0.549538612\n",
      "Trained batch 511 batch loss 0.552933753 epoch total loss 0.549545228\n",
      "Trained batch 512 batch loss 0.528069675 epoch total loss 0.549503326\n",
      "Trained batch 513 batch loss 0.506661475 epoch total loss 0.54941982\n",
      "Trained batch 514 batch loss 0.542788804 epoch total loss 0.549406886\n",
      "Trained batch 515 batch loss 0.52796 epoch total loss 0.549365222\n",
      "Trained batch 516 batch loss 0.602796376 epoch total loss 0.549468756\n",
      "Trained batch 517 batch loss 0.528978884 epoch total loss 0.549429178\n",
      "Trained batch 518 batch loss 0.521937549 epoch total loss 0.54937607\n",
      "Trained batch 519 batch loss 0.564181328 epoch total loss 0.549404621\n",
      "Trained batch 520 batch loss 0.569375873 epoch total loss 0.549443\n",
      "Trained batch 521 batch loss 0.530666828 epoch total loss 0.549406946\n",
      "Trained batch 522 batch loss 0.503740847 epoch total loss 0.549319506\n",
      "Trained batch 523 batch loss 0.51074481 epoch total loss 0.549245715\n",
      "Trained batch 524 batch loss 0.550705075 epoch total loss 0.549248576\n",
      "Trained batch 525 batch loss 0.522142112 epoch total loss 0.549196959\n",
      "Trained batch 526 batch loss 0.547441304 epoch total loss 0.549193621\n",
      "Trained batch 527 batch loss 0.562415 epoch total loss 0.549218714\n",
      "Trained batch 528 batch loss 0.536953926 epoch total loss 0.549195468\n",
      "Trained batch 529 batch loss 0.548898518 epoch total loss 0.549194932\n",
      "Trained batch 530 batch loss 0.533085585 epoch total loss 0.549164534\n",
      "Trained batch 531 batch loss 0.551492691 epoch total loss 0.549168885\n",
      "Trained batch 532 batch loss 0.58393681 epoch total loss 0.549234211\n",
      "Trained batch 533 batch loss 0.57140708 epoch total loss 0.549275815\n",
      "Trained batch 534 batch loss 0.575747609 epoch total loss 0.549325407\n",
      "Trained batch 535 batch loss 0.698960185 epoch total loss 0.549605072\n",
      "Trained batch 536 batch loss 0.601402938 epoch total loss 0.54970175\n",
      "Trained batch 537 batch loss 0.654942 epoch total loss 0.54989773\n",
      "Trained batch 538 batch loss 0.648967326 epoch total loss 0.550081849\n",
      "Trained batch 539 batch loss 0.655899286 epoch total loss 0.550278187\n",
      "Trained batch 540 batch loss 0.65186584 epoch total loss 0.550466299\n",
      "Trained batch 541 batch loss 0.666962147 epoch total loss 0.550681651\n",
      "Trained batch 542 batch loss 0.547433615 epoch total loss 0.550675631\n",
      "Trained batch 543 batch loss 0.507680476 epoch total loss 0.550596476\n",
      "Trained batch 544 batch loss 0.50336349 epoch total loss 0.550509632\n",
      "Trained batch 545 batch loss 0.482314795 epoch total loss 0.550384462\n",
      "Trained batch 546 batch loss 0.595416427 epoch total loss 0.550466955\n",
      "Trained batch 547 batch loss 0.629406929 epoch total loss 0.550611258\n",
      "Trained batch 548 batch loss 0.639975548 epoch total loss 0.550774336\n",
      "Trained batch 549 batch loss 0.646083891 epoch total loss 0.550947964\n",
      "Trained batch 550 batch loss 0.502459288 epoch total loss 0.550859809\n",
      "Trained batch 551 batch loss 0.554594159 epoch total loss 0.550866604\n",
      "Trained batch 552 batch loss 0.534510612 epoch total loss 0.550837\n",
      "Trained batch 553 batch loss 0.511565268 epoch total loss 0.550766\n",
      "Trained batch 554 batch loss 0.495743603 epoch total loss 0.55066669\n",
      "Trained batch 555 batch loss 0.523396134 epoch total loss 0.550617576\n",
      "Trained batch 556 batch loss 0.501769543 epoch total loss 0.550529718\n",
      "Trained batch 557 batch loss 0.528336406 epoch total loss 0.550489902\n",
      "Trained batch 558 batch loss 0.490736842 epoch total loss 0.550382793\n",
      "Trained batch 559 batch loss 0.484387487 epoch total loss 0.550264716\n",
      "Trained batch 560 batch loss 0.54148078 epoch total loss 0.550249\n",
      "Trained batch 561 batch loss 0.526001811 epoch total loss 0.550205767\n",
      "Trained batch 562 batch loss 0.527696908 epoch total loss 0.550165713\n",
      "Trained batch 563 batch loss 0.568573713 epoch total loss 0.550198436\n",
      "Trained batch 564 batch loss 0.551064193 epoch total loss 0.550199926\n",
      "Trained batch 565 batch loss 0.524106562 epoch total loss 0.550153792\n",
      "Trained batch 566 batch loss 0.55350244 epoch total loss 0.550159693\n",
      "Trained batch 567 batch loss 0.539376199 epoch total loss 0.550140679\n",
      "Trained batch 568 batch loss 0.502128065 epoch total loss 0.550056159\n",
      "Trained batch 569 batch loss 0.458901703 epoch total loss 0.549895942\n",
      "Trained batch 570 batch loss 0.505654097 epoch total loss 0.549818277\n",
      "Trained batch 571 batch loss 0.582811236 epoch total loss 0.549876094\n",
      "Trained batch 572 batch loss 0.58845973 epoch total loss 0.549943566\n",
      "Trained batch 573 batch loss 0.601388097 epoch total loss 0.550033331\n",
      "Trained batch 574 batch loss 0.557356477 epoch total loss 0.550046086\n",
      "Trained batch 575 batch loss 0.530333579 epoch total loss 0.550011754\n",
      "Trained batch 576 batch loss 0.468641281 epoch total loss 0.549870491\n",
      "Trained batch 577 batch loss 0.500861287 epoch total loss 0.549785554\n",
      "Trained batch 578 batch loss 0.485062242 epoch total loss 0.549673617\n",
      "Trained batch 579 batch loss 0.536243796 epoch total loss 0.549650431\n",
      "Trained batch 580 batch loss 0.5141626 epoch total loss 0.549589217\n",
      "Trained batch 581 batch loss 0.554541826 epoch total loss 0.54959774\n",
      "Trained batch 582 batch loss 0.493759811 epoch total loss 0.549501836\n",
      "Trained batch 583 batch loss 0.57055825 epoch total loss 0.549537957\n",
      "Trained batch 584 batch loss 0.643078148 epoch total loss 0.549698114\n",
      "Trained batch 585 batch loss 0.584720075 epoch total loss 0.549757957\n",
      "Trained batch 586 batch loss 0.5227983 epoch total loss 0.549711943\n",
      "Trained batch 587 batch loss 0.520341 epoch total loss 0.549661934\n",
      "Trained batch 588 batch loss 0.455205351 epoch total loss 0.5495013\n",
      "Trained batch 589 batch loss 0.46923 epoch total loss 0.549365\n",
      "Trained batch 590 batch loss 0.572610736 epoch total loss 0.549404383\n",
      "Trained batch 591 batch loss 0.520048499 epoch total loss 0.549354732\n",
      "Trained batch 592 batch loss 0.513383 epoch total loss 0.549294\n",
      "Trained batch 593 batch loss 0.572695136 epoch total loss 0.549333453\n",
      "Trained batch 594 batch loss 0.555850863 epoch total loss 0.54934442\n",
      "Trained batch 595 batch loss 0.513494492 epoch total loss 0.54928416\n",
      "Trained batch 596 batch loss 0.535159111 epoch total loss 0.549260437\n",
      "Trained batch 597 batch loss 0.569565 epoch total loss 0.549294472\n",
      "Trained batch 598 batch loss 0.618809044 epoch total loss 0.549410701\n",
      "Trained batch 599 batch loss 0.532833517 epoch total loss 0.549383044\n",
      "Trained batch 600 batch loss 0.467193305 epoch total loss 0.549246073\n",
      "Trained batch 601 batch loss 0.517109513 epoch total loss 0.549192607\n",
      "Trained batch 602 batch loss 0.435900033 epoch total loss 0.549004436\n",
      "Trained batch 603 batch loss 0.47394675 epoch total loss 0.548879921\n",
      "Trained batch 604 batch loss 0.487225503 epoch total loss 0.548777878\n",
      "Trained batch 605 batch loss 0.552916706 epoch total loss 0.548784673\n",
      "Trained batch 606 batch loss 0.521959841 epoch total loss 0.548740447\n",
      "Trained batch 607 batch loss 0.46932593 epoch total loss 0.548609614\n",
      "Trained batch 608 batch loss 0.494640112 epoch total loss 0.548520863\n",
      "Trained batch 609 batch loss 0.494010985 epoch total loss 0.548431337\n",
      "Trained batch 610 batch loss 0.498989016 epoch total loss 0.548350275\n",
      "Trained batch 611 batch loss 0.511373222 epoch total loss 0.548289776\n",
      "Trained batch 612 batch loss 0.489995718 epoch total loss 0.548194528\n",
      "Trained batch 613 batch loss 0.456360847 epoch total loss 0.548044741\n",
      "Trained batch 614 batch loss 0.47044751 epoch total loss 0.547918379\n",
      "Trained batch 615 batch loss 0.533735871 epoch total loss 0.547895253\n",
      "Trained batch 616 batch loss 0.430454373 epoch total loss 0.547704637\n",
      "Trained batch 617 batch loss 0.482317477 epoch total loss 0.54759866\n",
      "Trained batch 618 batch loss 0.472447067 epoch total loss 0.547477067\n",
      "Trained batch 619 batch loss 0.462469399 epoch total loss 0.547339737\n",
      "Trained batch 620 batch loss 0.459696501 epoch total loss 0.547198355\n",
      "Trained batch 621 batch loss 0.430433303 epoch total loss 0.547010303\n",
      "Trained batch 622 batch loss 0.505638897 epoch total loss 0.546943784\n",
      "Trained batch 623 batch loss 0.480447292 epoch total loss 0.546837032\n",
      "Trained batch 624 batch loss 0.533910096 epoch total loss 0.546816289\n",
      "Trained batch 625 batch loss 0.454778403 epoch total loss 0.546669066\n",
      "Trained batch 626 batch loss 0.48471728 epoch total loss 0.546570063\n",
      "Trained batch 627 batch loss 0.420851588 epoch total loss 0.546369553\n",
      "Trained batch 628 batch loss 0.507410884 epoch total loss 0.546307504\n",
      "Trained batch 629 batch loss 0.467108727 epoch total loss 0.54618156\n",
      "Trained batch 630 batch loss 0.530550718 epoch total loss 0.546156764\n",
      "Trained batch 631 batch loss 0.499921024 epoch total loss 0.54608345\n",
      "Trained batch 632 batch loss 0.494304836 epoch total loss 0.546001554\n",
      "Trained batch 633 batch loss 0.49882552 epoch total loss 0.545927048\n",
      "Trained batch 634 batch loss 0.621269047 epoch total loss 0.546045899\n",
      "Trained batch 635 batch loss 0.699504077 epoch total loss 0.546287537\n",
      "Trained batch 636 batch loss 0.704604208 epoch total loss 0.546536446\n",
      "Trained batch 637 batch loss 0.645615578 epoch total loss 0.546692\n",
      "Trained batch 638 batch loss 0.511077 epoch total loss 0.546636164\n",
      "Trained batch 639 batch loss 0.487430185 epoch total loss 0.546543539\n",
      "Trained batch 640 batch loss 0.549564838 epoch total loss 0.546548247\n",
      "Trained batch 641 batch loss 0.654655278 epoch total loss 0.546716869\n",
      "Trained batch 642 batch loss 0.581569552 epoch total loss 0.546771169\n",
      "Trained batch 643 batch loss 0.598205864 epoch total loss 0.546851158\n",
      "Trained batch 644 batch loss 0.547976732 epoch total loss 0.546852946\n",
      "Trained batch 645 batch loss 0.495724976 epoch total loss 0.546773672\n",
      "Trained batch 646 batch loss 0.55718565 epoch total loss 0.546789765\n",
      "Trained batch 647 batch loss 0.545466 epoch total loss 0.546787739\n",
      "Trained batch 648 batch loss 0.534092546 epoch total loss 0.546768129\n",
      "Trained batch 649 batch loss 0.55599 epoch total loss 0.546782374\n",
      "Trained batch 650 batch loss 0.434242457 epoch total loss 0.546609223\n",
      "Trained batch 651 batch loss 0.467056334 epoch total loss 0.546487033\n",
      "Trained batch 652 batch loss 0.605072856 epoch total loss 0.546576917\n",
      "Trained batch 653 batch loss 0.541034937 epoch total loss 0.546568394\n",
      "Trained batch 654 batch loss 0.550264299 epoch total loss 0.546574056\n",
      "Trained batch 655 batch loss 0.529749572 epoch total loss 0.546548367\n",
      "Trained batch 656 batch loss 0.482979357 epoch total loss 0.546451449\n",
      "Trained batch 657 batch loss 0.454140663 epoch total loss 0.546310961\n",
      "Trained batch 658 batch loss 0.434685081 epoch total loss 0.546141326\n",
      "Trained batch 659 batch loss 0.519667 epoch total loss 0.546101153\n",
      "Trained batch 660 batch loss 0.530303895 epoch total loss 0.546077192\n",
      "Trained batch 661 batch loss 0.580274165 epoch total loss 0.546128929\n",
      "Trained batch 662 batch loss 0.586333692 epoch total loss 0.546189666\n",
      "Trained batch 663 batch loss 0.517991 epoch total loss 0.546147108\n",
      "Trained batch 664 batch loss 0.581146955 epoch total loss 0.546199858\n",
      "Trained batch 665 batch loss 0.537588 epoch total loss 0.546186924\n",
      "Trained batch 666 batch loss 0.601201773 epoch total loss 0.546269476\n",
      "Trained batch 667 batch loss 0.581977606 epoch total loss 0.546323\n",
      "Trained batch 668 batch loss 0.720347941 epoch total loss 0.546583533\n",
      "Trained batch 669 batch loss 0.612645924 epoch total loss 0.546682298\n",
      "Trained batch 670 batch loss 0.597417355 epoch total loss 0.546758\n",
      "Trained batch 671 batch loss 0.504932642 epoch total loss 0.54669565\n",
      "Trained batch 672 batch loss 0.555285633 epoch total loss 0.546708465\n",
      "Trained batch 673 batch loss 0.582187057 epoch total loss 0.546761155\n",
      "Trained batch 674 batch loss 0.492591262 epoch total loss 0.546680808\n",
      "Trained batch 675 batch loss 0.531252325 epoch total loss 0.54665792\n",
      "Trained batch 676 batch loss 0.548665822 epoch total loss 0.5466609\n",
      "Trained batch 677 batch loss 0.544020534 epoch total loss 0.546657\n",
      "Trained batch 678 batch loss 0.556043148 epoch total loss 0.546670854\n",
      "Trained batch 679 batch loss 0.583632767 epoch total loss 0.546725273\n",
      "Trained batch 680 batch loss 0.565261662 epoch total loss 0.546752453\n",
      "Trained batch 681 batch loss 0.441022694 epoch total loss 0.546597183\n",
      "Trained batch 682 batch loss 0.384616554 epoch total loss 0.546359718\n",
      "Trained batch 683 batch loss 0.459690899 epoch total loss 0.54623282\n",
      "Trained batch 684 batch loss 0.353131175 epoch total loss 0.545950472\n",
      "Trained batch 685 batch loss 0.505315125 epoch total loss 0.545891106\n",
      "Trained batch 686 batch loss 0.569453537 epoch total loss 0.545925498\n",
      "Trained batch 687 batch loss 0.526833117 epoch total loss 0.545897663\n",
      "Trained batch 688 batch loss 0.611278057 epoch total loss 0.545992672\n",
      "Trained batch 689 batch loss 0.601687074 epoch total loss 0.546073496\n",
      "Trained batch 690 batch loss 0.655080616 epoch total loss 0.546231508\n",
      "Trained batch 691 batch loss 0.616027832 epoch total loss 0.546332538\n",
      "Trained batch 692 batch loss 0.622524858 epoch total loss 0.546442628\n",
      "Trained batch 693 batch loss 0.620881796 epoch total loss 0.546550035\n",
      "Trained batch 694 batch loss 0.593574822 epoch total loss 0.546617806\n",
      "Trained batch 695 batch loss 0.581958771 epoch total loss 0.546668649\n",
      "Trained batch 696 batch loss 0.565689802 epoch total loss 0.546696\n",
      "Trained batch 697 batch loss 0.634071231 epoch total loss 0.546821356\n",
      "Trained batch 698 batch loss 0.564424574 epoch total loss 0.546846569\n",
      "Trained batch 699 batch loss 0.521945 epoch total loss 0.546810925\n",
      "Trained batch 700 batch loss 0.567899525 epoch total loss 0.546841085\n",
      "Trained batch 701 batch loss 0.555962086 epoch total loss 0.546854079\n",
      "Trained batch 702 batch loss 0.596752644 epoch total loss 0.546925187\n",
      "Trained batch 703 batch loss 0.586196423 epoch total loss 0.546981037\n",
      "Trained batch 704 batch loss 0.657467186 epoch total loss 0.547138\n",
      "Trained batch 705 batch loss 0.561891079 epoch total loss 0.547158897\n",
      "Trained batch 706 batch loss 0.547631919 epoch total loss 0.547159553\n",
      "Trained batch 707 batch loss 0.514558733 epoch total loss 0.547113419\n",
      "Trained batch 708 batch loss 0.517561197 epoch total loss 0.547071695\n",
      "Trained batch 709 batch loss 0.529990137 epoch total loss 0.547047615\n",
      "Trained batch 710 batch loss 0.584785283 epoch total loss 0.547100723\n",
      "Trained batch 711 batch loss 0.446470141 epoch total loss 0.546959221\n",
      "Trained batch 712 batch loss 0.510342419 epoch total loss 0.546907783\n",
      "Trained batch 713 batch loss 0.563411176 epoch total loss 0.546930969\n",
      "Trained batch 714 batch loss 0.487642586 epoch total loss 0.54684788\n",
      "Trained batch 715 batch loss 0.491281569 epoch total loss 0.546770155\n",
      "Trained batch 716 batch loss 0.508048773 epoch total loss 0.546716094\n",
      "Trained batch 717 batch loss 0.474338472 epoch total loss 0.546615183\n",
      "Trained batch 718 batch loss 0.550392509 epoch total loss 0.546620429\n",
      "Trained batch 719 batch loss 0.548491716 epoch total loss 0.546623\n",
      "Trained batch 720 batch loss 0.635471642 epoch total loss 0.546746373\n",
      "Trained batch 721 batch loss 0.60937953 epoch total loss 0.546833277\n",
      "Trained batch 722 batch loss 0.632848144 epoch total loss 0.546952367\n",
      "Trained batch 723 batch loss 0.723732471 epoch total loss 0.547196865\n",
      "Trained batch 724 batch loss 0.619597256 epoch total loss 0.547296882\n",
      "Trained batch 725 batch loss 0.546115577 epoch total loss 0.547295272\n",
      "Trained batch 726 batch loss 0.588259161 epoch total loss 0.547351658\n",
      "Trained batch 727 batch loss 0.647502661 epoch total loss 0.547489405\n",
      "Trained batch 728 batch loss 0.630531847 epoch total loss 0.547603488\n",
      "Trained batch 729 batch loss 0.474278748 epoch total loss 0.547502875\n",
      "Trained batch 730 batch loss 0.500828207 epoch total loss 0.54743892\n",
      "Trained batch 731 batch loss 0.425530851 epoch total loss 0.547272205\n",
      "Trained batch 732 batch loss 0.478406787 epoch total loss 0.54717809\n",
      "Trained batch 733 batch loss 0.459332764 epoch total loss 0.547058225\n",
      "Trained batch 734 batch loss 0.451532662 epoch total loss 0.546928108\n",
      "Trained batch 735 batch loss 0.411157161 epoch total loss 0.546743393\n",
      "Trained batch 736 batch loss 0.429772139 epoch total loss 0.546584487\n",
      "Trained batch 737 batch loss 0.508696 epoch total loss 0.546533048\n",
      "Trained batch 738 batch loss 0.506469369 epoch total loss 0.546478748\n",
      "Trained batch 739 batch loss 0.507609189 epoch total loss 0.546426177\n",
      "Trained batch 740 batch loss 0.701032817 epoch total loss 0.546635091\n",
      "Trained batch 741 batch loss 0.577881098 epoch total loss 0.546677232\n",
      "Trained batch 742 batch loss 0.395617247 epoch total loss 0.546473682\n",
      "Trained batch 743 batch loss 0.441229343 epoch total loss 0.546332\n",
      "Trained batch 744 batch loss 0.433675408 epoch total loss 0.546180606\n",
      "Trained batch 745 batch loss 0.438300818 epoch total loss 0.546035767\n",
      "Trained batch 746 batch loss 0.431051075 epoch total loss 0.545881689\n",
      "Trained batch 747 batch loss 0.432328224 epoch total loss 0.545729697\n",
      "Trained batch 748 batch loss 0.47787717 epoch total loss 0.545639\n",
      "Trained batch 749 batch loss 0.655666053 epoch total loss 0.545785844\n",
      "Trained batch 750 batch loss 0.591730654 epoch total loss 0.545847118\n",
      "Trained batch 751 batch loss 0.572692037 epoch total loss 0.545882881\n",
      "Trained batch 752 batch loss 0.536924183 epoch total loss 0.54587096\n",
      "Trained batch 753 batch loss 0.637806654 epoch total loss 0.54599309\n",
      "Trained batch 754 batch loss 0.63045454 epoch total loss 0.546105087\n",
      "Trained batch 755 batch loss 0.565004945 epoch total loss 0.546130121\n",
      "Trained batch 756 batch loss 0.543491483 epoch total loss 0.546126604\n",
      "Trained batch 757 batch loss 0.401397049 epoch total loss 0.545935452\n",
      "Trained batch 758 batch loss 0.532878041 epoch total loss 0.545918226\n",
      "Trained batch 759 batch loss 0.494331062 epoch total loss 0.545850217\n",
      "Trained batch 760 batch loss 0.390954047 epoch total loss 0.545646429\n",
      "Trained batch 761 batch loss 0.557692 epoch total loss 0.545662224\n",
      "Trained batch 762 batch loss 0.581056237 epoch total loss 0.545708656\n",
      "Trained batch 763 batch loss 0.644564271 epoch total loss 0.545838237\n",
      "Trained batch 764 batch loss 0.441368848 epoch total loss 0.545701504\n",
      "Trained batch 765 batch loss 0.566702545 epoch total loss 0.545729\n",
      "Trained batch 766 batch loss 0.57645452 epoch total loss 0.545769095\n",
      "Trained batch 767 batch loss 0.602464616 epoch total loss 0.545843\n",
      "Trained batch 768 batch loss 0.505986631 epoch total loss 0.54579109\n",
      "Trained batch 769 batch loss 0.55866915 epoch total loss 0.545807838\n",
      "Trained batch 770 batch loss 0.612653 epoch total loss 0.545894623\n",
      "Trained batch 771 batch loss 0.689873 epoch total loss 0.546081364\n",
      "Trained batch 772 batch loss 0.676220953 epoch total loss 0.546249926\n",
      "Trained batch 773 batch loss 0.519905 epoch total loss 0.546215832\n",
      "Trained batch 774 batch loss 0.580248296 epoch total loss 0.54625982\n",
      "Trained batch 775 batch loss 0.542420208 epoch total loss 0.546254873\n",
      "Trained batch 776 batch loss 0.528672338 epoch total loss 0.546232224\n",
      "Trained batch 777 batch loss 0.567809105 epoch total loss 0.54626\n",
      "Trained batch 778 batch loss 0.511259258 epoch total loss 0.546215057\n",
      "Trained batch 779 batch loss 0.462070018 epoch total loss 0.546107\n",
      "Trained batch 780 batch loss 0.447099149 epoch total loss 0.545980096\n",
      "Trained batch 781 batch loss 0.508084 epoch total loss 0.545931578\n",
      "Trained batch 782 batch loss 0.441457212 epoch total loss 0.545798\n",
      "Trained batch 783 batch loss 0.475802332 epoch total loss 0.545708597\n",
      "Trained batch 784 batch loss 0.45599094 epoch total loss 0.545594156\n",
      "Trained batch 785 batch loss 0.553328037 epoch total loss 0.545604\n",
      "Trained batch 786 batch loss 0.487036049 epoch total loss 0.545529485\n",
      "Trained batch 787 batch loss 0.4884516 epoch total loss 0.545456946\n",
      "Trained batch 788 batch loss 0.442652255 epoch total loss 0.545326531\n",
      "Trained batch 789 batch loss 0.591173172 epoch total loss 0.545384645\n",
      "Trained batch 790 batch loss 0.593360543 epoch total loss 0.545445323\n",
      "Trained batch 791 batch loss 0.527792394 epoch total loss 0.545423031\n",
      "Trained batch 792 batch loss 0.585505962 epoch total loss 0.545473635\n",
      "Trained batch 793 batch loss 0.571805954 epoch total loss 0.545506895\n",
      "Trained batch 794 batch loss 0.579321325 epoch total loss 0.545549452\n",
      "Trained batch 795 batch loss 0.518900871 epoch total loss 0.545515895\n",
      "Trained batch 796 batch loss 0.572711408 epoch total loss 0.545550108\n",
      "Trained batch 797 batch loss 0.593722 epoch total loss 0.545610547\n",
      "Trained batch 798 batch loss 0.681708932 epoch total loss 0.545781076\n",
      "Trained batch 799 batch loss 0.651242435 epoch total loss 0.545913041\n",
      "Trained batch 800 batch loss 0.553468168 epoch total loss 0.545922518\n",
      "Trained batch 801 batch loss 0.574486613 epoch total loss 0.545958161\n",
      "Trained batch 802 batch loss 0.588475406 epoch total loss 0.546011209\n",
      "Trained batch 803 batch loss 0.61875093 epoch total loss 0.546101749\n",
      "Trained batch 804 batch loss 0.577734 epoch total loss 0.546141088\n",
      "Trained batch 805 batch loss 0.538730502 epoch total loss 0.546131909\n",
      "Trained batch 806 batch loss 0.57256186 epoch total loss 0.546164691\n",
      "Trained batch 807 batch loss 0.483878165 epoch total loss 0.546087503\n",
      "Trained batch 808 batch loss 0.486720115 epoch total loss 0.546014071\n",
      "Trained batch 809 batch loss 0.471733391 epoch total loss 0.54592222\n",
      "Trained batch 810 batch loss 0.621176124 epoch total loss 0.546015143\n",
      "Trained batch 811 batch loss 0.580538869 epoch total loss 0.546057701\n",
      "Trained batch 812 batch loss 0.594942808 epoch total loss 0.546117902\n",
      "Trained batch 813 batch loss 0.558309734 epoch total loss 0.546132922\n",
      "Trained batch 814 batch loss 0.633429468 epoch total loss 0.546240151\n",
      "Trained batch 815 batch loss 0.656497 epoch total loss 0.546375453\n",
      "Trained batch 816 batch loss 0.590940118 epoch total loss 0.546430051\n",
      "Trained batch 817 batch loss 0.593048275 epoch total loss 0.546487153\n",
      "Trained batch 818 batch loss 0.658081353 epoch total loss 0.546623528\n",
      "Trained batch 819 batch loss 0.605414391 epoch total loss 0.546695352\n",
      "Trained batch 820 batch loss 0.500473201 epoch total loss 0.546638966\n",
      "Trained batch 821 batch loss 0.411030829 epoch total loss 0.546473801\n",
      "Trained batch 822 batch loss 0.548689485 epoch total loss 0.546476483\n",
      "Trained batch 823 batch loss 0.548080325 epoch total loss 0.546478391\n",
      "Trained batch 824 batch loss 0.590871036 epoch total loss 0.546532333\n",
      "Trained batch 825 batch loss 0.536915421 epoch total loss 0.54652065\n",
      "Trained batch 826 batch loss 0.603053927 epoch total loss 0.546589136\n",
      "Trained batch 827 batch loss 0.61038065 epoch total loss 0.546666265\n",
      "Trained batch 828 batch loss 0.575145364 epoch total loss 0.546700656\n",
      "Trained batch 829 batch loss 0.531298697 epoch total loss 0.54668206\n",
      "Trained batch 830 batch loss 0.572809041 epoch total loss 0.546713531\n",
      "Trained batch 831 batch loss 0.460125506 epoch total loss 0.546609342\n",
      "Trained batch 832 batch loss 0.566370368 epoch total loss 0.546633124\n",
      "Trained batch 833 batch loss 0.437475264 epoch total loss 0.546502054\n",
      "Trained batch 834 batch loss 0.519915 epoch total loss 0.546470165\n",
      "Trained batch 835 batch loss 0.512948394 epoch total loss 0.546430051\n",
      "Trained batch 836 batch loss 0.621823728 epoch total loss 0.546520233\n",
      "Trained batch 837 batch loss 0.541575134 epoch total loss 0.546514273\n",
      "Trained batch 838 batch loss 0.558969796 epoch total loss 0.546529174\n",
      "Trained batch 839 batch loss 0.556668401 epoch total loss 0.546541214\n",
      "Trained batch 840 batch loss 0.5383026 epoch total loss 0.546531439\n",
      "Trained batch 841 batch loss 0.59965086 epoch total loss 0.54659456\n",
      "Trained batch 842 batch loss 0.593177319 epoch total loss 0.546649873\n",
      "Trained batch 843 batch loss 0.578565598 epoch total loss 0.546687722\n",
      "Trained batch 844 batch loss 0.639004111 epoch total loss 0.546797097\n",
      "Trained batch 845 batch loss 0.618759274 epoch total loss 0.546882272\n",
      "Trained batch 846 batch loss 0.63776058 epoch total loss 0.546989739\n",
      "Trained batch 847 batch loss 0.619288087 epoch total loss 0.547075093\n",
      "Trained batch 848 batch loss 0.516713083 epoch total loss 0.54703927\n",
      "Trained batch 849 batch loss 0.57545048 epoch total loss 0.547072709\n",
      "Trained batch 850 batch loss 0.641508877 epoch total loss 0.547183812\n",
      "Trained batch 851 batch loss 0.602845788 epoch total loss 0.547249258\n",
      "Trained batch 852 batch loss 0.666939497 epoch total loss 0.547389686\n",
      "Trained batch 853 batch loss 0.652128458 epoch total loss 0.547512531\n",
      "Trained batch 854 batch loss 0.566885471 epoch total loss 0.547535181\n",
      "Trained batch 855 batch loss 0.516268611 epoch total loss 0.547498643\n",
      "Trained batch 856 batch loss 0.49807623 epoch total loss 0.547440886\n",
      "Trained batch 857 batch loss 0.492421627 epoch total loss 0.547376692\n",
      "Trained batch 858 batch loss 0.529191375 epoch total loss 0.547355533\n",
      "Trained batch 859 batch loss 0.518345416 epoch total loss 0.547321737\n",
      "Trained batch 860 batch loss 0.522138 epoch total loss 0.547292471\n",
      "Trained batch 861 batch loss 0.470043 epoch total loss 0.547202706\n",
      "Trained batch 862 batch loss 0.447283626 epoch total loss 0.547086835\n",
      "Trained batch 863 batch loss 0.511270106 epoch total loss 0.54704529\n",
      "Trained batch 864 batch loss 0.459229618 epoch total loss 0.546943665\n",
      "Trained batch 865 batch loss 0.637615085 epoch total loss 0.54704845\n",
      "Trained batch 866 batch loss 0.519035757 epoch total loss 0.547016144\n",
      "Trained batch 867 batch loss 0.600449204 epoch total loss 0.547077775\n",
      "Trained batch 868 batch loss 0.632309735 epoch total loss 0.547176\n",
      "Trained batch 869 batch loss 0.565688908 epoch total loss 0.547197282\n",
      "Trained batch 870 batch loss 0.539996624 epoch total loss 0.547189\n",
      "Trained batch 871 batch loss 0.485018253 epoch total loss 0.547117651\n",
      "Trained batch 872 batch loss 0.608163774 epoch total loss 0.547187626\n",
      "Trained batch 873 batch loss 0.585853338 epoch total loss 0.547231913\n",
      "Trained batch 874 batch loss 0.569891095 epoch total loss 0.547257841\n",
      "Trained batch 875 batch loss 0.58018297 epoch total loss 0.547295451\n",
      "Trained batch 876 batch loss 0.520613492 epoch total loss 0.547265\n",
      "Trained batch 877 batch loss 0.504314899 epoch total loss 0.547216\n",
      "Trained batch 878 batch loss 0.590454459 epoch total loss 0.547265232\n",
      "Trained batch 879 batch loss 0.571623504 epoch total loss 0.547292948\n",
      "Trained batch 880 batch loss 0.557017922 epoch total loss 0.547304\n",
      "Trained batch 881 batch loss 0.687595367 epoch total loss 0.547463238\n",
      "Trained batch 882 batch loss 0.620560765 epoch total loss 0.547546089\n",
      "Trained batch 883 batch loss 0.603699207 epoch total loss 0.547609687\n",
      "Trained batch 884 batch loss 0.538138151 epoch total loss 0.547599\n",
      "Trained batch 885 batch loss 0.57209152 epoch total loss 0.547626674\n",
      "Trained batch 886 batch loss 0.573722839 epoch total loss 0.547656119\n",
      "Trained batch 887 batch loss 0.595604539 epoch total loss 0.54771018\n",
      "Trained batch 888 batch loss 0.606675148 epoch total loss 0.54777658\n",
      "Trained batch 889 batch loss 0.565386832 epoch total loss 0.547796428\n",
      "Trained batch 890 batch loss 0.56483233 epoch total loss 0.547815561\n",
      "Trained batch 891 batch loss 0.528095067 epoch total loss 0.547793448\n",
      "Trained batch 892 batch loss 0.608877063 epoch total loss 0.547861934\n",
      "Trained batch 893 batch loss 0.538787901 epoch total loss 0.547851741\n",
      "Trained batch 894 batch loss 0.536472082 epoch total loss 0.547839046\n",
      "Trained batch 895 batch loss 0.529560804 epoch total loss 0.547818601\n",
      "Trained batch 896 batch loss 0.558759451 epoch total loss 0.54783082\n",
      "Trained batch 897 batch loss 0.572968543 epoch total loss 0.547858834\n",
      "Trained batch 898 batch loss 0.60150671 epoch total loss 0.547918558\n",
      "Trained batch 899 batch loss 0.644022346 epoch total loss 0.548025489\n",
      "Trained batch 900 batch loss 0.535748 epoch total loss 0.548011839\n",
      "Trained batch 901 batch loss 0.664081395 epoch total loss 0.548140645\n",
      "Trained batch 902 batch loss 0.634640038 epoch total loss 0.548236549\n",
      "Trained batch 903 batch loss 0.564877629 epoch total loss 0.548254967\n",
      "Trained batch 904 batch loss 0.602149725 epoch total loss 0.548314571\n",
      "Trained batch 905 batch loss 0.636927366 epoch total loss 0.548412502\n",
      "Trained batch 906 batch loss 0.633275628 epoch total loss 0.5485062\n",
      "Trained batch 907 batch loss 0.555484056 epoch total loss 0.548513889\n",
      "Trained batch 908 batch loss 0.644319534 epoch total loss 0.54861939\n",
      "Trained batch 909 batch loss 0.619088471 epoch total loss 0.548696876\n",
      "Trained batch 910 batch loss 0.557751775 epoch total loss 0.54870683\n",
      "Trained batch 911 batch loss 0.656146 epoch total loss 0.548824787\n",
      "Trained batch 912 batch loss 0.627298653 epoch total loss 0.548910797\n",
      "Trained batch 913 batch loss 0.671928108 epoch total loss 0.549045563\n",
      "Trained batch 914 batch loss 0.700195789 epoch total loss 0.549210906\n",
      "Trained batch 915 batch loss 0.714906693 epoch total loss 0.549392045\n",
      "Trained batch 916 batch loss 0.636664152 epoch total loss 0.549487293\n",
      "Trained batch 917 batch loss 0.501252353 epoch total loss 0.549434662\n",
      "Trained batch 918 batch loss 0.529113173 epoch total loss 0.549412549\n",
      "Trained batch 919 batch loss 0.515204608 epoch total loss 0.549375296\n",
      "Trained batch 920 batch loss 0.502539039 epoch total loss 0.549324393\n",
      "Trained batch 921 batch loss 0.509745598 epoch total loss 0.549281418\n",
      "Trained batch 922 batch loss 0.554154873 epoch total loss 0.549286723\n",
      "Trained batch 923 batch loss 0.563701 epoch total loss 0.54930234\n",
      "Trained batch 924 batch loss 0.547041237 epoch total loss 0.549299836\n",
      "Trained batch 925 batch loss 0.581573784 epoch total loss 0.549334764\n",
      "Trained batch 926 batch loss 0.560125589 epoch total loss 0.549346387\n",
      "Trained batch 927 batch loss 0.64430809 epoch total loss 0.549448848\n",
      "Trained batch 928 batch loss 0.576236129 epoch total loss 0.549477696\n",
      "Trained batch 929 batch loss 0.65131104 epoch total loss 0.549587309\n",
      "Trained batch 930 batch loss 0.516513228 epoch total loss 0.549551725\n",
      "Trained batch 931 batch loss 0.708674371 epoch total loss 0.549722672\n",
      "Trained batch 932 batch loss 0.632217824 epoch total loss 0.549811184\n",
      "Trained batch 933 batch loss 0.674651623 epoch total loss 0.549944937\n",
      "Trained batch 934 batch loss 0.674162745 epoch total loss 0.550077915\n",
      "Trained batch 935 batch loss 0.695643842 epoch total loss 0.550233543\n",
      "Trained batch 936 batch loss 0.557826459 epoch total loss 0.550241649\n",
      "Trained batch 937 batch loss 0.538138509 epoch total loss 0.550228715\n",
      "Trained batch 938 batch loss 0.558815598 epoch total loss 0.550237894\n",
      "Trained batch 939 batch loss 0.570952773 epoch total loss 0.550259948\n",
      "Trained batch 940 batch loss 0.505954206 epoch total loss 0.55021286\n",
      "Trained batch 941 batch loss 0.478967428 epoch total loss 0.550137103\n",
      "Trained batch 942 batch loss 0.526074827 epoch total loss 0.550111532\n",
      "Trained batch 943 batch loss 0.586241245 epoch total loss 0.550149858\n",
      "Trained batch 944 batch loss 0.658513904 epoch total loss 0.550264657\n",
      "Trained batch 945 batch loss 0.558989704 epoch total loss 0.550273836\n",
      "Trained batch 946 batch loss 0.564133704 epoch total loss 0.550288498\n",
      "Trained batch 947 batch loss 0.538749933 epoch total loss 0.550276339\n",
      "Trained batch 948 batch loss 0.58196032 epoch total loss 0.550309777\n",
      "Trained batch 949 batch loss 0.503676593 epoch total loss 0.550260603\n",
      "Trained batch 950 batch loss 0.545178 epoch total loss 0.550255239\n",
      "Trained batch 951 batch loss 0.48385942 epoch total loss 0.550185442\n",
      "Trained batch 952 batch loss 0.571819663 epoch total loss 0.550208211\n",
      "Trained batch 953 batch loss 0.560018599 epoch total loss 0.550218463\n",
      "Trained batch 954 batch loss 0.551953375 epoch total loss 0.550220311\n",
      "Trained batch 955 batch loss 0.623736382 epoch total loss 0.55029726\n",
      "Trained batch 956 batch loss 0.540687561 epoch total loss 0.550287247\n",
      "Trained batch 957 batch loss 0.599830568 epoch total loss 0.550339043\n",
      "Trained batch 958 batch loss 0.521830261 epoch total loss 0.5503093\n",
      "Trained batch 959 batch loss 0.507462084 epoch total loss 0.550264597\n",
      "Trained batch 960 batch loss 0.539011359 epoch total loss 0.550252855\n",
      "Trained batch 961 batch loss 0.459163249 epoch total loss 0.550158083\n",
      "Trained batch 962 batch loss 0.482787311 epoch total loss 0.550088048\n",
      "Trained batch 963 batch loss 0.427402437 epoch total loss 0.549960673\n",
      "Trained batch 964 batch loss 0.462832928 epoch total loss 0.549870253\n",
      "Trained batch 965 batch loss 0.451959074 epoch total loss 0.549768806\n",
      "Trained batch 966 batch loss 0.615837336 epoch total loss 0.549837232\n",
      "Trained batch 967 batch loss 0.681504667 epoch total loss 0.549973428\n",
      "Trained batch 968 batch loss 0.688627362 epoch total loss 0.550116599\n",
      "Trained batch 969 batch loss 0.581109285 epoch total loss 0.550148606\n",
      "Trained batch 970 batch loss 0.57039839 epoch total loss 0.550169468\n",
      "Trained batch 971 batch loss 0.545289278 epoch total loss 0.550164402\n",
      "Trained batch 972 batch loss 0.484720737 epoch total loss 0.550097108\n",
      "Trained batch 973 batch loss 0.517560542 epoch total loss 0.55006367\n",
      "Trained batch 974 batch loss 0.491873085 epoch total loss 0.550003946\n",
      "Trained batch 975 batch loss 0.57762295 epoch total loss 0.550032318\n",
      "Trained batch 976 batch loss 0.536348879 epoch total loss 0.550018311\n",
      "Trained batch 977 batch loss 0.458969295 epoch total loss 0.549925148\n",
      "Trained batch 978 batch loss 0.519606948 epoch total loss 0.549894094\n",
      "Trained batch 979 batch loss 0.508266211 epoch total loss 0.549851596\n",
      "Trained batch 980 batch loss 0.485091031 epoch total loss 0.549785495\n",
      "Trained batch 981 batch loss 0.505943775 epoch total loss 0.549740791\n",
      "Trained batch 982 batch loss 0.578643143 epoch total loss 0.549770176\n",
      "Trained batch 983 batch loss 0.588365614 epoch total loss 0.549809456\n",
      "Trained batch 984 batch loss 0.470298529 epoch total loss 0.549728632\n",
      "Trained batch 985 batch loss 0.509804487 epoch total loss 0.54968816\n",
      "Trained batch 986 batch loss 0.579687595 epoch total loss 0.549718559\n",
      "Trained batch 987 batch loss 0.590645373 epoch total loss 0.549760044\n",
      "Trained batch 988 batch loss 0.537122 epoch total loss 0.549747229\n",
      "Trained batch 989 batch loss 0.59304148 epoch total loss 0.549791\n",
      "Trained batch 990 batch loss 0.561724424 epoch total loss 0.549803\n",
      "Trained batch 991 batch loss 0.575521529 epoch total loss 0.549828947\n",
      "Trained batch 992 batch loss 0.462122351 epoch total loss 0.549740493\n",
      "Trained batch 993 batch loss 0.504240453 epoch total loss 0.549694657\n",
      "Trained batch 994 batch loss 0.516855717 epoch total loss 0.549661636\n",
      "Trained batch 995 batch loss 0.515249431 epoch total loss 0.549627066\n",
      "Trained batch 996 batch loss 0.496048272 epoch total loss 0.549573243\n",
      "Trained batch 997 batch loss 0.562087297 epoch total loss 0.54958576\n",
      "Trained batch 998 batch loss 0.482647181 epoch total loss 0.549518704\n",
      "Trained batch 999 batch loss 0.591348588 epoch total loss 0.549560606\n",
      "Trained batch 1000 batch loss 0.540819347 epoch total loss 0.549551904\n",
      "Trained batch 1001 batch loss 0.533588707 epoch total loss 0.54953593\n",
      "Trained batch 1002 batch loss 0.460651278 epoch total loss 0.549447179\n",
      "Trained batch 1003 batch loss 0.489433974 epoch total loss 0.549387336\n",
      "Trained batch 1004 batch loss 0.501156 epoch total loss 0.549339354\n",
      "Trained batch 1005 batch loss 0.516333818 epoch total loss 0.549306512\n",
      "Trained batch 1006 batch loss 0.536188364 epoch total loss 0.549293458\n",
      "Trained batch 1007 batch loss 0.555466473 epoch total loss 0.549299598\n",
      "Trained batch 1008 batch loss 0.605294049 epoch total loss 0.549355149\n",
      "Trained batch 1009 batch loss 0.564094305 epoch total loss 0.549369752\n",
      "Trained batch 1010 batch loss 0.596140087 epoch total loss 0.549416065\n",
      "Trained batch 1011 batch loss 0.744155586 epoch total loss 0.549608648\n",
      "Trained batch 1012 batch loss 0.674229443 epoch total loss 0.549731851\n",
      "Trained batch 1013 batch loss 0.483887553 epoch total loss 0.549666822\n",
      "Trained batch 1014 batch loss 0.601623 epoch total loss 0.549718082\n",
      "Trained batch 1015 batch loss 0.608299136 epoch total loss 0.549775779\n",
      "Trained batch 1016 batch loss 0.525812507 epoch total loss 0.549752176\n",
      "Trained batch 1017 batch loss 0.532091498 epoch total loss 0.549734831\n",
      "Trained batch 1018 batch loss 0.572818637 epoch total loss 0.549757481\n",
      "Trained batch 1019 batch loss 0.65706265 epoch total loss 0.549862802\n",
      "Trained batch 1020 batch loss 0.555990458 epoch total loss 0.549868762\n",
      "Trained batch 1021 batch loss 0.576951265 epoch total loss 0.549895287\n",
      "Trained batch 1022 batch loss 0.558602095 epoch total loss 0.54990381\n",
      "Trained batch 1023 batch loss 0.546167791 epoch total loss 0.549900174\n",
      "Trained batch 1024 batch loss 0.517842174 epoch total loss 0.549868822\n",
      "Trained batch 1025 batch loss 0.581977725 epoch total loss 0.549900115\n",
      "Trained batch 1026 batch loss 0.56031394 epoch total loss 0.549910307\n",
      "Trained batch 1027 batch loss 0.526648343 epoch total loss 0.549887657\n",
      "Trained batch 1028 batch loss 0.507999122 epoch total loss 0.549846888\n",
      "Trained batch 1029 batch loss 0.544638455 epoch total loss 0.549841821\n",
      "Trained batch 1030 batch loss 0.507769465 epoch total loss 0.549800932\n",
      "Trained batch 1031 batch loss 0.612090111 epoch total loss 0.549861372\n",
      "Trained batch 1032 batch loss 0.541455567 epoch total loss 0.549853206\n",
      "Trained batch 1033 batch loss 0.547816336 epoch total loss 0.549851179\n",
      "Trained batch 1034 batch loss 0.481330812 epoch total loss 0.549784899\n",
      "Trained batch 1035 batch loss 0.512429476 epoch total loss 0.549748838\n",
      "Trained batch 1036 batch loss 0.48775804 epoch total loss 0.549689\n",
      "Trained batch 1037 batch loss 0.498286277 epoch total loss 0.549639404\n",
      "Trained batch 1038 batch loss 0.493486 epoch total loss 0.549585283\n",
      "Trained batch 1039 batch loss 0.568397462 epoch total loss 0.549603403\n",
      "Trained batch 1040 batch loss 0.528606892 epoch total loss 0.549583256\n",
      "Trained batch 1041 batch loss 0.502870917 epoch total loss 0.549538374\n",
      "Trained batch 1042 batch loss 0.517701209 epoch total loss 0.549507856\n",
      "Trained batch 1043 batch loss 0.508203268 epoch total loss 0.549468219\n",
      "Trained batch 1044 batch loss 0.548719227 epoch total loss 0.549467444\n",
      "Trained batch 1045 batch loss 0.64088726 epoch total loss 0.549554944\n",
      "Trained batch 1046 batch loss 0.663915098 epoch total loss 0.549664319\n",
      "Trained batch 1047 batch loss 0.767744064 epoch total loss 0.549872577\n",
      "Trained batch 1048 batch loss 0.568898141 epoch total loss 0.549890757\n",
      "Trained batch 1049 batch loss 0.509451807 epoch total loss 0.549852252\n",
      "Trained batch 1050 batch loss 0.491539657 epoch total loss 0.549796641\n",
      "Trained batch 1051 batch loss 0.504256904 epoch total loss 0.549753368\n",
      "Trained batch 1052 batch loss 0.405570686 epoch total loss 0.549616277\n",
      "Trained batch 1053 batch loss 0.495283127 epoch total loss 0.549564719\n",
      "Trained batch 1054 batch loss 0.482885212 epoch total loss 0.549501479\n",
      "Trained batch 1055 batch loss 0.515746 epoch total loss 0.549469471\n",
      "Trained batch 1056 batch loss 0.510441 epoch total loss 0.549432516\n",
      "Trained batch 1057 batch loss 0.58185184 epoch total loss 0.549463212\n",
      "Trained batch 1058 batch loss 0.609653056 epoch total loss 0.549520135\n",
      "Trained batch 1059 batch loss 0.656051 epoch total loss 0.549620688\n",
      "Trained batch 1060 batch loss 0.550118446 epoch total loss 0.549621165\n",
      "Trained batch 1061 batch loss 0.598762274 epoch total loss 0.549667478\n",
      "Trained batch 1062 batch loss 0.5187639 epoch total loss 0.549638391\n",
      "Trained batch 1063 batch loss 0.570111 epoch total loss 0.549657643\n",
      "Trained batch 1064 batch loss 0.562766671 epoch total loss 0.549669921\n",
      "Trained batch 1065 batch loss 0.496040761 epoch total loss 0.549619555\n",
      "Trained batch 1066 batch loss 0.469807476 epoch total loss 0.549544692\n",
      "Trained batch 1067 batch loss 0.555164516 epoch total loss 0.549549937\n",
      "Trained batch 1068 batch loss 0.561652243 epoch total loss 0.549561262\n",
      "Trained batch 1069 batch loss 0.519009471 epoch total loss 0.549532712\n",
      "Trained batch 1070 batch loss 0.55153954 epoch total loss 0.549534559\n",
      "Trained batch 1071 batch loss 0.489709556 epoch total loss 0.54947865\n",
      "Trained batch 1072 batch loss 0.669623613 epoch total loss 0.549590707\n",
      "Trained batch 1073 batch loss 0.548594594 epoch total loss 0.549589813\n",
      "Trained batch 1074 batch loss 0.677972198 epoch total loss 0.54970932\n",
      "Trained batch 1075 batch loss 0.615390778 epoch total loss 0.549770474\n",
      "Trained batch 1076 batch loss 0.593844056 epoch total loss 0.549811423\n",
      "Trained batch 1077 batch loss 0.491059721 epoch total loss 0.549756885\n",
      "Trained batch 1078 batch loss 0.415528476 epoch total loss 0.54963237\n",
      "Trained batch 1079 batch loss 0.515944242 epoch total loss 0.549601138\n",
      "Trained batch 1080 batch loss 0.486577868 epoch total loss 0.549542785\n",
      "Trained batch 1081 batch loss 0.485932559 epoch total loss 0.549484\n",
      "Trained batch 1082 batch loss 0.496846795 epoch total loss 0.549435318\n",
      "Trained batch 1083 batch loss 0.470537394 epoch total loss 0.549362421\n",
      "Trained batch 1084 batch loss 0.720810711 epoch total loss 0.549520612\n",
      "Trained batch 1085 batch loss 0.681152821 epoch total loss 0.549641967\n",
      "Trained batch 1086 batch loss 0.603661776 epoch total loss 0.549691677\n",
      "Trained batch 1087 batch loss 0.69232136 epoch total loss 0.549822867\n",
      "Trained batch 1088 batch loss 0.645457149 epoch total loss 0.549910784\n",
      "Trained batch 1089 batch loss 0.541196048 epoch total loss 0.549902797\n",
      "Trained batch 1090 batch loss 0.515679657 epoch total loss 0.549871385\n",
      "Trained batch 1091 batch loss 0.487680197 epoch total loss 0.549814343\n",
      "Trained batch 1092 batch loss 0.473219782 epoch total loss 0.549744189\n",
      "Trained batch 1093 batch loss 0.535828 epoch total loss 0.549731493\n",
      "Trained batch 1094 batch loss 0.63605547 epoch total loss 0.54981035\n",
      "Trained batch 1095 batch loss 0.597754598 epoch total loss 0.549854159\n",
      "Trained batch 1096 batch loss 0.642409503 epoch total loss 0.549938619\n",
      "Trained batch 1097 batch loss 0.635973334 epoch total loss 0.550017059\n",
      "Trained batch 1098 batch loss 0.631211758 epoch total loss 0.550091\n",
      "Trained batch 1099 batch loss 0.643446386 epoch total loss 0.550175965\n",
      "Trained batch 1100 batch loss 0.598954 epoch total loss 0.550220311\n",
      "Trained batch 1101 batch loss 0.565811872 epoch total loss 0.550234437\n",
      "Trained batch 1102 batch loss 0.626867473 epoch total loss 0.550304\n",
      "Trained batch 1103 batch loss 0.5522843 epoch total loss 0.550305784\n",
      "Trained batch 1104 batch loss 0.587655425 epoch total loss 0.550339639\n",
      "Trained batch 1105 batch loss 0.613573492 epoch total loss 0.55039686\n",
      "Trained batch 1106 batch loss 0.537594736 epoch total loss 0.550385296\n",
      "Trained batch 1107 batch loss 0.575526655 epoch total loss 0.550408\n",
      "Trained batch 1108 batch loss 0.582977831 epoch total loss 0.550437391\n",
      "Trained batch 1109 batch loss 0.601307869 epoch total loss 0.550483286\n",
      "Trained batch 1110 batch loss 0.634518 epoch total loss 0.550559\n",
      "Trained batch 1111 batch loss 0.506693363 epoch total loss 0.550519526\n",
      "Trained batch 1112 batch loss 0.545269787 epoch total loss 0.550514817\n",
      "Trained batch 1113 batch loss 0.543431 epoch total loss 0.550508499\n",
      "Trained batch 1114 batch loss 0.544622 epoch total loss 0.550503194\n",
      "Trained batch 1115 batch loss 0.568834841 epoch total loss 0.550519645\n",
      "Trained batch 1116 batch loss 0.590491593 epoch total loss 0.550555468\n",
      "Trained batch 1117 batch loss 0.569555759 epoch total loss 0.550572515\n",
      "Trained batch 1118 batch loss 0.575010419 epoch total loss 0.550594389\n",
      "Trained batch 1119 batch loss 0.618994236 epoch total loss 0.550655544\n",
      "Trained batch 1120 batch loss 0.570657551 epoch total loss 0.550673425\n",
      "Trained batch 1121 batch loss 0.593389809 epoch total loss 0.550711513\n",
      "Trained batch 1122 batch loss 0.584819436 epoch total loss 0.550741911\n",
      "Trained batch 1123 batch loss 0.617388606 epoch total loss 0.550801277\n",
      "Trained batch 1124 batch loss 0.630486071 epoch total loss 0.550872147\n",
      "Trained batch 1125 batch loss 0.573574185 epoch total loss 0.550892293\n",
      "Trained batch 1126 batch loss 0.602686763 epoch total loss 0.550938308\n",
      "Trained batch 1127 batch loss 0.619658828 epoch total loss 0.550999224\n",
      "Trained batch 1128 batch loss 0.529109836 epoch total loss 0.550979853\n",
      "Trained batch 1129 batch loss 0.590640783 epoch total loss 0.55101496\n",
      "Trained batch 1130 batch loss 0.582505941 epoch total loss 0.551042855\n",
      "Trained batch 1131 batch loss 0.513123274 epoch total loss 0.551009297\n",
      "Trained batch 1132 batch loss 0.5032745 epoch total loss 0.550967157\n",
      "Trained batch 1133 batch loss 0.495356411 epoch total loss 0.550918102\n",
      "Trained batch 1134 batch loss 0.480358481 epoch total loss 0.550855875\n",
      "Trained batch 1135 batch loss 0.567475796 epoch total loss 0.550870538\n",
      "Trained batch 1136 batch loss 0.614323497 epoch total loss 0.550926387\n",
      "Trained batch 1137 batch loss 0.556310058 epoch total loss 0.550931156\n",
      "Trained batch 1138 batch loss 0.573962152 epoch total loss 0.550951362\n",
      "Trained batch 1139 batch loss 0.577859402 epoch total loss 0.550975\n",
      "Trained batch 1140 batch loss 0.594768584 epoch total loss 0.55101347\n",
      "Trained batch 1141 batch loss 0.579937577 epoch total loss 0.551038802\n",
      "Trained batch 1142 batch loss 0.545441687 epoch total loss 0.551033914\n",
      "Trained batch 1143 batch loss 0.561880767 epoch total loss 0.551043451\n",
      "Trained batch 1144 batch loss 0.56077224 epoch total loss 0.551052\n",
      "Trained batch 1145 batch loss 0.512363791 epoch total loss 0.551018178\n",
      "Trained batch 1146 batch loss 0.603285432 epoch total loss 0.551063776\n",
      "Trained batch 1147 batch loss 0.605260909 epoch total loss 0.551111042\n",
      "Trained batch 1148 batch loss 0.588594198 epoch total loss 0.551143765\n",
      "Trained batch 1149 batch loss 0.605525792 epoch total loss 0.551191092\n",
      "Trained batch 1150 batch loss 0.582438946 epoch total loss 0.551218271\n",
      "Trained batch 1151 batch loss 0.502420604 epoch total loss 0.551175892\n",
      "Trained batch 1152 batch loss 0.516041875 epoch total loss 0.551145375\n",
      "Trained batch 1153 batch loss 0.502504766 epoch total loss 0.551103234\n",
      "Trained batch 1154 batch loss 0.500139594 epoch total loss 0.551059\n",
      "Trained batch 1155 batch loss 0.443556786 epoch total loss 0.550965965\n",
      "Trained batch 1156 batch loss 0.565468371 epoch total loss 0.550978482\n",
      "Trained batch 1157 batch loss 0.605409324 epoch total loss 0.551025569\n",
      "Trained batch 1158 batch loss 0.472048581 epoch total loss 0.550957322\n",
      "Trained batch 1159 batch loss 0.438176572 epoch total loss 0.550860047\n",
      "Trained batch 1160 batch loss 0.474770427 epoch total loss 0.550794482\n",
      "Trained batch 1161 batch loss 0.566186428 epoch total loss 0.550807714\n",
      "Trained batch 1162 batch loss 0.536751568 epoch total loss 0.550795615\n",
      "Trained batch 1163 batch loss 0.489463389 epoch total loss 0.550742805\n",
      "Trained batch 1164 batch loss 0.475595504 epoch total loss 0.550678253\n",
      "Trained batch 1165 batch loss 0.474624217 epoch total loss 0.550613\n",
      "Trained batch 1166 batch loss 0.448586166 epoch total loss 0.550525486\n",
      "Trained batch 1167 batch loss 0.462941527 epoch total loss 0.550450444\n",
      "Trained batch 1168 batch loss 0.492361724 epoch total loss 0.550400734\n",
      "Trained batch 1169 batch loss 0.4809196 epoch total loss 0.550341249\n",
      "Trained batch 1170 batch loss 0.5455935 epoch total loss 0.550337195\n",
      "Trained batch 1171 batch loss 0.546856105 epoch total loss 0.550334275\n",
      "Trained batch 1172 batch loss 0.595134914 epoch total loss 0.550372481\n",
      "Trained batch 1173 batch loss 0.655794144 epoch total loss 0.550462365\n",
      "Trained batch 1174 batch loss 0.568015099 epoch total loss 0.550477326\n",
      "Trained batch 1175 batch loss 0.511137486 epoch total loss 0.550443828\n",
      "Trained batch 1176 batch loss 0.579323769 epoch total loss 0.550468385\n",
      "Trained batch 1177 batch loss 0.571058035 epoch total loss 0.550485849\n",
      "Trained batch 1178 batch loss 0.585119486 epoch total loss 0.550515294\n",
      "Trained batch 1179 batch loss 0.609119654 epoch total loss 0.550565\n",
      "Trained batch 1180 batch loss 0.595739603 epoch total loss 0.55060333\n",
      "Trained batch 1181 batch loss 0.556471705 epoch total loss 0.550608277\n",
      "Trained batch 1182 batch loss 0.411910951 epoch total loss 0.550491\n",
      "Trained batch 1183 batch loss 0.518217146 epoch total loss 0.550463617\n",
      "Trained batch 1184 batch loss 0.584096 epoch total loss 0.550492048\n",
      "Trained batch 1185 batch loss 0.527513504 epoch total loss 0.550472677\n",
      "Trained batch 1186 batch loss 0.543843508 epoch total loss 0.550467074\n",
      "Trained batch 1187 batch loss 0.601139367 epoch total loss 0.550509751\n",
      "Trained batch 1188 batch loss 0.573454618 epoch total loss 0.550529063\n",
      "Trained batch 1189 batch loss 0.647956967 epoch total loss 0.55061096\n",
      "Trained batch 1190 batch loss 0.446377695 epoch total loss 0.550523341\n",
      "Trained batch 1191 batch loss 0.521452546 epoch total loss 0.550498903\n",
      "Trained batch 1192 batch loss 0.559165657 epoch total loss 0.550506175\n",
      "Trained batch 1193 batch loss 0.557671309 epoch total loss 0.550512195\n",
      "Trained batch 1194 batch loss 0.652698934 epoch total loss 0.550597787\n",
      "Trained batch 1195 batch loss 0.610895634 epoch total loss 0.550648272\n",
      "Trained batch 1196 batch loss 0.560388923 epoch total loss 0.550656378\n",
      "Trained batch 1197 batch loss 0.506435215 epoch total loss 0.550619423\n",
      "Trained batch 1198 batch loss 0.454195172 epoch total loss 0.550538957\n",
      "Trained batch 1199 batch loss 0.571737885 epoch total loss 0.5505566\n",
      "Trained batch 1200 batch loss 0.548600793 epoch total loss 0.550555\n",
      "Trained batch 1201 batch loss 0.524233282 epoch total loss 0.550533056\n",
      "Trained batch 1202 batch loss 0.603146672 epoch total loss 0.550576806\n",
      "Trained batch 1203 batch loss 0.547687054 epoch total loss 0.550574422\n",
      "Trained batch 1204 batch loss 0.537898064 epoch total loss 0.550563872\n",
      "Trained batch 1205 batch loss 0.554977298 epoch total loss 0.550567567\n",
      "Trained batch 1206 batch loss 0.552311063 epoch total loss 0.550569\n",
      "Trained batch 1207 batch loss 0.601706803 epoch total loss 0.550611317\n",
      "Trained batch 1208 batch loss 0.582992077 epoch total loss 0.550638139\n",
      "Trained batch 1209 batch loss 0.498669922 epoch total loss 0.550595164\n",
      "Trained batch 1210 batch loss 0.452408671 epoch total loss 0.550514\n",
      "Trained batch 1211 batch loss 0.465902358 epoch total loss 0.550444126\n",
      "Trained batch 1212 batch loss 0.627734959 epoch total loss 0.550507903\n",
      "Trained batch 1213 batch loss 0.652375579 epoch total loss 0.550591886\n",
      "Trained batch 1214 batch loss 0.534749389 epoch total loss 0.550578833\n",
      "Trained batch 1215 batch loss 0.507030964 epoch total loss 0.550543\n",
      "Trained batch 1216 batch loss 0.555838943 epoch total loss 0.550547361\n",
      "Trained batch 1217 batch loss 0.51009196 epoch total loss 0.550514102\n",
      "Trained batch 1218 batch loss 0.516311049 epoch total loss 0.550485969\n",
      "Trained batch 1219 batch loss 0.581754625 epoch total loss 0.550511599\n",
      "Trained batch 1220 batch loss 0.552348912 epoch total loss 0.550513148\n",
      "Trained batch 1221 batch loss 0.648101211 epoch total loss 0.550593\n",
      "Trained batch 1222 batch loss 0.638152361 epoch total loss 0.550664663\n",
      "Trained batch 1223 batch loss 0.490218312 epoch total loss 0.550615251\n",
      "Trained batch 1224 batch loss 0.568225622 epoch total loss 0.550629675\n",
      "Trained batch 1225 batch loss 0.630078137 epoch total loss 0.550694525\n",
      "Trained batch 1226 batch loss 0.577586234 epoch total loss 0.55071646\n",
      "Trained batch 1227 batch loss 0.568509638 epoch total loss 0.550730884\n",
      "Trained batch 1228 batch loss 0.505283594 epoch total loss 0.550693929\n",
      "Trained batch 1229 batch loss 0.557224929 epoch total loss 0.550699234\n",
      "Trained batch 1230 batch loss 0.522014439 epoch total loss 0.550675929\n",
      "Trained batch 1231 batch loss 0.542704582 epoch total loss 0.550669491\n",
      "Trained batch 1232 batch loss 0.635890126 epoch total loss 0.550738633\n",
      "Trained batch 1233 batch loss 0.56711 epoch total loss 0.550751925\n",
      "Trained batch 1234 batch loss 0.541944623 epoch total loss 0.550744772\n",
      "Trained batch 1235 batch loss 0.560304284 epoch total loss 0.550752521\n",
      "Trained batch 1236 batch loss 0.478143454 epoch total loss 0.55069381\n",
      "Trained batch 1237 batch loss 0.473953605 epoch total loss 0.550631762\n",
      "Trained batch 1238 batch loss 0.471541435 epoch total loss 0.550567865\n",
      "Trained batch 1239 batch loss 0.425068378 epoch total loss 0.550466537\n",
      "Trained batch 1240 batch loss 0.545967221 epoch total loss 0.550462961\n",
      "Trained batch 1241 batch loss 0.533594728 epoch total loss 0.550449312\n",
      "Trained batch 1242 batch loss 0.611830831 epoch total loss 0.550498724\n",
      "Trained batch 1243 batch loss 0.548600554 epoch total loss 0.550497174\n",
      "Trained batch 1244 batch loss 0.603333712 epoch total loss 0.550539672\n",
      "Trained batch 1245 batch loss 0.542749286 epoch total loss 0.550533354\n",
      "Trained batch 1246 batch loss 0.545394897 epoch total loss 0.550529301\n",
      "Trained batch 1247 batch loss 0.551412404 epoch total loss 0.550529957\n",
      "Trained batch 1248 batch loss 0.492982537 epoch total loss 0.550483823\n",
      "Trained batch 1249 batch loss 0.484833 epoch total loss 0.550431311\n",
      "Trained batch 1250 batch loss 0.485206 epoch total loss 0.550379157\n",
      "Trained batch 1251 batch loss 0.398692846 epoch total loss 0.550257862\n",
      "Trained batch 1252 batch loss 0.438518345 epoch total loss 0.550168633\n",
      "Trained batch 1253 batch loss 0.457261533 epoch total loss 0.550094545\n",
      "Trained batch 1254 batch loss 0.378482163 epoch total loss 0.549957693\n",
      "Trained batch 1255 batch loss 0.425070465 epoch total loss 0.549858153\n",
      "Trained batch 1256 batch loss 0.47644797 epoch total loss 0.549799681\n",
      "Trained batch 1257 batch loss 0.511507034 epoch total loss 0.549769223\n",
      "Trained batch 1258 batch loss 0.468683183 epoch total loss 0.54970479\n",
      "Trained batch 1259 batch loss 0.514080644 epoch total loss 0.549676538\n",
      "Trained batch 1260 batch loss 0.489473432 epoch total loss 0.549628735\n",
      "Trained batch 1261 batch loss 0.607817292 epoch total loss 0.549674869\n",
      "Trained batch 1262 batch loss 0.572027326 epoch total loss 0.549692571\n",
      "Trained batch 1263 batch loss 0.710147321 epoch total loss 0.549819648\n",
      "Trained batch 1264 batch loss 0.682175279 epoch total loss 0.549924314\n",
      "Trained batch 1265 batch loss 0.619502902 epoch total loss 0.549979329\n",
      "Trained batch 1266 batch loss 0.546278119 epoch total loss 0.549976408\n",
      "Trained batch 1267 batch loss 0.598480284 epoch total loss 0.550014734\n",
      "Trained batch 1268 batch loss 0.655002534 epoch total loss 0.550097525\n",
      "Trained batch 1269 batch loss 0.649660766 epoch total loss 0.550176\n",
      "Trained batch 1270 batch loss 0.528786182 epoch total loss 0.550159156\n",
      "Trained batch 1271 batch loss 0.45478785 epoch total loss 0.550084114\n",
      "Trained batch 1272 batch loss 0.592583597 epoch total loss 0.550117552\n",
      "Trained batch 1273 batch loss 0.58502835 epoch total loss 0.550145\n",
      "Trained batch 1274 batch loss 0.519836247 epoch total loss 0.550121188\n",
      "Trained batch 1275 batch loss 0.448166519 epoch total loss 0.550041199\n",
      "Trained batch 1276 batch loss 0.57135731 epoch total loss 0.550057888\n",
      "Trained batch 1277 batch loss 0.591511309 epoch total loss 0.550090373\n",
      "Trained batch 1278 batch loss 0.586672664 epoch total loss 0.550119\n",
      "Trained batch 1279 batch loss 0.532735407 epoch total loss 0.550105393\n",
      "Trained batch 1280 batch loss 0.576536357 epoch total loss 0.550126\n",
      "Trained batch 1281 batch loss 0.461016834 epoch total loss 0.550056458\n",
      "Trained batch 1282 batch loss 0.514683783 epoch total loss 0.550028861\n",
      "Trained batch 1283 batch loss 0.514343381 epoch total loss 0.550001085\n",
      "Trained batch 1284 batch loss 0.418670923 epoch total loss 0.549898803\n",
      "Trained batch 1285 batch loss 0.448334545 epoch total loss 0.549819767\n",
      "Trained batch 1286 batch loss 0.450901926 epoch total loss 0.549742877\n",
      "Trained batch 1287 batch loss 0.394399077 epoch total loss 0.549622178\n",
      "Trained batch 1288 batch loss 0.453786194 epoch total loss 0.549547791\n",
      "Trained batch 1289 batch loss 0.611922 epoch total loss 0.54959619\n",
      "Trained batch 1290 batch loss 0.633876801 epoch total loss 0.549661517\n",
      "Trained batch 1291 batch loss 0.697088718 epoch total loss 0.54977572\n",
      "Trained batch 1292 batch loss 0.718038499 epoch total loss 0.549905896\n",
      "Trained batch 1293 batch loss 0.614766598 epoch total loss 0.549956083\n",
      "Trained batch 1294 batch loss 0.668698847 epoch total loss 0.550047815\n",
      "Trained batch 1295 batch loss 0.701485157 epoch total loss 0.550164759\n",
      "Trained batch 1296 batch loss 0.569406748 epoch total loss 0.550179601\n",
      "Trained batch 1297 batch loss 0.524840176 epoch total loss 0.55016005\n",
      "Trained batch 1298 batch loss 0.642297089 epoch total loss 0.55023104\n",
      "Trained batch 1299 batch loss 0.653161705 epoch total loss 0.550310254\n",
      "Trained batch 1300 batch loss 0.661273956 epoch total loss 0.550395608\n",
      "Trained batch 1301 batch loss 0.587859929 epoch total loss 0.550424397\n",
      "Trained batch 1302 batch loss 0.593132436 epoch total loss 0.55045718\n",
      "Trained batch 1303 batch loss 0.555832863 epoch total loss 0.550461292\n",
      "Trained batch 1304 batch loss 0.665231586 epoch total loss 0.550549328\n",
      "Trained batch 1305 batch loss 0.678313 epoch total loss 0.550647199\n",
      "Trained batch 1306 batch loss 0.68713814 epoch total loss 0.550751686\n",
      "Trained batch 1307 batch loss 0.639084101 epoch total loss 0.550819337\n",
      "Trained batch 1308 batch loss 0.654117584 epoch total loss 0.550898254\n",
      "Trained batch 1309 batch loss 0.62277621 epoch total loss 0.550953209\n",
      "Trained batch 1310 batch loss 0.700310171 epoch total loss 0.551067233\n",
      "Trained batch 1311 batch loss 0.666911125 epoch total loss 0.551155627\n",
      "Trained batch 1312 batch loss 0.597173333 epoch total loss 0.551190674\n",
      "Trained batch 1313 batch loss 0.584624469 epoch total loss 0.551216125\n",
      "Trained batch 1314 batch loss 0.577328205 epoch total loss 0.551236\n",
      "Trained batch 1315 batch loss 0.550298095 epoch total loss 0.551235259\n",
      "Trained batch 1316 batch loss 0.602934659 epoch total loss 0.551274538\n",
      "Trained batch 1317 batch loss 0.513504207 epoch total loss 0.551245868\n",
      "Trained batch 1318 batch loss 0.568748653 epoch total loss 0.5512591\n",
      "Trained batch 1319 batch loss 0.635535181 epoch total loss 0.551323\n",
      "Trained batch 1320 batch loss 0.591348469 epoch total loss 0.551353335\n",
      "Trained batch 1321 batch loss 0.516921401 epoch total loss 0.551327288\n",
      "Trained batch 1322 batch loss 0.556803 epoch total loss 0.55133146\n",
      "Trained batch 1323 batch loss 0.575912952 epoch total loss 0.551350057\n",
      "Trained batch 1324 batch loss 0.580228865 epoch total loss 0.551371813\n",
      "Trained batch 1325 batch loss 0.5890311 epoch total loss 0.551400244\n",
      "Trained batch 1326 batch loss 0.555043578 epoch total loss 0.551403046\n",
      "Trained batch 1327 batch loss 0.563822508 epoch total loss 0.551412404\n",
      "Trained batch 1328 batch loss 0.52402 epoch total loss 0.55139178\n",
      "Trained batch 1329 batch loss 0.564362645 epoch total loss 0.551401556\n",
      "Trained batch 1330 batch loss 0.52254653 epoch total loss 0.551379859\n",
      "Trained batch 1331 batch loss 0.531171799 epoch total loss 0.55136466\n",
      "Trained batch 1332 batch loss 0.484326243 epoch total loss 0.551314354\n",
      "Trained batch 1333 batch loss 0.47285983 epoch total loss 0.551255465\n",
      "Trained batch 1334 batch loss 0.404025257 epoch total loss 0.551145136\n",
      "Trained batch 1335 batch loss 0.440945119 epoch total loss 0.551062584\n",
      "Trained batch 1336 batch loss 0.457519948 epoch total loss 0.550992548\n",
      "Trained batch 1337 batch loss 0.582893193 epoch total loss 0.55101639\n",
      "Trained batch 1338 batch loss 0.67461735 epoch total loss 0.551108778\n",
      "Trained batch 1339 batch loss 0.588701189 epoch total loss 0.551136851\n",
      "Trained batch 1340 batch loss 0.54883939 epoch total loss 0.551135123\n",
      "Trained batch 1341 batch loss 0.623836398 epoch total loss 0.551189363\n",
      "Trained batch 1342 batch loss 0.679780543 epoch total loss 0.551285148\n",
      "Trained batch 1343 batch loss 0.61184746 epoch total loss 0.551330268\n",
      "Trained batch 1344 batch loss 0.538187206 epoch total loss 0.551320553\n",
      "Trained batch 1345 batch loss 0.623784423 epoch total loss 0.551374376\n",
      "Trained batch 1346 batch loss 0.605762362 epoch total loss 0.551414847\n",
      "Trained batch 1347 batch loss 0.57539326 epoch total loss 0.55143261\n",
      "Trained batch 1348 batch loss 0.525417328 epoch total loss 0.551413298\n",
      "Trained batch 1349 batch loss 0.557970226 epoch total loss 0.551418185\n",
      "Trained batch 1350 batch loss 0.554824114 epoch total loss 0.551420689\n",
      "Trained batch 1351 batch loss 0.573154807 epoch total loss 0.551436782\n",
      "Trained batch 1352 batch loss 0.630119503 epoch total loss 0.551494956\n",
      "Trained batch 1353 batch loss 0.566203 epoch total loss 0.551505864\n",
      "Trained batch 1354 batch loss 0.505485952 epoch total loss 0.551471889\n",
      "Trained batch 1355 batch loss 0.485733956 epoch total loss 0.551423371\n",
      "Trained batch 1356 batch loss 0.516409755 epoch total loss 0.551397562\n",
      "Trained batch 1357 batch loss 0.522697151 epoch total loss 0.551376402\n",
      "Trained batch 1358 batch loss 0.556848109 epoch total loss 0.551380396\n",
      "Trained batch 1359 batch loss 0.48359853 epoch total loss 0.551330507\n",
      "Trained batch 1360 batch loss 0.514895141 epoch total loss 0.551303744\n",
      "Trained batch 1361 batch loss 0.532212 epoch total loss 0.551289737\n",
      "Trained batch 1362 batch loss 0.492925107 epoch total loss 0.551246881\n",
      "Trained batch 1363 batch loss 0.432627887 epoch total loss 0.551159799\n",
      "Trained batch 1364 batch loss 0.387269258 epoch total loss 0.551039636\n",
      "Trained batch 1365 batch loss 0.435216904 epoch total loss 0.550954819\n",
      "Trained batch 1366 batch loss 0.438486695 epoch total loss 0.550872505\n",
      "Trained batch 1367 batch loss 0.426363885 epoch total loss 0.550781429\n",
      "Trained batch 1368 batch loss 0.536364734 epoch total loss 0.550770879\n",
      "Trained batch 1369 batch loss 0.615645111 epoch total loss 0.550818324\n",
      "Trained batch 1370 batch loss 0.535290837 epoch total loss 0.55080694\n",
      "Trained batch 1371 batch loss 0.483443499 epoch total loss 0.550757825\n",
      "Trained batch 1372 batch loss 0.438013613 epoch total loss 0.550675631\n",
      "Trained batch 1373 batch loss 0.591184199 epoch total loss 0.550705135\n",
      "Trained batch 1374 batch loss 0.511761785 epoch total loss 0.550676823\n",
      "Trained batch 1375 batch loss 0.453508228 epoch total loss 0.550606132\n",
      "Trained batch 1376 batch loss 0.50003475 epoch total loss 0.550569415\n",
      "Trained batch 1377 batch loss 0.500947714 epoch total loss 0.550533414\n",
      "Trained batch 1378 batch loss 0.459718049 epoch total loss 0.550467491\n",
      "Trained batch 1379 batch loss 0.587084949 epoch total loss 0.550494\n",
      "Trained batch 1380 batch loss 0.580438733 epoch total loss 0.550515771\n",
      "Trained batch 1381 batch loss 0.561460912 epoch total loss 0.550523698\n",
      "Trained batch 1382 batch loss 0.595147491 epoch total loss 0.550555944\n",
      "Trained batch 1383 batch loss 0.580039561 epoch total loss 0.550577283\n",
      "Trained batch 1384 batch loss 0.623564482 epoch total loss 0.55063\n",
      "Trained batch 1385 batch loss 0.594319344 epoch total loss 0.550661504\n",
      "Trained batch 1386 batch loss 0.577621 epoch total loss 0.550681\n",
      "Trained batch 1387 batch loss 0.663484812 epoch total loss 0.550762355\n",
      "Trained batch 1388 batch loss 0.502397835 epoch total loss 0.550727487\n",
      "Trained batch 1389 batch loss 0.55983609 epoch total loss 0.550734\n",
      "Trained batch 1390 batch loss 0.533152103 epoch total loss 0.550721347\n",
      "Trained batch 1391 batch loss 0.521379709 epoch total loss 0.550700247\n",
      "Trained batch 1392 batch loss 0.520593822 epoch total loss 0.550678611\n",
      "Trained batch 1393 batch loss 0.537204921 epoch total loss 0.550668955\n",
      "Trained batch 1394 batch loss 0.490246207 epoch total loss 0.550625622\n",
      "Trained batch 1395 batch loss 0.455630153 epoch total loss 0.550557494\n",
      "Trained batch 1396 batch loss 0.416736364 epoch total loss 0.55046165\n",
      "Trained batch 1397 batch loss 0.413104236 epoch total loss 0.550363302\n",
      "Trained batch 1398 batch loss 0.466537952 epoch total loss 0.55030334\n",
      "Trained batch 1399 batch loss 0.462022513 epoch total loss 0.550240278\n",
      "Trained batch 1400 batch loss 0.484600037 epoch total loss 0.550193369\n",
      "Trained batch 1401 batch loss 0.469217479 epoch total loss 0.550135612\n",
      "Trained batch 1402 batch loss 0.669467509 epoch total loss 0.550220728\n",
      "Trained batch 1403 batch loss 0.581141114 epoch total loss 0.550242782\n",
      "Trained batch 1404 batch loss 0.576544642 epoch total loss 0.550261497\n",
      "Trained batch 1405 batch loss 0.582473636 epoch total loss 0.550284386\n",
      "Trained batch 1406 batch loss 0.66944623 epoch total loss 0.550369143\n",
      "Trained batch 1407 batch loss 0.626591742 epoch total loss 0.550423324\n",
      "Trained batch 1408 batch loss 0.637603819 epoch total loss 0.550485253\n",
      "Trained batch 1409 batch loss 0.643170655 epoch total loss 0.550551057\n",
      "Trained batch 1410 batch loss 0.608597755 epoch total loss 0.550592244\n",
      "Trained batch 1411 batch loss 0.562110543 epoch total loss 0.55060041\n",
      "Trained batch 1412 batch loss 0.621360064 epoch total loss 0.550650477\n",
      "Trained batch 1413 batch loss 0.625450671 epoch total loss 0.550703406\n",
      "Trained batch 1414 batch loss 0.506123066 epoch total loss 0.550671875\n",
      "Trained batch 1415 batch loss 0.496728897 epoch total loss 0.550633729\n",
      "Trained batch 1416 batch loss 0.487018824 epoch total loss 0.550588787\n",
      "Trained batch 1417 batch loss 0.447378457 epoch total loss 0.55051595\n",
      "Trained batch 1418 batch loss 0.578066945 epoch total loss 0.550535381\n",
      "Trained batch 1419 batch loss 0.537879944 epoch total loss 0.5505265\n",
      "Trained batch 1420 batch loss 0.614214718 epoch total loss 0.550571322\n",
      "Trained batch 1421 batch loss 0.557718277 epoch total loss 0.550576389\n",
      "Trained batch 1422 batch loss 0.530354679 epoch total loss 0.550562143\n",
      "Trained batch 1423 batch loss 0.494836867 epoch total loss 0.550523\n",
      "Trained batch 1424 batch loss 0.560683608 epoch total loss 0.550530076\n",
      "Trained batch 1425 batch loss 0.554067552 epoch total loss 0.550532579\n",
      "Trained batch 1426 batch loss 0.607792914 epoch total loss 0.550572693\n",
      "Trained batch 1427 batch loss 0.557812095 epoch total loss 0.55057776\n",
      "Trained batch 1428 batch loss 0.552553296 epoch total loss 0.55057919\n",
      "Trained batch 1429 batch loss 0.543666661 epoch total loss 0.550574303\n",
      "Trained batch 1430 batch loss 0.567774832 epoch total loss 0.550586343\n",
      "Trained batch 1431 batch loss 0.530226052 epoch total loss 0.550572097\n",
      "Trained batch 1432 batch loss 0.429465503 epoch total loss 0.550487518\n",
      "Trained batch 1433 batch loss 0.508390307 epoch total loss 0.550458074\n",
      "Trained batch 1434 batch loss 0.452739894 epoch total loss 0.550389946\n",
      "Trained batch 1435 batch loss 0.404342145 epoch total loss 0.5502882\n",
      "Trained batch 1436 batch loss 0.493828 epoch total loss 0.550248921\n",
      "Trained batch 1437 batch loss 0.628061175 epoch total loss 0.550303042\n",
      "Trained batch 1438 batch loss 0.437728763 epoch total loss 0.550224781\n",
      "Trained batch 1439 batch loss 0.492345154 epoch total loss 0.550184548\n",
      "Trained batch 1440 batch loss 0.534299254 epoch total loss 0.550173521\n",
      "Trained batch 1441 batch loss 0.547857821 epoch total loss 0.550171912\n",
      "Trained batch 1442 batch loss 0.531196415 epoch total loss 0.550158739\n",
      "Trained batch 1443 batch loss 0.501187503 epoch total loss 0.550124824\n",
      "Trained batch 1444 batch loss 0.40095374 epoch total loss 0.55002147\n",
      "Trained batch 1445 batch loss 0.454272091 epoch total loss 0.549955249\n",
      "Trained batch 1446 batch loss 0.537005067 epoch total loss 0.549946249\n",
      "Trained batch 1447 batch loss 0.536491752 epoch total loss 0.54993695\n",
      "Trained batch 1448 batch loss 0.548605621 epoch total loss 0.549936056\n",
      "Trained batch 1449 batch loss 0.639381886 epoch total loss 0.549997747\n",
      "Trained batch 1450 batch loss 0.49702698 epoch total loss 0.549961209\n",
      "Trained batch 1451 batch loss 0.515407205 epoch total loss 0.549937427\n",
      "Trained batch 1452 batch loss 0.509447455 epoch total loss 0.549909532\n",
      "Trained batch 1453 batch loss 0.539150715 epoch total loss 0.549902081\n",
      "Trained batch 1454 batch loss 0.543349147 epoch total loss 0.549897611\n",
      "Trained batch 1455 batch loss 0.464152038 epoch total loss 0.549838662\n",
      "Trained batch 1456 batch loss 0.485333115 epoch total loss 0.549794376\n",
      "Trained batch 1457 batch loss 0.466733754 epoch total loss 0.549737394\n",
      "Trained batch 1458 batch loss 0.539822817 epoch total loss 0.549730539\n",
      "Trained batch 1459 batch loss 0.534023762 epoch total loss 0.549719751\n",
      "Trained batch 1460 batch loss 0.49812746 epoch total loss 0.549684405\n",
      "Trained batch 1461 batch loss 0.562700152 epoch total loss 0.549693286\n",
      "Trained batch 1462 batch loss 0.557905078 epoch total loss 0.549698949\n",
      "Trained batch 1463 batch loss 0.600334525 epoch total loss 0.549733579\n",
      "Trained batch 1464 batch loss 0.576202929 epoch total loss 0.549751639\n",
      "Trained batch 1465 batch loss 0.655940413 epoch total loss 0.549824119\n",
      "Trained batch 1466 batch loss 0.678319812 epoch total loss 0.549911797\n",
      "Trained batch 1467 batch loss 0.660414755 epoch total loss 0.549987137\n",
      "Trained batch 1468 batch loss 0.649294853 epoch total loss 0.550054789\n",
      "Trained batch 1469 batch loss 0.526934743 epoch total loss 0.550039\n",
      "Trained batch 1470 batch loss 0.530733 epoch total loss 0.55002588\n",
      "Trained batch 1471 batch loss 0.505195558 epoch total loss 0.549995422\n",
      "Trained batch 1472 batch loss 0.479696751 epoch total loss 0.549947679\n",
      "Trained batch 1473 batch loss 0.582079291 epoch total loss 0.549969494\n",
      "Trained batch 1474 batch loss 0.594192624 epoch total loss 0.549999475\n",
      "Trained batch 1475 batch loss 0.575732827 epoch total loss 0.55001694\n",
      "Trained batch 1476 batch loss 0.680917323 epoch total loss 0.550105631\n",
      "Trained batch 1477 batch loss 0.662287652 epoch total loss 0.550181568\n",
      "Trained batch 1478 batch loss 0.668147326 epoch total loss 0.550261378\n",
      "Trained batch 1479 batch loss 0.655826926 epoch total loss 0.550332725\n",
      "Trained batch 1480 batch loss 0.595775783 epoch total loss 0.550363421\n",
      "Trained batch 1481 batch loss 0.588814199 epoch total loss 0.550389409\n",
      "Trained batch 1482 batch loss 0.551353693 epoch total loss 0.550390065\n",
      "Trained batch 1483 batch loss 0.482842803 epoch total loss 0.550344467\n",
      "Trained batch 1484 batch loss 0.510746241 epoch total loss 0.550317824\n",
      "Trained batch 1485 batch loss 0.584233284 epoch total loss 0.550340652\n",
      "Trained batch 1486 batch loss 0.639939249 epoch total loss 0.550401\n",
      "Trained batch 1487 batch loss 0.597869873 epoch total loss 0.550432861\n",
      "Trained batch 1488 batch loss 0.54212451 epoch total loss 0.550427258\n",
      "Trained batch 1489 batch loss 0.548260927 epoch total loss 0.550425828\n",
      "Trained batch 1490 batch loss 0.500169694 epoch total loss 0.550392091\n",
      "Trained batch 1491 batch loss 0.478178978 epoch total loss 0.550343633\n",
      "Trained batch 1492 batch loss 0.440891981 epoch total loss 0.550270319\n",
      "Trained batch 1493 batch loss 0.472121239 epoch total loss 0.550217927\n",
      "Trained batch 1494 batch loss 0.454103708 epoch total loss 0.550153613\n",
      "Trained batch 1495 batch loss 0.568243861 epoch total loss 0.550165713\n",
      "Trained batch 1496 batch loss 0.533001 epoch total loss 0.550154269\n",
      "Trained batch 1497 batch loss 0.589613914 epoch total loss 0.550180614\n",
      "Trained batch 1498 batch loss 0.493625224 epoch total loss 0.550142884\n",
      "Trained batch 1499 batch loss 0.479136467 epoch total loss 0.550095499\n",
      "Trained batch 1500 batch loss 0.417903721 epoch total loss 0.550007343\n",
      "Trained batch 1501 batch loss 0.516343772 epoch total loss 0.549984932\n",
      "Trained batch 1502 batch loss 0.590957463 epoch total loss 0.550012231\n",
      "Trained batch 1503 batch loss 0.492527455 epoch total loss 0.549973965\n",
      "Trained batch 1504 batch loss 0.576509833 epoch total loss 0.549991667\n",
      "Trained batch 1505 batch loss 0.545122 epoch total loss 0.549988389\n",
      "Trained batch 1506 batch loss 0.554361761 epoch total loss 0.54999131\n",
      "Trained batch 1507 batch loss 0.565041065 epoch total loss 0.550001323\n",
      "Trained batch 1508 batch loss 0.502060354 epoch total loss 0.549969554\n",
      "Trained batch 1509 batch loss 0.530804396 epoch total loss 0.549956858\n",
      "Trained batch 1510 batch loss 0.503447354 epoch total loss 0.549926043\n",
      "Trained batch 1511 batch loss 0.500389159 epoch total loss 0.54989326\n",
      "Trained batch 1512 batch loss 0.583496809 epoch total loss 0.549915433\n",
      "Trained batch 1513 batch loss 0.635235369 epoch total loss 0.549971879\n",
      "Trained batch 1514 batch loss 0.692939162 epoch total loss 0.550066292\n",
      "Trained batch 1515 batch loss 0.550291538 epoch total loss 0.550066411\n",
      "Trained batch 1516 batch loss 0.578273058 epoch total loss 0.550085\n",
      "Trained batch 1517 batch loss 0.58742 epoch total loss 0.550109625\n",
      "Trained batch 1518 batch loss 0.537431 epoch total loss 0.55010128\n",
      "Trained batch 1519 batch loss 0.487756699 epoch total loss 0.550060213\n",
      "Trained batch 1520 batch loss 0.571208715 epoch total loss 0.5500741\n",
      "Trained batch 1521 batch loss 0.585265815 epoch total loss 0.550097287\n",
      "Trained batch 1522 batch loss 0.583412826 epoch total loss 0.550119162\n",
      "Trained batch 1523 batch loss 0.567118526 epoch total loss 0.550130367\n",
      "Trained batch 1524 batch loss 0.378640413 epoch total loss 0.550017834\n",
      "Trained batch 1525 batch loss 0.406947851 epoch total loss 0.549924\n",
      "Trained batch 1526 batch loss 0.405862868 epoch total loss 0.549829602\n",
      "Trained batch 1527 batch loss 0.417617291 epoch total loss 0.549743\n",
      "Trained batch 1528 batch loss 0.422418475 epoch total loss 0.549659669\n",
      "Trained batch 1529 batch loss 0.514696121 epoch total loss 0.549636841\n",
      "Trained batch 1530 batch loss 0.495496273 epoch total loss 0.549601436\n",
      "Trained batch 1531 batch loss 0.531727493 epoch total loss 0.549589753\n",
      "Trained batch 1532 batch loss 0.555721462 epoch total loss 0.549593747\n",
      "Trained batch 1533 batch loss 0.548295677 epoch total loss 0.549592912\n",
      "Trained batch 1534 batch loss 0.645346224 epoch total loss 0.549655318\n",
      "Trained batch 1535 batch loss 0.503846407 epoch total loss 0.549625456\n",
      "Trained batch 1536 batch loss 0.478937507 epoch total loss 0.549579442\n",
      "Trained batch 1537 batch loss 0.510948062 epoch total loss 0.549554288\n",
      "Trained batch 1538 batch loss 0.573583961 epoch total loss 0.549569964\n",
      "Trained batch 1539 batch loss 0.611196935 epoch total loss 0.54961\n",
      "Trained batch 1540 batch loss 0.627151668 epoch total loss 0.549660325\n",
      "Trained batch 1541 batch loss 0.604893386 epoch total loss 0.549696207\n",
      "Trained batch 1542 batch loss 0.557410598 epoch total loss 0.549701214\n",
      "Trained batch 1543 batch loss 0.663175285 epoch total loss 0.549774766\n",
      "Trained batch 1544 batch loss 0.608335376 epoch total loss 0.549812675\n",
      "Trained batch 1545 batch loss 0.493575752 epoch total loss 0.549776256\n",
      "Trained batch 1546 batch loss 0.533250093 epoch total loss 0.549765587\n",
      "Trained batch 1547 batch loss 0.472010702 epoch total loss 0.54971534\n",
      "Trained batch 1548 batch loss 0.510406435 epoch total loss 0.549689889\n",
      "Trained batch 1549 batch loss 0.557837129 epoch total loss 0.549695194\n",
      "Trained batch 1550 batch loss 0.526339173 epoch total loss 0.549680114\n",
      "Trained batch 1551 batch loss 0.59971416 epoch total loss 0.54971242\n",
      "Trained batch 1552 batch loss 0.606078207 epoch total loss 0.549748719\n",
      "Trained batch 1553 batch loss 0.652506828 epoch total loss 0.54981488\n",
      "Trained batch 1554 batch loss 0.623162687 epoch total loss 0.549862087\n",
      "Trained batch 1555 batch loss 0.611860633 epoch total loss 0.549901962\n",
      "Trained batch 1556 batch loss 0.599642098 epoch total loss 0.54993397\n",
      "Trained batch 1557 batch loss 0.587030649 epoch total loss 0.549957812\n",
      "Trained batch 1558 batch loss 0.595722437 epoch total loss 0.549987137\n",
      "Trained batch 1559 batch loss 0.530481637 epoch total loss 0.54997462\n",
      "Trained batch 1560 batch loss 0.515616238 epoch total loss 0.549952626\n",
      "Trained batch 1561 batch loss 0.531135857 epoch total loss 0.549940586\n",
      "Trained batch 1562 batch loss 0.502674758 epoch total loss 0.549910307\n",
      "Trained batch 1563 batch loss 0.562344372 epoch total loss 0.549918234\n",
      "Trained batch 1564 batch loss 0.550738811 epoch total loss 0.549918771\n",
      "Trained batch 1565 batch loss 0.571759343 epoch total loss 0.549932718\n",
      "Trained batch 1566 batch loss 0.555123389 epoch total loss 0.549936056\n",
      "Trained batch 1567 batch loss 0.558460772 epoch total loss 0.54994148\n",
      "Trained batch 1568 batch loss 0.430647075 epoch total loss 0.549865425\n",
      "Trained batch 1569 batch loss 0.531845927 epoch total loss 0.549853921\n",
      "Trained batch 1570 batch loss 0.466342956 epoch total loss 0.549800754\n",
      "Trained batch 1571 batch loss 0.478668183 epoch total loss 0.549755454\n",
      "Trained batch 1572 batch loss 0.514952421 epoch total loss 0.549733341\n",
      "Trained batch 1573 batch loss 0.467927754 epoch total loss 0.549681365\n",
      "Trained batch 1574 batch loss 0.472378969 epoch total loss 0.549632192\n",
      "Trained batch 1575 batch loss 0.533852398 epoch total loss 0.549622178\n",
      "Trained batch 1576 batch loss 0.432127744 epoch total loss 0.549547672\n",
      "Trained batch 1577 batch loss 0.528075278 epoch total loss 0.549534\n",
      "Trained batch 1578 batch loss 0.488939822 epoch total loss 0.549495637\n",
      "Trained batch 1579 batch loss 0.554027557 epoch total loss 0.549498498\n",
      "Trained batch 1580 batch loss 0.404975355 epoch total loss 0.549407\n",
      "Trained batch 1581 batch loss 0.533918738 epoch total loss 0.54939723\n",
      "Trained batch 1582 batch loss 0.475738 epoch total loss 0.549350679\n",
      "Trained batch 1583 batch loss 0.493767291 epoch total loss 0.549315572\n",
      "Trained batch 1584 batch loss 0.55453831 epoch total loss 0.54931885\n",
      "Trained batch 1585 batch loss 0.42817685 epoch total loss 0.549242437\n",
      "Trained batch 1586 batch loss 0.49232778 epoch total loss 0.549206555\n",
      "Trained batch 1587 batch loss 0.515172184 epoch total loss 0.549185097\n",
      "Trained batch 1588 batch loss 0.535338402 epoch total loss 0.549176395\n",
      "Trained batch 1589 batch loss 0.580801785 epoch total loss 0.549196303\n",
      "Trained batch 1590 batch loss 0.617940485 epoch total loss 0.549239516\n",
      "Trained batch 1591 batch loss 0.556997895 epoch total loss 0.549244404\n",
      "Trained batch 1592 batch loss 0.526060641 epoch total loss 0.54922986\n",
      "Trained batch 1593 batch loss 0.644979477 epoch total loss 0.549289942\n",
      "Trained batch 1594 batch loss 0.598361075 epoch total loss 0.549320757\n",
      "Trained batch 1595 batch loss 0.593530059 epoch total loss 0.549348414\n",
      "Trained batch 1596 batch loss 0.578745425 epoch total loss 0.549366832\n",
      "Trained batch 1597 batch loss 0.515360296 epoch total loss 0.549345553\n",
      "Trained batch 1598 batch loss 0.53200233 epoch total loss 0.549334705\n",
      "Trained batch 1599 batch loss 0.580873 epoch total loss 0.549354434\n",
      "Trained batch 1600 batch loss 0.515393615 epoch total loss 0.549333215\n",
      "Trained batch 1601 batch loss 0.588853717 epoch total loss 0.549357891\n",
      "Trained batch 1602 batch loss 0.540118515 epoch total loss 0.549352109\n",
      "Trained batch 1603 batch loss 0.502807498 epoch total loss 0.549323082\n",
      "Trained batch 1604 batch loss 0.504164338 epoch total loss 0.549294889\n",
      "Trained batch 1605 batch loss 0.510480762 epoch total loss 0.549270749\n",
      "Trained batch 1606 batch loss 0.428642243 epoch total loss 0.549195647\n",
      "Trained batch 1607 batch loss 0.523147106 epoch total loss 0.549179435\n",
      "Trained batch 1608 batch loss 0.486776888 epoch total loss 0.549140573\n",
      "Trained batch 1609 batch loss 0.541874 epoch total loss 0.549136043\n",
      "Trained batch 1610 batch loss 0.566465557 epoch total loss 0.549146831\n",
      "Trained batch 1611 batch loss 0.559525609 epoch total loss 0.549153268\n",
      "Trained batch 1612 batch loss 0.595186949 epoch total loss 0.549181819\n",
      "Trained batch 1613 batch loss 0.573579788 epoch total loss 0.549196959\n",
      "Trained batch 1614 batch loss 0.580261171 epoch total loss 0.549216211\n",
      "Trained batch 1615 batch loss 0.575976193 epoch total loss 0.549232781\n",
      "Trained batch 1616 batch loss 0.579735696 epoch total loss 0.549251676\n",
      "Trained batch 1617 batch loss 0.55665189 epoch total loss 0.549256265\n",
      "Trained batch 1618 batch loss 0.528697968 epoch total loss 0.54924351\n",
      "Trained batch 1619 batch loss 0.563416839 epoch total loss 0.549252272\n",
      "Trained batch 1620 batch loss 0.576722443 epoch total loss 0.549269259\n",
      "Trained batch 1621 batch loss 0.662411094 epoch total loss 0.549339056\n",
      "Trained batch 1622 batch loss 0.592486084 epoch total loss 0.54936564\n",
      "Trained batch 1623 batch loss 0.586307 epoch total loss 0.549388409\n",
      "Trained batch 1624 batch loss 0.553181767 epoch total loss 0.549390733\n",
      "Trained batch 1625 batch loss 0.619057834 epoch total loss 0.549433589\n",
      "Trained batch 1626 batch loss 0.592639744 epoch total loss 0.549460173\n",
      "Trained batch 1627 batch loss 0.653163135 epoch total loss 0.54952389\n",
      "Trained batch 1628 batch loss 0.568740904 epoch total loss 0.549535692\n",
      "Trained batch 1629 batch loss 0.629508 epoch total loss 0.549584806\n",
      "Trained batch 1630 batch loss 0.582230747 epoch total loss 0.549604833\n",
      "Trained batch 1631 batch loss 0.576372385 epoch total loss 0.549621224\n",
      "Trained batch 1632 batch loss 0.615885675 epoch total loss 0.549661815\n",
      "Trained batch 1633 batch loss 0.560101867 epoch total loss 0.549668252\n",
      "Trained batch 1634 batch loss 0.541718 epoch total loss 0.549663365\n",
      "Trained batch 1635 batch loss 0.585146368 epoch total loss 0.549685061\n",
      "Trained batch 1636 batch loss 0.541039526 epoch total loss 0.549679756\n",
      "Trained batch 1637 batch loss 0.561196387 epoch total loss 0.549686849\n",
      "Trained batch 1638 batch loss 0.621139228 epoch total loss 0.54973048\n",
      "Trained batch 1639 batch loss 0.503318429 epoch total loss 0.549702108\n",
      "Trained batch 1640 batch loss 0.540635 epoch total loss 0.549696624\n",
      "Trained batch 1641 batch loss 0.564018846 epoch total loss 0.549705327\n",
      "Trained batch 1642 batch loss 0.608822465 epoch total loss 0.549741328\n",
      "Trained batch 1643 batch loss 0.574263215 epoch total loss 0.549756289\n",
      "Trained batch 1644 batch loss 0.597806156 epoch total loss 0.549785495\n",
      "Trained batch 1645 batch loss 0.499481261 epoch total loss 0.549754918\n",
      "Trained batch 1646 batch loss 0.45974192 epoch total loss 0.54970026\n",
      "Trained batch 1647 batch loss 0.523682654 epoch total loss 0.549684405\n",
      "Trained batch 1648 batch loss 0.513079345 epoch total loss 0.549662232\n",
      "Trained batch 1649 batch loss 0.530374229 epoch total loss 0.54965055\n",
      "Trained batch 1650 batch loss 0.514189243 epoch total loss 0.549629033\n",
      "Trained batch 1651 batch loss 0.545201361 epoch total loss 0.54962635\n",
      "Trained batch 1652 batch loss 0.464849055 epoch total loss 0.549575031\n",
      "Trained batch 1653 batch loss 0.491787225 epoch total loss 0.549540043\n",
      "Trained batch 1654 batch loss 0.528925419 epoch total loss 0.549527586\n",
      "Trained batch 1655 batch loss 0.514411807 epoch total loss 0.549506366\n",
      "Trained batch 1656 batch loss 0.517290533 epoch total loss 0.549486935\n",
      "Trained batch 1657 batch loss 0.551477551 epoch total loss 0.549488068\n",
      "Trained batch 1658 batch loss 0.520279408 epoch total loss 0.549470484\n",
      "Trained batch 1659 batch loss 0.479255974 epoch total loss 0.549428165\n",
      "Trained batch 1660 batch loss 0.49539268 epoch total loss 0.549395621\n",
      "Trained batch 1661 batch loss 0.679670751 epoch total loss 0.549474061\n",
      "Trained batch 1662 batch loss 0.536165357 epoch total loss 0.549466074\n",
      "Trained batch 1663 batch loss 0.540220618 epoch total loss 0.549460471\n",
      "Trained batch 1664 batch loss 0.56421113 epoch total loss 0.549469352\n",
      "Trained batch 1665 batch loss 0.524283707 epoch total loss 0.549454212\n",
      "Trained batch 1666 batch loss 0.483851969 epoch total loss 0.549414873\n",
      "Trained batch 1667 batch loss 0.397735208 epoch total loss 0.549323857\n",
      "Trained batch 1668 batch loss 0.369722545 epoch total loss 0.549216211\n",
      "Trained batch 1669 batch loss 0.40643084 epoch total loss 0.549130619\n",
      "Trained batch 1670 batch loss 0.473846376 epoch total loss 0.549085557\n",
      "Trained batch 1671 batch loss 0.525572956 epoch total loss 0.549071491\n",
      "Trained batch 1672 batch loss 0.59634614 epoch total loss 0.549099743\n",
      "Trained batch 1673 batch loss 0.62700206 epoch total loss 0.549146354\n",
      "Trained batch 1674 batch loss 0.556189 epoch total loss 0.549150527\n",
      "Trained batch 1675 batch loss 0.565006554 epoch total loss 0.54916\n",
      "Trained batch 1676 batch loss 0.525620937 epoch total loss 0.549146\n",
      "Trained batch 1677 batch loss 0.588096201 epoch total loss 0.549169183\n",
      "Trained batch 1678 batch loss 0.543785334 epoch total loss 0.549165964\n",
      "Trained batch 1679 batch loss 0.566551924 epoch total loss 0.549176276\n",
      "Trained batch 1680 batch loss 0.482295245 epoch total loss 0.549136519\n",
      "Trained batch 1681 batch loss 0.483427763 epoch total loss 0.549097419\n",
      "Trained batch 1682 batch loss 0.476691544 epoch total loss 0.549054325\n",
      "Trained batch 1683 batch loss 0.474810153 epoch total loss 0.549010217\n",
      "Trained batch 1684 batch loss 0.540020764 epoch total loss 0.549004912\n",
      "Trained batch 1685 batch loss 0.443189055 epoch total loss 0.548942089\n",
      "Trained batch 1686 batch loss 0.536123335 epoch total loss 0.54893446\n",
      "Trained batch 1687 batch loss 0.531648755 epoch total loss 0.548924267\n",
      "Trained batch 1688 batch loss 0.580839574 epoch total loss 0.548943162\n",
      "Trained batch 1689 batch loss 0.645437121 epoch total loss 0.549000263\n",
      "Trained batch 1690 batch loss 0.476809561 epoch total loss 0.548957586\n",
      "Trained batch 1691 batch loss 0.512087464 epoch total loss 0.548935771\n",
      "Trained batch 1692 batch loss 0.559055805 epoch total loss 0.548941731\n",
      "Trained batch 1693 batch loss 0.478822082 epoch total loss 0.548900366\n",
      "Trained batch 1694 batch loss 0.583258331 epoch total loss 0.548920631\n",
      "Trained batch 1695 batch loss 0.489285111 epoch total loss 0.548885405\n",
      "Trained batch 1696 batch loss 0.6552881 epoch total loss 0.548948169\n",
      "Trained batch 1697 batch loss 0.629455805 epoch total loss 0.548995614\n",
      "Trained batch 1698 batch loss 0.505377412 epoch total loss 0.548969865\n",
      "Trained batch 1699 batch loss 0.530243218 epoch total loss 0.548958898\n",
      "Trained batch 1700 batch loss 0.639347851 epoch total loss 0.549012065\n",
      "Trained batch 1701 batch loss 0.71613133 epoch total loss 0.549110293\n",
      "Trained batch 1702 batch loss 0.630238 epoch total loss 0.549158\n",
      "Trained batch 1703 batch loss 0.601316273 epoch total loss 0.549188614\n",
      "Trained batch 1704 batch loss 0.660156488 epoch total loss 0.549253702\n",
      "Trained batch 1705 batch loss 0.720751762 epoch total loss 0.549354315\n",
      "Trained batch 1706 batch loss 0.719260395 epoch total loss 0.549453914\n",
      "Trained batch 1707 batch loss 0.593828499 epoch total loss 0.549479902\n",
      "Trained batch 1708 batch loss 0.478975117 epoch total loss 0.549438596\n",
      "Trained batch 1709 batch loss 0.488438874 epoch total loss 0.549402952\n",
      "Trained batch 1710 batch loss 0.46307677 epoch total loss 0.549352467\n",
      "Trained batch 1711 batch loss 0.558079123 epoch total loss 0.549357593\n",
      "Trained batch 1712 batch loss 0.619519591 epoch total loss 0.549398541\n",
      "Trained batch 1713 batch loss 0.562395394 epoch total loss 0.549406111\n",
      "Trained batch 1714 batch loss 0.49914217 epoch total loss 0.549376786\n",
      "Trained batch 1715 batch loss 0.466970265 epoch total loss 0.549328744\n",
      "Trained batch 1716 batch loss 0.473413378 epoch total loss 0.549284518\n",
      "Trained batch 1717 batch loss 0.468630642 epoch total loss 0.549237549\n",
      "Trained batch 1718 batch loss 0.503378332 epoch total loss 0.549210846\n",
      "Trained batch 1719 batch loss 0.486245394 epoch total loss 0.54917419\n",
      "Trained batch 1720 batch loss 0.466436803 epoch total loss 0.549126089\n",
      "Trained batch 1721 batch loss 0.509504497 epoch total loss 0.549103081\n",
      "Trained batch 1722 batch loss 0.511300802 epoch total loss 0.549081147\n",
      "Trained batch 1723 batch loss 0.422706962 epoch total loss 0.549007773\n",
      "Trained batch 1724 batch loss 0.416523367 epoch total loss 0.548930943\n",
      "Trained batch 1725 batch loss 0.398041785 epoch total loss 0.548843503\n",
      "Trained batch 1726 batch loss 0.46090585 epoch total loss 0.548792541\n",
      "Trained batch 1727 batch loss 0.550548315 epoch total loss 0.548793554\n",
      "Trained batch 1728 batch loss 0.528856814 epoch total loss 0.548782\n",
      "Trained batch 1729 batch loss 0.518980265 epoch total loss 0.548764765\n",
      "Trained batch 1730 batch loss 0.536234438 epoch total loss 0.548757553\n",
      "Trained batch 1731 batch loss 0.493552804 epoch total loss 0.548725605\n",
      "Trained batch 1732 batch loss 0.443049908 epoch total loss 0.548664629\n",
      "Trained batch 1733 batch loss 0.493409425 epoch total loss 0.548632741\n",
      "Trained batch 1734 batch loss 0.444711447 epoch total loss 0.548572779\n",
      "Trained batch 1735 batch loss 0.495065689 epoch total loss 0.548541963\n",
      "Trained batch 1736 batch loss 0.559238315 epoch total loss 0.548548102\n",
      "Trained batch 1737 batch loss 0.593125582 epoch total loss 0.548573792\n",
      "Trained batch 1738 batch loss 0.665016472 epoch total loss 0.548640788\n",
      "Trained batch 1739 batch loss 0.524539948 epoch total loss 0.548626959\n",
      "Trained batch 1740 batch loss 0.571442544 epoch total loss 0.548640072\n",
      "Trained batch 1741 batch loss 0.567312 epoch total loss 0.548650801\n",
      "Trained batch 1742 batch loss 0.643982172 epoch total loss 0.548705518\n",
      "Trained batch 1743 batch loss 0.589477599 epoch total loss 0.548728943\n",
      "Trained batch 1744 batch loss 0.487074912 epoch total loss 0.548693538\n",
      "Trained batch 1745 batch loss 0.548550129 epoch total loss 0.548693478\n",
      "Trained batch 1746 batch loss 0.5641886 epoch total loss 0.548702359\n",
      "Trained batch 1747 batch loss 0.604723394 epoch total loss 0.548734426\n",
      "Trained batch 1748 batch loss 0.60939759 epoch total loss 0.548769116\n",
      "Trained batch 1749 batch loss 0.534593523 epoch total loss 0.548761\n",
      "Trained batch 1750 batch loss 0.484713495 epoch total loss 0.548724413\n",
      "Trained batch 1751 batch loss 0.53194344 epoch total loss 0.548714817\n",
      "Trained batch 1752 batch loss 0.469825029 epoch total loss 0.548669815\n",
      "Trained batch 1753 batch loss 0.523572505 epoch total loss 0.54865551\n",
      "Trained batch 1754 batch loss 0.500278533 epoch total loss 0.548627913\n",
      "Trained batch 1755 batch loss 0.566407502 epoch total loss 0.548638046\n",
      "Trained batch 1756 batch loss 0.572154582 epoch total loss 0.548651457\n",
      "Trained batch 1757 batch loss 0.606449 epoch total loss 0.548684359\n",
      "Trained batch 1758 batch loss 0.59668541 epoch total loss 0.548711658\n",
      "Trained batch 1759 batch loss 0.629951596 epoch total loss 0.548757851\n",
      "Trained batch 1760 batch loss 0.598578572 epoch total loss 0.548786104\n",
      "Trained batch 1761 batch loss 0.594489694 epoch total loss 0.548812091\n",
      "Trained batch 1762 batch loss 0.552451909 epoch total loss 0.548814118\n",
      "Trained batch 1763 batch loss 0.568927526 epoch total loss 0.548825502\n",
      "Trained batch 1764 batch loss 0.497003675 epoch total loss 0.548796177\n",
      "Trained batch 1765 batch loss 0.555465758 epoch total loss 0.548799932\n",
      "Trained batch 1766 batch loss 0.603546739 epoch total loss 0.548831\n",
      "Trained batch 1767 batch loss 0.516443074 epoch total loss 0.548812628\n",
      "Trained batch 1768 batch loss 0.513988554 epoch total loss 0.548792899\n",
      "Trained batch 1769 batch loss 0.502330899 epoch total loss 0.548766613\n",
      "Trained batch 1770 batch loss 0.484393537 epoch total loss 0.548730254\n",
      "Trained batch 1771 batch loss 0.505659044 epoch total loss 0.548705935\n",
      "Trained batch 1772 batch loss 0.508054376 epoch total loss 0.548683\n",
      "Trained batch 1773 batch loss 0.594280124 epoch total loss 0.548708737\n",
      "Trained batch 1774 batch loss 0.545922935 epoch total loss 0.548707128\n",
      "Trained batch 1775 batch loss 0.652319372 epoch total loss 0.54876554\n",
      "Trained batch 1776 batch loss 0.596336782 epoch total loss 0.548792303\n",
      "Trained batch 1777 batch loss 0.538628936 epoch total loss 0.548786581\n",
      "Trained batch 1778 batch loss 0.543718457 epoch total loss 0.54878372\n",
      "Trained batch 1779 batch loss 0.461581081 epoch total loss 0.548734725\n",
      "Trained batch 1780 batch loss 0.43660149 epoch total loss 0.548671722\n",
      "Trained batch 1781 batch loss 0.426524073 epoch total loss 0.548603117\n",
      "Trained batch 1782 batch loss 0.447036505 epoch total loss 0.548546135\n",
      "Trained batch 1783 batch loss 0.44832021 epoch total loss 0.548489928\n",
      "Trained batch 1784 batch loss 0.479842961 epoch total loss 0.548451424\n",
      "Trained batch 1785 batch loss 0.559423149 epoch total loss 0.548457623\n",
      "Trained batch 1786 batch loss 0.629746914 epoch total loss 0.548503101\n",
      "Trained batch 1787 batch loss 0.608568192 epoch total loss 0.548536777\n",
      "Trained batch 1788 batch loss 0.670787752 epoch total loss 0.548605144\n",
      "Trained batch 1789 batch loss 0.730686188 epoch total loss 0.548706889\n",
      "Trained batch 1790 batch loss 0.734702766 epoch total loss 0.54881078\n",
      "Trained batch 1791 batch loss 0.649505138 epoch total loss 0.548867\n",
      "Trained batch 1792 batch loss 0.517801285 epoch total loss 0.548849702\n",
      "Trained batch 1793 batch loss 0.489149928 epoch total loss 0.548816383\n",
      "Trained batch 1794 batch loss 0.498977304 epoch total loss 0.548788607\n",
      "Trained batch 1795 batch loss 0.558882594 epoch total loss 0.54879421\n",
      "Trained batch 1796 batch loss 0.514829755 epoch total loss 0.548775315\n",
      "Trained batch 1797 batch loss 0.557317615 epoch total loss 0.548780084\n",
      "Trained batch 1798 batch loss 0.574605584 epoch total loss 0.548794389\n",
      "Trained batch 1799 batch loss 0.62918365 epoch total loss 0.548839092\n",
      "Trained batch 1800 batch loss 0.589606762 epoch total loss 0.548861742\n",
      "Trained batch 1801 batch loss 0.580871463 epoch total loss 0.548879564\n",
      "Trained batch 1802 batch loss 0.579476595 epoch total loss 0.548896492\n",
      "Trained batch 1803 batch loss 0.565996885 epoch total loss 0.548905969\n",
      "Trained batch 1804 batch loss 0.563042939 epoch total loss 0.548913836\n",
      "Trained batch 1805 batch loss 0.595686853 epoch total loss 0.548939764\n",
      "Trained batch 1806 batch loss 0.586339474 epoch total loss 0.548960447\n",
      "Trained batch 1807 batch loss 0.606238246 epoch total loss 0.548992157\n",
      "Trained batch 1808 batch loss 0.51585114 epoch total loss 0.548973858\n",
      "Trained batch 1809 batch loss 0.513802171 epoch total loss 0.548954427\n",
      "Trained batch 1810 batch loss 0.518113792 epoch total loss 0.54893738\n",
      "Trained batch 1811 batch loss 0.501924276 epoch total loss 0.548911452\n",
      "Trained batch 1812 batch loss 0.479387403 epoch total loss 0.548873067\n",
      "Trained batch 1813 batch loss 0.437910616 epoch total loss 0.548811853\n",
      "Trained batch 1814 batch loss 0.506188691 epoch total loss 0.548788369\n",
      "Trained batch 1815 batch loss 0.534203291 epoch total loss 0.548780322\n",
      "Trained batch 1816 batch loss 0.521330059 epoch total loss 0.548765182\n",
      "Trained batch 1817 batch loss 0.615598 epoch total loss 0.548801959\n",
      "Trained batch 1818 batch loss 0.586116374 epoch total loss 0.548822463\n",
      "Trained batch 1819 batch loss 0.513343453 epoch total loss 0.548803\n",
      "Trained batch 1820 batch loss 0.406667471 epoch total loss 0.54872489\n",
      "Trained batch 1821 batch loss 0.452995062 epoch total loss 0.548672318\n",
      "Trained batch 1822 batch loss 0.535418093 epoch total loss 0.548665047\n",
      "Trained batch 1823 batch loss 0.595863938 epoch total loss 0.548691\n",
      "Trained batch 1824 batch loss 0.60779053 epoch total loss 0.54872334\n",
      "Trained batch 1825 batch loss 0.580212414 epoch total loss 0.548740625\n",
      "Trained batch 1826 batch loss 0.5967803 epoch total loss 0.548766911\n",
      "Trained batch 1827 batch loss 0.57586205 epoch total loss 0.548781753\n",
      "Trained batch 1828 batch loss 0.596737564 epoch total loss 0.548808\n",
      "Trained batch 1829 batch loss 0.61516577 epoch total loss 0.548844278\n",
      "Trained batch 1830 batch loss 0.577916205 epoch total loss 0.548860192\n",
      "Trained batch 1831 batch loss 0.554992139 epoch total loss 0.54886353\n",
      "Trained batch 1832 batch loss 0.610028267 epoch total loss 0.548896909\n",
      "Trained batch 1833 batch loss 0.629667103 epoch total loss 0.548940957\n",
      "Trained batch 1834 batch loss 0.574459553 epoch total loss 0.548954904\n",
      "Trained batch 1835 batch loss 0.472438216 epoch total loss 0.548913181\n",
      "Trained batch 1836 batch loss 0.464406341 epoch total loss 0.548867166\n",
      "Trained batch 1837 batch loss 0.512785196 epoch total loss 0.548847497\n",
      "Trained batch 1838 batch loss 0.577170849 epoch total loss 0.548862875\n",
      "Trained batch 1839 batch loss 0.45334655 epoch total loss 0.548810959\n",
      "Trained batch 1840 batch loss 0.51166141 epoch total loss 0.548790753\n",
      "Trained batch 1841 batch loss 0.464500219 epoch total loss 0.548745\n",
      "Trained batch 1842 batch loss 0.447500408 epoch total loss 0.54869\n",
      "Trained batch 1843 batch loss 0.406107843 epoch total loss 0.548612654\n",
      "Trained batch 1844 batch loss 0.556562662 epoch total loss 0.548617\n",
      "Trained batch 1845 batch loss 0.506993711 epoch total loss 0.548594415\n",
      "Trained batch 1846 batch loss 0.513595283 epoch total loss 0.548575461\n",
      "Trained batch 1847 batch loss 0.497133553 epoch total loss 0.548547626\n",
      "Trained batch 1848 batch loss 0.441490173 epoch total loss 0.54848969\n",
      "Trained batch 1849 batch loss 0.365456372 epoch total loss 0.548390687\n",
      "Trained batch 1850 batch loss 0.393473357 epoch total loss 0.548307\n",
      "Trained batch 1851 batch loss 0.41965425 epoch total loss 0.548237503\n",
      "Trained batch 1852 batch loss 0.417758554 epoch total loss 0.54816705\n",
      "Trained batch 1853 batch loss 0.411275655 epoch total loss 0.54809314\n",
      "Trained batch 1854 batch loss 0.402871609 epoch total loss 0.548014879\n",
      "Trained batch 1855 batch loss 0.430010796 epoch total loss 0.547951221\n",
      "Trained batch 1856 batch loss 0.568210602 epoch total loss 0.547962129\n",
      "Trained batch 1857 batch loss 0.57409519 epoch total loss 0.547976255\n",
      "Trained batch 1858 batch loss 0.564238489 epoch total loss 0.547984958\n",
      "Trained batch 1859 batch loss 0.515306771 epoch total loss 0.547967374\n",
      "Trained batch 1860 batch loss 0.526788533 epoch total loss 0.547956\n",
      "Trained batch 1861 batch loss 0.46602 epoch total loss 0.547912\n",
      "Trained batch 1862 batch loss 0.552045166 epoch total loss 0.547914207\n",
      "Trained batch 1863 batch loss 0.461552858 epoch total loss 0.547867835\n",
      "Trained batch 1864 batch loss 0.525887728 epoch total loss 0.547856033\n",
      "Trained batch 1865 batch loss 0.604797482 epoch total loss 0.54788655\n",
      "Trained batch 1866 batch loss 0.689421713 epoch total loss 0.547962427\n",
      "Trained batch 1867 batch loss 0.61018312 epoch total loss 0.547995746\n",
      "Trained batch 1868 batch loss 0.614791334 epoch total loss 0.548031509\n",
      "Trained batch 1869 batch loss 0.508532643 epoch total loss 0.548010349\n",
      "Trained batch 1870 batch loss 0.531496942 epoch total loss 0.548001528\n",
      "Trained batch 1871 batch loss 0.521737278 epoch total loss 0.547987461\n",
      "Trained batch 1872 batch loss 0.530416191 epoch total loss 0.547978044\n",
      "Trained batch 1873 batch loss 0.572998524 epoch total loss 0.547991455\n",
      "Trained batch 1874 batch loss 0.578988373 epoch total loss 0.548007965\n",
      "Trained batch 1875 batch loss 0.632810473 epoch total loss 0.548053205\n",
      "Trained batch 1876 batch loss 0.535601676 epoch total loss 0.548046589\n",
      "Trained batch 1877 batch loss 0.599042952 epoch total loss 0.548073709\n",
      "Trained batch 1878 batch loss 0.534405708 epoch total loss 0.548066437\n",
      "Trained batch 1879 batch loss 0.565220475 epoch total loss 0.548075557\n",
      "Trained batch 1880 batch loss 0.561336219 epoch total loss 0.54808259\n",
      "Trained batch 1881 batch loss 0.566949368 epoch total loss 0.548092604\n",
      "Trained batch 1882 batch loss 0.677749753 epoch total loss 0.548161447\n",
      "Trained batch 1883 batch loss 0.564377308 epoch total loss 0.54817009\n",
      "Trained batch 1884 batch loss 0.565085173 epoch total loss 0.54817903\n",
      "Trained batch 1885 batch loss 0.523639321 epoch total loss 0.548166037\n",
      "Trained batch 1886 batch loss 0.544167161 epoch total loss 0.54816395\n",
      "Trained batch 1887 batch loss 0.550000966 epoch total loss 0.548164904\n",
      "Trained batch 1888 batch loss 0.555373549 epoch total loss 0.548168778\n",
      "Trained batch 1889 batch loss 0.555375576 epoch total loss 0.548172593\n",
      "Trained batch 1890 batch loss 0.620682895 epoch total loss 0.548211\n",
      "Trained batch 1891 batch loss 0.612910151 epoch total loss 0.548245192\n",
      "Trained batch 1892 batch loss 0.482817173 epoch total loss 0.548210621\n",
      "Trained batch 1893 batch loss 0.596691132 epoch total loss 0.548236191\n",
      "Trained batch 1894 batch loss 0.454825401 epoch total loss 0.548186898\n",
      "Trained batch 1895 batch loss 0.492566288 epoch total loss 0.548157573\n",
      "Trained batch 1896 batch loss 0.457172781 epoch total loss 0.548109531\n",
      "Trained batch 1897 batch loss 0.440712333 epoch total loss 0.548052907\n",
      "Trained batch 1898 batch loss 0.46997419 epoch total loss 0.54801178\n",
      "Trained batch 1899 batch loss 0.452641487 epoch total loss 0.547961533\n",
      "Trained batch 1900 batch loss 0.454075873 epoch total loss 0.547912121\n",
      "Trained batch 1901 batch loss 0.485696256 epoch total loss 0.547879457\n",
      "Trained batch 1902 batch loss 0.558263898 epoch total loss 0.547884881\n",
      "Trained batch 1903 batch loss 0.556682467 epoch total loss 0.547889471\n",
      "Trained batch 1904 batch loss 0.514704347 epoch total loss 0.547872\n",
      "Trained batch 1905 batch loss 0.544934034 epoch total loss 0.547870457\n",
      "Trained batch 1906 batch loss 0.386100203 epoch total loss 0.54778558\n",
      "Trained batch 1907 batch loss 0.441381484 epoch total loss 0.54772979\n",
      "Trained batch 1908 batch loss 0.518808782 epoch total loss 0.547714651\n",
      "Trained batch 1909 batch loss 0.58785671 epoch total loss 0.547735691\n",
      "Trained batch 1910 batch loss 0.587788 epoch total loss 0.547756672\n",
      "Trained batch 1911 batch loss 0.695533872 epoch total loss 0.547834\n",
      "Trained batch 1912 batch loss 0.648258 epoch total loss 0.54788655\n",
      "Trained batch 1913 batch loss 0.591421187 epoch total loss 0.547909319\n",
      "Trained batch 1914 batch loss 0.628325462 epoch total loss 0.547951281\n",
      "Trained batch 1915 batch loss 0.611947894 epoch total loss 0.547984719\n",
      "Trained batch 1916 batch loss 0.551718235 epoch total loss 0.547986686\n",
      "Trained batch 1917 batch loss 0.510209262 epoch total loss 0.547967\n",
      "Trained batch 1918 batch loss 0.589379311 epoch total loss 0.547988594\n",
      "Trained batch 1919 batch loss 0.580306709 epoch total loss 0.548005462\n",
      "Trained batch 1920 batch loss 0.559271336 epoch total loss 0.548011363\n",
      "Trained batch 1921 batch loss 0.567272067 epoch total loss 0.548021376\n",
      "Trained batch 1922 batch loss 0.543335915 epoch total loss 0.548018932\n",
      "Trained batch 1923 batch loss 0.455327064 epoch total loss 0.547970712\n",
      "Trained batch 1924 batch loss 0.601987243 epoch total loss 0.547998786\n",
      "Trained batch 1925 batch loss 0.549232125 epoch total loss 0.547999382\n",
      "Trained batch 1926 batch loss 0.529631257 epoch total loss 0.547989845\n",
      "Trained batch 1927 batch loss 0.622097075 epoch total loss 0.54802829\n",
      "Trained batch 1928 batch loss 0.595301151 epoch total loss 0.548052847\n",
      "Trained batch 1929 batch loss 0.586538076 epoch total loss 0.548072815\n",
      "Trained batch 1930 batch loss 0.539147437 epoch total loss 0.548068166\n",
      "Trained batch 1931 batch loss 0.533744752 epoch total loss 0.548060715\n",
      "Trained batch 1932 batch loss 0.533962488 epoch total loss 0.548053443\n",
      "Trained batch 1933 batch loss 0.568169653 epoch total loss 0.548063815\n",
      "Trained batch 1934 batch loss 0.618616521 epoch total loss 0.548100293\n",
      "Trained batch 1935 batch loss 0.528003335 epoch total loss 0.548089921\n",
      "Trained batch 1936 batch loss 0.531971216 epoch total loss 0.548081577\n",
      "Trained batch 1937 batch loss 0.409733951 epoch total loss 0.54801017\n",
      "Trained batch 1938 batch loss 0.507273674 epoch total loss 0.54798919\n",
      "Trained batch 1939 batch loss 0.50840348 epoch total loss 0.547968805\n",
      "Trained batch 1940 batch loss 0.548396289 epoch total loss 0.547969\n",
      "Trained batch 1941 batch loss 0.541876554 epoch total loss 0.547965825\n",
      "Trained batch 1942 batch loss 0.573669195 epoch total loss 0.547979057\n",
      "Trained batch 1943 batch loss 0.478067279 epoch total loss 0.547943056\n",
      "Trained batch 1944 batch loss 0.485328913 epoch total loss 0.547910869\n",
      "Trained batch 1945 batch loss 0.413412422 epoch total loss 0.547841728\n",
      "Trained batch 1946 batch loss 0.532731712 epoch total loss 0.54783392\n",
      "Trained batch 1947 batch loss 0.457271814 epoch total loss 0.547787428\n",
      "Trained batch 1948 batch loss 0.533536196 epoch total loss 0.547780097\n",
      "Trained batch 1949 batch loss 0.518982887 epoch total loss 0.547765374\n",
      "Trained batch 1950 batch loss 0.587361813 epoch total loss 0.547785699\n",
      "Trained batch 1951 batch loss 0.553947091 epoch total loss 0.547788858\n",
      "Trained batch 1952 batch loss 0.568328083 epoch total loss 0.547799408\n",
      "Trained batch 1953 batch loss 0.528874457 epoch total loss 0.547789752\n",
      "Trained batch 1954 batch loss 0.528115571 epoch total loss 0.547779679\n",
      "Trained batch 1955 batch loss 0.494403094 epoch total loss 0.547752321\n",
      "Trained batch 1956 batch loss 0.495195508 epoch total loss 0.547725499\n",
      "Trained batch 1957 batch loss 0.521953404 epoch total loss 0.547712326\n",
      "Trained batch 1958 batch loss 0.535058141 epoch total loss 0.547705889\n",
      "Trained batch 1959 batch loss 0.502664089 epoch total loss 0.547682881\n",
      "Trained batch 1960 batch loss 0.5126279 epoch total loss 0.54766494\n",
      "Trained batch 1961 batch loss 0.559565663 epoch total loss 0.547671\n",
      "Trained batch 1962 batch loss 0.502298236 epoch total loss 0.547647953\n",
      "Trained batch 1963 batch loss 0.584056437 epoch total loss 0.54766649\n",
      "Trained batch 1964 batch loss 0.524145067 epoch total loss 0.54765451\n",
      "Trained batch 1965 batch loss 0.587656558 epoch total loss 0.547674894\n",
      "Trained batch 1966 batch loss 0.58997947 epoch total loss 0.547696412\n",
      "Trained batch 1967 batch loss 0.64138937 epoch total loss 0.547744036\n",
      "Trained batch 1968 batch loss 0.597281694 epoch total loss 0.547769189\n",
      "Trained batch 1969 batch loss 0.668557346 epoch total loss 0.547830522\n",
      "Trained batch 1970 batch loss 0.563144565 epoch total loss 0.54783833\n",
      "Trained batch 1971 batch loss 0.550466597 epoch total loss 0.547839582\n",
      "Trained batch 1972 batch loss 0.595410883 epoch total loss 0.547863781\n",
      "Trained batch 1973 batch loss 0.505624294 epoch total loss 0.547842324\n",
      "Trained batch 1974 batch loss 0.501623 epoch total loss 0.547818899\n",
      "Trained batch 1975 batch loss 0.495510429 epoch total loss 0.547792435\n",
      "Trained batch 1976 batch loss 0.517618895 epoch total loss 0.547777116\n",
      "Trained batch 1977 batch loss 0.453739643 epoch total loss 0.547729552\n",
      "Trained batch 1978 batch loss 0.531471372 epoch total loss 0.547721326\n",
      "Trained batch 1979 batch loss 0.42461434 epoch total loss 0.547659099\n",
      "Trained batch 1980 batch loss 0.544341922 epoch total loss 0.54765743\n",
      "Trained batch 1981 batch loss 0.583899081 epoch total loss 0.547675669\n",
      "Trained batch 1982 batch loss 0.648943782 epoch total loss 0.54772681\n",
      "Trained batch 1983 batch loss 0.612275302 epoch total loss 0.547759354\n",
      "Trained batch 1984 batch loss 0.586844683 epoch total loss 0.547779\n",
      "Trained batch 1985 batch loss 0.612119555 epoch total loss 0.547811389\n",
      "Trained batch 1986 batch loss 0.581346095 epoch total loss 0.547828257\n",
      "Trained batch 1987 batch loss 0.564203084 epoch total loss 0.547836483\n",
      "Trained batch 1988 batch loss 0.511503875 epoch total loss 0.547818244\n",
      "Trained batch 1989 batch loss 0.532558918 epoch total loss 0.547810555\n",
      "Trained batch 1990 batch loss 0.544643879 epoch total loss 0.547809\n",
      "Trained batch 1991 batch loss 0.534899354 epoch total loss 0.547802508\n",
      "Trained batch 1992 batch loss 0.592560291 epoch total loss 0.547825\n",
      "Trained batch 1993 batch loss 0.486949772 epoch total loss 0.547794402\n",
      "Trained batch 1994 batch loss 0.49305436 epoch total loss 0.547767\n",
      "Trained batch 1995 batch loss 0.468390107 epoch total loss 0.547727168\n",
      "Trained batch 1996 batch loss 0.481977731 epoch total loss 0.547694206\n",
      "Trained batch 1997 batch loss 0.468097568 epoch total loss 0.54765439\n",
      "Trained batch 1998 batch loss 0.585875332 epoch total loss 0.547673464\n",
      "Trained batch 1999 batch loss 0.649537563 epoch total loss 0.547724426\n",
      "Trained batch 2000 batch loss 0.585558355 epoch total loss 0.54774332\n",
      "Trained batch 2001 batch loss 0.588719845 epoch total loss 0.547763824\n",
      "Trained batch 2002 batch loss 0.543708861 epoch total loss 0.547761798\n",
      "Trained batch 2003 batch loss 0.496156394 epoch total loss 0.547736049\n",
      "Trained batch 2004 batch loss 0.569065034 epoch total loss 0.547746718\n",
      "Trained batch 2005 batch loss 0.755796194 epoch total loss 0.54785049\n",
      "Trained batch 2006 batch loss 0.668854475 epoch total loss 0.54791075\n",
      "Trained batch 2007 batch loss 0.745035887 epoch total loss 0.548009\n",
      "Trained batch 2008 batch loss 0.645407557 epoch total loss 0.548057437\n",
      "Trained batch 2009 batch loss 0.573435068 epoch total loss 0.548070133\n",
      "Trained batch 2010 batch loss 0.562324047 epoch total loss 0.548077226\n",
      "Trained batch 2011 batch loss 0.522854626 epoch total loss 0.548064709\n",
      "Trained batch 2012 batch loss 0.543293893 epoch total loss 0.548062325\n",
      "Trained batch 2013 batch loss 0.476576447 epoch total loss 0.5480268\n",
      "Trained batch 2014 batch loss 0.407781273 epoch total loss 0.547957182\n",
      "Trained batch 2015 batch loss 0.372186571 epoch total loss 0.54787\n",
      "Trained batch 2016 batch loss 0.431106895 epoch total loss 0.547812104\n",
      "Trained batch 2017 batch loss 0.528416276 epoch total loss 0.547802508\n",
      "Trained batch 2018 batch loss 0.5222224 epoch total loss 0.547789812\n",
      "Trained batch 2019 batch loss 0.581969142 epoch total loss 0.54780668\n",
      "Trained batch 2020 batch loss 0.433610231 epoch total loss 0.547750175\n",
      "Trained batch 2021 batch loss 0.48154673 epoch total loss 0.547717392\n",
      "Trained batch 2022 batch loss 0.389475137 epoch total loss 0.547639191\n",
      "Trained batch 2023 batch loss 0.39050433 epoch total loss 0.547561526\n",
      "Trained batch 2024 batch loss 0.423200071 epoch total loss 0.547500074\n",
      "Trained batch 2025 batch loss 0.42367968 epoch total loss 0.54743892\n",
      "Trained batch 2026 batch loss 0.445890278 epoch total loss 0.547388852\n",
      "Trained batch 2027 batch loss 0.422925532 epoch total loss 0.547327459\n",
      "Trained batch 2028 batch loss 0.481311917 epoch total loss 0.547294915\n",
      "Trained batch 2029 batch loss 0.574011624 epoch total loss 0.547308\n",
      "Trained batch 2030 batch loss 0.477639765 epoch total loss 0.547273755\n",
      "Trained batch 2031 batch loss 0.463968754 epoch total loss 0.547232747\n",
      "Trained batch 2032 batch loss 0.58699733 epoch total loss 0.547252297\n",
      "Trained batch 2033 batch loss 0.593250334 epoch total loss 0.547274947\n",
      "Trained batch 2034 batch loss 0.503006339 epoch total loss 0.547253191\n",
      "Trained batch 2035 batch loss 0.539303422 epoch total loss 0.547249317\n",
      "Trained batch 2036 batch loss 0.574199855 epoch total loss 0.547262549\n",
      "Trained batch 2037 batch loss 0.535392463 epoch total loss 0.547256708\n",
      "Trained batch 2038 batch loss 0.64963609 epoch total loss 0.547306955\n",
      "Trained batch 2039 batch loss 0.656593561 epoch total loss 0.547360599\n",
      "Trained batch 2040 batch loss 0.58979094 epoch total loss 0.547381401\n",
      "Trained batch 2041 batch loss 0.576238751 epoch total loss 0.547395587\n",
      "Trained batch 2042 batch loss 0.558389425 epoch total loss 0.547400951\n",
      "Trained batch 2043 batch loss 0.550216198 epoch total loss 0.547402322\n",
      "Trained batch 2044 batch loss 0.708227038 epoch total loss 0.547481\n",
      "Trained batch 2045 batch loss 0.604655862 epoch total loss 0.547508955\n",
      "Trained batch 2046 batch loss 0.516104221 epoch total loss 0.547493577\n",
      "Trained batch 2047 batch loss 0.48831895 epoch total loss 0.547464669\n",
      "Trained batch 2048 batch loss 0.561088 epoch total loss 0.547471285\n",
      "Trained batch 2049 batch loss 0.53203094 epoch total loss 0.547463715\n",
      "Trained batch 2050 batch loss 0.51123178 epoch total loss 0.547446072\n",
      "Trained batch 2051 batch loss 0.528173 epoch total loss 0.547436655\n",
      "Trained batch 2052 batch loss 0.556435823 epoch total loss 0.547441\n",
      "Trained batch 2053 batch loss 0.567604 epoch total loss 0.54745084\n",
      "Trained batch 2054 batch loss 0.555084407 epoch total loss 0.547454596\n",
      "Trained batch 2055 batch loss 0.584880948 epoch total loss 0.547472775\n",
      "Trained batch 2056 batch loss 0.511188745 epoch total loss 0.547455132\n",
      "Trained batch 2057 batch loss 0.535607159 epoch total loss 0.54744941\n",
      "Trained batch 2058 batch loss 0.568145335 epoch total loss 0.547459424\n",
      "Trained batch 2059 batch loss 0.583630681 epoch total loss 0.547477\n",
      "Trained batch 2060 batch loss 0.584403753 epoch total loss 0.547494888\n",
      "Trained batch 2061 batch loss 0.571741939 epoch total loss 0.54750669\n",
      "Trained batch 2062 batch loss 0.485177875 epoch total loss 0.54747647\n",
      "Trained batch 2063 batch loss 0.46517688 epoch total loss 0.547436595\n",
      "Trained batch 2064 batch loss 0.574867189 epoch total loss 0.547449887\n",
      "Trained batch 2065 batch loss 0.549285352 epoch total loss 0.547450781\n",
      "Trained batch 2066 batch loss 0.545366168 epoch total loss 0.547449768\n",
      "Trained batch 2067 batch loss 0.575560272 epoch total loss 0.547463357\n",
      "Trained batch 2068 batch loss 0.532431364 epoch total loss 0.547456145\n",
      "Trained batch 2069 batch loss 0.503363073 epoch total loss 0.547434866\n",
      "Trained batch 2070 batch loss 0.521427929 epoch total loss 0.54742229\n",
      "Trained batch 2071 batch loss 0.528458059 epoch total loss 0.54741317\n",
      "Trained batch 2072 batch loss 0.60226649 epoch total loss 0.547439635\n",
      "Trained batch 2073 batch loss 0.521389723 epoch total loss 0.547427058\n",
      "Trained batch 2074 batch loss 0.529427171 epoch total loss 0.547418356\n",
      "Trained batch 2075 batch loss 0.53634423 epoch total loss 0.547413051\n",
      "Trained batch 2076 batch loss 0.562807 epoch total loss 0.547420502\n",
      "Trained batch 2077 batch loss 0.570689321 epoch total loss 0.547431707\n",
      "Trained batch 2078 batch loss 0.550494909 epoch total loss 0.547433197\n",
      "Trained batch 2079 batch loss 0.550666034 epoch total loss 0.547434747\n",
      "Trained batch 2080 batch loss 0.482520372 epoch total loss 0.547403514\n",
      "Trained batch 2081 batch loss 0.474619925 epoch total loss 0.547368586\n",
      "Trained batch 2082 batch loss 0.494849533 epoch total loss 0.547343373\n",
      "Trained batch 2083 batch loss 0.449519217 epoch total loss 0.547296345\n",
      "Trained batch 2084 batch loss 0.457113504 epoch total loss 0.547253072\n",
      "Trained batch 2085 batch loss 0.518504798 epoch total loss 0.547239363\n",
      "Trained batch 2086 batch loss 0.519994557 epoch total loss 0.54722631\n",
      "Trained batch 2087 batch loss 0.579154432 epoch total loss 0.547241569\n",
      "Trained batch 2088 batch loss 0.556129515 epoch total loss 0.5472458\n",
      "Trained batch 2089 batch loss 0.54765439 epoch total loss 0.547246\n",
      "Trained batch 2090 batch loss 0.509457767 epoch total loss 0.547227919\n",
      "Trained batch 2091 batch loss 0.492761642 epoch total loss 0.547201872\n",
      "Trained batch 2092 batch loss 0.438952625 epoch total loss 0.547150135\n",
      "Trained batch 2093 batch loss 0.541044593 epoch total loss 0.547147214\n",
      "Trained batch 2094 batch loss 0.508501351 epoch total loss 0.547128737\n",
      "Trained batch 2095 batch loss 0.561623 epoch total loss 0.547135711\n",
      "Trained batch 2096 batch loss 0.505782723 epoch total loss 0.547115922\n",
      "Trained batch 2097 batch loss 0.514886081 epoch total loss 0.547100544\n",
      "Trained batch 2098 batch loss 0.550426662 epoch total loss 0.547102153\n",
      "Trained batch 2099 batch loss 0.544825315 epoch total loss 0.54710108\n",
      "Trained batch 2100 batch loss 0.552021503 epoch total loss 0.547103405\n",
      "Trained batch 2101 batch loss 0.47686851 epoch total loss 0.547069967\n",
      "Trained batch 2102 batch loss 0.555860162 epoch total loss 0.547074199\n",
      "Trained batch 2103 batch loss 0.545946598 epoch total loss 0.547073662\n",
      "Trained batch 2104 batch loss 0.63254 epoch total loss 0.547114253\n",
      "Trained batch 2105 batch loss 0.500593185 epoch total loss 0.547092199\n",
      "Trained batch 2106 batch loss 0.476922095 epoch total loss 0.54705888\n",
      "Trained batch 2107 batch loss 0.533345759 epoch total loss 0.547052324\n",
      "Trained batch 2108 batch loss 0.581422925 epoch total loss 0.547068655\n",
      "Trained batch 2109 batch loss 0.582745194 epoch total loss 0.547085583\n",
      "Trained batch 2110 batch loss 0.533148706 epoch total loss 0.547078967\n",
      "Trained batch 2111 batch loss 0.482132137 epoch total loss 0.547048271\n",
      "Trained batch 2112 batch loss 0.548975527 epoch total loss 0.547049165\n",
      "Trained batch 2113 batch loss 0.471818835 epoch total loss 0.547013521\n",
      "Trained batch 2114 batch loss 0.631877661 epoch total loss 0.547053635\n",
      "Trained batch 2115 batch loss 0.679886281 epoch total loss 0.547116458\n",
      "Trained batch 2116 batch loss 0.528356254 epoch total loss 0.547107577\n",
      "Trained batch 2117 batch loss 0.526360512 epoch total loss 0.547097802\n",
      "Trained batch 2118 batch loss 0.488164485 epoch total loss 0.547069967\n",
      "Trained batch 2119 batch loss 0.561881065 epoch total loss 0.547077\n",
      "Trained batch 2120 batch loss 0.649342179 epoch total loss 0.547125161\n",
      "Trained batch 2121 batch loss 0.427299589 epoch total loss 0.547068655\n",
      "Trained batch 2122 batch loss 0.480684936 epoch total loss 0.547037423\n",
      "Trained batch 2123 batch loss 0.52444005 epoch total loss 0.547026753\n",
      "Trained batch 2124 batch loss 0.585389853 epoch total loss 0.547044814\n",
      "Trained batch 2125 batch loss 0.510920763 epoch total loss 0.547027826\n",
      "Trained batch 2126 batch loss 0.571023822 epoch total loss 0.547039092\n",
      "Trained batch 2127 batch loss 0.485436738 epoch total loss 0.547010183\n",
      "Trained batch 2128 batch loss 0.497026831 epoch total loss 0.546986699\n",
      "Trained batch 2129 batch loss 0.499494463 epoch total loss 0.546964407\n",
      "Trained batch 2130 batch loss 0.572595656 epoch total loss 0.546976447\n",
      "Trained batch 2131 batch loss 0.561705291 epoch total loss 0.546983302\n",
      "Trained batch 2132 batch loss 0.558355391 epoch total loss 0.546988666\n",
      "Trained batch 2133 batch loss 0.558683932 epoch total loss 0.54699415\n",
      "Trained batch 2134 batch loss 0.504719436 epoch total loss 0.546974361\n",
      "Trained batch 2135 batch loss 0.501724482 epoch total loss 0.546953142\n",
      "Trained batch 2136 batch loss 0.580292165 epoch total loss 0.546968758\n",
      "Trained batch 2137 batch loss 0.546600163 epoch total loss 0.546968639\n",
      "Trained batch 2138 batch loss 0.595738232 epoch total loss 0.546991408\n",
      "Trained batch 2139 batch loss 0.468895286 epoch total loss 0.54695487\n",
      "Trained batch 2140 batch loss 0.488643587 epoch total loss 0.546927631\n",
      "Trained batch 2141 batch loss 0.566261649 epoch total loss 0.546936691\n",
      "Trained batch 2142 batch loss 0.607905 epoch total loss 0.546965182\n",
      "Trained batch 2143 batch loss 0.579636872 epoch total loss 0.546980381\n",
      "Trained batch 2144 batch loss 0.553876281 epoch total loss 0.5469836\n",
      "Trained batch 2145 batch loss 0.574061751 epoch total loss 0.546996236\n",
      "Trained batch 2146 batch loss 0.603204131 epoch total loss 0.547022402\n",
      "Trained batch 2147 batch loss 0.590444088 epoch total loss 0.547042608\n",
      "Trained batch 2148 batch loss 0.479020476 epoch total loss 0.547010958\n",
      "Trained batch 2149 batch loss 0.509904385 epoch total loss 0.546993673\n",
      "Trained batch 2150 batch loss 0.532517 epoch total loss 0.546986878\n",
      "Trained batch 2151 batch loss 0.560582161 epoch total loss 0.546993196\n",
      "Trained batch 2152 batch loss 0.482144386 epoch total loss 0.546963096\n",
      "Trained batch 2153 batch loss 0.541175 epoch total loss 0.546960413\n",
      "Trained batch 2154 batch loss 0.568303049 epoch total loss 0.546970308\n",
      "Trained batch 2155 batch loss 0.569211662 epoch total loss 0.546980619\n",
      "Trained batch 2156 batch loss 0.580401421 epoch total loss 0.546996176\n",
      "Trained batch 2157 batch loss 0.588532686 epoch total loss 0.547015429\n",
      "Trained batch 2158 batch loss 0.669070661 epoch total loss 0.547072\n",
      "Trained batch 2159 batch loss 0.619198561 epoch total loss 0.547105372\n",
      "Trained batch 2160 batch loss 0.600374401 epoch total loss 0.54713\n",
      "Trained batch 2161 batch loss 0.650040269 epoch total loss 0.547177613\n",
      "Trained batch 2162 batch loss 0.616291344 epoch total loss 0.54720962\n",
      "Trained batch 2163 batch loss 0.595979035 epoch total loss 0.547232151\n",
      "Trained batch 2164 batch loss 0.602746248 epoch total loss 0.547257781\n",
      "Trained batch 2165 batch loss 0.646837234 epoch total loss 0.547303796\n",
      "Trained batch 2166 batch loss 0.616107404 epoch total loss 0.547335565\n",
      "Trained batch 2167 batch loss 0.492861301 epoch total loss 0.547310472\n",
      "Trained batch 2168 batch loss 0.490715295 epoch total loss 0.547284365\n",
      "Trained batch 2169 batch loss 0.474559426 epoch total loss 0.547250807\n",
      "Trained batch 2170 batch loss 0.452541858 epoch total loss 0.547207177\n",
      "Trained batch 2171 batch loss 0.400226116 epoch total loss 0.547139466\n",
      "Trained batch 2172 batch loss 0.461589158 epoch total loss 0.547100067\n",
      "Trained batch 2173 batch loss 0.515585661 epoch total loss 0.547085583\n",
      "Trained batch 2174 batch loss 0.581091702 epoch total loss 0.5471012\n",
      "Trained batch 2175 batch loss 0.625040412 epoch total loss 0.547137\n",
      "Trained batch 2176 batch loss 0.64417237 epoch total loss 0.547181606\n",
      "Trained batch 2177 batch loss 0.576763391 epoch total loss 0.547195256\n",
      "Trained batch 2178 batch loss 0.516046762 epoch total loss 0.547180891\n",
      "Trained batch 2179 batch loss 0.693658471 epoch total loss 0.547248125\n",
      "Trained batch 2180 batch loss 0.57656455 epoch total loss 0.547261536\n",
      "Trained batch 2181 batch loss 0.579297125 epoch total loss 0.547276258\n",
      "Trained batch 2182 batch loss 0.562839508 epoch total loss 0.547283411\n",
      "Trained batch 2183 batch loss 0.699469805 epoch total loss 0.547353089\n",
      "Trained batch 2184 batch loss 0.542250395 epoch total loss 0.547350764\n",
      "Trained batch 2185 batch loss 0.679791033 epoch total loss 0.547411382\n",
      "Trained batch 2186 batch loss 0.543801427 epoch total loss 0.547409713\n",
      "Trained batch 2187 batch loss 0.498577714 epoch total loss 0.547387421\n",
      "Trained batch 2188 batch loss 0.660091341 epoch total loss 0.54743886\n",
      "Trained batch 2189 batch loss 0.58337152 epoch total loss 0.547455311\n",
      "Trained batch 2190 batch loss 0.611159921 epoch total loss 0.547484398\n",
      "Trained batch 2191 batch loss 0.580075085 epoch total loss 0.547499299\n",
      "Trained batch 2192 batch loss 0.578320384 epoch total loss 0.547513366\n",
      "Trained batch 2193 batch loss 0.541377425 epoch total loss 0.547510564\n",
      "Trained batch 2194 batch loss 0.4935655 epoch total loss 0.547485948\n",
      "Trained batch 2195 batch loss 0.481483847 epoch total loss 0.547455847\n",
      "Trained batch 2196 batch loss 0.499618471 epoch total loss 0.547434092\n",
      "Trained batch 2197 batch loss 0.532203 epoch total loss 0.547427177\n",
      "Trained batch 2198 batch loss 0.409934044 epoch total loss 0.547364593\n",
      "Trained batch 2199 batch loss 0.43378979 epoch total loss 0.547313\n",
      "Trained batch 2200 batch loss 0.470258921 epoch total loss 0.547277927\n",
      "Trained batch 2201 batch loss 0.517377 epoch total loss 0.547264338\n",
      "Trained batch 2202 batch loss 0.551548123 epoch total loss 0.547266245\n",
      "Trained batch 2203 batch loss 0.552980185 epoch total loss 0.547268867\n",
      "Trained batch 2204 batch loss 0.615105569 epoch total loss 0.547299623\n",
      "Trained batch 2205 batch loss 0.521085382 epoch total loss 0.547287762\n",
      "Trained batch 2206 batch loss 0.553940535 epoch total loss 0.547290802\n",
      "Trained batch 2207 batch loss 0.574683964 epoch total loss 0.5473032\n",
      "Trained batch 2208 batch loss 0.618590117 epoch total loss 0.547335446\n",
      "Trained batch 2209 batch loss 0.592578471 epoch total loss 0.54735595\n",
      "Trained batch 2210 batch loss 0.583026409 epoch total loss 0.547372043\n",
      "Trained batch 2211 batch loss 0.557097435 epoch total loss 0.547376454\n",
      "Trained batch 2212 batch loss 0.512573957 epoch total loss 0.547360718\n",
      "Trained batch 2213 batch loss 0.601136684 epoch total loss 0.547385097\n",
      "Trained batch 2214 batch loss 0.572189748 epoch total loss 0.547396243\n",
      "Trained batch 2215 batch loss 0.519655943 epoch total loss 0.547383726\n",
      "Trained batch 2216 batch loss 0.58051151 epoch total loss 0.547398686\n",
      "Trained batch 2217 batch loss 0.560041904 epoch total loss 0.547404408\n",
      "Trained batch 2218 batch loss 0.536596477 epoch total loss 0.547399521\n",
      "Trained batch 2219 batch loss 0.520665 epoch total loss 0.547387481\n",
      "Trained batch 2220 batch loss 0.534406722 epoch total loss 0.547381639\n",
      "Trained batch 2221 batch loss 0.641322732 epoch total loss 0.547423959\n",
      "Trained batch 2222 batch loss 0.681391239 epoch total loss 0.547484279\n",
      "Trained batch 2223 batch loss 0.61827594 epoch total loss 0.547516108\n",
      "Trained batch 2224 batch loss 0.61607188 epoch total loss 0.547546923\n",
      "Trained batch 2225 batch loss 0.608603597 epoch total loss 0.547574401\n",
      "Trained batch 2226 batch loss 0.602559865 epoch total loss 0.547599077\n",
      "Trained batch 2227 batch loss 0.56377846 epoch total loss 0.547606349\n",
      "Trained batch 2228 batch loss 0.518746614 epoch total loss 0.547593415\n",
      "Trained batch 2229 batch loss 0.579296112 epoch total loss 0.54760766\n",
      "Trained batch 2230 batch loss 0.499934 epoch total loss 0.547586262\n",
      "Trained batch 2231 batch loss 0.539671481 epoch total loss 0.547582686\n",
      "Trained batch 2232 batch loss 0.505654335 epoch total loss 0.54756391\n",
      "Trained batch 2233 batch loss 0.479814619 epoch total loss 0.547533572\n",
      "Trained batch 2234 batch loss 0.649537146 epoch total loss 0.547579229\n",
      "Trained batch 2235 batch loss 0.638171375 epoch total loss 0.54761976\n",
      "Trained batch 2236 batch loss 0.593346655 epoch total loss 0.547640204\n",
      "Trained batch 2237 batch loss 0.580809951 epoch total loss 0.547655046\n",
      "Trained batch 2238 batch loss 0.693457484 epoch total loss 0.547720194\n",
      "Trained batch 2239 batch loss 0.680526 epoch total loss 0.54777956\n",
      "Trained batch 2240 batch loss 0.702610254 epoch total loss 0.547848701\n",
      "Trained batch 2241 batch loss 0.575237751 epoch total loss 0.547860861\n",
      "Trained batch 2242 batch loss 0.473130137 epoch total loss 0.547827542\n",
      "Trained batch 2243 batch loss 0.518959403 epoch total loss 0.547814667\n",
      "Trained batch 2244 batch loss 0.536016166 epoch total loss 0.547809422\n",
      "Trained batch 2245 batch loss 0.690955222 epoch total loss 0.547873139\n",
      "Trained batch 2246 batch loss 0.552124918 epoch total loss 0.547875047\n",
      "Trained batch 2247 batch loss 0.503752828 epoch total loss 0.547855437\n",
      "Trained batch 2248 batch loss 0.584825873 epoch total loss 0.547871888\n",
      "Trained batch 2249 batch loss 0.652837038 epoch total loss 0.547918558\n",
      "Trained batch 2250 batch loss 0.648299 epoch total loss 0.547963142\n",
      "Trained batch 2251 batch loss 0.636399209 epoch total loss 0.548002422\n",
      "Trained batch 2252 batch loss 0.564616203 epoch total loss 0.548009813\n",
      "Trained batch 2253 batch loss 0.544075251 epoch total loss 0.548008\n",
      "Trained batch 2254 batch loss 0.436745912 epoch total loss 0.547958672\n",
      "Trained batch 2255 batch loss 0.50847888 epoch total loss 0.547941148\n",
      "Trained batch 2256 batch loss 0.538026273 epoch total loss 0.547936797\n",
      "Trained batch 2257 batch loss 0.684488416 epoch total loss 0.547997296\n",
      "Trained batch 2258 batch loss 0.442112982 epoch total loss 0.547950387\n",
      "Trained batch 2259 batch loss 0.514087 epoch total loss 0.547935367\n",
      "Trained batch 2260 batch loss 0.682766914 epoch total loss 0.547995031\n",
      "Trained batch 2261 batch loss 0.67592442 epoch total loss 0.548051596\n",
      "Trained batch 2262 batch loss 0.527426898 epoch total loss 0.548042476\n",
      "Trained batch 2263 batch loss 0.56008482 epoch total loss 0.548047781\n",
      "Trained batch 2264 batch loss 0.644174159 epoch total loss 0.548090279\n",
      "Trained batch 2265 batch loss 0.635309041 epoch total loss 0.548128724\n",
      "Trained batch 2266 batch loss 0.659232557 epoch total loss 0.548177719\n",
      "Trained batch 2267 batch loss 0.666201949 epoch total loss 0.548229814\n",
      "Trained batch 2268 batch loss 0.690435946 epoch total loss 0.548292518\n",
      "Trained batch 2269 batch loss 0.614136338 epoch total loss 0.548321545\n",
      "Trained batch 2270 batch loss 0.531676173 epoch total loss 0.548314214\n",
      "Trained batch 2271 batch loss 0.53722626 epoch total loss 0.548309326\n",
      "Trained batch 2272 batch loss 0.633099735 epoch total loss 0.548346639\n",
      "Trained batch 2273 batch loss 0.555179358 epoch total loss 0.548349619\n",
      "Trained batch 2274 batch loss 0.556824327 epoch total loss 0.548353374\n",
      "Trained batch 2275 batch loss 0.542285144 epoch total loss 0.548350692\n",
      "Trained batch 2276 batch loss 0.55810082 epoch total loss 0.548355\n",
      "Trained batch 2277 batch loss 0.592381358 epoch total loss 0.548374295\n",
      "Trained batch 2278 batch loss 0.587481141 epoch total loss 0.548391521\n",
      "Trained batch 2279 batch loss 0.637923 epoch total loss 0.5484308\n",
      "Trained batch 2280 batch loss 0.598287225 epoch total loss 0.548452675\n",
      "Trained batch 2281 batch loss 0.617254853 epoch total loss 0.548482835\n",
      "Trained batch 2282 batch loss 0.635497451 epoch total loss 0.548521\n",
      "Trained batch 2283 batch loss 0.607046545 epoch total loss 0.548546612\n",
      "Trained batch 2284 batch loss 0.736098409 epoch total loss 0.548628747\n",
      "Trained batch 2285 batch loss 0.574079156 epoch total loss 0.548639894\n",
      "Trained batch 2286 batch loss 0.631514072 epoch total loss 0.548676074\n",
      "Trained batch 2287 batch loss 0.656345487 epoch total loss 0.548723161\n",
      "Trained batch 2288 batch loss 0.644602299 epoch total loss 0.548765123\n",
      "Trained batch 2289 batch loss 0.649266183 epoch total loss 0.548809052\n",
      "Trained batch 2290 batch loss 0.682590723 epoch total loss 0.548867464\n",
      "Trained batch 2291 batch loss 0.666425884 epoch total loss 0.548918784\n",
      "Trained batch 2292 batch loss 0.612100303 epoch total loss 0.548946321\n",
      "Trained batch 2293 batch loss 0.651991725 epoch total loss 0.548991263\n",
      "Trained batch 2294 batch loss 0.61193645 epoch total loss 0.549018681\n",
      "Trained batch 2295 batch loss 0.611679554 epoch total loss 0.549046\n",
      "Trained batch 2296 batch loss 0.606906056 epoch total loss 0.549071193\n",
      "Trained batch 2297 batch loss 0.542709172 epoch total loss 0.549068451\n",
      "Trained batch 2298 batch loss 0.58344835 epoch total loss 0.549083412\n",
      "Trained batch 2299 batch loss 0.58845 epoch total loss 0.549100578\n",
      "Trained batch 2300 batch loss 0.664558828 epoch total loss 0.549150765\n",
      "Trained batch 2301 batch loss 0.650969744 epoch total loss 0.549195051\n",
      "Trained batch 2302 batch loss 0.572677553 epoch total loss 0.549205184\n",
      "Trained batch 2303 batch loss 0.602913558 epoch total loss 0.549228549\n",
      "Trained batch 2304 batch loss 0.559998572 epoch total loss 0.549233198\n",
      "Trained batch 2305 batch loss 0.43415314 epoch total loss 0.549183309\n",
      "Trained batch 2306 batch loss 0.519898534 epoch total loss 0.549170613\n",
      "Trained batch 2307 batch loss 0.381578088 epoch total loss 0.549097955\n",
      "Trained batch 2308 batch loss 0.542403877 epoch total loss 0.549095035\n",
      "Trained batch 2309 batch loss 0.54694593 epoch total loss 0.549094141\n",
      "Trained batch 2310 batch loss 0.430146962 epoch total loss 0.549042702\n",
      "Trained batch 2311 batch loss 0.421568 epoch total loss 0.548987508\n",
      "Trained batch 2312 batch loss 0.546129286 epoch total loss 0.548986256\n",
      "Trained batch 2313 batch loss 0.59509176 epoch total loss 0.549006164\n",
      "Trained batch 2314 batch loss 0.482833475 epoch total loss 0.548977554\n",
      "Trained batch 2315 batch loss 0.530433059 epoch total loss 0.548969567\n",
      "Trained batch 2316 batch loss 0.614485085 epoch total loss 0.548997819\n",
      "Trained batch 2317 batch loss 0.588242173 epoch total loss 0.549014807\n",
      "Trained batch 2318 batch loss 0.510647297 epoch total loss 0.548998237\n",
      "Trained batch 2319 batch loss 0.513761342 epoch total loss 0.548983037\n",
      "Trained batch 2320 batch loss 0.53050518 epoch total loss 0.54897511\n",
      "Trained batch 2321 batch loss 0.582681835 epoch total loss 0.548989594\n",
      "Trained batch 2322 batch loss 0.533168137 epoch total loss 0.548982799\n",
      "Trained batch 2323 batch loss 0.617836833 epoch total loss 0.549012423\n",
      "Trained batch 2324 batch loss 0.52877593 epoch total loss 0.54900372\n",
      "Trained batch 2325 batch loss 0.565798402 epoch total loss 0.549010932\n",
      "Trained batch 2326 batch loss 0.636483908 epoch total loss 0.549048543\n",
      "Trained batch 2327 batch loss 0.746691585 epoch total loss 0.54913348\n",
      "Trained batch 2328 batch loss 0.677903712 epoch total loss 0.549188793\n",
      "Trained batch 2329 batch loss 0.713690341 epoch total loss 0.549259424\n",
      "Trained batch 2330 batch loss 0.704660356 epoch total loss 0.549326122\n",
      "Trained batch 2331 batch loss 0.606149137 epoch total loss 0.54935056\n",
      "Trained batch 2332 batch loss 0.513935626 epoch total loss 0.549335361\n",
      "Trained batch 2333 batch loss 0.471489727 epoch total loss 0.549302\n",
      "Trained batch 2334 batch loss 0.470721602 epoch total loss 0.549268305\n",
      "Trained batch 2335 batch loss 0.512585282 epoch total loss 0.54925257\n",
      "Trained batch 2336 batch loss 0.59194839 epoch total loss 0.549270868\n",
      "Trained batch 2337 batch loss 0.575086236 epoch total loss 0.549281895\n",
      "Trained batch 2338 batch loss 0.639404774 epoch total loss 0.5493204\n",
      "Trained batch 2339 batch loss 0.628898382 epoch total loss 0.549354434\n",
      "Trained batch 2340 batch loss 0.578378856 epoch total loss 0.549366832\n",
      "Trained batch 2341 batch loss 0.672467232 epoch total loss 0.549419463\n",
      "Trained batch 2342 batch loss 0.61142242 epoch total loss 0.549445927\n",
      "Trained batch 2343 batch loss 0.568207204 epoch total loss 0.549454\n",
      "Trained batch 2344 batch loss 0.640785575 epoch total loss 0.549492896\n",
      "Trained batch 2345 batch loss 0.532929957 epoch total loss 0.549485862\n",
      "Trained batch 2346 batch loss 0.527275264 epoch total loss 0.549476385\n",
      "Trained batch 2347 batch loss 0.616819501 epoch total loss 0.549505055\n",
      "Trained batch 2348 batch loss 0.556953073 epoch total loss 0.549508274\n",
      "Trained batch 2349 batch loss 0.591689408 epoch total loss 0.549526215\n",
      "Trained batch 2350 batch loss 0.497909725 epoch total loss 0.54950422\n",
      "Trained batch 2351 batch loss 0.545853376 epoch total loss 0.54950273\n",
      "Trained batch 2352 batch loss 0.578024924 epoch total loss 0.54951483\n",
      "Trained batch 2353 batch loss 0.598765671 epoch total loss 0.549535751\n",
      "Trained batch 2354 batch loss 0.541886628 epoch total loss 0.549532473\n",
      "Trained batch 2355 batch loss 0.503773212 epoch total loss 0.549513042\n",
      "Trained batch 2356 batch loss 0.469215751 epoch total loss 0.549479\n",
      "Trained batch 2357 batch loss 0.542466044 epoch total loss 0.549476\n",
      "Trained batch 2358 batch loss 0.49855876 epoch total loss 0.549454451\n",
      "Trained batch 2359 batch loss 0.561869 epoch total loss 0.549459696\n",
      "Trained batch 2360 batch loss 0.605334818 epoch total loss 0.549483359\n",
      "Trained batch 2361 batch loss 0.71681565 epoch total loss 0.549554229\n",
      "Trained batch 2362 batch loss 0.58175832 epoch total loss 0.549567878\n",
      "Trained batch 2363 batch loss 0.55414176 epoch total loss 0.549569845\n",
      "Trained batch 2364 batch loss 0.644430161 epoch total loss 0.549609959\n",
      "Trained batch 2365 batch loss 0.620334089 epoch total loss 0.549639881\n",
      "Trained batch 2366 batch loss 0.555428863 epoch total loss 0.549642324\n",
      "Trained batch 2367 batch loss 0.718099773 epoch total loss 0.549713492\n",
      "Trained batch 2368 batch loss 0.524374723 epoch total loss 0.549702823\n",
      "Trained batch 2369 batch loss 0.573648393 epoch total loss 0.549712896\n",
      "Trained batch 2370 batch loss 0.621753216 epoch total loss 0.549743295\n",
      "Trained batch 2371 batch loss 0.560798407 epoch total loss 0.549747944\n",
      "Trained batch 2372 batch loss 0.627124667 epoch total loss 0.549780548\n",
      "Trained batch 2373 batch loss 0.602273703 epoch total loss 0.549802661\n",
      "Trained batch 2374 batch loss 0.583541632 epoch total loss 0.549816847\n",
      "Trained batch 2375 batch loss 0.54174149 epoch total loss 0.549813449\n",
      "Trained batch 2376 batch loss 0.686389625 epoch total loss 0.549870968\n",
      "Trained batch 2377 batch loss 0.53394413 epoch total loss 0.549864233\n",
      "Trained batch 2378 batch loss 0.540777206 epoch total loss 0.549860418\n",
      "Trained batch 2379 batch loss 0.504802883 epoch total loss 0.549841464\n",
      "Trained batch 2380 batch loss 0.538201511 epoch total loss 0.549836576\n",
      "Trained batch 2381 batch loss 0.486857653 epoch total loss 0.549810112\n",
      "Trained batch 2382 batch loss 0.482354462 epoch total loss 0.549781799\n",
      "Trained batch 2383 batch loss 0.482729346 epoch total loss 0.549753666\n",
      "Trained batch 2384 batch loss 0.446176589 epoch total loss 0.549710214\n",
      "Trained batch 2385 batch loss 0.55688262 epoch total loss 0.549713254\n",
      "Trained batch 2386 batch loss 0.581923306 epoch total loss 0.549726725\n",
      "Trained batch 2387 batch loss 0.609306097 epoch total loss 0.549751639\n",
      "Trained batch 2388 batch loss 0.475351214 epoch total loss 0.549720466\n",
      "Trained batch 2389 batch loss 0.550758302 epoch total loss 0.549720943\n",
      "Trained batch 2390 batch loss 0.571072698 epoch total loss 0.549729884\n",
      "Trained batch 2391 batch loss 0.55757463 epoch total loss 0.549733162\n",
      "Trained batch 2392 batch loss 0.635184407 epoch total loss 0.549768865\n",
      "Trained batch 2393 batch loss 0.569639564 epoch total loss 0.54977715\n",
      "Trained batch 2394 batch loss 0.609044135 epoch total loss 0.549801886\n",
      "Trained batch 2395 batch loss 0.553061366 epoch total loss 0.549803257\n",
      "Trained batch 2396 batch loss 0.594500899 epoch total loss 0.549821913\n",
      "Trained batch 2397 batch loss 0.525670826 epoch total loss 0.54981184\n",
      "Trained batch 2398 batch loss 0.499380201 epoch total loss 0.5497908\n",
      "Trained batch 2399 batch loss 0.532353759 epoch total loss 0.549783528\n",
      "Trained batch 2400 batch loss 0.490387946 epoch total loss 0.549758732\n",
      "Trained batch 2401 batch loss 0.482869327 epoch total loss 0.549730897\n",
      "Trained batch 2402 batch loss 0.446908832 epoch total loss 0.549688101\n",
      "Trained batch 2403 batch loss 0.514003634 epoch total loss 0.549673259\n",
      "Trained batch 2404 batch loss 0.562786043 epoch total loss 0.549678683\n",
      "Trained batch 2405 batch loss 0.569561481 epoch total loss 0.549686968\n",
      "Trained batch 2406 batch loss 0.483686894 epoch total loss 0.54965955\n",
      "Trained batch 2407 batch loss 0.562455595 epoch total loss 0.549664855\n",
      "Trained batch 2408 batch loss 0.551871479 epoch total loss 0.549665809\n",
      "Trained batch 2409 batch loss 0.507052183 epoch total loss 0.549648106\n",
      "Trained batch 2410 batch loss 0.49305433 epoch total loss 0.549624622\n",
      "Trained batch 2411 batch loss 0.427939028 epoch total loss 0.549574137\n",
      "Trained batch 2412 batch loss 0.491547018 epoch total loss 0.549550116\n",
      "Trained batch 2413 batch loss 0.475307077 epoch total loss 0.54951936\n",
      "Trained batch 2414 batch loss 0.55156666 epoch total loss 0.549520195\n",
      "Trained batch 2415 batch loss 0.539932191 epoch total loss 0.549516201\n",
      "Trained batch 2416 batch loss 0.59436506 epoch total loss 0.549534798\n",
      "Trained batch 2417 batch loss 0.743988335 epoch total loss 0.549615264\n",
      "Trained batch 2418 batch loss 0.617892325 epoch total loss 0.549643517\n",
      "Trained batch 2419 batch loss 0.526717842 epoch total loss 0.549634039\n",
      "Trained batch 2420 batch loss 0.505362093 epoch total loss 0.549615741\n",
      "Trained batch 2421 batch loss 0.57156074 epoch total loss 0.549624801\n",
      "Trained batch 2422 batch loss 0.46952641 epoch total loss 0.54959172\n",
      "Trained batch 2423 batch loss 0.444001436 epoch total loss 0.54954809\n",
      "Trained batch 2424 batch loss 0.378756016 epoch total loss 0.549477637\n",
      "Trained batch 2425 batch loss 0.291203648 epoch total loss 0.549371183\n",
      "Trained batch 2426 batch loss 0.278402805 epoch total loss 0.549259484\n",
      "Trained batch 2427 batch loss 0.34323591 epoch total loss 0.549174607\n",
      "Trained batch 2428 batch loss 0.364793807 epoch total loss 0.54909867\n",
      "Trained batch 2429 batch loss 0.426234186 epoch total loss 0.549048066\n",
      "Trained batch 2430 batch loss 0.49691391 epoch total loss 0.549026668\n",
      "Trained batch 2431 batch loss 0.606554687 epoch total loss 0.549050331\n",
      "Trained batch 2432 batch loss 0.551949382 epoch total loss 0.549051523\n",
      "Trained batch 2433 batch loss 0.583464503 epoch total loss 0.549065709\n",
      "Trained batch 2434 batch loss 0.578554571 epoch total loss 0.549077809\n",
      "Trained batch 2435 batch loss 0.566067815 epoch total loss 0.549084783\n",
      "Trained batch 2436 batch loss 0.61219573 epoch total loss 0.549110711\n",
      "Trained batch 2437 batch loss 0.592475951 epoch total loss 0.549128532\n",
      "Trained batch 2438 batch loss 0.599512279 epoch total loss 0.549149156\n",
      "Trained batch 2439 batch loss 0.458457023 epoch total loss 0.549112\n",
      "Trained batch 2440 batch loss 0.51450336 epoch total loss 0.549097836\n",
      "Trained batch 2441 batch loss 0.466230631 epoch total loss 0.549063861\n",
      "Trained batch 2442 batch loss 0.40063411 epoch total loss 0.549003065\n",
      "Trained batch 2443 batch loss 0.466093123 epoch total loss 0.54896915\n",
      "Trained batch 2444 batch loss 0.559115946 epoch total loss 0.548973262\n",
      "Trained batch 2445 batch loss 0.66228 epoch total loss 0.549019575\n",
      "Trained batch 2446 batch loss 0.595795393 epoch total loss 0.549038708\n",
      "Trained batch 2447 batch loss 0.591791928 epoch total loss 0.549056232\n",
      "Trained batch 2448 batch loss 0.611440539 epoch total loss 0.549081683\n",
      "Trained batch 2449 batch loss 0.596097231 epoch total loss 0.549100876\n",
      "Trained batch 2450 batch loss 0.606045842 epoch total loss 0.549124122\n",
      "Trained batch 2451 batch loss 0.632642806 epoch total loss 0.549158216\n",
      "Trained batch 2452 batch loss 0.657053888 epoch total loss 0.549202263\n",
      "Trained batch 2453 batch loss 0.632267535 epoch total loss 0.549236119\n",
      "Trained batch 2454 batch loss 0.552292049 epoch total loss 0.54923737\n",
      "Trained batch 2455 batch loss 0.586655 epoch total loss 0.549252629\n",
      "Trained batch 2456 batch loss 0.56174022 epoch total loss 0.549257696\n",
      "Trained batch 2457 batch loss 0.564661264 epoch total loss 0.549264\n",
      "Trained batch 2458 batch loss 0.651383281 epoch total loss 0.549305558\n",
      "Trained batch 2459 batch loss 0.577984631 epoch total loss 0.549317181\n",
      "Trained batch 2460 batch loss 0.543777704 epoch total loss 0.549315\n",
      "Trained batch 2461 batch loss 0.550565779 epoch total loss 0.549315453\n",
      "Trained batch 2462 batch loss 0.633876681 epoch total loss 0.549349844\n",
      "Trained batch 2463 batch loss 0.563041031 epoch total loss 0.549355388\n",
      "Trained batch 2464 batch loss 0.60771817 epoch total loss 0.549379051\n",
      "Trained batch 2465 batch loss 0.603776515 epoch total loss 0.549401104\n",
      "Trained batch 2466 batch loss 0.57402885 epoch total loss 0.549411058\n",
      "Trained batch 2467 batch loss 0.710386157 epoch total loss 0.549476266\n",
      "Trained batch 2468 batch loss 0.645585358 epoch total loss 0.549515247\n",
      "Trained batch 2469 batch loss 0.534768939 epoch total loss 0.549509287\n",
      "Trained batch 2470 batch loss 0.551797867 epoch total loss 0.549510181\n",
      "Trained batch 2471 batch loss 0.579455495 epoch total loss 0.549522281\n",
      "Trained batch 2472 batch loss 0.51768136 epoch total loss 0.549509466\n",
      "Trained batch 2473 batch loss 0.55649519 epoch total loss 0.549512267\n",
      "Trained batch 2474 batch loss 0.616441846 epoch total loss 0.549539328\n",
      "Trained batch 2475 batch loss 0.595083714 epoch total loss 0.549557745\n",
      "Trained batch 2476 batch loss 0.569372535 epoch total loss 0.549565732\n",
      "Trained batch 2477 batch loss 0.554522514 epoch total loss 0.549567759\n",
      "Trained batch 2478 batch loss 0.602461934 epoch total loss 0.549589097\n",
      "Trained batch 2479 batch loss 0.57874912 epoch total loss 0.54960084\n",
      "Trained batch 2480 batch loss 0.626589954 epoch total loss 0.549631894\n",
      "Trained batch 2481 batch loss 0.58532238 epoch total loss 0.549646258\n",
      "Trained batch 2482 batch loss 0.636067569 epoch total loss 0.549681067\n",
      "Trained batch 2483 batch loss 0.554294825 epoch total loss 0.549683\n",
      "Trained batch 2484 batch loss 0.578953 epoch total loss 0.549694777\n",
      "Trained batch 2485 batch loss 0.611946523 epoch total loss 0.54971981\n",
      "Trained batch 2486 batch loss 0.536151528 epoch total loss 0.549714327\n",
      "Trained batch 2487 batch loss 0.570323 epoch total loss 0.549722612\n",
      "Trained batch 2488 batch loss 0.568678 epoch total loss 0.549730241\n",
      "Trained batch 2489 batch loss 0.51764971 epoch total loss 0.549717367\n",
      "Trained batch 2490 batch loss 0.475420982 epoch total loss 0.549687564\n",
      "Trained batch 2491 batch loss 0.541926622 epoch total loss 0.549684405\n",
      "Trained batch 2492 batch loss 0.520143509 epoch total loss 0.549672604\n",
      "Trained batch 2493 batch loss 0.474232078 epoch total loss 0.549642324\n",
      "Trained batch 2494 batch loss 0.525111854 epoch total loss 0.54963249\n",
      "Trained batch 2495 batch loss 0.529364884 epoch total loss 0.549624383\n",
      "Trained batch 2496 batch loss 0.565558195 epoch total loss 0.549630761\n",
      "Trained batch 2497 batch loss 0.505282223 epoch total loss 0.549613\n",
      "Trained batch 2498 batch loss 0.566366911 epoch total loss 0.549619734\n",
      "Trained batch 2499 batch loss 0.47001937 epoch total loss 0.549587846\n",
      "Trained batch 2500 batch loss 0.497673899 epoch total loss 0.549567103\n",
      "Trained batch 2501 batch loss 0.478124917 epoch total loss 0.549538553\n",
      "Trained batch 2502 batch loss 0.46134761 epoch total loss 0.549503267\n",
      "Trained batch 2503 batch loss 0.433304608 epoch total loss 0.549456835\n",
      "Trained batch 2504 batch loss 0.467382699 epoch total loss 0.549424112\n",
      "Trained batch 2505 batch loss 0.464330405 epoch total loss 0.549390137\n",
      "Trained batch 2506 batch loss 0.551006913 epoch total loss 0.549390793\n",
      "Trained batch 2507 batch loss 0.545799911 epoch total loss 0.549389362\n",
      "Trained batch 2508 batch loss 0.565467238 epoch total loss 0.54939574\n",
      "Trained batch 2509 batch loss 0.519734383 epoch total loss 0.549383938\n",
      "Trained batch 2510 batch loss 0.591005862 epoch total loss 0.549400568\n",
      "Trained batch 2511 batch loss 0.543496311 epoch total loss 0.549398184\n",
      "Trained batch 2512 batch loss 0.47484073 epoch total loss 0.549368501\n",
      "Trained batch 2513 batch loss 0.604203463 epoch total loss 0.549390316\n",
      "Trained batch 2514 batch loss 0.562366128 epoch total loss 0.549395502\n",
      "Trained batch 2515 batch loss 0.628744543 epoch total loss 0.549427092\n",
      "Trained batch 2516 batch loss 0.673259676 epoch total loss 0.549476266\n",
      "Trained batch 2517 batch loss 0.690023303 epoch total loss 0.549532115\n",
      "Trained batch 2518 batch loss 0.645381749 epoch total loss 0.549570203\n",
      "Trained batch 2519 batch loss 0.54524678 epoch total loss 0.549568474\n",
      "Trained batch 2520 batch loss 0.55352968 epoch total loss 0.549570084\n",
      "Trained batch 2521 batch loss 0.626826227 epoch total loss 0.54960072\n",
      "Trained batch 2522 batch loss 0.532565653 epoch total loss 0.549594\n",
      "Trained batch 2523 batch loss 0.654933095 epoch total loss 0.549635708\n",
      "Trained batch 2524 batch loss 0.685767591 epoch total loss 0.549689651\n",
      "Trained batch 2525 batch loss 0.59008944 epoch total loss 0.549705684\n",
      "Trained batch 2526 batch loss 0.646411121 epoch total loss 0.54974395\n",
      "Trained batch 2527 batch loss 0.586342871 epoch total loss 0.549758434\n",
      "Trained batch 2528 batch loss 0.631630778 epoch total loss 0.5497908\n",
      "Trained batch 2529 batch loss 0.505481482 epoch total loss 0.549773276\n",
      "Trained batch 2530 batch loss 0.507210135 epoch total loss 0.549756467\n",
      "Trained batch 2531 batch loss 0.563939691 epoch total loss 0.54976207\n",
      "Trained batch 2532 batch loss 0.554020703 epoch total loss 0.549763739\n",
      "Trained batch 2533 batch loss 0.605452359 epoch total loss 0.549785733\n",
      "Trained batch 2534 batch loss 0.508327 epoch total loss 0.549769402\n",
      "Trained batch 2535 batch loss 0.483736128 epoch total loss 0.549743354\n",
      "Trained batch 2536 batch loss 0.606228352 epoch total loss 0.549765587\n",
      "Trained batch 2537 batch loss 0.625836313 epoch total loss 0.549795568\n",
      "Trained batch 2538 batch loss 0.561857939 epoch total loss 0.549800336\n",
      "Trained batch 2539 batch loss 0.555674434 epoch total loss 0.549802661\n",
      "Trained batch 2540 batch loss 0.60937041 epoch total loss 0.549826145\n",
      "Trained batch 2541 batch loss 0.605595946 epoch total loss 0.54984808\n",
      "Trained batch 2542 batch loss 0.636264384 epoch total loss 0.549882054\n",
      "Trained batch 2543 batch loss 0.605473578 epoch total loss 0.549903929\n",
      "Trained batch 2544 batch loss 0.548856437 epoch total loss 0.549903512\n",
      "Trained batch 2545 batch loss 0.572635353 epoch total loss 0.549912393\n",
      "Trained batch 2546 batch loss 0.651788533 epoch total loss 0.549952388\n",
      "Trained batch 2547 batch loss 0.648943543 epoch total loss 0.54999125\n",
      "Trained batch 2548 batch loss 0.522042632 epoch total loss 0.549980342\n",
      "Trained batch 2549 batch loss 0.464398265 epoch total loss 0.549946725\n",
      "Trained batch 2550 batch loss 0.455264598 epoch total loss 0.549909592\n",
      "Trained batch 2551 batch loss 0.484770536 epoch total loss 0.549884081\n",
      "Trained batch 2552 batch loss 0.440005183 epoch total loss 0.549841046\n",
      "Trained batch 2553 batch loss 0.567122757 epoch total loss 0.549847841\n",
      "Trained batch 2554 batch loss 0.578095436 epoch total loss 0.549858868\n",
      "Trained batch 2555 batch loss 0.635783195 epoch total loss 0.549892485\n",
      "Trained batch 2556 batch loss 0.586262703 epoch total loss 0.549906731\n",
      "Trained batch 2557 batch loss 0.568672955 epoch total loss 0.549914122\n",
      "Trained batch 2558 batch loss 0.517982125 epoch total loss 0.549901605\n",
      "Trained batch 2559 batch loss 0.583687365 epoch total loss 0.549914837\n",
      "Trained batch 2560 batch loss 0.575459898 epoch total loss 0.549924791\n",
      "Trained batch 2561 batch loss 0.601315856 epoch total loss 0.549944878\n",
      "Trained batch 2562 batch loss 0.509354472 epoch total loss 0.549929\n",
      "Trained batch 2563 batch loss 0.562434554 epoch total loss 0.54993391\n",
      "Trained batch 2564 batch loss 0.552701652 epoch total loss 0.549935\n",
      "Trained batch 2565 batch loss 0.638509274 epoch total loss 0.549969554\n",
      "Trained batch 2566 batch loss 0.625319302 epoch total loss 0.549998939\n",
      "Trained batch 2567 batch loss 0.576553106 epoch total loss 0.550009251\n",
      "Trained batch 2568 batch loss 0.530261338 epoch total loss 0.550001562\n",
      "Trained batch 2569 batch loss 0.579412043 epoch total loss 0.550013065\n",
      "Trained batch 2570 batch loss 0.604838729 epoch total loss 0.550034404\n",
      "Trained batch 2571 batch loss 0.57382381 epoch total loss 0.550043643\n",
      "Trained batch 2572 batch loss 0.482504308 epoch total loss 0.550017416\n",
      "Trained batch 2573 batch loss 0.575985074 epoch total loss 0.55002749\n",
      "Trained batch 2574 batch loss 0.567252755 epoch total loss 0.550034165\n",
      "Trained batch 2575 batch loss 0.599508166 epoch total loss 0.550053358\n",
      "Trained batch 2576 batch loss 0.684333682 epoch total loss 0.550105512\n",
      "Trained batch 2577 batch loss 0.708957374 epoch total loss 0.550167143\n",
      "Trained batch 2578 batch loss 0.439473182 epoch total loss 0.550124228\n",
      "Trained batch 2579 batch loss 0.40760383 epoch total loss 0.550068915\n",
      "Trained batch 2580 batch loss 0.427014947 epoch total loss 0.550021231\n",
      "Trained batch 2581 batch loss 0.478142 epoch total loss 0.549993396\n",
      "Trained batch 2582 batch loss 0.500631 epoch total loss 0.549974263\n",
      "Trained batch 2583 batch loss 0.552127719 epoch total loss 0.549975097\n",
      "Trained batch 2584 batch loss 0.619681 epoch total loss 0.550002038\n",
      "Trained batch 2585 batch loss 0.5192855 epoch total loss 0.549990177\n",
      "Trained batch 2586 batch loss 0.531572342 epoch total loss 0.549983084\n",
      "Trained batch 2587 batch loss 0.480752856 epoch total loss 0.549956322\n",
      "Trained batch 2588 batch loss 0.488355368 epoch total loss 0.549932539\n",
      "Trained batch 2589 batch loss 0.551663399 epoch total loss 0.549933195\n",
      "Trained batch 2590 batch loss 0.558478236 epoch total loss 0.549936473\n",
      "Trained batch 2591 batch loss 0.55138427 epoch total loss 0.549937\n",
      "Trained batch 2592 batch loss 0.652915955 epoch total loss 0.549976766\n",
      "Trained batch 2593 batch loss 0.580844641 epoch total loss 0.549988687\n",
      "Trained batch 2594 batch loss 0.643952429 epoch total loss 0.550024867\n",
      "Trained batch 2595 batch loss 0.614361 epoch total loss 0.550049663\n",
      "Trained batch 2596 batch loss 0.50868082 epoch total loss 0.550033748\n",
      "Trained batch 2597 batch loss 0.580634654 epoch total loss 0.55004555\n",
      "Trained batch 2598 batch loss 0.546631813 epoch total loss 0.550044239\n",
      "Trained batch 2599 batch loss 0.704295158 epoch total loss 0.550103605\n",
      "Trained batch 2600 batch loss 0.690575361 epoch total loss 0.550157607\n",
      "Trained batch 2601 batch loss 0.611770391 epoch total loss 0.550181329\n",
      "Trained batch 2602 batch loss 0.683134139 epoch total loss 0.55023241\n",
      "Trained batch 2603 batch loss 0.586316705 epoch total loss 0.550246239\n",
      "Trained batch 2604 batch loss 0.585015595 epoch total loss 0.55025959\n",
      "Trained batch 2605 batch loss 0.591810584 epoch total loss 0.550275564\n",
      "Trained batch 2606 batch loss 0.636730433 epoch total loss 0.550308704\n",
      "Trained batch 2607 batch loss 0.595480323 epoch total loss 0.550326049\n",
      "Trained batch 2608 batch loss 0.608632445 epoch total loss 0.550348401\n",
      "Trained batch 2609 batch loss 0.677562475 epoch total loss 0.550397158\n",
      "Trained batch 2610 batch loss 0.576704264 epoch total loss 0.550407231\n",
      "Trained batch 2611 batch loss 0.547107756 epoch total loss 0.550406\n",
      "Trained batch 2612 batch loss 0.566971183 epoch total loss 0.550412357\n",
      "Trained batch 2613 batch loss 0.584387898 epoch total loss 0.550425291\n",
      "Trained batch 2614 batch loss 0.530523 epoch total loss 0.550417721\n",
      "Trained batch 2615 batch loss 0.69542712 epoch total loss 0.550473154\n",
      "Trained batch 2616 batch loss 0.602719843 epoch total loss 0.550493121\n",
      "Trained batch 2617 batch loss 0.538775325 epoch total loss 0.550488651\n",
      "Trained batch 2618 batch loss 0.626188636 epoch total loss 0.550517559\n",
      "Trained batch 2619 batch loss 0.609108329 epoch total loss 0.55054\n",
      "Trained batch 2620 batch loss 0.607107401 epoch total loss 0.550561547\n",
      "Trained batch 2621 batch loss 0.588866174 epoch total loss 0.55057615\n",
      "Trained batch 2622 batch loss 0.578123569 epoch total loss 0.550586641\n",
      "Trained batch 2623 batch loss 0.554375529 epoch total loss 0.550588071\n",
      "Trained batch 2624 batch loss 0.535058 epoch total loss 0.55058217\n",
      "Trained batch 2625 batch loss 0.587066412 epoch total loss 0.550596058\n",
      "Trained batch 2626 batch loss 0.591061711 epoch total loss 0.550611436\n",
      "Trained batch 2627 batch loss 0.72006619 epoch total loss 0.550675929\n",
      "Trained batch 2628 batch loss 0.61378324 epoch total loss 0.550699949\n",
      "Trained batch 2629 batch loss 0.597091675 epoch total loss 0.550717592\n",
      "Trained batch 2630 batch loss 0.581277847 epoch total loss 0.550729215\n",
      "Trained batch 2631 batch loss 0.534454048 epoch total loss 0.550723\n",
      "Trained batch 2632 batch loss 0.584803402 epoch total loss 0.550736\n",
      "Trained batch 2633 batch loss 0.546596527 epoch total loss 0.550734401\n",
      "Trained batch 2634 batch loss 0.452463359 epoch total loss 0.550697148\n",
      "Trained batch 2635 batch loss 0.4018718 epoch total loss 0.550640643\n",
      "Trained batch 2636 batch loss 0.449044853 epoch total loss 0.550602138\n",
      "Trained batch 2637 batch loss 0.460043788 epoch total loss 0.550567806\n",
      "Trained batch 2638 batch loss 0.587247074 epoch total loss 0.550581694\n",
      "Trained batch 2639 batch loss 0.55938518 epoch total loss 0.550585032\n",
      "Trained batch 2640 batch loss 0.537263274 epoch total loss 0.550579965\n",
      "Trained batch 2641 batch loss 0.5176211 epoch total loss 0.550567448\n",
      "Trained batch 2642 batch loss 0.532397926 epoch total loss 0.550560594\n",
      "Trained batch 2643 batch loss 0.666239262 epoch total loss 0.550604343\n",
      "Trained batch 2644 batch loss 0.599546909 epoch total loss 0.550622821\n",
      "Trained batch 2645 batch loss 0.538588762 epoch total loss 0.550618291\n",
      "Trained batch 2646 batch loss 0.548799753 epoch total loss 0.550617635\n",
      "Trained batch 2647 batch loss 0.519687235 epoch total loss 0.550605893\n",
      "Trained batch 2648 batch loss 0.580754578 epoch total loss 0.550617337\n",
      "Trained batch 2649 batch loss 0.58125484 epoch total loss 0.550628901\n",
      "Trained batch 2650 batch loss 0.583459318 epoch total loss 0.550641298\n",
      "Trained batch 2651 batch loss 0.640006661 epoch total loss 0.550675035\n",
      "Trained batch 2652 batch loss 0.670568228 epoch total loss 0.550720215\n",
      "Trained batch 2653 batch loss 0.615813613 epoch total loss 0.550744772\n",
      "Trained batch 2654 batch loss 0.638902903 epoch total loss 0.550778\n",
      "Trained batch 2655 batch loss 0.624520063 epoch total loss 0.550805748\n",
      "Trained batch 2656 batch loss 0.684567213 epoch total loss 0.550856113\n",
      "Trained batch 2657 batch loss 0.565187216 epoch total loss 0.550861537\n",
      "Trained batch 2658 batch loss 0.594882905 epoch total loss 0.550878048\n",
      "Trained batch 2659 batch loss 0.663366377 epoch total loss 0.550920367\n",
      "Trained batch 2660 batch loss 0.573694706 epoch total loss 0.55092895\n",
      "Trained batch 2661 batch loss 0.537412047 epoch total loss 0.550923824\n",
      "Trained batch 2662 batch loss 0.5637272 epoch total loss 0.550928652\n",
      "Trained batch 2663 batch loss 0.550565422 epoch total loss 0.550928473\n",
      "Trained batch 2664 batch loss 0.491895497 epoch total loss 0.55090636\n",
      "Trained batch 2665 batch loss 0.465846509 epoch total loss 0.550874412\n",
      "Trained batch 2666 batch loss 0.477617562 epoch total loss 0.550846934\n",
      "Trained batch 2667 batch loss 0.54466778 epoch total loss 0.550844669\n",
      "Trained batch 2668 batch loss 0.583280563 epoch total loss 0.550856769\n",
      "Trained batch 2669 batch loss 0.617325 epoch total loss 0.550881684\n",
      "Trained batch 2670 batch loss 0.569737673 epoch total loss 0.550888717\n",
      "Trained batch 2671 batch loss 0.5945822 epoch total loss 0.550905108\n",
      "Trained batch 2672 batch loss 0.566755295 epoch total loss 0.550911069\n",
      "Trained batch 2673 batch loss 0.612037957 epoch total loss 0.550933897\n",
      "Trained batch 2674 batch loss 0.477265716 epoch total loss 0.55090636\n",
      "Trained batch 2675 batch loss 0.562562585 epoch total loss 0.550910771\n",
      "Trained batch 2676 batch loss 0.560590565 epoch total loss 0.550914347\n",
      "Trained batch 2677 batch loss 0.456587732 epoch total loss 0.550879121\n",
      "Trained batch 2678 batch loss 0.518549562 epoch total loss 0.550867\n",
      "Trained batch 2679 batch loss 0.417208821 epoch total loss 0.550817132\n",
      "Trained batch 2680 batch loss 0.431444019 epoch total loss 0.550772607\n",
      "Trained batch 2681 batch loss 0.430499077 epoch total loss 0.550727725\n",
      "Trained batch 2682 batch loss 0.556468129 epoch total loss 0.55072993\n",
      "Trained batch 2683 batch loss 0.540285707 epoch total loss 0.550726\n",
      "Trained batch 2684 batch loss 0.509234667 epoch total loss 0.550710559\n",
      "Trained batch 2685 batch loss 0.521003306 epoch total loss 0.550699532\n",
      "Trained batch 2686 batch loss 0.447776049 epoch total loss 0.550661206\n",
      "Trained batch 2687 batch loss 0.541855812 epoch total loss 0.550657928\n",
      "Trained batch 2688 batch loss 0.539744675 epoch total loss 0.550653875\n",
      "Trained batch 2689 batch loss 0.541224658 epoch total loss 0.550650358\n",
      "Trained batch 2690 batch loss 0.422193885 epoch total loss 0.550602615\n",
      "Trained batch 2691 batch loss 0.380194902 epoch total loss 0.550539315\n",
      "Trained batch 2692 batch loss 0.450129628 epoch total loss 0.550502\n",
      "Trained batch 2693 batch loss 0.428355813 epoch total loss 0.550456643\n",
      "Trained batch 2694 batch loss 0.639272869 epoch total loss 0.550489604\n",
      "Trained batch 2695 batch loss 0.616323709 epoch total loss 0.550514042\n",
      "Trained batch 2696 batch loss 0.511114478 epoch total loss 0.550499439\n",
      "Trained batch 2697 batch loss 0.60846293 epoch total loss 0.550520957\n",
      "Trained batch 2698 batch loss 0.576625109 epoch total loss 0.550530612\n",
      "Trained batch 2699 batch loss 0.528336287 epoch total loss 0.550522387\n",
      "Trained batch 2700 batch loss 0.597369373 epoch total loss 0.550539792\n",
      "Trained batch 2701 batch loss 0.560872197 epoch total loss 0.550543606\n",
      "Trained batch 2702 batch loss 0.58407408 epoch total loss 0.550556064\n",
      "Trained batch 2703 batch loss 0.570446193 epoch total loss 0.550563395\n",
      "Trained batch 2704 batch loss 0.519337118 epoch total loss 0.550551832\n",
      "Trained batch 2705 batch loss 0.479637146 epoch total loss 0.550525606\n",
      "Trained batch 2706 batch loss 0.464695066 epoch total loss 0.550493896\n",
      "Trained batch 2707 batch loss 0.485878706 epoch total loss 0.55047\n",
      "Trained batch 2708 batch loss 0.47345674 epoch total loss 0.550441563\n",
      "Trained batch 2709 batch loss 0.494726419 epoch total loss 0.550421059\n",
      "Trained batch 2710 batch loss 0.583391547 epoch total loss 0.550433218\n",
      "Trained batch 2711 batch loss 0.59013772 epoch total loss 0.550447822\n",
      "Trained batch 2712 batch loss 0.509295881 epoch total loss 0.550432622\n",
      "Trained batch 2713 batch loss 0.552318573 epoch total loss 0.550433338\n",
      "Trained batch 2714 batch loss 0.556608617 epoch total loss 0.550435662\n",
      "Trained batch 2715 batch loss 0.5862028 epoch total loss 0.550448835\n",
      "Trained batch 2716 batch loss 0.664729238 epoch total loss 0.550490856\n",
      "Trained batch 2717 batch loss 0.616604149 epoch total loss 0.550515175\n",
      "Trained batch 2718 batch loss 0.538795352 epoch total loss 0.550510883\n",
      "Trained batch 2719 batch loss 0.550820589 epoch total loss 0.550511\n",
      "Trained batch 2720 batch loss 0.611559391 epoch total loss 0.550533414\n",
      "Trained batch 2721 batch loss 0.634154737 epoch total loss 0.55056417\n",
      "Trained batch 2722 batch loss 0.621075809 epoch total loss 0.550590098\n",
      "Trained batch 2723 batch loss 0.56679225 epoch total loss 0.550596\n",
      "Trained batch 2724 batch loss 0.614663363 epoch total loss 0.550619543\n",
      "Trained batch 2725 batch loss 0.660142779 epoch total loss 0.550659716\n",
      "Trained batch 2726 batch loss 0.615256488 epoch total loss 0.550683379\n",
      "Trained batch 2727 batch loss 0.559573054 epoch total loss 0.550686657\n",
      "Trained batch 2728 batch loss 0.572643161 epoch total loss 0.550694704\n",
      "Trained batch 2729 batch loss 0.693155468 epoch total loss 0.550746918\n",
      "Trained batch 2730 batch loss 0.653451443 epoch total loss 0.550784528\n",
      "Trained batch 2731 batch loss 0.580952048 epoch total loss 0.550795555\n",
      "Trained batch 2732 batch loss 0.59425962 epoch total loss 0.55081147\n",
      "Trained batch 2733 batch loss 0.514718056 epoch total loss 0.550798297\n",
      "Trained batch 2734 batch loss 0.559828281 epoch total loss 0.550801575\n",
      "Trained batch 2735 batch loss 0.541036963 epoch total loss 0.550798\n",
      "Trained batch 2736 batch loss 0.579300404 epoch total loss 0.55080843\n",
      "Trained batch 2737 batch loss 0.591871738 epoch total loss 0.55082345\n",
      "Trained batch 2738 batch loss 0.606938124 epoch total loss 0.550843954\n",
      "Trained batch 2739 batch loss 0.588947773 epoch total loss 0.550857842\n",
      "Trained batch 2740 batch loss 0.566734314 epoch total loss 0.550863683\n",
      "Trained batch 2741 batch loss 0.505535483 epoch total loss 0.550847113\n",
      "Trained batch 2742 batch loss 0.499433666 epoch total loss 0.550828338\n",
      "Trained batch 2743 batch loss 0.537979 epoch total loss 0.550823689\n",
      "Trained batch 2744 batch loss 0.611605763 epoch total loss 0.550845802\n",
      "Trained batch 2745 batch loss 0.528691173 epoch total loss 0.550837755\n",
      "Trained batch 2746 batch loss 0.514741302 epoch total loss 0.550824583\n",
      "Trained batch 2747 batch loss 0.464641 epoch total loss 0.550793231\n",
      "Trained batch 2748 batch loss 0.517907202 epoch total loss 0.55078125\n",
      "Trained batch 2749 batch loss 0.608574092 epoch total loss 0.550802231\n",
      "Trained batch 2750 batch loss 0.57176137 epoch total loss 0.55080986\n",
      "Trained batch 2751 batch loss 0.554802597 epoch total loss 0.55081135\n",
      "Trained batch 2752 batch loss 0.532420635 epoch total loss 0.550804675\n",
      "Trained batch 2753 batch loss 0.508402407 epoch total loss 0.550789297\n",
      "Trained batch 2754 batch loss 0.528241396 epoch total loss 0.550781071\n",
      "Trained batch 2755 batch loss 0.51301378 epoch total loss 0.550767362\n",
      "Trained batch 2756 batch loss 0.537023067 epoch total loss 0.550762355\n",
      "Trained batch 2757 batch loss 0.535520077 epoch total loss 0.550756872\n",
      "Trained batch 2758 batch loss 0.481478572 epoch total loss 0.550731719\n",
      "Trained batch 2759 batch loss 0.504835069 epoch total loss 0.550715089\n",
      "Trained batch 2760 batch loss 0.478131831 epoch total loss 0.550688803\n",
      "Trained batch 2761 batch loss 0.541253865 epoch total loss 0.550685406\n",
      "Trained batch 2762 batch loss 0.51746273 epoch total loss 0.550673366\n",
      "Trained batch 2763 batch loss 0.594487906 epoch total loss 0.55068922\n",
      "Trained batch 2764 batch loss 0.488398165 epoch total loss 0.55066669\n",
      "Trained batch 2765 batch loss 0.533283472 epoch total loss 0.550660431\n",
      "Trained batch 2766 batch loss 0.559342086 epoch total loss 0.550663531\n",
      "Trained batch 2767 batch loss 0.520141482 epoch total loss 0.550652504\n",
      "Trained batch 2768 batch loss 0.50253576 epoch total loss 0.550635159\n",
      "Trained batch 2769 batch loss 0.52411288 epoch total loss 0.550625563\n",
      "Trained batch 2770 batch loss 0.54537183 epoch total loss 0.550623715\n",
      "Trained batch 2771 batch loss 0.524827182 epoch total loss 0.550614357\n",
      "Trained batch 2772 batch loss 0.456793904 epoch total loss 0.550580502\n",
      "Trained batch 2773 batch loss 0.511727333 epoch total loss 0.550566494\n",
      "Trained batch 2774 batch loss 0.631258309 epoch total loss 0.550595582\n",
      "Trained batch 2775 batch loss 0.564683259 epoch total loss 0.550600648\n",
      "Trained batch 2776 batch loss 0.564430296 epoch total loss 0.550605655\n",
      "Epoch 7 train loss 0.5506056547164917\n",
      "Validated batch 1 batch loss 0.552288413\n",
      "Validated batch 2 batch loss 0.638592839\n",
      "Validated batch 3 batch loss 0.534728765\n",
      "Validated batch 4 batch loss 0.568308\n",
      "Validated batch 5 batch loss 0.4805246\n",
      "Validated batch 6 batch loss 0.633283377\n",
      "Validated batch 7 batch loss 0.485817164\n",
      "Validated batch 8 batch loss 0.610190451\n",
      "Validated batch 9 batch loss 0.560220957\n",
      "Validated batch 10 batch loss 0.520150959\n",
      "Validated batch 11 batch loss 0.645339906\n",
      "Validated batch 12 batch loss 0.581530154\n",
      "Validated batch 13 batch loss 0.661116302\n",
      "Validated batch 14 batch loss 0.504302859\n",
      "Validated batch 15 batch loss 0.553771377\n",
      "Validated batch 16 batch loss 0.528344452\n",
      "Validated batch 17 batch loss 0.547330379\n",
      "Validated batch 18 batch loss 0.707661808\n",
      "Validated batch 19 batch loss 0.620209575\n",
      "Validated batch 20 batch loss 0.587280333\n",
      "Validated batch 21 batch loss 0.515195251\n",
      "Validated batch 22 batch loss 0.530288875\n",
      "Validated batch 23 batch loss 0.462533832\n",
      "Validated batch 24 batch loss 0.539711297\n",
      "Validated batch 25 batch loss 0.542087495\n",
      "Validated batch 26 batch loss 0.626860261\n",
      "Validated batch 27 batch loss 0.62971735\n",
      "Validated batch 28 batch loss 0.555690169\n",
      "Validated batch 29 batch loss 0.661814749\n",
      "Validated batch 30 batch loss 0.635782421\n",
      "Validated batch 31 batch loss 0.654939234\n",
      "Validated batch 32 batch loss 0.61877054\n",
      "Validated batch 33 batch loss 0.521076202\n",
      "Validated batch 34 batch loss 0.640331209\n",
      "Validated batch 35 batch loss 0.703934133\n",
      "Validated batch 36 batch loss 0.564348817\n",
      "Validated batch 37 batch loss 0.575714171\n",
      "Validated batch 38 batch loss 0.626030862\n",
      "Validated batch 39 batch loss 0.621860445\n",
      "Validated batch 40 batch loss 0.536343634\n",
      "Validated batch 41 batch loss 0.62862891\n",
      "Validated batch 42 batch loss 0.532428384\n",
      "Validated batch 43 batch loss 0.418067932\n",
      "Validated batch 44 batch loss 0.485211\n",
      "Validated batch 45 batch loss 0.523310304\n",
      "Validated batch 46 batch loss 0.594690442\n",
      "Validated batch 47 batch loss 0.491125703\n",
      "Validated batch 48 batch loss 0.619513512\n",
      "Validated batch 49 batch loss 0.612495124\n",
      "Validated batch 50 batch loss 0.479204237\n",
      "Validated batch 51 batch loss 0.629221916\n",
      "Validated batch 52 batch loss 0.469963908\n",
      "Validated batch 53 batch loss 0.533076763\n",
      "Validated batch 54 batch loss 0.578740239\n",
      "Validated batch 55 batch loss 0.579829216\n",
      "Validated batch 56 batch loss 0.468498558\n",
      "Validated batch 57 batch loss 0.706308126\n",
      "Validated batch 58 batch loss 0.506752908\n",
      "Validated batch 59 batch loss 0.526668966\n",
      "Validated batch 60 batch loss 0.569119096\n",
      "Validated batch 61 batch loss 0.499292254\n",
      "Validated batch 62 batch loss 0.464067161\n",
      "Validated batch 63 batch loss 0.496017218\n",
      "Validated batch 64 batch loss 0.564208269\n",
      "Validated batch 65 batch loss 0.610119\n",
      "Validated batch 66 batch loss 0.483817726\n",
      "Validated batch 67 batch loss 0.512150884\n",
      "Validated batch 68 batch loss 0.533083737\n",
      "Validated batch 69 batch loss 0.606948078\n",
      "Validated batch 70 batch loss 0.470673203\n",
      "Validated batch 71 batch loss 0.479794025\n",
      "Validated batch 72 batch loss 0.577247858\n",
      "Validated batch 73 batch loss 0.543350577\n",
      "Validated batch 74 batch loss 0.557653725\n",
      "Validated batch 75 batch loss 0.670375645\n",
      "Validated batch 76 batch loss 0.623526\n",
      "Validated batch 77 batch loss 0.596939266\n",
      "Validated batch 78 batch loss 0.552960515\n",
      "Validated batch 79 batch loss 0.553370476\n",
      "Validated batch 80 batch loss 0.576655626\n",
      "Validated batch 81 batch loss 0.652592897\n",
      "Validated batch 82 batch loss 0.573311329\n",
      "Validated batch 83 batch loss 0.455884188\n",
      "Validated batch 84 batch loss 0.472187459\n",
      "Validated batch 85 batch loss 0.456105\n",
      "Validated batch 86 batch loss 0.590581417\n",
      "Validated batch 87 batch loss 0.504865885\n",
      "Validated batch 88 batch loss 0.463165492\n",
      "Validated batch 89 batch loss 0.550742328\n",
      "Validated batch 90 batch loss 0.583398581\n",
      "Validated batch 91 batch loss 0.69470036\n",
      "Validated batch 92 batch loss 0.602492213\n",
      "Validated batch 93 batch loss 0.556970954\n",
      "Validated batch 94 batch loss 0.584217489\n",
      "Validated batch 95 batch loss 0.568277836\n",
      "Validated batch 96 batch loss 0.550834656\n",
      "Validated batch 97 batch loss 0.552567\n",
      "Validated batch 98 batch loss 0.553702831\n",
      "Validated batch 99 batch loss 0.524174869\n",
      "Validated batch 100 batch loss 0.631896377\n",
      "Validated batch 101 batch loss 0.606431544\n",
      "Validated batch 102 batch loss 0.591138721\n",
      "Validated batch 103 batch loss 0.722536206\n",
      "Validated batch 104 batch loss 0.615243912\n",
      "Validated batch 105 batch loss 0.524470448\n",
      "Validated batch 106 batch loss 0.562034488\n",
      "Validated batch 107 batch loss 0.565667212\n",
      "Validated batch 108 batch loss 0.617190838\n",
      "Validated batch 109 batch loss 0.546359181\n",
      "Validated batch 110 batch loss 0.586909056\n",
      "Validated batch 111 batch loss 0.599583685\n",
      "Validated batch 112 batch loss 0.600963235\n",
      "Validated batch 113 batch loss 0.588120103\n",
      "Validated batch 114 batch loss 0.622725785\n",
      "Validated batch 115 batch loss 0.535303116\n",
      "Validated batch 116 batch loss 0.505337238\n",
      "Validated batch 117 batch loss 0.611041188\n",
      "Validated batch 118 batch loss 0.750670969\n",
      "Validated batch 119 batch loss 0.620194\n",
      "Validated batch 120 batch loss 0.490473479\n",
      "Validated batch 121 batch loss 0.609470844\n",
      "Validated batch 122 batch loss 0.639874637\n",
      "Validated batch 123 batch loss 0.53009671\n",
      "Validated batch 124 batch loss 0.609226882\n",
      "Validated batch 125 batch loss 0.631573498\n",
      "Validated batch 126 batch loss 0.621245503\n",
      "Validated batch 127 batch loss 0.595697284\n",
      "Validated batch 128 batch loss 0.437014341\n",
      "Validated batch 129 batch loss 0.589037\n",
      "Validated batch 130 batch loss 0.607793212\n",
      "Validated batch 131 batch loss 0.573554695\n",
      "Validated batch 132 batch loss 0.524582505\n",
      "Validated batch 133 batch loss 0.580247104\n",
      "Validated batch 134 batch loss 0.534009933\n",
      "Validated batch 135 batch loss 0.594874144\n",
      "Validated batch 136 batch loss 0.554034114\n",
      "Validated batch 137 batch loss 0.508205891\n",
      "Validated batch 138 batch loss 0.562251568\n",
      "Validated batch 139 batch loss 0.572568655\n",
      "Validated batch 140 batch loss 0.580613673\n",
      "Validated batch 141 batch loss 0.600348\n",
      "Validated batch 142 batch loss 0.582598805\n",
      "Validated batch 143 batch loss 0.558053792\n",
      "Validated batch 144 batch loss 0.572322071\n",
      "Validated batch 145 batch loss 0.51237756\n",
      "Validated batch 146 batch loss 0.568555117\n",
      "Validated batch 147 batch loss 0.551642656\n",
      "Validated batch 148 batch loss 0.566248238\n",
      "Validated batch 149 batch loss 0.563824952\n",
      "Validated batch 150 batch loss 0.653283954\n",
      "Validated batch 151 batch loss 0.599049807\n",
      "Validated batch 152 batch loss 0.5961411\n",
      "Validated batch 153 batch loss 0.594998956\n",
      "Validated batch 154 batch loss 0.653133929\n",
      "Validated batch 155 batch loss 0.615767777\n",
      "Validated batch 156 batch loss 0.553703249\n",
      "Validated batch 157 batch loss 0.546152532\n",
      "Validated batch 158 batch loss 0.528665721\n",
      "Validated batch 159 batch loss 0.585126221\n",
      "Validated batch 160 batch loss 0.573114693\n",
      "Validated batch 161 batch loss 0.605398476\n",
      "Validated batch 162 batch loss 0.498785824\n",
      "Validated batch 163 batch loss 0.568860412\n",
      "Validated batch 164 batch loss 0.606939375\n",
      "Validated batch 165 batch loss 0.679165304\n",
      "Validated batch 166 batch loss 0.600008\n",
      "Validated batch 167 batch loss 0.620969117\n",
      "Validated batch 168 batch loss 0.592561066\n",
      "Validated batch 169 batch loss 0.593690455\n",
      "Validated batch 170 batch loss 0.612710416\n",
      "Validated batch 171 batch loss 0.569039643\n",
      "Validated batch 172 batch loss 0.756890059\n",
      "Validated batch 173 batch loss 0.524903059\n",
      "Validated batch 174 batch loss 0.543249428\n",
      "Validated batch 175 batch loss 0.560038567\n",
      "Validated batch 176 batch loss 0.620908439\n",
      "Validated batch 177 batch loss 0.531322718\n",
      "Validated batch 178 batch loss 0.456662655\n",
      "Validated batch 179 batch loss 0.484456569\n",
      "Validated batch 180 batch loss 0.586573362\n",
      "Validated batch 181 batch loss 0.600346923\n",
      "Validated batch 182 batch loss 0.626937\n",
      "Validated batch 183 batch loss 0.500352144\n",
      "Validated batch 184 batch loss 0.537056148\n",
      "Validated batch 185 batch loss 0.562077641\n",
      "Validated batch 186 batch loss 0.533103585\n",
      "Validated batch 187 batch loss 0.550242901\n",
      "Validated batch 188 batch loss 0.50369978\n",
      "Validated batch 189 batch loss 0.502585351\n",
      "Validated batch 190 batch loss 0.548080325\n",
      "Validated batch 191 batch loss 0.559896052\n",
      "Validated batch 192 batch loss 0.488351852\n",
      "Validated batch 193 batch loss 0.598337471\n",
      "Validated batch 194 batch loss 0.49941051\n",
      "Validated batch 195 batch loss 0.562721372\n",
      "Validated batch 196 batch loss 0.62983495\n",
      "Validated batch 197 batch loss 0.506493866\n",
      "Validated batch 198 batch loss 0.618412614\n",
      "Validated batch 199 batch loss 0.579570949\n",
      "Validated batch 200 batch loss 0.647803545\n",
      "Validated batch 201 batch loss 0.590584755\n",
      "Validated batch 202 batch loss 0.580524862\n",
      "Validated batch 203 batch loss 0.611128032\n",
      "Validated batch 204 batch loss 0.521779716\n",
      "Validated batch 205 batch loss 0.545377076\n",
      "Validated batch 206 batch loss 0.591134\n",
      "Validated batch 207 batch loss 0.648918271\n",
      "Validated batch 208 batch loss 0.663426101\n",
      "Validated batch 209 batch loss 0.565454304\n",
      "Validated batch 210 batch loss 0.608140111\n",
      "Validated batch 211 batch loss 0.632039845\n",
      "Validated batch 212 batch loss 0.621108472\n",
      "Validated batch 213 batch loss 0.606616378\n",
      "Validated batch 214 batch loss 0.624550223\n",
      "Validated batch 215 batch loss 0.647857726\n",
      "Validated batch 216 batch loss 0.636710823\n",
      "Validated batch 217 batch loss 0.644033194\n",
      "Validated batch 218 batch loss 0.595215678\n",
      "Validated batch 219 batch loss 0.629358053\n",
      "Validated batch 220 batch loss 0.458357245\n",
      "Validated batch 221 batch loss 0.568867922\n",
      "Validated batch 222 batch loss 0.627691865\n",
      "Validated batch 223 batch loss 0.665180624\n",
      "Validated batch 224 batch loss 0.608902097\n",
      "Validated batch 225 batch loss 0.655823708\n",
      "Validated batch 226 batch loss 0.564513206\n",
      "Validated batch 227 batch loss 0.518984735\n",
      "Validated batch 228 batch loss 0.58184278\n",
      "Validated batch 229 batch loss 0.608612895\n",
      "Validated batch 230 batch loss 0.535481155\n",
      "Validated batch 231 batch loss 0.573196352\n",
      "Validated batch 232 batch loss 0.478028148\n",
      "Validated batch 233 batch loss 0.511770189\n",
      "Validated batch 234 batch loss 0.563613653\n",
      "Validated batch 235 batch loss 0.608460128\n",
      "Validated batch 236 batch loss 0.572837651\n",
      "Validated batch 237 batch loss 0.523096383\n",
      "Validated batch 238 batch loss 0.502711833\n",
      "Validated batch 239 batch loss 0.577650666\n",
      "Validated batch 240 batch loss 0.582146049\n",
      "Validated batch 241 batch loss 0.687925696\n",
      "Validated batch 242 batch loss 0.629673779\n",
      "Validated batch 243 batch loss 0.519750953\n",
      "Validated batch 244 batch loss 0.473213345\n",
      "Validated batch 245 batch loss 0.552421689\n",
      "Validated batch 246 batch loss 0.537371397\n",
      "Validated batch 247 batch loss 0.572442234\n",
      "Validated batch 248 batch loss 0.53909409\n",
      "Validated batch 249 batch loss 0.576095521\n",
      "Validated batch 250 batch loss 0.602986693\n",
      "Validated batch 251 batch loss 0.605640948\n",
      "Validated batch 252 batch loss 0.566484928\n",
      "Validated batch 253 batch loss 0.537875414\n",
      "Validated batch 254 batch loss 0.513508439\n",
      "Validated batch 255 batch loss 0.368443877\n",
      "Validated batch 256 batch loss 0.493708342\n",
      "Validated batch 257 batch loss 0.573587596\n",
      "Validated batch 258 batch loss 0.529484808\n",
      "Validated batch 259 batch loss 0.513581\n",
      "Validated batch 260 batch loss 0.489291757\n",
      "Validated batch 261 batch loss 0.533343196\n",
      "Validated batch 262 batch loss 0.546583652\n",
      "Validated batch 263 batch loss 0.651609898\n",
      "Validated batch 264 batch loss 0.559817195\n",
      "Validated batch 265 batch loss 0.528131\n",
      "Validated batch 266 batch loss 0.449476153\n",
      "Validated batch 267 batch loss 0.564237356\n",
      "Validated batch 268 batch loss 0.553786039\n",
      "Validated batch 269 batch loss 0.61647445\n",
      "Validated batch 270 batch loss 0.621135116\n",
      "Validated batch 271 batch loss 0.5803985\n",
      "Validated batch 272 batch loss 0.570532858\n",
      "Validated batch 273 batch loss 0.565027118\n",
      "Validated batch 274 batch loss 0.56174016\n",
      "Validated batch 275 batch loss 0.491764784\n",
      "Validated batch 276 batch loss 0.472199827\n",
      "Validated batch 277 batch loss 0.583177507\n",
      "Validated batch 278 batch loss 0.59558624\n",
      "Validated batch 279 batch loss 0.598447859\n",
      "Validated batch 280 batch loss 0.54708612\n",
      "Validated batch 281 batch loss 0.599127352\n",
      "Validated batch 282 batch loss 0.520169199\n",
      "Validated batch 283 batch loss 0.557561159\n",
      "Validated batch 284 batch loss 0.572637677\n",
      "Validated batch 285 batch loss 0.515799046\n",
      "Validated batch 286 batch loss 0.50536418\n",
      "Validated batch 287 batch loss 0.614632249\n",
      "Validated batch 288 batch loss 0.579676092\n",
      "Validated batch 289 batch loss 0.5507285\n",
      "Validated batch 290 batch loss 0.608936369\n",
      "Validated batch 291 batch loss 0.587461829\n",
      "Validated batch 292 batch loss 0.492833078\n",
      "Validated batch 293 batch loss 0.523933589\n",
      "Validated batch 294 batch loss 0.567032099\n",
      "Validated batch 295 batch loss 0.591612279\n",
      "Validated batch 296 batch loss 0.560073137\n",
      "Validated batch 297 batch loss 0.551853478\n",
      "Validated batch 298 batch loss 0.591715395\n",
      "Validated batch 299 batch loss 0.664795876\n",
      "Validated batch 300 batch loss 0.564262033\n",
      "Validated batch 301 batch loss 0.585729897\n",
      "Validated batch 302 batch loss 0.613705814\n",
      "Validated batch 303 batch loss 0.500545323\n",
      "Validated batch 304 batch loss 0.530936241\n",
      "Validated batch 305 batch loss 0.623707294\n",
      "Validated batch 306 batch loss 0.464524031\n",
      "Validated batch 307 batch loss 0.594893\n",
      "Validated batch 308 batch loss 0.550562739\n",
      "Validated batch 309 batch loss 0.490172356\n",
      "Validated batch 310 batch loss 0.598751128\n",
      "Validated batch 311 batch loss 0.546762049\n",
      "Validated batch 312 batch loss 0.608526051\n",
      "Validated batch 313 batch loss 0.560371161\n",
      "Validated batch 314 batch loss 0.554555595\n",
      "Validated batch 315 batch loss 0.63676095\n",
      "Validated batch 316 batch loss 0.656886637\n",
      "Validated batch 317 batch loss 0.7143839\n",
      "Validated batch 318 batch loss 0.587050915\n",
      "Validated batch 319 batch loss 0.509914279\n",
      "Validated batch 320 batch loss 0.598233402\n",
      "Validated batch 321 batch loss 0.525655806\n",
      "Validated batch 322 batch loss 0.501277328\n",
      "Validated batch 323 batch loss 0.490670681\n",
      "Validated batch 324 batch loss 0.534713209\n",
      "Validated batch 325 batch loss 0.523961365\n",
      "Validated batch 326 batch loss 0.478965223\n",
      "Validated batch 327 batch loss 0.558487892\n",
      "Validated batch 328 batch loss 0.583850324\n",
      "Validated batch 329 batch loss 0.515246749\n",
      "Validated batch 330 batch loss 0.567638278\n",
      "Validated batch 331 batch loss 0.507647932\n",
      "Validated batch 332 batch loss 0.540675104\n",
      "Validated batch 333 batch loss 0.540136099\n",
      "Validated batch 334 batch loss 0.645357609\n",
      "Validated batch 335 batch loss 0.545803726\n",
      "Validated batch 336 batch loss 0.503468812\n",
      "Validated batch 337 batch loss 0.605481565\n",
      "Validated batch 338 batch loss 0.545303822\n",
      "Validated batch 339 batch loss 0.63341713\n",
      "Validated batch 340 batch loss 0.611645937\n",
      "Validated batch 341 batch loss 0.478549\n",
      "Validated batch 342 batch loss 0.493194401\n",
      "Validated batch 343 batch loss 0.648576379\n",
      "Validated batch 344 batch loss 0.552229941\n",
      "Validated batch 345 batch loss 0.572712898\n",
      "Validated batch 346 batch loss 0.598373175\n",
      "Validated batch 347 batch loss 0.483363092\n",
      "Validated batch 348 batch loss 0.529996634\n",
      "Validated batch 349 batch loss 0.642135322\n",
      "Validated batch 350 batch loss 0.515158594\n",
      "Validated batch 351 batch loss 0.612587\n",
      "Validated batch 352 batch loss 0.572502792\n",
      "Validated batch 353 batch loss 0.538324833\n",
      "Validated batch 354 batch loss 0.521723509\n",
      "Validated batch 355 batch loss 0.591511965\n",
      "Validated batch 356 batch loss 0.654142559\n",
      "Validated batch 357 batch loss 0.602638304\n",
      "Validated batch 358 batch loss 0.533388734\n",
      "Validated batch 359 batch loss 0.630224228\n",
      "Validated batch 360 batch loss 0.543070436\n",
      "Validated batch 361 batch loss 0.435145736\n",
      "Validated batch 362 batch loss 0.580655932\n",
      "Validated batch 363 batch loss 0.62732327\n",
      "Validated batch 364 batch loss 0.561775\n",
      "Validated batch 365 batch loss 0.553383946\n",
      "Validated batch 366 batch loss 0.570431352\n",
      "Validated batch 367 batch loss 0.46093291\n",
      "Validated batch 368 batch loss 0.588765919\n",
      "Validated batch 369 batch loss 0.549571395\n",
      "Epoch 7 val loss 0.5679338574409485\n",
      "Model /aiffel/mpii/weights/shg_model-epoch-7-loss-0.5679.h5 saved.\n",
      "Start epoch 8 with learning rate 0.0007\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 0.605622292 epoch total loss 0.605622292\n",
      "Trained batch 2 batch loss 0.525204301 epoch total loss 0.565413296\n",
      "Trained batch 3 batch loss 0.584462643 epoch total loss 0.571763098\n",
      "Trained batch 4 batch loss 0.513204098 epoch total loss 0.557123303\n",
      "Trained batch 5 batch loss 0.560763419 epoch total loss 0.557851315\n",
      "Trained batch 6 batch loss 0.535461724 epoch total loss 0.554119706\n",
      "Trained batch 7 batch loss 0.528857231 epoch total loss 0.550510764\n",
      "Trained batch 8 batch loss 0.534958661 epoch total loss 0.548566759\n",
      "Trained batch 9 batch loss 0.460723311 epoch total loss 0.538806379\n",
      "Trained batch 10 batch loss 0.503479719 epoch total loss 0.535273731\n",
      "Trained batch 11 batch loss 0.587533653 epoch total loss 0.540024638\n",
      "Trained batch 12 batch loss 0.501428843 epoch total loss 0.536808312\n",
      "Trained batch 13 batch loss 0.479473233 epoch total loss 0.532397926\n",
      "Trained batch 14 batch loss 0.47803694 epoch total loss 0.528515\n",
      "Trained batch 15 batch loss 0.521157503 epoch total loss 0.528024495\n",
      "Trained batch 16 batch loss 0.542401314 epoch total loss 0.528923035\n",
      "Trained batch 17 batch loss 0.500979185 epoch total loss 0.527279317\n",
      "Trained batch 18 batch loss 0.565780282 epoch total loss 0.52941823\n",
      "Trained batch 19 batch loss 0.621502519 epoch total loss 0.534264803\n",
      "Trained batch 20 batch loss 0.615776122 epoch total loss 0.53834039\n",
      "Trained batch 21 batch loss 0.564931691 epoch total loss 0.539606631\n",
      "Trained batch 22 batch loss 0.609258056 epoch total loss 0.542772591\n",
      "Trained batch 23 batch loss 0.608599246 epoch total loss 0.545634627\n",
      "Trained batch 24 batch loss 0.608281195 epoch total loss 0.548244894\n",
      "Trained batch 25 batch loss 0.590102792 epoch total loss 0.549919248\n",
      "Trained batch 26 batch loss 0.593649089 epoch total loss 0.551601171\n",
      "Trained batch 27 batch loss 0.564625382 epoch total loss 0.552083552\n",
      "Trained batch 28 batch loss 0.481579274 epoch total loss 0.549565494\n",
      "Trained batch 29 batch loss 0.605228424 epoch total loss 0.551484942\n",
      "Trained batch 30 batch loss 0.550865412 epoch total loss 0.55146426\n",
      "Trained batch 31 batch loss 0.639942765 epoch total loss 0.554318368\n",
      "Trained batch 32 batch loss 0.529820323 epoch total loss 0.553552806\n",
      "Trained batch 33 batch loss 0.495919466 epoch total loss 0.55180639\n",
      "Trained batch 34 batch loss 0.537949443 epoch total loss 0.551398754\n",
      "Trained batch 35 batch loss 0.531464 epoch total loss 0.550829232\n",
      "Trained batch 36 batch loss 0.517187119 epoch total loss 0.549894691\n",
      "Trained batch 37 batch loss 0.478531241 epoch total loss 0.547965944\n",
      "Trained batch 38 batch loss 0.612636089 epoch total loss 0.549667835\n",
      "Trained batch 39 batch loss 0.467876107 epoch total loss 0.547570586\n",
      "Trained batch 40 batch loss 0.471190661 epoch total loss 0.545661092\n",
      "Trained batch 41 batch loss 0.430359185 epoch total loss 0.542848885\n",
      "Trained batch 42 batch loss 0.480952144 epoch total loss 0.541375101\n",
      "Trained batch 43 batch loss 0.470967621 epoch total loss 0.539737761\n",
      "Trained batch 44 batch loss 0.5241 epoch total loss 0.539382339\n",
      "Trained batch 45 batch loss 0.561708 epoch total loss 0.539878488\n",
      "Trained batch 46 batch loss 0.539980054 epoch total loss 0.539880693\n",
      "Trained batch 47 batch loss 0.505966544 epoch total loss 0.53915906\n",
      "Trained batch 48 batch loss 0.507119417 epoch total loss 0.538491607\n",
      "Trained batch 49 batch loss 0.512336135 epoch total loss 0.537957847\n",
      "Trained batch 50 batch loss 0.535816431 epoch total loss 0.537915\n",
      "Trained batch 51 batch loss 0.583846152 epoch total loss 0.538815618\n",
      "Trained batch 52 batch loss 0.549982607 epoch total loss 0.539030373\n",
      "Trained batch 53 batch loss 0.475036472 epoch total loss 0.537822962\n",
      "Trained batch 54 batch loss 0.454313457 epoch total loss 0.53627646\n",
      "Trained batch 55 batch loss 0.488546968 epoch total loss 0.535408616\n",
      "Trained batch 56 batch loss 0.357754022 epoch total loss 0.532236218\n",
      "Trained batch 57 batch loss 0.401772827 epoch total loss 0.5299474\n",
      "Trained batch 58 batch loss 0.499453187 epoch total loss 0.529421628\n",
      "Trained batch 59 batch loss 0.525106311 epoch total loss 0.529348493\n",
      "Trained batch 60 batch loss 0.606468499 epoch total loss 0.530633807\n",
      "Trained batch 61 batch loss 0.558537364 epoch total loss 0.531091273\n",
      "Trained batch 62 batch loss 0.638662934 epoch total loss 0.532826304\n",
      "Trained batch 63 batch loss 0.597279191 epoch total loss 0.533849359\n",
      "Trained batch 64 batch loss 0.504500091 epoch total loss 0.53339082\n",
      "Trained batch 65 batch loss 0.561045349 epoch total loss 0.533816278\n",
      "Trained batch 66 batch loss 0.647768259 epoch total loss 0.535542846\n",
      "Trained batch 67 batch loss 0.612496912 epoch total loss 0.536691427\n",
      "Trained batch 68 batch loss 0.529395 epoch total loss 0.536584139\n",
      "Trained batch 69 batch loss 0.521042168 epoch total loss 0.536358893\n",
      "Trained batch 70 batch loss 0.584553063 epoch total loss 0.537047386\n",
      "Trained batch 71 batch loss 0.581235826 epoch total loss 0.537669718\n",
      "Trained batch 72 batch loss 0.53209579 epoch total loss 0.537592292\n",
      "Trained batch 73 batch loss 0.600901902 epoch total loss 0.538459539\n",
      "Trained batch 74 batch loss 0.544437051 epoch total loss 0.538540363\n",
      "Trained batch 75 batch loss 0.544124901 epoch total loss 0.53861481\n",
      "Trained batch 76 batch loss 0.544236779 epoch total loss 0.538688779\n",
      "Trained batch 77 batch loss 0.553058147 epoch total loss 0.538875401\n",
      "Trained batch 78 batch loss 0.631138504 epoch total loss 0.540058255\n",
      "Trained batch 79 batch loss 0.524898589 epoch total loss 0.539866328\n",
      "Trained batch 80 batch loss 0.566089511 epoch total loss 0.540194154\n",
      "Trained batch 81 batch loss 0.592570245 epoch total loss 0.540840745\n",
      "Trained batch 82 batch loss 0.64693284 epoch total loss 0.542134583\n",
      "Trained batch 83 batch loss 0.577258527 epoch total loss 0.542557776\n",
      "Trained batch 84 batch loss 0.545936346 epoch total loss 0.542598\n",
      "Trained batch 85 batch loss 0.520169139 epoch total loss 0.542334139\n",
      "Trained batch 86 batch loss 0.598526657 epoch total loss 0.542987525\n",
      "Trained batch 87 batch loss 0.515844047 epoch total loss 0.542675495\n",
      "Trained batch 88 batch loss 0.562459469 epoch total loss 0.542900324\n",
      "Trained batch 89 batch loss 0.567573249 epoch total loss 0.543177545\n",
      "Trained batch 90 batch loss 0.53532809 epoch total loss 0.543090284\n",
      "Trained batch 91 batch loss 0.484191954 epoch total loss 0.542443097\n",
      "Trained batch 92 batch loss 0.519069731 epoch total loss 0.542189\n",
      "Trained batch 93 batch loss 0.515684128 epoch total loss 0.541904\n",
      "Trained batch 94 batch loss 0.518210769 epoch total loss 0.541651964\n",
      "Trained batch 95 batch loss 0.511988759 epoch total loss 0.541339695\n",
      "Trained batch 96 batch loss 0.571799397 epoch total loss 0.541657031\n",
      "Trained batch 97 batch loss 0.634331226 epoch total loss 0.542612433\n",
      "Trained batch 98 batch loss 0.551527202 epoch total loss 0.54270339\n",
      "Trained batch 99 batch loss 0.511971295 epoch total loss 0.542392969\n",
      "Trained batch 100 batch loss 0.694293141 epoch total loss 0.543912\n",
      "Trained batch 101 batch loss 0.605405569 epoch total loss 0.544520795\n",
      "Trained batch 102 batch loss 0.603454 epoch total loss 0.545098603\n",
      "Trained batch 103 batch loss 0.62381196 epoch total loss 0.545862794\n",
      "Trained batch 104 batch loss 0.570904672 epoch total loss 0.546103597\n",
      "Trained batch 105 batch loss 0.520804524 epoch total loss 0.545862675\n",
      "Trained batch 106 batch loss 0.561395347 epoch total loss 0.546009183\n",
      "Trained batch 107 batch loss 0.595347643 epoch total loss 0.546470284\n",
      "Trained batch 108 batch loss 0.550420046 epoch total loss 0.546506822\n",
      "Trained batch 109 batch loss 0.627064466 epoch total loss 0.54724592\n",
      "Trained batch 110 batch loss 0.565228462 epoch total loss 0.547409356\n",
      "Trained batch 111 batch loss 0.557056308 epoch total loss 0.547496259\n",
      "Trained batch 112 batch loss 0.464158624 epoch total loss 0.546752155\n",
      "Trained batch 113 batch loss 0.578665376 epoch total loss 0.547034621\n",
      "Trained batch 114 batch loss 0.574397 epoch total loss 0.547274649\n",
      "Trained batch 115 batch loss 0.565166414 epoch total loss 0.547430217\n",
      "Trained batch 116 batch loss 0.472144097 epoch total loss 0.546781182\n",
      "Trained batch 117 batch loss 0.519583583 epoch total loss 0.546548784\n",
      "Trained batch 118 batch loss 0.532915473 epoch total loss 0.54643321\n",
      "Trained batch 119 batch loss 0.544478834 epoch total loss 0.546416759\n",
      "Trained batch 120 batch loss 0.589885831 epoch total loss 0.546779037\n",
      "Trained batch 121 batch loss 0.491445 epoch total loss 0.54632175\n",
      "Trained batch 122 batch loss 0.509372413 epoch total loss 0.546018898\n",
      "Trained batch 123 batch loss 0.495000422 epoch total loss 0.54560411\n",
      "Trained batch 124 batch loss 0.512724578 epoch total loss 0.545339\n",
      "Trained batch 125 batch loss 0.53524667 epoch total loss 0.545258224\n",
      "Trained batch 126 batch loss 0.456903756 epoch total loss 0.544557\n",
      "Trained batch 127 batch loss 0.47756356 epoch total loss 0.544029474\n",
      "Trained batch 128 batch loss 0.538312316 epoch total loss 0.54398483\n",
      "Trained batch 129 batch loss 0.43467623 epoch total loss 0.543137491\n",
      "Trained batch 130 batch loss 0.486348629 epoch total loss 0.542700648\n",
      "Trained batch 131 batch loss 0.468812793 epoch total loss 0.54213661\n",
      "Trained batch 132 batch loss 0.444262058 epoch total loss 0.541395128\n",
      "Trained batch 133 batch loss 0.386834949 epoch total loss 0.540233\n",
      "Trained batch 134 batch loss 0.459512591 epoch total loss 0.539630592\n",
      "Trained batch 135 batch loss 0.595631957 epoch total loss 0.54004544\n",
      "Trained batch 136 batch loss 0.591959774 epoch total loss 0.540427148\n",
      "Trained batch 137 batch loss 0.626355529 epoch total loss 0.541054368\n",
      "Trained batch 138 batch loss 0.632245243 epoch total loss 0.541715205\n",
      "Trained batch 139 batch loss 0.543668211 epoch total loss 0.541729271\n",
      "Trained batch 140 batch loss 0.614900112 epoch total loss 0.542251885\n",
      "Trained batch 141 batch loss 0.552154541 epoch total loss 0.542322159\n",
      "Trained batch 142 batch loss 0.571994543 epoch total loss 0.542531073\n",
      "Trained batch 143 batch loss 0.55714047 epoch total loss 0.542633295\n",
      "Trained batch 144 batch loss 0.690001845 epoch total loss 0.543656647\n",
      "Trained batch 145 batch loss 0.632984102 epoch total loss 0.544272721\n",
      "Trained batch 146 batch loss 0.54685092 epoch total loss 0.544290364\n",
      "Trained batch 147 batch loss 0.550545216 epoch total loss 0.544332922\n",
      "Trained batch 148 batch loss 0.557786465 epoch total loss 0.544423819\n",
      "Trained batch 149 batch loss 0.568981349 epoch total loss 0.544588625\n",
      "Trained batch 150 batch loss 0.601404488 epoch total loss 0.544967413\n",
      "Trained batch 151 batch loss 0.606036484 epoch total loss 0.54537183\n",
      "Trained batch 152 batch loss 0.621575654 epoch total loss 0.545873165\n",
      "Trained batch 153 batch loss 0.50561738 epoch total loss 0.54561\n",
      "Trained batch 154 batch loss 0.522848904 epoch total loss 0.545462251\n",
      "Trained batch 155 batch loss 0.509113848 epoch total loss 0.545227766\n",
      "Trained batch 156 batch loss 0.445331633 epoch total loss 0.544587433\n",
      "Trained batch 157 batch loss 0.48152113 epoch total loss 0.544185698\n",
      "Trained batch 158 batch loss 0.485818923 epoch total loss 0.543816268\n",
      "Trained batch 159 batch loss 0.46892947 epoch total loss 0.543345332\n",
      "Trained batch 160 batch loss 0.381314546 epoch total loss 0.542332649\n",
      "Trained batch 161 batch loss 0.434588015 epoch total loss 0.541663408\n",
      "Trained batch 162 batch loss 0.518122196 epoch total loss 0.541518092\n",
      "Trained batch 163 batch loss 0.545908034 epoch total loss 0.541545\n",
      "Trained batch 164 batch loss 0.533574045 epoch total loss 0.541496396\n",
      "Trained batch 165 batch loss 0.598873377 epoch total loss 0.541844189\n",
      "Trained batch 166 batch loss 0.541610599 epoch total loss 0.541842759\n",
      "Trained batch 167 batch loss 0.535457 epoch total loss 0.541804492\n",
      "Trained batch 168 batch loss 0.540518761 epoch total loss 0.541796863\n",
      "Trained batch 169 batch loss 0.561488032 epoch total loss 0.54191339\n",
      "Trained batch 170 batch loss 0.568452477 epoch total loss 0.542069495\n",
      "Trained batch 171 batch loss 0.592915833 epoch total loss 0.542366803\n",
      "Trained batch 172 batch loss 0.529144287 epoch total loss 0.54229\n",
      "Trained batch 173 batch loss 0.519814372 epoch total loss 0.542160034\n",
      "Trained batch 174 batch loss 0.627655268 epoch total loss 0.542651355\n",
      "Trained batch 175 batch loss 0.628051162 epoch total loss 0.543139398\n",
      "Trained batch 176 batch loss 0.405450642 epoch total loss 0.542357\n",
      "Trained batch 177 batch loss 0.542511582 epoch total loss 0.542357922\n",
      "Trained batch 178 batch loss 0.633795 epoch total loss 0.542871654\n",
      "Trained batch 179 batch loss 0.553442419 epoch total loss 0.542930663\n",
      "Trained batch 180 batch loss 0.558907092 epoch total loss 0.543019414\n",
      "Trained batch 181 batch loss 0.57472 epoch total loss 0.543194592\n",
      "Trained batch 182 batch loss 0.623628378 epoch total loss 0.54363656\n",
      "Trained batch 183 batch loss 0.710865498 epoch total loss 0.544550359\n",
      "Trained batch 184 batch loss 0.617542863 epoch total loss 0.544947088\n",
      "Trained batch 185 batch loss 0.696536303 epoch total loss 0.545766473\n",
      "Trained batch 186 batch loss 0.688743055 epoch total loss 0.546535134\n",
      "Trained batch 187 batch loss 0.555918932 epoch total loss 0.546585321\n",
      "Trained batch 188 batch loss 0.504252195 epoch total loss 0.546360135\n",
      "Trained batch 189 batch loss 0.509813607 epoch total loss 0.546166778\n",
      "Trained batch 190 batch loss 0.573879421 epoch total loss 0.54631263\n",
      "Trained batch 191 batch loss 0.564341068 epoch total loss 0.546407\n",
      "Trained batch 192 batch loss 0.523176253 epoch total loss 0.546286047\n",
      "Trained batch 193 batch loss 0.49049896 epoch total loss 0.545996964\n",
      "Trained batch 194 batch loss 0.596218646 epoch total loss 0.546255887\n",
      "Trained batch 195 batch loss 0.581409037 epoch total loss 0.546436131\n",
      "Trained batch 196 batch loss 0.575696 epoch total loss 0.546585441\n",
      "Trained batch 197 batch loss 0.603040278 epoch total loss 0.546872\n",
      "Trained batch 198 batch loss 0.625093341 epoch total loss 0.547267079\n",
      "Trained batch 199 batch loss 0.591167331 epoch total loss 0.547487676\n",
      "Trained batch 200 batch loss 0.663777232 epoch total loss 0.548069119\n",
      "Trained batch 201 batch loss 0.618130922 epoch total loss 0.548417687\n",
      "Trained batch 202 batch loss 0.63254118 epoch total loss 0.548834145\n",
      "Trained batch 203 batch loss 0.560993552 epoch total loss 0.548894048\n",
      "Trained batch 204 batch loss 0.616278052 epoch total loss 0.549224377\n",
      "Trained batch 205 batch loss 0.634359181 epoch total loss 0.549639642\n",
      "Trained batch 206 batch loss 0.655196667 epoch total loss 0.550152063\n",
      "Trained batch 207 batch loss 0.630268693 epoch total loss 0.550539136\n",
      "Trained batch 208 batch loss 0.62186569 epoch total loss 0.550882041\n",
      "Trained batch 209 batch loss 0.675464571 epoch total loss 0.551478088\n",
      "Trained batch 210 batch loss 0.596358776 epoch total loss 0.55169183\n",
      "Trained batch 211 batch loss 0.623436153 epoch total loss 0.552031875\n",
      "Trained batch 212 batch loss 0.560361385 epoch total loss 0.552071154\n",
      "Trained batch 213 batch loss 0.597991943 epoch total loss 0.552286744\n",
      "Trained batch 214 batch loss 0.626589775 epoch total loss 0.552633941\n",
      "Trained batch 215 batch loss 0.562761486 epoch total loss 0.552681\n",
      "Trained batch 216 batch loss 0.565648913 epoch total loss 0.55274111\n",
      "Trained batch 217 batch loss 0.519843 epoch total loss 0.552589476\n",
      "Trained batch 218 batch loss 0.554803133 epoch total loss 0.552599609\n",
      "Trained batch 219 batch loss 0.606241941 epoch total loss 0.552844584\n",
      "Trained batch 220 batch loss 0.503080308 epoch total loss 0.552618384\n",
      "Trained batch 221 batch loss 0.496722698 epoch total loss 0.552365422\n",
      "Trained batch 222 batch loss 0.453945696 epoch total loss 0.551922143\n",
      "Trained batch 223 batch loss 0.453490674 epoch total loss 0.551480711\n",
      "Trained batch 224 batch loss 0.480276525 epoch total loss 0.551162839\n",
      "Trained batch 225 batch loss 0.545457482 epoch total loss 0.551137507\n",
      "Trained batch 226 batch loss 0.621433 epoch total loss 0.551448524\n",
      "Trained batch 227 batch loss 0.602996588 epoch total loss 0.551675618\n",
      "Trained batch 228 batch loss 0.492654622 epoch total loss 0.551416755\n",
      "Trained batch 229 batch loss 0.651307881 epoch total loss 0.551852942\n",
      "Trained batch 230 batch loss 0.566997886 epoch total loss 0.551918805\n",
      "Trained batch 231 batch loss 0.525479615 epoch total loss 0.551804364\n",
      "Trained batch 232 batch loss 0.551061153 epoch total loss 0.551801145\n",
      "Trained batch 233 batch loss 0.545301318 epoch total loss 0.55177325\n",
      "Trained batch 234 batch loss 0.519095302 epoch total loss 0.551633596\n",
      "Trained batch 235 batch loss 0.526551247 epoch total loss 0.551526845\n",
      "Trained batch 236 batch loss 0.590619087 epoch total loss 0.551692486\n",
      "Trained batch 237 batch loss 0.622939765 epoch total loss 0.551993132\n",
      "Trained batch 238 batch loss 0.627790511 epoch total loss 0.552311599\n",
      "Trained batch 239 batch loss 0.515035808 epoch total loss 0.552155614\n",
      "Trained batch 240 batch loss 0.439931124 epoch total loss 0.551688\n",
      "Trained batch 241 batch loss 0.474627495 epoch total loss 0.551368237\n",
      "Trained batch 242 batch loss 0.516224563 epoch total loss 0.551223\n",
      "Trained batch 243 batch loss 0.458913147 epoch total loss 0.55084312\n",
      "Trained batch 244 batch loss 0.505062044 epoch total loss 0.550655484\n",
      "Trained batch 245 batch loss 0.528957486 epoch total loss 0.550567\n",
      "Trained batch 246 batch loss 0.535963297 epoch total loss 0.550507605\n",
      "Trained batch 247 batch loss 0.489441305 epoch total loss 0.550260365\n",
      "Trained batch 248 batch loss 0.459005088 epoch total loss 0.549892366\n",
      "Trained batch 249 batch loss 0.44452095 epoch total loss 0.549469173\n",
      "Trained batch 250 batch loss 0.487340748 epoch total loss 0.549220622\n",
      "Trained batch 251 batch loss 0.568472326 epoch total loss 0.549297333\n",
      "Trained batch 252 batch loss 0.527628839 epoch total loss 0.549211323\n",
      "Trained batch 253 batch loss 0.417598158 epoch total loss 0.548691154\n",
      "Trained batch 254 batch loss 0.528814 epoch total loss 0.548612893\n",
      "Trained batch 255 batch loss 0.499545574 epoch total loss 0.548420429\n",
      "Trained batch 256 batch loss 0.451664329 epoch total loss 0.548042476\n",
      "Trained batch 257 batch loss 0.417630702 epoch total loss 0.547535062\n",
      "Trained batch 258 batch loss 0.433403969 epoch total loss 0.547092676\n",
      "Trained batch 259 batch loss 0.528953 epoch total loss 0.547022641\n",
      "Trained batch 260 batch loss 0.556056798 epoch total loss 0.54705739\n",
      "Trained batch 261 batch loss 0.500359535 epoch total loss 0.546878517\n",
      "Trained batch 262 batch loss 0.502839565 epoch total loss 0.546710432\n",
      "Trained batch 263 batch loss 0.458472729 epoch total loss 0.546374857\n",
      "Trained batch 264 batch loss 0.472664505 epoch total loss 0.546095729\n",
      "Trained batch 265 batch loss 0.475227386 epoch total loss 0.545828283\n",
      "Trained batch 266 batch loss 0.448192865 epoch total loss 0.545461297\n",
      "Trained batch 267 batch loss 0.447193503 epoch total loss 0.545093179\n",
      "Trained batch 268 batch loss 0.515108168 epoch total loss 0.544981301\n",
      "Trained batch 269 batch loss 0.573020637 epoch total loss 0.545085549\n",
      "Trained batch 270 batch loss 0.533202 epoch total loss 0.545041502\n",
      "Trained batch 271 batch loss 0.55123359 epoch total loss 0.54506439\n",
      "Trained batch 272 batch loss 0.598412514 epoch total loss 0.545260549\n",
      "Trained batch 273 batch loss 0.589355826 epoch total loss 0.545422077\n",
      "Trained batch 274 batch loss 0.523142397 epoch total loss 0.545340776\n",
      "Trained batch 275 batch loss 0.540954113 epoch total loss 0.545324802\n",
      "Trained batch 276 batch loss 0.530710101 epoch total loss 0.545271873\n",
      "Trained batch 277 batch loss 0.459634125 epoch total loss 0.544962764\n",
      "Trained batch 278 batch loss 0.439294815 epoch total loss 0.544582665\n",
      "Trained batch 279 batch loss 0.525421262 epoch total loss 0.544514\n",
      "Trained batch 280 batch loss 0.476495087 epoch total loss 0.544271111\n",
      "Trained batch 281 batch loss 0.586359382 epoch total loss 0.544420898\n",
      "Trained batch 282 batch loss 0.49719882 epoch total loss 0.544253409\n",
      "Trained batch 283 batch loss 0.496795654 epoch total loss 0.544085741\n",
      "Trained batch 284 batch loss 0.55146718 epoch total loss 0.544111729\n",
      "Trained batch 285 batch loss 0.554553449 epoch total loss 0.544148326\n",
      "Trained batch 286 batch loss 0.604217708 epoch total loss 0.544358373\n",
      "Trained batch 287 batch loss 0.599471331 epoch total loss 0.544550419\n",
      "Trained batch 288 batch loss 0.473534495 epoch total loss 0.544303834\n",
      "Trained batch 289 batch loss 0.601042867 epoch total loss 0.544500172\n",
      "Trained batch 290 batch loss 0.608391881 epoch total loss 0.544720531\n",
      "Trained batch 291 batch loss 0.567345679 epoch total loss 0.544798255\n",
      "Trained batch 292 batch loss 0.483030647 epoch total loss 0.544586778\n",
      "Trained batch 293 batch loss 0.539887488 epoch total loss 0.544570744\n",
      "Trained batch 294 batch loss 0.482640624 epoch total loss 0.544360042\n",
      "Trained batch 295 batch loss 0.625629127 epoch total loss 0.544635534\n",
      "Trained batch 296 batch loss 0.509021163 epoch total loss 0.544515193\n",
      "Trained batch 297 batch loss 0.545324206 epoch total loss 0.544517875\n",
      "Trained batch 298 batch loss 0.559060395 epoch total loss 0.544566751\n",
      "Trained batch 299 batch loss 0.475741655 epoch total loss 0.544336557\n",
      "Trained batch 300 batch loss 0.563539803 epoch total loss 0.544400513\n",
      "Trained batch 301 batch loss 0.578006923 epoch total loss 0.544512153\n",
      "Trained batch 302 batch loss 0.557248712 epoch total loss 0.544554353\n",
      "Trained batch 303 batch loss 0.541390955 epoch total loss 0.544543922\n",
      "Trained batch 304 batch loss 0.645193219 epoch total loss 0.544874966\n",
      "Trained batch 305 batch loss 0.491604984 epoch total loss 0.544700325\n",
      "Trained batch 306 batch loss 0.601779461 epoch total loss 0.544886887\n",
      "Trained batch 307 batch loss 0.614499331 epoch total loss 0.545113623\n",
      "Trained batch 308 batch loss 0.541074395 epoch total loss 0.54510051\n",
      "Trained batch 309 batch loss 0.543259561 epoch total loss 0.54509455\n",
      "Trained batch 310 batch loss 0.60549289 epoch total loss 0.545289397\n",
      "Trained batch 311 batch loss 0.583385527 epoch total loss 0.545411944\n",
      "Trained batch 312 batch loss 0.532337844 epoch total loss 0.54537\n",
      "Trained batch 313 batch loss 0.543342829 epoch total loss 0.545363545\n",
      "Trained batch 314 batch loss 0.591877 epoch total loss 0.545511663\n",
      "Trained batch 315 batch loss 0.560736775 epoch total loss 0.54556\n",
      "Trained batch 316 batch loss 0.532724679 epoch total loss 0.545519412\n",
      "Trained batch 317 batch loss 0.553579926 epoch total loss 0.545544803\n",
      "Trained batch 318 batch loss 0.574928105 epoch total loss 0.54563719\n",
      "Trained batch 319 batch loss 0.563200533 epoch total loss 0.545692205\n",
      "Trained batch 320 batch loss 0.537012339 epoch total loss 0.545665145\n",
      "Trained batch 321 batch loss 0.529825628 epoch total loss 0.545615792\n",
      "Trained batch 322 batch loss 0.518518388 epoch total loss 0.545531631\n",
      "Trained batch 323 batch loss 0.512049079 epoch total loss 0.545428038\n",
      "Trained batch 324 batch loss 0.508351326 epoch total loss 0.545313537\n",
      "Trained batch 325 batch loss 0.462645173 epoch total loss 0.545059204\n",
      "Trained batch 326 batch loss 0.460206538 epoch total loss 0.544798911\n",
      "Trained batch 327 batch loss 0.537832379 epoch total loss 0.544777572\n",
      "Trained batch 328 batch loss 0.499583781 epoch total loss 0.544639826\n",
      "Trained batch 329 batch loss 0.586401045 epoch total loss 0.544766724\n",
      "Trained batch 330 batch loss 0.496957839 epoch total loss 0.544621885\n",
      "Trained batch 331 batch loss 0.494165242 epoch total loss 0.544469476\n",
      "Trained batch 332 batch loss 0.482134044 epoch total loss 0.544281721\n",
      "Trained batch 333 batch loss 0.461036 epoch total loss 0.54403168\n",
      "Trained batch 334 batch loss 0.440393448 epoch total loss 0.543721437\n",
      "Trained batch 335 batch loss 0.441713691 epoch total loss 0.543416917\n",
      "Trained batch 336 batch loss 0.404118478 epoch total loss 0.543002307\n",
      "Trained batch 337 batch loss 0.489165127 epoch total loss 0.542842567\n",
      "Trained batch 338 batch loss 0.501061499 epoch total loss 0.542718947\n",
      "Trained batch 339 batch loss 0.502543807 epoch total loss 0.542600453\n",
      "Trained batch 340 batch loss 0.52763021 epoch total loss 0.542556465\n",
      "Trained batch 341 batch loss 0.547065258 epoch total loss 0.542569637\n",
      "Trained batch 342 batch loss 0.543341398 epoch total loss 0.542571902\n",
      "Trained batch 343 batch loss 0.526355743 epoch total loss 0.542524576\n",
      "Trained batch 344 batch loss 0.563777268 epoch total loss 0.542586386\n",
      "Trained batch 345 batch loss 0.565654576 epoch total loss 0.542653263\n",
      "Trained batch 346 batch loss 0.503684163 epoch total loss 0.54254061\n",
      "Trained batch 347 batch loss 0.548226893 epoch total loss 0.542557\n",
      "Trained batch 348 batch loss 0.539828956 epoch total loss 0.542549193\n",
      "Trained batch 349 batch loss 0.440001428 epoch total loss 0.542255342\n",
      "Trained batch 350 batch loss 0.51334995 epoch total loss 0.54217279\n",
      "Trained batch 351 batch loss 0.455877393 epoch total loss 0.54192692\n",
      "Trained batch 352 batch loss 0.434513688 epoch total loss 0.541621745\n",
      "Trained batch 353 batch loss 0.561344862 epoch total loss 0.541677594\n",
      "Trained batch 354 batch loss 0.655863881 epoch total loss 0.542000175\n",
      "Trained batch 355 batch loss 0.462336302 epoch total loss 0.541775763\n",
      "Trained batch 356 batch loss 0.485680252 epoch total loss 0.541618228\n",
      "Trained batch 357 batch loss 0.507534266 epoch total loss 0.541522741\n",
      "Trained batch 358 batch loss 0.533733487 epoch total loss 0.541501\n",
      "Trained batch 359 batch loss 0.537648261 epoch total loss 0.541490257\n",
      "Trained batch 360 batch loss 0.410589367 epoch total loss 0.541126609\n",
      "Trained batch 361 batch loss 0.459899724 epoch total loss 0.540901601\n",
      "Trained batch 362 batch loss 0.521166921 epoch total loss 0.540847123\n",
      "Trained batch 363 batch loss 0.532271266 epoch total loss 0.54082346\n",
      "Trained batch 364 batch loss 0.453230679 epoch total loss 0.540582836\n",
      "Trained batch 365 batch loss 0.486989707 epoch total loss 0.540436\n",
      "Trained batch 366 batch loss 0.478310525 epoch total loss 0.540266275\n",
      "Trained batch 367 batch loss 0.5205217 epoch total loss 0.540212452\n",
      "Trained batch 368 batch loss 0.544687927 epoch total loss 0.540224671\n",
      "Trained batch 369 batch loss 0.612573445 epoch total loss 0.540420711\n",
      "Trained batch 370 batch loss 0.526462734 epoch total loss 0.540383\n",
      "Trained batch 371 batch loss 0.461394548 epoch total loss 0.540170074\n",
      "Trained batch 372 batch loss 0.572431 epoch total loss 0.540256858\n",
      "Trained batch 373 batch loss 0.528385103 epoch total loss 0.540224969\n",
      "Trained batch 374 batch loss 0.56578213 epoch total loss 0.540293336\n",
      "Trained batch 375 batch loss 0.549186409 epoch total loss 0.540317\n",
      "Trained batch 376 batch loss 0.483671218 epoch total loss 0.540166378\n",
      "Trained batch 377 batch loss 0.485675216 epoch total loss 0.540021837\n",
      "Trained batch 378 batch loss 0.512571335 epoch total loss 0.539949179\n",
      "Trained batch 379 batch loss 0.599112868 epoch total loss 0.540105283\n",
      "Trained batch 380 batch loss 0.608261704 epoch total loss 0.540284634\n",
      "Trained batch 381 batch loss 0.601615548 epoch total loss 0.540445626\n",
      "Trained batch 382 batch loss 0.464573622 epoch total loss 0.540246964\n",
      "Trained batch 383 batch loss 0.529059887 epoch total loss 0.540217757\n",
      "Trained batch 384 batch loss 0.490253985 epoch total loss 0.54008764\n",
      "Trained batch 385 batch loss 0.560479641 epoch total loss 0.540140629\n",
      "Trained batch 386 batch loss 0.467615664 epoch total loss 0.539952755\n",
      "Trained batch 387 batch loss 0.549460053 epoch total loss 0.539977252\n",
      "Trained batch 388 batch loss 0.546879411 epoch total loss 0.539995074\n",
      "Trained batch 389 batch loss 0.494261742 epoch total loss 0.539877474\n",
      "Trained batch 390 batch loss 0.523072422 epoch total loss 0.53983438\n",
      "Trained batch 391 batch loss 0.550253928 epoch total loss 0.539861\n",
      "Trained batch 392 batch loss 0.540348232 epoch total loss 0.539862275\n",
      "Trained batch 393 batch loss 0.53528893 epoch total loss 0.539850652\n",
      "Trained batch 394 batch loss 0.494619668 epoch total loss 0.539735794\n",
      "Trained batch 395 batch loss 0.49194473 epoch total loss 0.539614856\n",
      "Trained batch 396 batch loss 0.430335671 epoch total loss 0.539338827\n",
      "Trained batch 397 batch loss 0.504513621 epoch total loss 0.539251149\n",
      "Trained batch 398 batch loss 0.564443648 epoch total loss 0.539314449\n",
      "Trained batch 399 batch loss 0.50117439 epoch total loss 0.539218843\n",
      "Trained batch 400 batch loss 0.48695147 epoch total loss 0.53908819\n",
      "Trained batch 401 batch loss 0.460157454 epoch total loss 0.538891315\n",
      "Trained batch 402 batch loss 0.494002104 epoch total loss 0.538779676\n",
      "Trained batch 403 batch loss 0.564643204 epoch total loss 0.53884387\n",
      "Trained batch 404 batch loss 0.588018954 epoch total loss 0.538965523\n",
      "Trained batch 405 batch loss 0.574165583 epoch total loss 0.539052486\n",
      "Trained batch 406 batch loss 0.463209152 epoch total loss 0.538865685\n",
      "Trained batch 407 batch loss 0.516138911 epoch total loss 0.538809836\n",
      "Trained batch 408 batch loss 0.508180559 epoch total loss 0.538734794\n",
      "Trained batch 409 batch loss 0.388078481 epoch total loss 0.538366437\n",
      "Trained batch 410 batch loss 0.305070043 epoch total loss 0.537797391\n",
      "Trained batch 411 batch loss 0.416777134 epoch total loss 0.537502944\n",
      "Trained batch 412 batch loss 0.450981677 epoch total loss 0.537292957\n",
      "Trained batch 413 batch loss 0.545349717 epoch total loss 0.537312448\n",
      "Trained batch 414 batch loss 0.596562326 epoch total loss 0.537455559\n",
      "Trained batch 415 batch loss 0.609924734 epoch total loss 0.5376302\n",
      "Trained batch 416 batch loss 0.548874855 epoch total loss 0.537657201\n",
      "Trained batch 417 batch loss 0.502873778 epoch total loss 0.537573814\n",
      "Trained batch 418 batch loss 0.522382081 epoch total loss 0.537537456\n",
      "Trained batch 419 batch loss 0.541577 epoch total loss 0.537547112\n",
      "Trained batch 420 batch loss 0.535873413 epoch total loss 0.537543118\n",
      "Trained batch 421 batch loss 0.48098439 epoch total loss 0.537408769\n",
      "Trained batch 422 batch loss 0.515548468 epoch total loss 0.537357\n",
      "Trained batch 423 batch loss 0.463146031 epoch total loss 0.537181556\n",
      "Trained batch 424 batch loss 0.353294522 epoch total loss 0.536747873\n",
      "Trained batch 425 batch loss 0.544621 epoch total loss 0.53676641\n",
      "Trained batch 426 batch loss 0.417894274 epoch total loss 0.536487341\n",
      "Trained batch 427 batch loss 0.504248619 epoch total loss 0.536411822\n",
      "Trained batch 428 batch loss 0.524789572 epoch total loss 0.536384702\n",
      "Trained batch 429 batch loss 0.45848462 epoch total loss 0.536203086\n",
      "Trained batch 430 batch loss 0.586074769 epoch total loss 0.536319077\n",
      "Trained batch 431 batch loss 0.563115478 epoch total loss 0.536381245\n",
      "Trained batch 432 batch loss 0.550084949 epoch total loss 0.536412954\n",
      "Trained batch 433 batch loss 0.498884618 epoch total loss 0.536326289\n",
      "Trained batch 434 batch loss 0.49825877 epoch total loss 0.536238551\n",
      "Trained batch 435 batch loss 0.543754935 epoch total loss 0.536255836\n",
      "Trained batch 436 batch loss 0.579887033 epoch total loss 0.536355913\n",
      "Trained batch 437 batch loss 0.54546243 epoch total loss 0.536376715\n",
      "Trained batch 438 batch loss 0.557457507 epoch total loss 0.536424875\n",
      "Trained batch 439 batch loss 0.517074227 epoch total loss 0.536380827\n",
      "Trained batch 440 batch loss 0.546015382 epoch total loss 0.536402702\n",
      "Trained batch 441 batch loss 0.615218401 epoch total loss 0.536581457\n",
      "Trained batch 442 batch loss 0.539066672 epoch total loss 0.536587059\n",
      "Trained batch 443 batch loss 0.588163853 epoch total loss 0.536703467\n",
      "Trained batch 444 batch loss 0.624359608 epoch total loss 0.536900878\n",
      "Trained batch 445 batch loss 0.637539208 epoch total loss 0.537127078\n",
      "Trained batch 446 batch loss 0.640205 epoch total loss 0.537358165\n",
      "Trained batch 447 batch loss 0.536296308 epoch total loss 0.537355781\n",
      "Trained batch 448 batch loss 0.635341048 epoch total loss 0.53757453\n",
      "Trained batch 449 batch loss 0.530411124 epoch total loss 0.537558556\n",
      "Trained batch 450 batch loss 0.50758636 epoch total loss 0.537492\n",
      "Trained batch 451 batch loss 0.531237304 epoch total loss 0.537478089\n",
      "Trained batch 452 batch loss 0.504642069 epoch total loss 0.537405431\n",
      "Trained batch 453 batch loss 0.495169073 epoch total loss 0.53731221\n",
      "Trained batch 454 batch loss 0.542924106 epoch total loss 0.537324548\n",
      "Trained batch 455 batch loss 0.529344678 epoch total loss 0.537307\n",
      "Trained batch 456 batch loss 0.634973109 epoch total loss 0.537521183\n",
      "Trained batch 457 batch loss 0.539540172 epoch total loss 0.537525594\n",
      "Trained batch 458 batch loss 0.588670194 epoch total loss 0.537637293\n",
      "Trained batch 459 batch loss 0.625829577 epoch total loss 0.537829399\n",
      "Trained batch 460 batch loss 0.727322638 epoch total loss 0.538241327\n",
      "Trained batch 461 batch loss 0.684509516 epoch total loss 0.538558602\n",
      "Trained batch 462 batch loss 0.618820131 epoch total loss 0.53873235\n",
      "Trained batch 463 batch loss 0.544314563 epoch total loss 0.53874439\n",
      "Trained batch 464 batch loss 0.464618325 epoch total loss 0.53858465\n",
      "Trained batch 465 batch loss 0.467237025 epoch total loss 0.538431227\n",
      "Trained batch 466 batch loss 0.654839039 epoch total loss 0.53868103\n",
      "Trained batch 467 batch loss 0.580306113 epoch total loss 0.538770139\n",
      "Trained batch 468 batch loss 0.517732322 epoch total loss 0.538725197\n",
      "Trained batch 469 batch loss 0.47146225 epoch total loss 0.538581789\n",
      "Trained batch 470 batch loss 0.610080421 epoch total loss 0.5387339\n",
      "Trained batch 471 batch loss 0.648878932 epoch total loss 0.538967788\n",
      "Trained batch 472 batch loss 0.590134561 epoch total loss 0.539076149\n",
      "Trained batch 473 batch loss 0.664392769 epoch total loss 0.539341152\n",
      "Trained batch 474 batch loss 0.506073713 epoch total loss 0.539270937\n",
      "Trained batch 475 batch loss 0.437405527 epoch total loss 0.53905648\n",
      "Trained batch 476 batch loss 0.413374871 epoch total loss 0.538792372\n",
      "Trained batch 477 batch loss 0.464275181 epoch total loss 0.538636148\n",
      "Trained batch 478 batch loss 0.487199187 epoch total loss 0.538528562\n",
      "Trained batch 479 batch loss 0.45505321 epoch total loss 0.538354278\n",
      "Trained batch 480 batch loss 0.55953908 epoch total loss 0.538398445\n",
      "Trained batch 481 batch loss 0.46822843 epoch total loss 0.538252532\n",
      "Trained batch 482 batch loss 0.50917083 epoch total loss 0.538192272\n",
      "Trained batch 483 batch loss 0.473751813 epoch total loss 0.538058817\n",
      "Trained batch 484 batch loss 0.507766962 epoch total loss 0.537996292\n",
      "Trained batch 485 batch loss 0.493987441 epoch total loss 0.537905514\n",
      "Trained batch 486 batch loss 0.527769685 epoch total loss 0.537884712\n",
      "Trained batch 487 batch loss 0.557269216 epoch total loss 0.537924528\n",
      "Trained batch 488 batch loss 0.511326969 epoch total loss 0.53787\n",
      "Trained batch 489 batch loss 0.639079332 epoch total loss 0.538076937\n",
      "Trained batch 490 batch loss 0.573995769 epoch total loss 0.538150251\n",
      "Trained batch 491 batch loss 0.575413 epoch total loss 0.538226128\n",
      "Trained batch 492 batch loss 0.528169036 epoch total loss 0.538205683\n",
      "Trained batch 493 batch loss 0.627235115 epoch total loss 0.538386285\n",
      "Trained batch 494 batch loss 0.6353212 epoch total loss 0.538582504\n",
      "Trained batch 495 batch loss 0.604513049 epoch total loss 0.53871572\n",
      "Trained batch 496 batch loss 0.559186876 epoch total loss 0.538756967\n",
      "Trained batch 497 batch loss 0.612392187 epoch total loss 0.538905144\n",
      "Trained batch 498 batch loss 0.485252827 epoch total loss 0.538797379\n",
      "Trained batch 499 batch loss 0.525264204 epoch total loss 0.538770318\n",
      "Trained batch 500 batch loss 0.564095 epoch total loss 0.538820922\n",
      "Trained batch 501 batch loss 0.510667682 epoch total loss 0.538764775\n",
      "Trained batch 502 batch loss 0.426898599 epoch total loss 0.538541913\n",
      "Trained batch 503 batch loss 0.477765411 epoch total loss 0.538421094\n",
      "Trained batch 504 batch loss 0.497979045 epoch total loss 0.538340867\n",
      "Trained batch 505 batch loss 0.499747813 epoch total loss 0.538264453\n",
      "Trained batch 506 batch loss 0.556751251 epoch total loss 0.538301\n",
      "Trained batch 507 batch loss 0.6116395 epoch total loss 0.538445652\n",
      "Trained batch 508 batch loss 0.551718831 epoch total loss 0.538471818\n",
      "Trained batch 509 batch loss 0.660809517 epoch total loss 0.538712144\n",
      "Trained batch 510 batch loss 0.564580858 epoch total loss 0.538762808\n",
      "Trained batch 511 batch loss 0.519579172 epoch total loss 0.538725317\n",
      "Trained batch 512 batch loss 0.584310591 epoch total loss 0.538814366\n",
      "Trained batch 513 batch loss 0.540587723 epoch total loss 0.538817823\n",
      "Trained batch 514 batch loss 0.487128198 epoch total loss 0.53871727\n",
      "Trained batch 515 batch loss 0.525637269 epoch total loss 0.538691819\n",
      "Trained batch 516 batch loss 0.526393414 epoch total loss 0.538668036\n",
      "Trained batch 517 batch loss 0.561591148 epoch total loss 0.538712323\n",
      "Trained batch 518 batch loss 0.519004 epoch total loss 0.538674295\n",
      "Trained batch 519 batch loss 0.458914518 epoch total loss 0.538520634\n",
      "Trained batch 520 batch loss 0.446299523 epoch total loss 0.53834331\n",
      "Trained batch 521 batch loss 0.479128242 epoch total loss 0.538229644\n",
      "Trained batch 522 batch loss 0.495038807 epoch total loss 0.538146853\n",
      "Trained batch 523 batch loss 0.472237527 epoch total loss 0.53802079\n",
      "Trained batch 524 batch loss 0.591597676 epoch total loss 0.538123\n",
      "Trained batch 525 batch loss 0.667098582 epoch total loss 0.538368702\n",
      "Trained batch 526 batch loss 0.709256768 epoch total loss 0.538693547\n",
      "Trained batch 527 batch loss 0.53786236 epoch total loss 0.538692\n",
      "Trained batch 528 batch loss 0.599568844 epoch total loss 0.538807333\n",
      "Trained batch 529 batch loss 0.466759622 epoch total loss 0.538671136\n",
      "Trained batch 530 batch loss 0.465936363 epoch total loss 0.538533926\n",
      "Trained batch 531 batch loss 0.406655192 epoch total loss 0.538285553\n",
      "Trained batch 532 batch loss 0.37791276 epoch total loss 0.537984073\n",
      "Trained batch 533 batch loss 0.488113463 epoch total loss 0.537890494\n",
      "Trained batch 534 batch loss 0.4621948 epoch total loss 0.537748754\n",
      "Trained batch 535 batch loss 0.566148818 epoch total loss 0.537801862\n",
      "Trained batch 536 batch loss 0.485505879 epoch total loss 0.537704289\n",
      "Trained batch 537 batch loss 0.589033842 epoch total loss 0.537799835\n",
      "Trained batch 538 batch loss 0.660300374 epoch total loss 0.538027585\n",
      "Trained batch 539 batch loss 0.564950049 epoch total loss 0.538077474\n",
      "Trained batch 540 batch loss 0.567599833 epoch total loss 0.538132191\n",
      "Trained batch 541 batch loss 0.511211 epoch total loss 0.538082361\n",
      "Trained batch 542 batch loss 0.554733098 epoch total loss 0.538113058\n",
      "Trained batch 543 batch loss 0.599374175 epoch total loss 0.538225889\n",
      "Trained batch 544 batch loss 0.50624 epoch total loss 0.538167059\n",
      "Trained batch 545 batch loss 0.517698824 epoch total loss 0.538129508\n",
      "Trained batch 546 batch loss 0.521801233 epoch total loss 0.538099587\n",
      "Trained batch 547 batch loss 0.488844693 epoch total loss 0.538009524\n",
      "Trained batch 548 batch loss 0.547021627 epoch total loss 0.538026\n",
      "Trained batch 549 batch loss 0.536496162 epoch total loss 0.538023174\n",
      "Trained batch 550 batch loss 0.517687619 epoch total loss 0.537986219\n",
      "Trained batch 551 batch loss 0.542531133 epoch total loss 0.537994504\n",
      "Trained batch 552 batch loss 0.644595861 epoch total loss 0.538187623\n",
      "Trained batch 553 batch loss 0.636752546 epoch total loss 0.538365841\n",
      "Trained batch 554 batch loss 0.586301684 epoch total loss 0.538452387\n",
      "Trained batch 555 batch loss 0.551448941 epoch total loss 0.538475811\n",
      "Trained batch 556 batch loss 0.519353092 epoch total loss 0.53844142\n",
      "Trained batch 557 batch loss 0.474027723 epoch total loss 0.538325727\n",
      "Trained batch 558 batch loss 0.457630932 epoch total loss 0.538181126\n",
      "Trained batch 559 batch loss 0.495122403 epoch total loss 0.538104117\n",
      "Trained batch 560 batch loss 0.52034235 epoch total loss 0.538072407\n",
      "Trained batch 561 batch loss 0.440822303 epoch total loss 0.537899077\n",
      "Trained batch 562 batch loss 0.453415722 epoch total loss 0.537748754\n",
      "Trained batch 563 batch loss 0.579822183 epoch total loss 0.537823558\n",
      "Trained batch 564 batch loss 0.572379 epoch total loss 0.537884831\n",
      "Trained batch 565 batch loss 0.528590441 epoch total loss 0.537868381\n",
      "Trained batch 566 batch loss 0.596125 epoch total loss 0.537971318\n",
      "Trained batch 567 batch loss 0.586894453 epoch total loss 0.538057566\n",
      "Trained batch 568 batch loss 0.590196371 epoch total loss 0.538149416\n",
      "Trained batch 569 batch loss 0.562521219 epoch total loss 0.538192213\n",
      "Trained batch 570 batch loss 0.47627914 epoch total loss 0.538083613\n",
      "Trained batch 571 batch loss 0.579318 epoch total loss 0.538155854\n",
      "Trained batch 572 batch loss 0.460545838 epoch total loss 0.538020134\n",
      "Trained batch 573 batch loss 0.537526488 epoch total loss 0.5380193\n",
      "Trained batch 574 batch loss 0.500041544 epoch total loss 0.537953138\n",
      "Trained batch 575 batch loss 0.552611649 epoch total loss 0.537978649\n",
      "Trained batch 576 batch loss 0.598823905 epoch total loss 0.538084269\n",
      "Trained batch 577 batch loss 0.601006 epoch total loss 0.538193285\n",
      "Trained batch 578 batch loss 0.553701282 epoch total loss 0.538220167\n",
      "Trained batch 579 batch loss 0.481516 epoch total loss 0.538122177\n",
      "Trained batch 580 batch loss 0.664356112 epoch total loss 0.538339853\n",
      "Trained batch 581 batch loss 0.551925778 epoch total loss 0.538363278\n",
      "Trained batch 582 batch loss 0.580213308 epoch total loss 0.538435161\n",
      "Trained batch 583 batch loss 0.577809274 epoch total loss 0.538502693\n",
      "Trained batch 584 batch loss 0.579593539 epoch total loss 0.538573086\n",
      "Trained batch 585 batch loss 0.623191655 epoch total loss 0.538717747\n",
      "Trained batch 586 batch loss 0.632248759 epoch total loss 0.538877368\n",
      "Trained batch 587 batch loss 0.620069623 epoch total loss 0.539015651\n",
      "Trained batch 588 batch loss 0.571351588 epoch total loss 0.539070666\n",
      "Trained batch 589 batch loss 0.646535099 epoch total loss 0.539253116\n",
      "Trained batch 590 batch loss 0.581267536 epoch total loss 0.539324343\n",
      "Trained batch 591 batch loss 0.664459884 epoch total loss 0.539536059\n",
      "Trained batch 592 batch loss 0.646905124 epoch total loss 0.539717436\n",
      "Trained batch 593 batch loss 0.638230205 epoch total loss 0.539883614\n",
      "Trained batch 594 batch loss 0.498017311 epoch total loss 0.539813101\n",
      "Trained batch 595 batch loss 0.536817 epoch total loss 0.539808035\n",
      "Trained batch 596 batch loss 0.52318716 epoch total loss 0.5397802\n",
      "Trained batch 597 batch loss 0.505827188 epoch total loss 0.539723277\n",
      "Trained batch 598 batch loss 0.545146823 epoch total loss 0.539732337\n",
      "Trained batch 599 batch loss 0.521063447 epoch total loss 0.539701164\n",
      "Trained batch 600 batch loss 0.492290974 epoch total loss 0.539622128\n",
      "Trained batch 601 batch loss 0.458572656 epoch total loss 0.539487302\n",
      "Trained batch 602 batch loss 0.492735803 epoch total loss 0.539409637\n",
      "Trained batch 603 batch loss 0.462918371 epoch total loss 0.539282799\n",
      "Trained batch 604 batch loss 0.485600889 epoch total loss 0.539193928\n",
      "Trained batch 605 batch loss 0.596145034 epoch total loss 0.539288044\n",
      "Trained batch 606 batch loss 0.500632882 epoch total loss 0.539224267\n",
      "Trained batch 607 batch loss 0.571352661 epoch total loss 0.539277196\n",
      "Trained batch 608 batch loss 0.647347569 epoch total loss 0.539454937\n",
      "Trained batch 609 batch loss 0.670102656 epoch total loss 0.539669454\n",
      "Trained batch 610 batch loss 0.66199863 epoch total loss 0.539869964\n",
      "Trained batch 611 batch loss 0.583277941 epoch total loss 0.539941\n",
      "Trained batch 612 batch loss 0.508963 epoch total loss 0.539890409\n",
      "Trained batch 613 batch loss 0.465629518 epoch total loss 0.539769292\n",
      "Trained batch 614 batch loss 0.559738755 epoch total loss 0.539801836\n",
      "Trained batch 615 batch loss 0.380325615 epoch total loss 0.539542556\n",
      "Trained batch 616 batch loss 0.509063244 epoch total loss 0.539493084\n",
      "Trained batch 617 batch loss 0.615964353 epoch total loss 0.539617\n",
      "Trained batch 618 batch loss 0.677142918 epoch total loss 0.539839566\n",
      "Trained batch 619 batch loss 0.600800812 epoch total loss 0.539938033\n",
      "Trained batch 620 batch loss 0.59560132 epoch total loss 0.540027857\n",
      "Trained batch 621 batch loss 0.591573715 epoch total loss 0.540110886\n",
      "Trained batch 622 batch loss 0.601621211 epoch total loss 0.54020977\n",
      "Trained batch 623 batch loss 0.611110806 epoch total loss 0.540323555\n",
      "Trained batch 624 batch loss 0.565791488 epoch total loss 0.540364385\n",
      "Trained batch 625 batch loss 0.618595481 epoch total loss 0.540489554\n",
      "Trained batch 626 batch loss 0.574231267 epoch total loss 0.540543437\n",
      "Trained batch 627 batch loss 0.58199 epoch total loss 0.540609539\n",
      "Trained batch 628 batch loss 0.604639053 epoch total loss 0.540711522\n",
      "Trained batch 629 batch loss 0.515315473 epoch total loss 0.54067117\n",
      "Trained batch 630 batch loss 0.509130359 epoch total loss 0.540621102\n",
      "Trained batch 631 batch loss 0.620305 epoch total loss 0.540747344\n",
      "Trained batch 632 batch loss 0.638486862 epoch total loss 0.540902\n",
      "Trained batch 633 batch loss 0.538202643 epoch total loss 0.540897727\n",
      "Trained batch 634 batch loss 0.485845417 epoch total loss 0.540810883\n",
      "Trained batch 635 batch loss 0.525548697 epoch total loss 0.540786862\n",
      "Trained batch 636 batch loss 0.563959956 epoch total loss 0.540823281\n",
      "Trained batch 637 batch loss 0.56935519 epoch total loss 0.540868104\n",
      "Trained batch 638 batch loss 0.566551 epoch total loss 0.540908396\n",
      "Trained batch 639 batch loss 0.547822893 epoch total loss 0.540919185\n",
      "Trained batch 640 batch loss 0.586941361 epoch total loss 0.540991127\n",
      "Trained batch 641 batch loss 0.5855968 epoch total loss 0.541060686\n",
      "Trained batch 642 batch loss 0.548779488 epoch total loss 0.541072726\n",
      "Trained batch 643 batch loss 0.597906053 epoch total loss 0.54116112\n",
      "Trained batch 644 batch loss 0.576114535 epoch total loss 0.54121536\n",
      "Trained batch 645 batch loss 0.594994307 epoch total loss 0.541298747\n",
      "Trained batch 646 batch loss 0.621468067 epoch total loss 0.541422844\n",
      "Trained batch 647 batch loss 0.589300752 epoch total loss 0.541496813\n",
      "Trained batch 648 batch loss 0.636240065 epoch total loss 0.541643\n",
      "Trained batch 649 batch loss 0.542484879 epoch total loss 0.541644335\n",
      "Trained batch 650 batch loss 0.585806847 epoch total loss 0.541712284\n",
      "Trained batch 651 batch loss 0.459917277 epoch total loss 0.541586637\n",
      "Trained batch 652 batch loss 0.405688 epoch total loss 0.54137826\n",
      "Trained batch 653 batch loss 0.528949618 epoch total loss 0.541359246\n",
      "Trained batch 654 batch loss 0.409801364 epoch total loss 0.541158\n",
      "Trained batch 655 batch loss 0.49197191 epoch total loss 0.541083\n",
      "Trained batch 656 batch loss 0.441170692 epoch total loss 0.540930629\n",
      "Trained batch 657 batch loss 0.539144397 epoch total loss 0.540927947\n",
      "Trained batch 658 batch loss 0.473729 epoch total loss 0.540825784\n",
      "Trained batch 659 batch loss 0.508900166 epoch total loss 0.540777385\n",
      "Trained batch 660 batch loss 0.483945966 epoch total loss 0.540691257\n",
      "Trained batch 661 batch loss 0.491059244 epoch total loss 0.540616155\n",
      "Trained batch 662 batch loss 0.49868983 epoch total loss 0.540552855\n",
      "Trained batch 663 batch loss 0.509126902 epoch total loss 0.540505409\n",
      "Trained batch 664 batch loss 0.36967662 epoch total loss 0.540248156\n",
      "Trained batch 665 batch loss 0.552671492 epoch total loss 0.540266871\n",
      "Trained batch 666 batch loss 0.530487478 epoch total loss 0.540252209\n",
      "Trained batch 667 batch loss 0.533101797 epoch total loss 0.54024148\n",
      "Trained batch 668 batch loss 0.56889 epoch total loss 0.540284336\n",
      "Trained batch 669 batch loss 0.520572 epoch total loss 0.540254891\n",
      "Trained batch 670 batch loss 0.485752851 epoch total loss 0.540173531\n",
      "Trained batch 671 batch loss 0.474381506 epoch total loss 0.540075481\n",
      "Trained batch 672 batch loss 0.548924088 epoch total loss 0.540088654\n",
      "Trained batch 673 batch loss 0.538017452 epoch total loss 0.540085614\n",
      "Trained batch 674 batch loss 0.453915119 epoch total loss 0.539957762\n",
      "Trained batch 675 batch loss 0.397841901 epoch total loss 0.539747179\n",
      "Trained batch 676 batch loss 0.49009344 epoch total loss 0.539673686\n",
      "Trained batch 677 batch loss 0.534532 epoch total loss 0.539666116\n",
      "Trained batch 678 batch loss 0.444973648 epoch total loss 0.539526463\n",
      "Trained batch 679 batch loss 0.479434431 epoch total loss 0.53943795\n",
      "Trained batch 680 batch loss 0.558132112 epoch total loss 0.539465487\n",
      "Trained batch 681 batch loss 0.511350393 epoch total loss 0.539424181\n",
      "Trained batch 682 batch loss 0.586583912 epoch total loss 0.539493322\n",
      "Trained batch 683 batch loss 0.596493 epoch total loss 0.539576769\n",
      "Trained batch 684 batch loss 0.682702661 epoch total loss 0.539786041\n",
      "Trained batch 685 batch loss 0.616501808 epoch total loss 0.539898038\n",
      "Trained batch 686 batch loss 0.675618291 epoch total loss 0.540095925\n",
      "Trained batch 687 batch loss 0.611851335 epoch total loss 0.540200353\n",
      "Trained batch 688 batch loss 0.576584458 epoch total loss 0.540253282\n",
      "Trained batch 689 batch loss 0.552477956 epoch total loss 0.540271044\n",
      "Trained batch 690 batch loss 0.478270382 epoch total loss 0.54018116\n",
      "Trained batch 691 batch loss 0.503409147 epoch total loss 0.540128\n",
      "Trained batch 692 batch loss 0.502841592 epoch total loss 0.54007405\n",
      "Trained batch 693 batch loss 0.544198513 epoch total loss 0.54008\n",
      "Trained batch 694 batch loss 0.544765353 epoch total loss 0.540086746\n",
      "Trained batch 695 batch loss 0.489134938 epoch total loss 0.540013492\n",
      "Trained batch 696 batch loss 0.495593607 epoch total loss 0.539949656\n",
      "Trained batch 697 batch loss 0.447920203 epoch total loss 0.539817631\n",
      "Trained batch 698 batch loss 0.485651493 epoch total loss 0.53974\n",
      "Trained batch 699 batch loss 0.535075188 epoch total loss 0.539733291\n",
      "Trained batch 700 batch loss 0.526898801 epoch total loss 0.539715\n",
      "Trained batch 701 batch loss 0.529486418 epoch total loss 0.539700389\n",
      "Trained batch 702 batch loss 0.451711893 epoch total loss 0.53957504\n",
      "Trained batch 703 batch loss 0.552553058 epoch total loss 0.539593518\n",
      "Trained batch 704 batch loss 0.541557 epoch total loss 0.539596319\n",
      "Trained batch 705 batch loss 0.625417829 epoch total loss 0.539718032\n",
      "Trained batch 706 batch loss 0.626448512 epoch total loss 0.539840877\n",
      "Trained batch 707 batch loss 0.553487897 epoch total loss 0.539860189\n",
      "Trained batch 708 batch loss 0.534368336 epoch total loss 0.53985244\n",
      "Trained batch 709 batch loss 0.550548375 epoch total loss 0.53986752\n",
      "Trained batch 710 batch loss 0.547927499 epoch total loss 0.539878845\n",
      "Trained batch 711 batch loss 0.669105291 epoch total loss 0.54006058\n",
      "Trained batch 712 batch loss 0.630498648 epoch total loss 0.540187597\n",
      "Trained batch 713 batch loss 0.563974261 epoch total loss 0.540220916\n",
      "Trained batch 714 batch loss 0.548096955 epoch total loss 0.540231943\n",
      "Trained batch 715 batch loss 0.55955559 epoch total loss 0.540259\n",
      "Trained batch 716 batch loss 0.611453116 epoch total loss 0.540358424\n",
      "Trained batch 717 batch loss 0.568962097 epoch total loss 0.540398359\n",
      "Trained batch 718 batch loss 0.582826674 epoch total loss 0.540457428\n",
      "Trained batch 719 batch loss 0.57586807 epoch total loss 0.540506661\n",
      "Trained batch 720 batch loss 0.451037616 epoch total loss 0.540382445\n",
      "Trained batch 721 batch loss 0.529956341 epoch total loss 0.540367961\n",
      "Trained batch 722 batch loss 0.454525352 epoch total loss 0.540249109\n",
      "Trained batch 723 batch loss 0.539313674 epoch total loss 0.540247798\n",
      "Trained batch 724 batch loss 0.611973643 epoch total loss 0.540346861\n",
      "Trained batch 725 batch loss 0.611757 epoch total loss 0.540445328\n",
      "Trained batch 726 batch loss 0.568066955 epoch total loss 0.540483356\n",
      "Trained batch 727 batch loss 0.565679193 epoch total loss 0.540518045\n",
      "Trained batch 728 batch loss 0.657614112 epoch total loss 0.540678859\n",
      "Trained batch 729 batch loss 0.575630605 epoch total loss 0.54072684\n",
      "Trained batch 730 batch loss 0.600659549 epoch total loss 0.540808916\n",
      "Trained batch 731 batch loss 0.590873361 epoch total loss 0.540877402\n",
      "Trained batch 732 batch loss 0.56377691 epoch total loss 0.540908694\n",
      "Trained batch 733 batch loss 0.552904546 epoch total loss 0.540925086\n",
      "Trained batch 734 batch loss 0.471963823 epoch total loss 0.540831089\n",
      "Trained batch 735 batch loss 0.486778677 epoch total loss 0.540757596\n",
      "Trained batch 736 batch loss 0.54998368 epoch total loss 0.540770113\n",
      "Trained batch 737 batch loss 0.580167 epoch total loss 0.540823579\n",
      "Trained batch 738 batch loss 0.500323892 epoch total loss 0.540768743\n",
      "Trained batch 739 batch loss 0.479851633 epoch total loss 0.540686309\n",
      "Trained batch 740 batch loss 0.44224605 epoch total loss 0.540553272\n",
      "Trained batch 741 batch loss 0.477013886 epoch total loss 0.54046756\n",
      "Trained batch 742 batch loss 0.456824422 epoch total loss 0.540354788\n",
      "Trained batch 743 batch loss 0.439879864 epoch total loss 0.540219605\n",
      "Trained batch 744 batch loss 0.505414128 epoch total loss 0.540172756\n",
      "Trained batch 745 batch loss 0.535642743 epoch total loss 0.540166676\n",
      "Trained batch 746 batch loss 0.561389923 epoch total loss 0.540195167\n",
      "Trained batch 747 batch loss 0.536948383 epoch total loss 0.540190816\n",
      "Trained batch 748 batch loss 0.553779125 epoch total loss 0.540209\n",
      "Trained batch 749 batch loss 0.52388376 epoch total loss 0.54018718\n",
      "Trained batch 750 batch loss 0.533071816 epoch total loss 0.540177763\n",
      "Trained batch 751 batch loss 0.685471058 epoch total loss 0.540371239\n",
      "Trained batch 752 batch loss 0.596108854 epoch total loss 0.540445328\n",
      "Trained batch 753 batch loss 0.547443211 epoch total loss 0.540454626\n",
      "Trained batch 754 batch loss 0.687877715 epoch total loss 0.540650129\n",
      "Trained batch 755 batch loss 0.50941509 epoch total loss 0.540608764\n",
      "Trained batch 756 batch loss 0.496137 epoch total loss 0.540549934\n",
      "Trained batch 757 batch loss 0.540000141 epoch total loss 0.540549219\n",
      "Trained batch 758 batch loss 0.61493057 epoch total loss 0.540647388\n",
      "Trained batch 759 batch loss 0.532237172 epoch total loss 0.540636241\n",
      "Trained batch 760 batch loss 0.494825959 epoch total loss 0.540576\n",
      "Trained batch 761 batch loss 0.491725624 epoch total loss 0.540511787\n",
      "Trained batch 762 batch loss 0.505886853 epoch total loss 0.540466368\n",
      "Trained batch 763 batch loss 0.577760458 epoch total loss 0.540515244\n",
      "Trained batch 764 batch loss 0.620857 epoch total loss 0.540620387\n",
      "Trained batch 765 batch loss 0.587140322 epoch total loss 0.540681183\n",
      "Trained batch 766 batch loss 0.616719246 epoch total loss 0.540780425\n",
      "Trained batch 767 batch loss 0.584839046 epoch total loss 0.540837884\n",
      "Trained batch 768 batch loss 0.569302738 epoch total loss 0.540874958\n",
      "Trained batch 769 batch loss 0.525473237 epoch total loss 0.540854931\n",
      "Trained batch 770 batch loss 0.528980374 epoch total loss 0.540839553\n",
      "Trained batch 771 batch loss 0.599996924 epoch total loss 0.540916264\n",
      "Trained batch 772 batch loss 0.503943145 epoch total loss 0.540868402\n",
      "Trained batch 773 batch loss 0.656778872 epoch total loss 0.541018307\n",
      "Trained batch 774 batch loss 0.610461771 epoch total loss 0.541108072\n",
      "Trained batch 775 batch loss 0.647054672 epoch total loss 0.541244745\n",
      "Trained batch 776 batch loss 0.619879842 epoch total loss 0.541346073\n",
      "Trained batch 777 batch loss 0.543065 epoch total loss 0.541348279\n",
      "Trained batch 778 batch loss 0.539098501 epoch total loss 0.541345417\n",
      "Trained batch 779 batch loss 0.515752614 epoch total loss 0.541312516\n",
      "Trained batch 780 batch loss 0.53829056 epoch total loss 0.541308701\n",
      "Trained batch 781 batch loss 0.512549222 epoch total loss 0.541271865\n",
      "Trained batch 782 batch loss 0.615597963 epoch total loss 0.541366875\n",
      "Trained batch 783 batch loss 0.635950327 epoch total loss 0.541487694\n",
      "Trained batch 784 batch loss 0.514230192 epoch total loss 0.541452885\n",
      "Trained batch 785 batch loss 0.564407468 epoch total loss 0.541482151\n",
      "Trained batch 786 batch loss 0.485926926 epoch total loss 0.541411519\n",
      "Trained batch 787 batch loss 0.487162143 epoch total loss 0.541342556\n",
      "Trained batch 788 batch loss 0.466561079 epoch total loss 0.541247666\n",
      "Trained batch 789 batch loss 0.448156834 epoch total loss 0.541129649\n",
      "Trained batch 790 batch loss 0.426597506 epoch total loss 0.54098469\n",
      "Trained batch 791 batch loss 0.450232744 epoch total loss 0.540869951\n",
      "Trained batch 792 batch loss 0.522969306 epoch total loss 0.540847361\n",
      "Trained batch 793 batch loss 0.596111655 epoch total loss 0.540917039\n",
      "Trained batch 794 batch loss 0.704860449 epoch total loss 0.541123509\n",
      "Trained batch 795 batch loss 0.62898618 epoch total loss 0.541234076\n",
      "Trained batch 796 batch loss 0.566126943 epoch total loss 0.541265309\n",
      "Trained batch 797 batch loss 0.47079128 epoch total loss 0.541176915\n",
      "Trained batch 798 batch loss 0.542691648 epoch total loss 0.541178823\n",
      "Trained batch 799 batch loss 0.448711157 epoch total loss 0.54106307\n",
      "Trained batch 800 batch loss 0.457990736 epoch total loss 0.540959179\n",
      "Trained batch 801 batch loss 0.600059569 epoch total loss 0.54103297\n",
      "Trained batch 802 batch loss 0.544389248 epoch total loss 0.541037202\n",
      "Trained batch 803 batch loss 0.515807688 epoch total loss 0.54100579\n",
      "Trained batch 804 batch loss 0.464467525 epoch total loss 0.540910602\n",
      "Trained batch 805 batch loss 0.482071221 epoch total loss 0.540837526\n",
      "Trained batch 806 batch loss 0.511717618 epoch total loss 0.540801406\n",
      "Trained batch 807 batch loss 0.431185186 epoch total loss 0.540665567\n",
      "Trained batch 808 batch loss 0.556090355 epoch total loss 0.54068464\n",
      "Trained batch 809 batch loss 0.553005099 epoch total loss 0.540699899\n",
      "Trained batch 810 batch loss 0.546177268 epoch total loss 0.540706635\n",
      "Trained batch 811 batch loss 0.494737566 epoch total loss 0.540649951\n",
      "Trained batch 812 batch loss 0.534460783 epoch total loss 0.540642321\n",
      "Trained batch 813 batch loss 0.530106 epoch total loss 0.540629387\n",
      "Trained batch 814 batch loss 0.552387416 epoch total loss 0.540643871\n",
      "Trained batch 815 batch loss 0.515187621 epoch total loss 0.540612638\n",
      "Trained batch 816 batch loss 0.605552197 epoch total loss 0.54069221\n",
      "Trained batch 817 batch loss 0.50239718 epoch total loss 0.540645361\n",
      "Trained batch 818 batch loss 0.533023953 epoch total loss 0.540636063\n",
      "Trained batch 819 batch loss 0.456849813 epoch total loss 0.540533781\n",
      "Trained batch 820 batch loss 0.499251306 epoch total loss 0.540483415\n",
      "Trained batch 821 batch loss 0.560284793 epoch total loss 0.540507495\n",
      "Trained batch 822 batch loss 0.507516861 epoch total loss 0.540467322\n",
      "Trained batch 823 batch loss 0.549672961 epoch total loss 0.540478528\n",
      "Trained batch 824 batch loss 0.564968288 epoch total loss 0.54050827\n",
      "Trained batch 825 batch loss 0.676002681 epoch total loss 0.540672481\n",
      "Trained batch 826 batch loss 0.654889941 epoch total loss 0.540810764\n",
      "Trained batch 827 batch loss 0.572977424 epoch total loss 0.540849626\n",
      "Trained batch 828 batch loss 0.566822231 epoch total loss 0.540881038\n",
      "Trained batch 829 batch loss 0.652939439 epoch total loss 0.541016221\n",
      "Trained batch 830 batch loss 0.581674 epoch total loss 0.541065216\n",
      "Trained batch 831 batch loss 0.558589756 epoch total loss 0.541086257\n",
      "Trained batch 832 batch loss 0.677031636 epoch total loss 0.541249692\n",
      "Trained batch 833 batch loss 0.611665487 epoch total loss 0.541334212\n",
      "Trained batch 834 batch loss 0.594886601 epoch total loss 0.541398406\n",
      "Trained batch 835 batch loss 0.521918237 epoch total loss 0.541375101\n",
      "Trained batch 836 batch loss 0.638901472 epoch total loss 0.541491747\n",
      "Trained batch 837 batch loss 0.487378538 epoch total loss 0.541427076\n",
      "Trained batch 838 batch loss 0.517529249 epoch total loss 0.541398525\n",
      "Trained batch 839 batch loss 0.505304575 epoch total loss 0.54135555\n",
      "Trained batch 840 batch loss 0.502791405 epoch total loss 0.541309595\n",
      "Trained batch 841 batch loss 0.606721282 epoch total loss 0.541387379\n",
      "Trained batch 842 batch loss 0.473657459 epoch total loss 0.541307\n",
      "Trained batch 843 batch loss 0.563139796 epoch total loss 0.541332841\n",
      "Trained batch 844 batch loss 0.494161934 epoch total loss 0.541277\n",
      "Trained batch 845 batch loss 0.62610817 epoch total loss 0.541377366\n",
      "Trained batch 846 batch loss 0.577480853 epoch total loss 0.541420043\n",
      "Trained batch 847 batch loss 0.51871562 epoch total loss 0.54139322\n",
      "Trained batch 848 batch loss 0.597622156 epoch total loss 0.54145956\n",
      "Trained batch 849 batch loss 0.591464281 epoch total loss 0.54151845\n",
      "Trained batch 850 batch loss 0.602011621 epoch total loss 0.541589618\n",
      "Trained batch 851 batch loss 0.603408754 epoch total loss 0.541662216\n",
      "Trained batch 852 batch loss 0.525581598 epoch total loss 0.541643322\n",
      "Trained batch 853 batch loss 0.505674303 epoch total loss 0.541601181\n",
      "Trained batch 854 batch loss 0.651229501 epoch total loss 0.54172951\n",
      "Trained batch 855 batch loss 0.580888748 epoch total loss 0.541775346\n",
      "Trained batch 856 batch loss 0.500113726 epoch total loss 0.541726708\n",
      "Trained batch 857 batch loss 0.42251572 epoch total loss 0.541587591\n",
      "Trained batch 858 batch loss 0.429223835 epoch total loss 0.54145664\n",
      "Trained batch 859 batch loss 0.456234604 epoch total loss 0.541357458\n",
      "Trained batch 860 batch loss 0.536056042 epoch total loss 0.541351259\n",
      "Trained batch 861 batch loss 0.467883945 epoch total loss 0.541265905\n",
      "Trained batch 862 batch loss 0.600178897 epoch total loss 0.541334271\n",
      "Trained batch 863 batch loss 0.539507627 epoch total loss 0.541332185\n",
      "Trained batch 864 batch loss 0.575270534 epoch total loss 0.541371465\n",
      "Trained batch 865 batch loss 0.604190111 epoch total loss 0.541444063\n",
      "Trained batch 866 batch loss 0.51054 epoch total loss 0.54140836\n",
      "Trained batch 867 batch loss 0.505782604 epoch total loss 0.541367292\n",
      "Trained batch 868 batch loss 0.586237 epoch total loss 0.54141897\n",
      "Trained batch 869 batch loss 0.521973789 epoch total loss 0.541396558\n",
      "Trained batch 870 batch loss 0.61677134 epoch total loss 0.541483223\n",
      "Trained batch 871 batch loss 0.513165891 epoch total loss 0.541450679\n",
      "Trained batch 872 batch loss 0.574396729 epoch total loss 0.541488469\n",
      "Trained batch 873 batch loss 0.608754933 epoch total loss 0.541565537\n",
      "Trained batch 874 batch loss 0.557271063 epoch total loss 0.541583538\n",
      "Trained batch 875 batch loss 0.54745239 epoch total loss 0.541590214\n",
      "Trained batch 876 batch loss 0.564018786 epoch total loss 0.541615844\n",
      "Trained batch 877 batch loss 0.63208884 epoch total loss 0.541719\n",
      "Trained batch 878 batch loss 0.699571848 epoch total loss 0.541898787\n",
      "Trained batch 879 batch loss 0.68910867 epoch total loss 0.542066276\n",
      "Trained batch 880 batch loss 0.55799216 epoch total loss 0.542084336\n",
      "Trained batch 881 batch loss 0.561894298 epoch total loss 0.542106867\n",
      "Trained batch 882 batch loss 0.521991909 epoch total loss 0.542084038\n",
      "Trained batch 883 batch loss 0.585914075 epoch total loss 0.542133689\n",
      "Trained batch 884 batch loss 0.624119163 epoch total loss 0.542226434\n",
      "Trained batch 885 batch loss 0.65164876 epoch total loss 0.542350054\n",
      "Trained batch 886 batch loss 0.614666 epoch total loss 0.542431653\n",
      "Trained batch 887 batch loss 0.563694715 epoch total loss 0.542455614\n",
      "Trained batch 888 batch loss 0.604995191 epoch total loss 0.542526066\n",
      "Trained batch 889 batch loss 0.453365266 epoch total loss 0.542425752\n",
      "Trained batch 890 batch loss 0.617106557 epoch total loss 0.542509675\n",
      "Trained batch 891 batch loss 0.531949282 epoch total loss 0.542497814\n",
      "Trained batch 892 batch loss 0.500977635 epoch total loss 0.542451262\n",
      "Trained batch 893 batch loss 0.569793105 epoch total loss 0.542481899\n",
      "Trained batch 894 batch loss 0.489543289 epoch total loss 0.542422652\n",
      "Trained batch 895 batch loss 0.550584853 epoch total loss 0.542431772\n",
      "Trained batch 896 batch loss 0.59857583 epoch total loss 0.542494416\n",
      "Trained batch 897 batch loss 0.570503116 epoch total loss 0.542525649\n",
      "Trained batch 898 batch loss 0.56375 epoch total loss 0.542549312\n",
      "Trained batch 899 batch loss 0.551975965 epoch total loss 0.542559743\n",
      "Trained batch 900 batch loss 0.557972491 epoch total loss 0.542576909\n",
      "Trained batch 901 batch loss 0.570035338 epoch total loss 0.542607367\n",
      "Trained batch 902 batch loss 0.597530246 epoch total loss 0.542668283\n",
      "Trained batch 903 batch loss 0.563494384 epoch total loss 0.54269135\n",
      "Trained batch 904 batch loss 0.452679724 epoch total loss 0.542591751\n",
      "Trained batch 905 batch loss 0.532313943 epoch total loss 0.542580426\n",
      "Trained batch 906 batch loss 0.603040695 epoch total loss 0.542647123\n",
      "Trained batch 907 batch loss 0.50097847 epoch total loss 0.542601168\n",
      "Trained batch 908 batch loss 0.534736 epoch total loss 0.542592525\n",
      "Trained batch 909 batch loss 0.571051419 epoch total loss 0.542623818\n",
      "Trained batch 910 batch loss 0.531815231 epoch total loss 0.542611957\n",
      "Trained batch 911 batch loss 0.507471502 epoch total loss 0.542573392\n",
      "Trained batch 912 batch loss 0.510012031 epoch total loss 0.542537689\n",
      "Trained batch 913 batch loss 0.563559651 epoch total loss 0.542560697\n",
      "Trained batch 914 batch loss 0.558568537 epoch total loss 0.54257822\n",
      "Trained batch 915 batch loss 0.604578733 epoch total loss 0.542646\n",
      "Trained batch 916 batch loss 0.528915823 epoch total loss 0.54263103\n",
      "Trained batch 917 batch loss 0.567611217 epoch total loss 0.542658269\n",
      "Trained batch 918 batch loss 0.613799334 epoch total loss 0.542735755\n",
      "Trained batch 919 batch loss 0.509739935 epoch total loss 0.542699814\n",
      "Trained batch 920 batch loss 0.571594536 epoch total loss 0.542731225\n",
      "Trained batch 921 batch loss 0.772297561 epoch total loss 0.542980492\n",
      "Trained batch 922 batch loss 0.662059665 epoch total loss 0.543109655\n",
      "Trained batch 923 batch loss 0.761013865 epoch total loss 0.543345749\n",
      "Trained batch 924 batch loss 0.566944957 epoch total loss 0.54337132\n",
      "Trained batch 925 batch loss 0.536385417 epoch total loss 0.54336375\n",
      "Trained batch 926 batch loss 0.536336303 epoch total loss 0.54335618\n",
      "Trained batch 927 batch loss 0.558444142 epoch total loss 0.543372452\n",
      "Trained batch 928 batch loss 0.503103495 epoch total loss 0.54332906\n",
      "Trained batch 929 batch loss 0.432302684 epoch total loss 0.543209553\n",
      "Trained batch 930 batch loss 0.380354285 epoch total loss 0.543034434\n",
      "Trained batch 931 batch loss 0.353014886 epoch total loss 0.542830348\n",
      "Trained batch 932 batch loss 0.430963069 epoch total loss 0.542710304\n",
      "Trained batch 933 batch loss 0.502803 epoch total loss 0.542667508\n",
      "Trained batch 934 batch loss 0.601716 epoch total loss 0.542730749\n",
      "Trained batch 935 batch loss 0.510925055 epoch total loss 0.542696714\n",
      "Trained batch 936 batch loss 0.425465018 epoch total loss 0.542571485\n",
      "Trained batch 937 batch loss 0.358356833 epoch total loss 0.542374909\n",
      "Trained batch 938 batch loss 0.354390949 epoch total loss 0.542174518\n",
      "Trained batch 939 batch loss 0.401777714 epoch total loss 0.54202497\n",
      "Trained batch 940 batch loss 0.398823678 epoch total loss 0.541872621\n",
      "Trained batch 941 batch loss 0.466278881 epoch total loss 0.541792333\n",
      "Trained batch 942 batch loss 0.41422379 epoch total loss 0.541656911\n",
      "Trained batch 943 batch loss 0.483806521 epoch total loss 0.541595519\n",
      "Trained batch 944 batch loss 0.450509936 epoch total loss 0.541499\n",
      "Trained batch 945 batch loss 0.533250451 epoch total loss 0.541490316\n",
      "Trained batch 946 batch loss 0.456105709 epoch total loss 0.5414\n",
      "Trained batch 947 batch loss 0.483424306 epoch total loss 0.541338801\n",
      "Trained batch 948 batch loss 0.561413646 epoch total loss 0.541359961\n",
      "Trained batch 949 batch loss 0.522714913 epoch total loss 0.541340292\n",
      "Trained batch 950 batch loss 0.449082255 epoch total loss 0.541243196\n",
      "Trained batch 951 batch loss 0.546434879 epoch total loss 0.541248679\n",
      "Trained batch 952 batch loss 0.488939881 epoch total loss 0.541193724\n",
      "Trained batch 953 batch loss 0.59617734 epoch total loss 0.541251421\n",
      "Trained batch 954 batch loss 0.487437129 epoch total loss 0.541195035\n",
      "Trained batch 955 batch loss 0.486134946 epoch total loss 0.541137397\n",
      "Trained batch 956 batch loss 0.477259547 epoch total loss 0.541070521\n",
      "Trained batch 957 batch loss 0.535254061 epoch total loss 0.541064501\n",
      "Trained batch 958 batch loss 0.602031529 epoch total loss 0.541128159\n",
      "Trained batch 959 batch loss 0.616844535 epoch total loss 0.541207075\n",
      "Trained batch 960 batch loss 0.587990522 epoch total loss 0.541255832\n",
      "Trained batch 961 batch loss 0.650569916 epoch total loss 0.541369557\n",
      "Trained batch 962 batch loss 0.614343286 epoch total loss 0.541445434\n",
      "Trained batch 963 batch loss 0.594825208 epoch total loss 0.541500866\n",
      "Trained batch 964 batch loss 0.576408625 epoch total loss 0.541537106\n",
      "Trained batch 965 batch loss 0.603440583 epoch total loss 0.541601241\n",
      "Trained batch 966 batch loss 0.529923677 epoch total loss 0.541589141\n",
      "Trained batch 967 batch loss 0.591444492 epoch total loss 0.541640699\n",
      "Trained batch 968 batch loss 0.588275611 epoch total loss 0.541688859\n",
      "Trained batch 969 batch loss 0.524631262 epoch total loss 0.541671276\n",
      "Trained batch 970 batch loss 0.526322484 epoch total loss 0.541655421\n",
      "Trained batch 971 batch loss 0.523274 epoch total loss 0.541636467\n",
      "Trained batch 972 batch loss 0.477153957 epoch total loss 0.541570127\n",
      "Trained batch 973 batch loss 0.473769337 epoch total loss 0.541500449\n",
      "Trained batch 974 batch loss 0.521199584 epoch total loss 0.541479588\n",
      "Trained batch 975 batch loss 0.491654426 epoch total loss 0.541428447\n",
      "Trained batch 976 batch loss 0.603999496 epoch total loss 0.541492581\n",
      "Trained batch 977 batch loss 0.55662632 epoch total loss 0.541508079\n",
      "Trained batch 978 batch loss 0.613552451 epoch total loss 0.54158175\n",
      "Trained batch 979 batch loss 0.596768439 epoch total loss 0.541638076\n",
      "Trained batch 980 batch loss 0.586943507 epoch total loss 0.54168427\n",
      "Trained batch 981 batch loss 0.517484426 epoch total loss 0.541659594\n",
      "Trained batch 982 batch loss 0.578772068 epoch total loss 0.541697383\n",
      "Trained batch 983 batch loss 0.556256294 epoch total loss 0.541712224\n",
      "Trained batch 984 batch loss 0.515431285 epoch total loss 0.541685522\n",
      "Trained batch 985 batch loss 0.44437179 epoch total loss 0.541586757\n",
      "Trained batch 986 batch loss 0.441831023 epoch total loss 0.541485608\n",
      "Trained batch 987 batch loss 0.452405572 epoch total loss 0.541395307\n",
      "Trained batch 988 batch loss 0.466506928 epoch total loss 0.541319489\n",
      "Trained batch 989 batch loss 0.423945427 epoch total loss 0.541200817\n",
      "Trained batch 990 batch loss 0.548814714 epoch total loss 0.541208506\n",
      "Trained batch 991 batch loss 0.497892439 epoch total loss 0.541164815\n",
      "Trained batch 992 batch loss 0.531061411 epoch total loss 0.541154623\n",
      "Trained batch 993 batch loss 0.489417255 epoch total loss 0.541102529\n",
      "Trained batch 994 batch loss 0.47739023 epoch total loss 0.541038454\n",
      "Trained batch 995 batch loss 0.460481972 epoch total loss 0.54095751\n",
      "Trained batch 996 batch loss 0.492917091 epoch total loss 0.54090929\n",
      "Trained batch 997 batch loss 0.556691647 epoch total loss 0.540925145\n",
      "Trained batch 998 batch loss 0.498037606 epoch total loss 0.54088217\n",
      "Trained batch 999 batch loss 0.522584736 epoch total loss 0.540863872\n",
      "Trained batch 1000 batch loss 0.544411838 epoch total loss 0.540867448\n",
      "Trained batch 1001 batch loss 0.536411583 epoch total loss 0.540863\n",
      "Trained batch 1002 batch loss 0.586419284 epoch total loss 0.540908456\n",
      "Trained batch 1003 batch loss 0.57354933 epoch total loss 0.540941\n",
      "Trained batch 1004 batch loss 0.641222477 epoch total loss 0.541040897\n",
      "Trained batch 1005 batch loss 0.581310093 epoch total loss 0.541080952\n",
      "Trained batch 1006 batch loss 0.556895912 epoch total loss 0.541096687\n",
      "Trained batch 1007 batch loss 0.582321763 epoch total loss 0.541137636\n",
      "Trained batch 1008 batch loss 0.550317526 epoch total loss 0.541146696\n",
      "Trained batch 1009 batch loss 0.619214892 epoch total loss 0.541224062\n",
      "Trained batch 1010 batch loss 0.544641256 epoch total loss 0.54122746\n",
      "Trained batch 1011 batch loss 0.672751 epoch total loss 0.541357517\n",
      "Trained batch 1012 batch loss 0.566939831 epoch total loss 0.54138279\n",
      "Trained batch 1013 batch loss 0.567190111 epoch total loss 0.5414083\n",
      "Trained batch 1014 batch loss 0.614946187 epoch total loss 0.54148078\n",
      "Trained batch 1015 batch loss 0.57185775 epoch total loss 0.541510701\n",
      "Trained batch 1016 batch loss 0.617337167 epoch total loss 0.541585326\n",
      "Trained batch 1017 batch loss 0.513778865 epoch total loss 0.541557968\n",
      "Trained batch 1018 batch loss 0.54874438 epoch total loss 0.541565061\n",
      "Trained batch 1019 batch loss 0.590067863 epoch total loss 0.541612685\n",
      "Trained batch 1020 batch loss 0.522691 epoch total loss 0.541594148\n",
      "Trained batch 1021 batch loss 0.65597254 epoch total loss 0.541706145\n",
      "Trained batch 1022 batch loss 0.65397191 epoch total loss 0.541816\n",
      "Trained batch 1023 batch loss 0.611837089 epoch total loss 0.541884422\n",
      "Trained batch 1024 batch loss 0.546907306 epoch total loss 0.541889369\n",
      "Trained batch 1025 batch loss 0.574687481 epoch total loss 0.541921377\n",
      "Trained batch 1026 batch loss 0.494393468 epoch total loss 0.541875064\n",
      "Trained batch 1027 batch loss 0.611241579 epoch total loss 0.541942596\n",
      "Trained batch 1028 batch loss 0.509049118 epoch total loss 0.541910589\n",
      "Trained batch 1029 batch loss 0.449187547 epoch total loss 0.541820467\n",
      "Trained batch 1030 batch loss 0.426056236 epoch total loss 0.541708112\n",
      "Trained batch 1031 batch loss 0.385521382 epoch total loss 0.541556597\n",
      "Trained batch 1032 batch loss 0.421363473 epoch total loss 0.541440129\n",
      "Trained batch 1033 batch loss 0.554787874 epoch total loss 0.541453063\n",
      "Trained batch 1034 batch loss 0.528734803 epoch total loss 0.541440785\n",
      "Trained batch 1035 batch loss 0.560952 epoch total loss 0.54145968\n",
      "Trained batch 1036 batch loss 0.520479679 epoch total loss 0.541439474\n",
      "Trained batch 1037 batch loss 0.516979158 epoch total loss 0.54141587\n",
      "Trained batch 1038 batch loss 0.63089 epoch total loss 0.541502059\n",
      "Trained batch 1039 batch loss 0.558677614 epoch total loss 0.541518569\n",
      "Trained batch 1040 batch loss 0.582063 epoch total loss 0.54155761\n",
      "Trained batch 1041 batch loss 0.5657897 epoch total loss 0.541580915\n",
      "Trained batch 1042 batch loss 0.609754205 epoch total loss 0.541646302\n",
      "Trained batch 1043 batch loss 0.599425316 epoch total loss 0.541701674\n",
      "Trained batch 1044 batch loss 0.611718297 epoch total loss 0.54176873\n",
      "Trained batch 1045 batch loss 0.568705678 epoch total loss 0.541794538\n",
      "Trained batch 1046 batch loss 0.523403585 epoch total loss 0.541776955\n",
      "Trained batch 1047 batch loss 0.597374797 epoch total loss 0.54183\n",
      "Trained batch 1048 batch loss 0.586568356 epoch total loss 0.54187268\n",
      "Trained batch 1049 batch loss 0.607481 epoch total loss 0.541935205\n",
      "Trained batch 1050 batch loss 0.555237591 epoch total loss 0.541947901\n",
      "Trained batch 1051 batch loss 0.582420409 epoch total loss 0.541986406\n",
      "Trained batch 1052 batch loss 0.537019134 epoch total loss 0.541981697\n",
      "Trained batch 1053 batch loss 0.563677549 epoch total loss 0.542002261\n",
      "Trained batch 1054 batch loss 0.573416173 epoch total loss 0.542032063\n",
      "Trained batch 1055 batch loss 0.537363887 epoch total loss 0.542027652\n",
      "Trained batch 1056 batch loss 0.553049 epoch total loss 0.542038083\n",
      "Trained batch 1057 batch loss 0.528759301 epoch total loss 0.542025506\n",
      "Trained batch 1058 batch loss 0.482741 epoch total loss 0.541969478\n",
      "Trained batch 1059 batch loss 0.486448377 epoch total loss 0.541917\n",
      "Trained batch 1060 batch loss 0.520528436 epoch total loss 0.54189682\n",
      "Trained batch 1061 batch loss 0.561216593 epoch total loss 0.541915059\n",
      "Trained batch 1062 batch loss 0.5597 epoch total loss 0.541931808\n",
      "Trained batch 1063 batch loss 0.531390965 epoch total loss 0.541921854\n",
      "Trained batch 1064 batch loss 0.622142434 epoch total loss 0.541997254\n",
      "Trained batch 1065 batch loss 0.596474349 epoch total loss 0.542048395\n",
      "Trained batch 1066 batch loss 0.624125659 epoch total loss 0.542125404\n",
      "Trained batch 1067 batch loss 0.540757298 epoch total loss 0.542124152\n",
      "Trained batch 1068 batch loss 0.52384156 epoch total loss 0.542107046\n",
      "Trained batch 1069 batch loss 0.547317863 epoch total loss 0.542111933\n",
      "Trained batch 1070 batch loss 0.538428903 epoch total loss 0.542108476\n",
      "Trained batch 1071 batch loss 0.622110128 epoch total loss 0.54218322\n",
      "Trained batch 1072 batch loss 0.509352148 epoch total loss 0.542152584\n",
      "Trained batch 1073 batch loss 0.49466306 epoch total loss 0.542108357\n",
      "Trained batch 1074 batch loss 0.564827621 epoch total loss 0.542129517\n",
      "Trained batch 1075 batch loss 0.456204265 epoch total loss 0.542049527\n",
      "Trained batch 1076 batch loss 0.481679499 epoch total loss 0.541993439\n",
      "Trained batch 1077 batch loss 0.455992758 epoch total loss 0.541913569\n",
      "Trained batch 1078 batch loss 0.558712482 epoch total loss 0.541929185\n",
      "Trained batch 1079 batch loss 0.581391513 epoch total loss 0.541965783\n",
      "Trained batch 1080 batch loss 0.578410208 epoch total loss 0.541999519\n",
      "Trained batch 1081 batch loss 0.623791099 epoch total loss 0.542075157\n",
      "Trained batch 1082 batch loss 0.57422173 epoch total loss 0.5421049\n",
      "Trained batch 1083 batch loss 0.583596289 epoch total loss 0.542143226\n",
      "Trained batch 1084 batch loss 0.58894068 epoch total loss 0.542186379\n",
      "Trained batch 1085 batch loss 0.53536272 epoch total loss 0.542180061\n",
      "Trained batch 1086 batch loss 0.584489644 epoch total loss 0.542219043\n",
      "Trained batch 1087 batch loss 0.610236645 epoch total loss 0.542281568\n",
      "Trained batch 1088 batch loss 0.525207341 epoch total loss 0.542265892\n",
      "Trained batch 1089 batch loss 0.599373221 epoch total loss 0.542318344\n",
      "Trained batch 1090 batch loss 0.557798266 epoch total loss 0.54233253\n",
      "Trained batch 1091 batch loss 0.527467906 epoch total loss 0.542318881\n",
      "Trained batch 1092 batch loss 0.587157786 epoch total loss 0.542359948\n",
      "Trained batch 1093 batch loss 0.562379241 epoch total loss 0.542378306\n",
      "Trained batch 1094 batch loss 0.608656943 epoch total loss 0.542438865\n",
      "Trained batch 1095 batch loss 0.667037606 epoch total loss 0.54255265\n",
      "Trained batch 1096 batch loss 0.605740428 epoch total loss 0.542610288\n",
      "Trained batch 1097 batch loss 0.473711193 epoch total loss 0.542547464\n",
      "Trained batch 1098 batch loss 0.536869 epoch total loss 0.542542279\n",
      "Trained batch 1099 batch loss 0.511424661 epoch total loss 0.542513967\n",
      "Trained batch 1100 batch loss 0.481446654 epoch total loss 0.542458415\n",
      "Trained batch 1101 batch loss 0.506236 epoch total loss 0.542425513\n",
      "Trained batch 1102 batch loss 0.501065671 epoch total loss 0.542387962\n",
      "Trained batch 1103 batch loss 0.406807661 epoch total loss 0.542265058\n",
      "Trained batch 1104 batch loss 0.414035052 epoch total loss 0.542148948\n",
      "Trained batch 1105 batch loss 0.498112828 epoch total loss 0.542109072\n",
      "Trained batch 1106 batch loss 0.414370716 epoch total loss 0.541993558\n",
      "Trained batch 1107 batch loss 0.481313765 epoch total loss 0.541938782\n",
      "Trained batch 1108 batch loss 0.506658196 epoch total loss 0.541906893\n",
      "Trained batch 1109 batch loss 0.45977965 epoch total loss 0.541832864\n",
      "Trained batch 1110 batch loss 0.474974334 epoch total loss 0.541772604\n",
      "Trained batch 1111 batch loss 0.399383962 epoch total loss 0.541644514\n",
      "Trained batch 1112 batch loss 0.367068589 epoch total loss 0.541487515\n",
      "Trained batch 1113 batch loss 0.329650342 epoch total loss 0.541297138\n",
      "Trained batch 1114 batch loss 0.410271823 epoch total loss 0.541179538\n",
      "Trained batch 1115 batch loss 0.400884688 epoch total loss 0.541053712\n",
      "Trained batch 1116 batch loss 0.339311093 epoch total loss 0.540872931\n",
      "Trained batch 1117 batch loss 0.452874094 epoch total loss 0.540794134\n",
      "Trained batch 1118 batch loss 0.50450325 epoch total loss 0.540761709\n",
      "Trained batch 1119 batch loss 0.45564422 epoch total loss 0.540685654\n",
      "Trained batch 1120 batch loss 0.533752501 epoch total loss 0.540679455\n",
      "Trained batch 1121 batch loss 0.529326558 epoch total loss 0.540669262\n",
      "Trained batch 1122 batch loss 0.569921434 epoch total loss 0.540695369\n",
      "Trained batch 1123 batch loss 0.488476634 epoch total loss 0.540648878\n",
      "Trained batch 1124 batch loss 0.505995572 epoch total loss 0.540618\n",
      "Trained batch 1125 batch loss 0.504365265 epoch total loss 0.540585816\n",
      "Trained batch 1126 batch loss 0.468894094 epoch total loss 0.540522158\n",
      "Trained batch 1127 batch loss 0.617045403 epoch total loss 0.540590048\n",
      "Trained batch 1128 batch loss 0.501255393 epoch total loss 0.540555239\n",
      "Trained batch 1129 batch loss 0.564668477 epoch total loss 0.540576577\n",
      "Trained batch 1130 batch loss 0.560369253 epoch total loss 0.540594101\n",
      "Trained batch 1131 batch loss 0.588558793 epoch total loss 0.540636539\n",
      "Trained batch 1132 batch loss 0.539390683 epoch total loss 0.540635407\n",
      "Trained batch 1133 batch loss 0.600598454 epoch total loss 0.540688336\n",
      "Trained batch 1134 batch loss 0.549774885 epoch total loss 0.540696323\n",
      "Trained batch 1135 batch loss 0.546990693 epoch total loss 0.540701926\n",
      "Trained batch 1136 batch loss 0.550110698 epoch total loss 0.540710151\n",
      "Trained batch 1137 batch loss 0.50465095 epoch total loss 0.540678442\n",
      "Trained batch 1138 batch loss 0.543872952 epoch total loss 0.540681243\n",
      "Trained batch 1139 batch loss 0.505155921 epoch total loss 0.54065007\n",
      "Trained batch 1140 batch loss 0.540566266 epoch total loss 0.54065\n",
      "Trained batch 1141 batch loss 0.537642 epoch total loss 0.540647388\n",
      "Trained batch 1142 batch loss 0.464264333 epoch total loss 0.540580511\n",
      "Trained batch 1143 batch loss 0.504541039 epoch total loss 0.540549\n",
      "Trained batch 1144 batch loss 0.499158412 epoch total loss 0.5405128\n",
      "Trained batch 1145 batch loss 0.565223873 epoch total loss 0.540534377\n",
      "Trained batch 1146 batch loss 0.536952138 epoch total loss 0.540531218\n",
      "Trained batch 1147 batch loss 0.551308751 epoch total loss 0.540540636\n",
      "Trained batch 1148 batch loss 0.453385919 epoch total loss 0.540464699\n",
      "Trained batch 1149 batch loss 0.46848765 epoch total loss 0.540402114\n",
      "Trained batch 1150 batch loss 0.544287443 epoch total loss 0.540405512\n",
      "Trained batch 1151 batch loss 0.498049021 epoch total loss 0.540368676\n",
      "Trained batch 1152 batch loss 0.552006483 epoch total loss 0.540378809\n",
      "Trained batch 1153 batch loss 0.508235 epoch total loss 0.540350914\n",
      "Trained batch 1154 batch loss 0.491138518 epoch total loss 0.540308297\n",
      "Trained batch 1155 batch loss 0.428943187 epoch total loss 0.540211856\n",
      "Trained batch 1156 batch loss 0.607524 epoch total loss 0.54027009\n",
      "Trained batch 1157 batch loss 0.502879262 epoch total loss 0.540237784\n",
      "Trained batch 1158 batch loss 0.528986216 epoch total loss 0.540228069\n",
      "Trained batch 1159 batch loss 0.48815009 epoch total loss 0.540183127\n",
      "Trained batch 1160 batch loss 0.568185 epoch total loss 0.540207267\n",
      "Trained batch 1161 batch loss 0.559695244 epoch total loss 0.540224075\n",
      "Trained batch 1162 batch loss 0.420882702 epoch total loss 0.540121377\n",
      "Trained batch 1163 batch loss 0.489272326 epoch total loss 0.540077627\n",
      "Trained batch 1164 batch loss 0.521374226 epoch total loss 0.540061533\n",
      "Trained batch 1165 batch loss 0.554890037 epoch total loss 0.540074289\n",
      "Trained batch 1166 batch loss 0.457151979 epoch total loss 0.540003181\n",
      "Trained batch 1167 batch loss 0.504802406 epoch total loss 0.539973\n",
      "Trained batch 1168 batch loss 0.556419849 epoch total loss 0.539987087\n",
      "Trained batch 1169 batch loss 0.561430693 epoch total loss 0.540005386\n",
      "Trained batch 1170 batch loss 0.652858496 epoch total loss 0.540101826\n",
      "Trained batch 1171 batch loss 0.525071263 epoch total loss 0.540089\n",
      "Trained batch 1172 batch loss 0.571978271 epoch total loss 0.540116191\n",
      "Trained batch 1173 batch loss 0.638374209 epoch total loss 0.540199935\n",
      "Trained batch 1174 batch loss 0.639235198 epoch total loss 0.540284276\n",
      "Trained batch 1175 batch loss 0.578705907 epoch total loss 0.540317\n",
      "Trained batch 1176 batch loss 0.481984049 epoch total loss 0.540267408\n",
      "Trained batch 1177 batch loss 0.446478 epoch total loss 0.540187716\n",
      "Trained batch 1178 batch loss 0.46398434 epoch total loss 0.540123045\n",
      "Trained batch 1179 batch loss 0.392769426 epoch total loss 0.539998055\n",
      "Trained batch 1180 batch loss 0.476639241 epoch total loss 0.539944351\n",
      "Trained batch 1181 batch loss 0.457465559 epoch total loss 0.539874494\n",
      "Trained batch 1182 batch loss 0.514643 epoch total loss 0.539853156\n",
      "Trained batch 1183 batch loss 0.401996881 epoch total loss 0.539736629\n",
      "Trained batch 1184 batch loss 0.52643609 epoch total loss 0.539725363\n",
      "Trained batch 1185 batch loss 0.529053688 epoch total loss 0.539716363\n",
      "Trained batch 1186 batch loss 0.466548175 epoch total loss 0.539654672\n",
      "Trained batch 1187 batch loss 0.489542276 epoch total loss 0.539612472\n",
      "Trained batch 1188 batch loss 0.59024328 epoch total loss 0.539655149\n",
      "Trained batch 1189 batch loss 0.535806298 epoch total loss 0.53965193\n",
      "Trained batch 1190 batch loss 0.641470432 epoch total loss 0.539737463\n",
      "Trained batch 1191 batch loss 0.471544892 epoch total loss 0.539680243\n",
      "Trained batch 1192 batch loss 0.429237783 epoch total loss 0.539587617\n",
      "Trained batch 1193 batch loss 0.551984668 epoch total loss 0.539598\n",
      "Trained batch 1194 batch loss 0.521770418 epoch total loss 0.539583087\n",
      "Trained batch 1195 batch loss 0.55941 epoch total loss 0.539599657\n",
      "Trained batch 1196 batch loss 0.563516796 epoch total loss 0.539619684\n",
      "Trained batch 1197 batch loss 0.561903238 epoch total loss 0.539638281\n",
      "Trained batch 1198 batch loss 0.609151483 epoch total loss 0.539696276\n",
      "Trained batch 1199 batch loss 0.688289344 epoch total loss 0.539820194\n",
      "Trained batch 1200 batch loss 0.710320592 epoch total loss 0.539962292\n",
      "Trained batch 1201 batch loss 0.639741421 epoch total loss 0.54004544\n",
      "Trained batch 1202 batch loss 0.705190659 epoch total loss 0.540182829\n",
      "Trained batch 1203 batch loss 0.633887 epoch total loss 0.540260732\n",
      "Trained batch 1204 batch loss 0.521906674 epoch total loss 0.540245473\n",
      "Trained batch 1205 batch loss 0.432326645 epoch total loss 0.540155888\n",
      "Trained batch 1206 batch loss 0.465876311 epoch total loss 0.540094316\n",
      "Trained batch 1207 batch loss 0.49983722 epoch total loss 0.540060937\n",
      "Trained batch 1208 batch loss 0.510853171 epoch total loss 0.540036798\n",
      "Trained batch 1209 batch loss 0.610449493 epoch total loss 0.540095031\n",
      "Trained batch 1210 batch loss 0.62436372 epoch total loss 0.540164709\n",
      "Trained batch 1211 batch loss 0.734706581 epoch total loss 0.540325344\n",
      "Trained batch 1212 batch loss 0.66754204 epoch total loss 0.540430307\n",
      "Trained batch 1213 batch loss 0.510917187 epoch total loss 0.540406\n",
      "Trained batch 1214 batch loss 0.587716222 epoch total loss 0.54044497\n",
      "Trained batch 1215 batch loss 0.57495147 epoch total loss 0.540473342\n",
      "Trained batch 1216 batch loss 0.551449 epoch total loss 0.540482342\n",
      "Trained batch 1217 batch loss 0.589198589 epoch total loss 0.540522397\n",
      "Trained batch 1218 batch loss 0.504551888 epoch total loss 0.540492892\n",
      "Trained batch 1219 batch loss 0.531026602 epoch total loss 0.540485084\n",
      "Trained batch 1220 batch loss 0.554853678 epoch total loss 0.540496886\n",
      "Trained batch 1221 batch loss 0.498523891 epoch total loss 0.540462494\n",
      "Trained batch 1222 batch loss 0.473018348 epoch total loss 0.5404073\n",
      "Trained batch 1223 batch loss 0.533979714 epoch total loss 0.540402055\n",
      "Trained batch 1224 batch loss 0.465930402 epoch total loss 0.540341258\n",
      "Trained batch 1225 batch loss 0.531207323 epoch total loss 0.540333748\n",
      "Trained batch 1226 batch loss 0.621845663 epoch total loss 0.540400267\n",
      "Trained batch 1227 batch loss 0.487727255 epoch total loss 0.540357292\n",
      "Trained batch 1228 batch loss 0.564888477 epoch total loss 0.540377319\n",
      "Trained batch 1229 batch loss 0.592003524 epoch total loss 0.540419281\n",
      "Trained batch 1230 batch loss 0.596335411 epoch total loss 0.540464699\n",
      "Trained batch 1231 batch loss 0.590601087 epoch total loss 0.540505409\n",
      "Trained batch 1232 batch loss 0.542844832 epoch total loss 0.540507317\n",
      "Trained batch 1233 batch loss 0.560213447 epoch total loss 0.54052335\n",
      "Trained batch 1234 batch loss 0.547722697 epoch total loss 0.540529191\n",
      "Trained batch 1235 batch loss 0.434522033 epoch total loss 0.540443301\n",
      "Trained batch 1236 batch loss 0.49797225 epoch total loss 0.540408969\n",
      "Trained batch 1237 batch loss 0.58181 epoch total loss 0.540442407\n",
      "Trained batch 1238 batch loss 0.597411633 epoch total loss 0.540488422\n",
      "Trained batch 1239 batch loss 0.57405293 epoch total loss 0.540515542\n",
      "Trained batch 1240 batch loss 0.557706058 epoch total loss 0.54052937\n",
      "Trained batch 1241 batch loss 0.556928217 epoch total loss 0.540542603\n",
      "Trained batch 1242 batch loss 0.621611536 epoch total loss 0.54060781\n",
      "Trained batch 1243 batch loss 0.542660594 epoch total loss 0.540609479\n",
      "Trained batch 1244 batch loss 0.53866595 epoch total loss 0.540607929\n",
      "Trained batch 1245 batch loss 0.578934252 epoch total loss 0.540638745\n",
      "Trained batch 1246 batch loss 0.594509244 epoch total loss 0.540681958\n",
      "Trained batch 1247 batch loss 0.491436809 epoch total loss 0.54064244\n",
      "Trained batch 1248 batch loss 0.553053439 epoch total loss 0.540652394\n",
      "Trained batch 1249 batch loss 0.501388073 epoch total loss 0.540621\n",
      "Trained batch 1250 batch loss 0.523931265 epoch total loss 0.540607631\n",
      "Trained batch 1251 batch loss 0.486756355 epoch total loss 0.540564597\n",
      "Trained batch 1252 batch loss 0.530031621 epoch total loss 0.540556133\n",
      "Trained batch 1253 batch loss 0.647887647 epoch total loss 0.540641844\n",
      "Trained batch 1254 batch loss 0.561702 epoch total loss 0.540658593\n",
      "Trained batch 1255 batch loss 0.527701318 epoch total loss 0.540648282\n",
      "Trained batch 1256 batch loss 0.561005056 epoch total loss 0.540664554\n",
      "Trained batch 1257 batch loss 0.577339232 epoch total loss 0.5406937\n",
      "Trained batch 1258 batch loss 0.452736288 epoch total loss 0.540623784\n",
      "Trained batch 1259 batch loss 0.532492936 epoch total loss 0.540617347\n",
      "Trained batch 1260 batch loss 0.565052688 epoch total loss 0.540636718\n",
      "Trained batch 1261 batch loss 0.567235529 epoch total loss 0.540657818\n",
      "Trained batch 1262 batch loss 0.605373263 epoch total loss 0.540709078\n",
      "Trained batch 1263 batch loss 0.677183 epoch total loss 0.540817142\n",
      "Trained batch 1264 batch loss 0.457288265 epoch total loss 0.54075104\n",
      "Trained batch 1265 batch loss 0.533447385 epoch total loss 0.540745258\n",
      "Trained batch 1266 batch loss 0.589136481 epoch total loss 0.540783465\n",
      "Trained batch 1267 batch loss 0.584171414 epoch total loss 0.540817738\n",
      "Trained batch 1268 batch loss 0.5587731 epoch total loss 0.540831923\n",
      "Trained batch 1269 batch loss 0.576855063 epoch total loss 0.540860295\n",
      "Trained batch 1270 batch loss 0.469333231 epoch total loss 0.540803969\n",
      "Trained batch 1271 batch loss 0.598061085 epoch total loss 0.54084903\n",
      "Trained batch 1272 batch loss 0.538449407 epoch total loss 0.540847182\n",
      "Trained batch 1273 batch loss 0.665481091 epoch total loss 0.540945053\n",
      "Trained batch 1274 batch loss 0.582284272 epoch total loss 0.540977478\n",
      "Trained batch 1275 batch loss 0.63054949 epoch total loss 0.541047752\n",
      "Trained batch 1276 batch loss 0.635529339 epoch total loss 0.54112184\n",
      "Trained batch 1277 batch loss 0.622628689 epoch total loss 0.541185617\n",
      "Trained batch 1278 batch loss 0.693764746 epoch total loss 0.541305065\n",
      "Trained batch 1279 batch loss 0.538130164 epoch total loss 0.541302562\n",
      "Trained batch 1280 batch loss 0.479339033 epoch total loss 0.541254163\n",
      "Trained batch 1281 batch loss 0.536768734 epoch total loss 0.541250646\n",
      "Trained batch 1282 batch loss 0.611725748 epoch total loss 0.541305602\n",
      "Trained batch 1283 batch loss 0.567836225 epoch total loss 0.541326284\n",
      "Trained batch 1284 batch loss 0.566856384 epoch total loss 0.541346133\n",
      "Trained batch 1285 batch loss 0.458919346 epoch total loss 0.541282\n",
      "Trained batch 1286 batch loss 0.639360309 epoch total loss 0.541358232\n",
      "Trained batch 1287 batch loss 0.585239291 epoch total loss 0.541392386\n",
      "Trained batch 1288 batch loss 0.537748337 epoch total loss 0.541389525\n",
      "Trained batch 1289 batch loss 0.601615667 epoch total loss 0.541436255\n",
      "Trained batch 1290 batch loss 0.554758728 epoch total loss 0.541446567\n",
      "Trained batch 1291 batch loss 0.542342484 epoch total loss 0.541447282\n",
      "Trained batch 1292 batch loss 0.545487285 epoch total loss 0.541450381\n",
      "Trained batch 1293 batch loss 0.628460884 epoch total loss 0.541517675\n",
      "Trained batch 1294 batch loss 0.58867085 epoch total loss 0.541554153\n",
      "Trained batch 1295 batch loss 0.515565 epoch total loss 0.541534066\n",
      "Trained batch 1296 batch loss 0.569286704 epoch total loss 0.541555464\n",
      "Trained batch 1297 batch loss 0.646050334 epoch total loss 0.54163605\n",
      "Trained batch 1298 batch loss 0.503762186 epoch total loss 0.541606903\n",
      "Trained batch 1299 batch loss 0.621301293 epoch total loss 0.541668236\n",
      "Trained batch 1300 batch loss 0.537952065 epoch total loss 0.541665375\n",
      "Trained batch 1301 batch loss 0.549584746 epoch total loss 0.541671455\n",
      "Trained batch 1302 batch loss 0.510258257 epoch total loss 0.541647315\n",
      "Trained batch 1303 batch loss 0.484799683 epoch total loss 0.541603684\n",
      "Trained batch 1304 batch loss 0.518111 epoch total loss 0.541585684\n",
      "Trained batch 1305 batch loss 0.582931638 epoch total loss 0.541617393\n",
      "Trained batch 1306 batch loss 0.532319963 epoch total loss 0.541610301\n",
      "Trained batch 1307 batch loss 0.589049935 epoch total loss 0.5416466\n",
      "Trained batch 1308 batch loss 0.574935794 epoch total loss 0.541672051\n",
      "Trained batch 1309 batch loss 0.55228889 epoch total loss 0.541680157\n",
      "Trained batch 1310 batch loss 0.609082758 epoch total loss 0.541731596\n",
      "Trained batch 1311 batch loss 0.58696419 epoch total loss 0.541766107\n",
      "Trained batch 1312 batch loss 0.543502092 epoch total loss 0.541767418\n",
      "Trained batch 1313 batch loss 0.559939623 epoch total loss 0.541781306\n",
      "Trained batch 1314 batch loss 0.539191902 epoch total loss 0.54177928\n",
      "Trained batch 1315 batch loss 0.575611591 epoch total loss 0.541805\n",
      "Trained batch 1316 batch loss 0.616065502 epoch total loss 0.541861475\n",
      "Trained batch 1317 batch loss 0.610478938 epoch total loss 0.541913569\n",
      "Trained batch 1318 batch loss 0.571496 epoch total loss 0.54193604\n",
      "Trained batch 1319 batch loss 0.546403468 epoch total loss 0.541939378\n",
      "Trained batch 1320 batch loss 0.537431121 epoch total loss 0.541936\n",
      "Trained batch 1321 batch loss 0.579026341 epoch total loss 0.541964054\n",
      "Trained batch 1322 batch loss 0.597596169 epoch total loss 0.542006135\n",
      "Trained batch 1323 batch loss 0.540970445 epoch total loss 0.54200536\n",
      "Trained batch 1324 batch loss 0.571371198 epoch total loss 0.542027473\n",
      "Trained batch 1325 batch loss 0.617773414 epoch total loss 0.542084694\n",
      "Trained batch 1326 batch loss 0.60751 epoch total loss 0.542134\n",
      "Trained batch 1327 batch loss 0.517772 epoch total loss 0.542115629\n",
      "Trained batch 1328 batch loss 0.57958746 epoch total loss 0.542143881\n",
      "Trained batch 1329 batch loss 0.600780427 epoch total loss 0.542188\n",
      "Trained batch 1330 batch loss 0.504688 epoch total loss 0.542159796\n",
      "Trained batch 1331 batch loss 0.561938226 epoch total loss 0.542174637\n",
      "Trained batch 1332 batch loss 0.560349584 epoch total loss 0.542188287\n",
      "Trained batch 1333 batch loss 0.548522115 epoch total loss 0.542193055\n",
      "Trained batch 1334 batch loss 0.478282183 epoch total loss 0.542145133\n",
      "Trained batch 1335 batch loss 0.594289839 epoch total loss 0.542184234\n",
      "Trained batch 1336 batch loss 0.536371648 epoch total loss 0.542179883\n",
      "Trained batch 1337 batch loss 0.608383894 epoch total loss 0.542229414\n",
      "Trained batch 1338 batch loss 0.517204225 epoch total loss 0.542210698\n",
      "Trained batch 1339 batch loss 0.453310162 epoch total loss 0.542144299\n",
      "Trained batch 1340 batch loss 0.56918025 epoch total loss 0.542164445\n",
      "Trained batch 1341 batch loss 0.581837654 epoch total loss 0.542194068\n",
      "Trained batch 1342 batch loss 0.472725183 epoch total loss 0.542142272\n",
      "Trained batch 1343 batch loss 0.490688086 epoch total loss 0.542103946\n",
      "Trained batch 1344 batch loss 0.507352173 epoch total loss 0.542078078\n",
      "Trained batch 1345 batch loss 0.649403691 epoch total loss 0.542157888\n",
      "Trained batch 1346 batch loss 0.672949851 epoch total loss 0.542255044\n",
      "Trained batch 1347 batch loss 0.636252582 epoch total loss 0.542324841\n",
      "Trained batch 1348 batch loss 0.724841952 epoch total loss 0.542460203\n",
      "Trained batch 1349 batch loss 0.712729931 epoch total loss 0.542586446\n",
      "Trained batch 1350 batch loss 0.656627357 epoch total loss 0.542670906\n",
      "Trained batch 1351 batch loss 0.448389173 epoch total loss 0.542601109\n",
      "Trained batch 1352 batch loss 0.478786409 epoch total loss 0.542553902\n",
      "Trained batch 1353 batch loss 0.49271208 epoch total loss 0.542517066\n",
      "Trained batch 1354 batch loss 0.505843639 epoch total loss 0.54249\n",
      "Trained batch 1355 batch loss 0.500264168 epoch total loss 0.542458773\n",
      "Trained batch 1356 batch loss 0.491344482 epoch total loss 0.542421103\n",
      "Trained batch 1357 batch loss 0.59978348 epoch total loss 0.542463362\n",
      "Trained batch 1358 batch loss 0.542695642 epoch total loss 0.542463541\n",
      "Trained batch 1359 batch loss 0.594404757 epoch total loss 0.542501807\n",
      "Trained batch 1360 batch loss 0.633144259 epoch total loss 0.542568445\n",
      "Trained batch 1361 batch loss 0.569525361 epoch total loss 0.542588234\n",
      "Trained batch 1362 batch loss 0.57920444 epoch total loss 0.542615116\n",
      "Trained batch 1363 batch loss 0.617530286 epoch total loss 0.542670131\n",
      "Trained batch 1364 batch loss 0.51913929 epoch total loss 0.542652905\n",
      "Trained batch 1365 batch loss 0.673569202 epoch total loss 0.542748809\n",
      "Trained batch 1366 batch loss 0.662013829 epoch total loss 0.54283607\n",
      "Trained batch 1367 batch loss 0.584941089 epoch total loss 0.542866886\n",
      "Trained batch 1368 batch loss 0.650733709 epoch total loss 0.542945802\n",
      "Trained batch 1369 batch loss 0.65393883 epoch total loss 0.543026865\n",
      "Trained batch 1370 batch loss 0.629973 epoch total loss 0.543090284\n",
      "Trained batch 1371 batch loss 0.568830431 epoch total loss 0.543109059\n",
      "Trained batch 1372 batch loss 0.515891731 epoch total loss 0.543089211\n",
      "Trained batch 1373 batch loss 0.529878 epoch total loss 0.543079615\n",
      "Trained batch 1374 batch loss 0.523319 epoch total loss 0.54306525\n",
      "Trained batch 1375 batch loss 0.522246242 epoch total loss 0.543050051\n",
      "Trained batch 1376 batch loss 0.602264106 epoch total loss 0.543093085\n",
      "Trained batch 1377 batch loss 0.585633874 epoch total loss 0.54312396\n",
      "Trained batch 1378 batch loss 0.588436127 epoch total loss 0.543156862\n",
      "Trained batch 1379 batch loss 0.511214912 epoch total loss 0.543133736\n",
      "Trained batch 1380 batch loss 0.537821591 epoch total loss 0.543129861\n",
      "Trained batch 1381 batch loss 0.575075 epoch total loss 0.543153\n",
      "Trained batch 1382 batch loss 0.550204754 epoch total loss 0.543158114\n",
      "Trained batch 1383 batch loss 0.530303061 epoch total loss 0.543148816\n",
      "Trained batch 1384 batch loss 0.558802247 epoch total loss 0.543160081\n",
      "Trained batch 1385 batch loss 0.465160608 epoch total loss 0.543103755\n",
      "Trained batch 1386 batch loss 0.562015712 epoch total loss 0.543117404\n",
      "Trained batch 1387 batch loss 0.510780394 epoch total loss 0.543094099\n",
      "Trained batch 1388 batch loss 0.550023317 epoch total loss 0.543099105\n",
      "Trained batch 1389 batch loss 0.521249831 epoch total loss 0.54308337\n",
      "Trained batch 1390 batch loss 0.544819057 epoch total loss 0.543084621\n",
      "Trained batch 1391 batch loss 0.523054 epoch total loss 0.543070257\n",
      "Trained batch 1392 batch loss 0.548824608 epoch total loss 0.543074369\n",
      "Trained batch 1393 batch loss 0.531593084 epoch total loss 0.543066144\n",
      "Trained batch 1394 batch loss 0.509106398 epoch total loss 0.543041766\n",
      "Trained batch 1395 batch loss 0.494852722 epoch total loss 0.543007255\n",
      "Trained batch 1396 batch loss 0.454156637 epoch total loss 0.542943597\n",
      "Trained batch 1397 batch loss 0.435105324 epoch total loss 0.542866409\n",
      "Trained batch 1398 batch loss 0.426172346 epoch total loss 0.542782962\n",
      "Trained batch 1399 batch loss 0.433024287 epoch total loss 0.542704523\n",
      "Trained batch 1400 batch loss 0.389682859 epoch total loss 0.542595208\n",
      "Trained batch 1401 batch loss 0.565885782 epoch total loss 0.542611837\n",
      "Trained batch 1402 batch loss 0.630770326 epoch total loss 0.54267472\n",
      "Trained batch 1403 batch loss 0.544150293 epoch total loss 0.542675734\n",
      "Trained batch 1404 batch loss 0.615759492 epoch total loss 0.542727828\n",
      "Trained batch 1405 batch loss 0.574455678 epoch total loss 0.542750418\n",
      "Trained batch 1406 batch loss 0.660947561 epoch total loss 0.542834461\n",
      "Trained batch 1407 batch loss 0.638002098 epoch total loss 0.542902112\n",
      "Trained batch 1408 batch loss 0.635400891 epoch total loss 0.542967796\n",
      "Trained batch 1409 batch loss 0.663719416 epoch total loss 0.543053508\n",
      "Trained batch 1410 batch loss 0.542754114 epoch total loss 0.543053269\n",
      "Trained batch 1411 batch loss 0.586172402 epoch total loss 0.543083787\n",
      "Trained batch 1412 batch loss 0.593388736 epoch total loss 0.543119431\n",
      "Trained batch 1413 batch loss 0.544861078 epoch total loss 0.543120682\n",
      "Trained batch 1414 batch loss 0.566341937 epoch total loss 0.543137074\n",
      "Trained batch 1415 batch loss 0.463727325 epoch total loss 0.543081\n",
      "Trained batch 1416 batch loss 0.482932031 epoch total loss 0.543038487\n",
      "Trained batch 1417 batch loss 0.45091188 epoch total loss 0.542973459\n",
      "Trained batch 1418 batch loss 0.498195052 epoch total loss 0.542941868\n",
      "Trained batch 1419 batch loss 0.581981242 epoch total loss 0.542969406\n",
      "Trained batch 1420 batch loss 0.55570662 epoch total loss 0.542978406\n",
      "Trained batch 1421 batch loss 0.564982176 epoch total loss 0.542993903\n",
      "Trained batch 1422 batch loss 0.505648196 epoch total loss 0.542967618\n",
      "Trained batch 1423 batch loss 0.518403351 epoch total loss 0.542950392\n",
      "Trained batch 1424 batch loss 0.584100664 epoch total loss 0.5429793\n",
      "Trained batch 1425 batch loss 0.515630782 epoch total loss 0.542960107\n",
      "Trained batch 1426 batch loss 0.642128766 epoch total loss 0.543029666\n",
      "Trained batch 1427 batch loss 0.521259129 epoch total loss 0.543014407\n",
      "Trained batch 1428 batch loss 0.594388664 epoch total loss 0.543050349\n",
      "Trained batch 1429 batch loss 0.532083929 epoch total loss 0.54304266\n",
      "Trained batch 1430 batch loss 0.530888319 epoch total loss 0.543034196\n",
      "Trained batch 1431 batch loss 0.59120512 epoch total loss 0.543067813\n",
      "Trained batch 1432 batch loss 0.565730214 epoch total loss 0.543083668\n",
      "Trained batch 1433 batch loss 0.586257875 epoch total loss 0.543113768\n",
      "Trained batch 1434 batch loss 0.558775246 epoch total loss 0.543124676\n",
      "Trained batch 1435 batch loss 0.482623041 epoch total loss 0.543082535\n",
      "Trained batch 1436 batch loss 0.570337951 epoch total loss 0.54310149\n",
      "Trained batch 1437 batch loss 0.562117457 epoch total loss 0.543114722\n",
      "Trained batch 1438 batch loss 0.479169637 epoch total loss 0.543070257\n",
      "Trained batch 1439 batch loss 0.559648097 epoch total loss 0.54308176\n",
      "Trained batch 1440 batch loss 0.522363961 epoch total loss 0.543067396\n",
      "Trained batch 1441 batch loss 0.510212779 epoch total loss 0.543044567\n",
      "Trained batch 1442 batch loss 0.450193554 epoch total loss 0.542980194\n",
      "Trained batch 1443 batch loss 0.434253663 epoch total loss 0.542904854\n",
      "Trained batch 1444 batch loss 0.50901258 epoch total loss 0.54288137\n",
      "Trained batch 1445 batch loss 0.529015601 epoch total loss 0.542871773\n",
      "Trained batch 1446 batch loss 0.509678 epoch total loss 0.542848825\n",
      "Trained batch 1447 batch loss 0.444642723 epoch total loss 0.542780936\n",
      "Trained batch 1448 batch loss 0.487468362 epoch total loss 0.542742789\n",
      "Trained batch 1449 batch loss 0.429995507 epoch total loss 0.542664945\n",
      "Trained batch 1450 batch loss 0.532857299 epoch total loss 0.54265821\n",
      "Trained batch 1451 batch loss 0.466249049 epoch total loss 0.542605519\n",
      "Trained batch 1452 batch loss 0.436128199 epoch total loss 0.542532206\n",
      "Trained batch 1453 batch loss 0.509563 epoch total loss 0.542509556\n",
      "Trained batch 1454 batch loss 0.411280453 epoch total loss 0.542419255\n",
      "Trained batch 1455 batch loss 0.434603333 epoch total loss 0.542345166\n",
      "Trained batch 1456 batch loss 0.537679791 epoch total loss 0.542341948\n",
      "Trained batch 1457 batch loss 0.489124626 epoch total loss 0.54230547\n",
      "Trained batch 1458 batch loss 0.639534116 epoch total loss 0.542372108\n",
      "Trained batch 1459 batch loss 0.574166715 epoch total loss 0.542393923\n",
      "Trained batch 1460 batch loss 0.644740283 epoch total loss 0.542464\n",
      "Trained batch 1461 batch loss 0.52305305 epoch total loss 0.542450726\n",
      "Trained batch 1462 batch loss 0.498118758 epoch total loss 0.542420387\n",
      "Trained batch 1463 batch loss 0.554194927 epoch total loss 0.542428434\n",
      "Trained batch 1464 batch loss 0.523431778 epoch total loss 0.5424155\n",
      "Trained batch 1465 batch loss 0.536707 epoch total loss 0.542411566\n",
      "Trained batch 1466 batch loss 0.532594323 epoch total loss 0.54240489\n",
      "Trained batch 1467 batch loss 0.543142319 epoch total loss 0.542405367\n",
      "Trained batch 1468 batch loss 0.479931474 epoch total loss 0.542362809\n",
      "Trained batch 1469 batch loss 0.466131926 epoch total loss 0.542310894\n",
      "Trained batch 1470 batch loss 0.556316555 epoch total loss 0.54232043\n",
      "Trained batch 1471 batch loss 0.547735929 epoch total loss 0.542324126\n",
      "Trained batch 1472 batch loss 0.61133194 epoch total loss 0.542371\n",
      "Trained batch 1473 batch loss 0.534861326 epoch total loss 0.542365909\n",
      "Trained batch 1474 batch loss 0.525404 epoch total loss 0.542354405\n",
      "Trained batch 1475 batch loss 0.588649094 epoch total loss 0.542385757\n",
      "Trained batch 1476 batch loss 0.512403131 epoch total loss 0.542365432\n",
      "Trained batch 1477 batch loss 0.594023526 epoch total loss 0.54240036\n",
      "Trained batch 1478 batch loss 0.616422534 epoch total loss 0.542450428\n",
      "Trained batch 1479 batch loss 0.585594296 epoch total loss 0.542479575\n",
      "Trained batch 1480 batch loss 0.505263567 epoch total loss 0.542454422\n",
      "Trained batch 1481 batch loss 0.508903086 epoch total loss 0.542431772\n",
      "Trained batch 1482 batch loss 0.54561162 epoch total loss 0.542433918\n",
      "Trained batch 1483 batch loss 0.559242129 epoch total loss 0.542445302\n",
      "Trained batch 1484 batch loss 0.59973222 epoch total loss 0.542483866\n",
      "Trained batch 1485 batch loss 0.594906271 epoch total loss 0.542519212\n",
      "Trained batch 1486 batch loss 0.514547765 epoch total loss 0.542500377\n",
      "Trained batch 1487 batch loss 0.541822195 epoch total loss 0.5424999\n",
      "Trained batch 1488 batch loss 0.532272041 epoch total loss 0.542493045\n",
      "Trained batch 1489 batch loss 0.51139164 epoch total loss 0.542472124\n",
      "Trained batch 1490 batch loss 0.557897091 epoch total loss 0.542482495\n",
      "Trained batch 1491 batch loss 0.547952 epoch total loss 0.542486191\n",
      "Trained batch 1492 batch loss 0.562593222 epoch total loss 0.542499721\n",
      "Trained batch 1493 batch loss 0.552365541 epoch total loss 0.542506278\n",
      "Trained batch 1494 batch loss 0.515927 epoch total loss 0.542488515\n",
      "Trained batch 1495 batch loss 0.467054129 epoch total loss 0.54243803\n",
      "Trained batch 1496 batch loss 0.435322374 epoch total loss 0.542366445\n",
      "Trained batch 1497 batch loss 0.513263524 epoch total loss 0.542346954\n",
      "Trained batch 1498 batch loss 0.479062915 epoch total loss 0.542304754\n",
      "Trained batch 1499 batch loss 0.467075497 epoch total loss 0.542254567\n",
      "Trained batch 1500 batch loss 0.586010098 epoch total loss 0.542283714\n",
      "Trained batch 1501 batch loss 0.660333514 epoch total loss 0.542362392\n",
      "Trained batch 1502 batch loss 0.618047893 epoch total loss 0.542412758\n",
      "Trained batch 1503 batch loss 0.57025212 epoch total loss 0.542431295\n",
      "Trained batch 1504 batch loss 0.704483867 epoch total loss 0.542539\n",
      "Trained batch 1505 batch loss 0.565068722 epoch total loss 0.542554\n",
      "Trained batch 1506 batch loss 0.599414 epoch total loss 0.542591751\n",
      "Trained batch 1507 batch loss 0.586662889 epoch total loss 0.542621\n",
      "Trained batch 1508 batch loss 0.578226686 epoch total loss 0.54264462\n",
      "Trained batch 1509 batch loss 0.632513225 epoch total loss 0.542704165\n",
      "Trained batch 1510 batch loss 0.518555105 epoch total loss 0.542688191\n",
      "Trained batch 1511 batch loss 0.443658054 epoch total loss 0.542622626\n",
      "Trained batch 1512 batch loss 0.440646 epoch total loss 0.542555213\n",
      "Trained batch 1513 batch loss 0.441142708 epoch total loss 0.542488217\n",
      "Trained batch 1514 batch loss 0.403717399 epoch total loss 0.542396605\n",
      "Trained batch 1515 batch loss 0.406884342 epoch total loss 0.542307138\n",
      "Trained batch 1516 batch loss 0.414196253 epoch total loss 0.542222619\n",
      "Trained batch 1517 batch loss 0.465699673 epoch total loss 0.542172134\n",
      "Trained batch 1518 batch loss 0.499732 epoch total loss 0.542144239\n",
      "Trained batch 1519 batch loss 0.655382812 epoch total loss 0.542218745\n",
      "Trained batch 1520 batch loss 0.658728361 epoch total loss 0.542295456\n",
      "Trained batch 1521 batch loss 0.671700954 epoch total loss 0.542380512\n",
      "Trained batch 1522 batch loss 0.53792721 epoch total loss 0.542377591\n",
      "Trained batch 1523 batch loss 0.54685 epoch total loss 0.542380512\n",
      "Trained batch 1524 batch loss 0.651527822 epoch total loss 0.542452157\n",
      "Trained batch 1525 batch loss 0.501268506 epoch total loss 0.542425156\n",
      "Trained batch 1526 batch loss 0.479099572 epoch total loss 0.542383671\n",
      "Trained batch 1527 batch loss 0.55055964 epoch total loss 0.542389035\n",
      "Trained batch 1528 batch loss 0.553407133 epoch total loss 0.542396247\n",
      "Trained batch 1529 batch loss 0.573226571 epoch total loss 0.542416394\n",
      "Trained batch 1530 batch loss 0.562510669 epoch total loss 0.542429507\n",
      "Trained batch 1531 batch loss 0.612492681 epoch total loss 0.542475283\n",
      "Trained batch 1532 batch loss 0.505387247 epoch total loss 0.542451084\n",
      "Trained batch 1533 batch loss 0.483650506 epoch total loss 0.542412698\n",
      "Trained batch 1534 batch loss 0.498717189 epoch total loss 0.542384207\n",
      "Trained batch 1535 batch loss 0.449728429 epoch total loss 0.542323828\n",
      "Trained batch 1536 batch loss 0.442819417 epoch total loss 0.542259037\n",
      "Trained batch 1537 batch loss 0.460755795 epoch total loss 0.542206049\n",
      "Trained batch 1538 batch loss 0.485539913 epoch total loss 0.542169154\n",
      "Trained batch 1539 batch loss 0.554855943 epoch total loss 0.542177439\n",
      "Trained batch 1540 batch loss 0.540330708 epoch total loss 0.542176247\n",
      "Trained batch 1541 batch loss 0.540025294 epoch total loss 0.542174876\n",
      "Trained batch 1542 batch loss 0.418005586 epoch total loss 0.54209435\n",
      "Trained batch 1543 batch loss 0.499584377 epoch total loss 0.542066813\n",
      "Trained batch 1544 batch loss 0.485404491 epoch total loss 0.542030096\n",
      "Trained batch 1545 batch loss 0.529709458 epoch total loss 0.542022109\n",
      "Trained batch 1546 batch loss 0.524451 epoch total loss 0.542010784\n",
      "Trained batch 1547 batch loss 0.540653527 epoch total loss 0.54200989\n",
      "Trained batch 1548 batch loss 0.581380069 epoch total loss 0.542035341\n",
      "Trained batch 1549 batch loss 0.547811151 epoch total loss 0.542039037\n",
      "Trained batch 1550 batch loss 0.618669271 epoch total loss 0.542088449\n",
      "Trained batch 1551 batch loss 0.502117336 epoch total loss 0.5420627\n",
      "Trained batch 1552 batch loss 0.520992041 epoch total loss 0.54204911\n",
      "Trained batch 1553 batch loss 0.488673091 epoch total loss 0.542014718\n",
      "Trained batch 1554 batch loss 0.488030523 epoch total loss 0.54198\n",
      "Trained batch 1555 batch loss 0.569847 epoch total loss 0.54199791\n",
      "Trained batch 1556 batch loss 0.555542648 epoch total loss 0.542006612\n",
      "Trained batch 1557 batch loss 0.613227487 epoch total loss 0.542052388\n",
      "Trained batch 1558 batch loss 0.629480839 epoch total loss 0.542108476\n",
      "Trained batch 1559 batch loss 0.584686041 epoch total loss 0.542135775\n",
      "Trained batch 1560 batch loss 0.551315427 epoch total loss 0.542141616\n",
      "Trained batch 1561 batch loss 0.540529072 epoch total loss 0.542140603\n",
      "Trained batch 1562 batch loss 0.59538132 epoch total loss 0.542174697\n",
      "Trained batch 1563 batch loss 0.571359515 epoch total loss 0.542193353\n",
      "Trained batch 1564 batch loss 0.52289 epoch total loss 0.542181\n",
      "Trained batch 1565 batch loss 0.525886059 epoch total loss 0.542170584\n",
      "Trained batch 1566 batch loss 0.520780385 epoch total loss 0.542156935\n",
      "Trained batch 1567 batch loss 0.535307825 epoch total loss 0.542152524\n",
      "Trained batch 1568 batch loss 0.606595755 epoch total loss 0.542193651\n",
      "Trained batch 1569 batch loss 0.622258186 epoch total loss 0.542244673\n",
      "Trained batch 1570 batch loss 0.573675036 epoch total loss 0.54226464\n",
      "Trained batch 1571 batch loss 0.470416218 epoch total loss 0.542218924\n",
      "Trained batch 1572 batch loss 0.431958616 epoch total loss 0.542148769\n",
      "Trained batch 1573 batch loss 0.535603106 epoch total loss 0.542144597\n",
      "Trained batch 1574 batch loss 0.529878914 epoch total loss 0.542136848\n",
      "Trained batch 1575 batch loss 0.528686285 epoch total loss 0.542128265\n",
      "Trained batch 1576 batch loss 0.553751826 epoch total loss 0.542135656\n",
      "Trained batch 1577 batch loss 0.517120361 epoch total loss 0.542119861\n",
      "Trained batch 1578 batch loss 0.462567866 epoch total loss 0.542069435\n",
      "Trained batch 1579 batch loss 0.436391026 epoch total loss 0.542002499\n",
      "Trained batch 1580 batch loss 0.426339209 epoch total loss 0.541929305\n",
      "Trained batch 1581 batch loss 0.426653177 epoch total loss 0.541856349\n",
      "Trained batch 1582 batch loss 0.42017 epoch total loss 0.541779459\n",
      "Trained batch 1583 batch loss 0.439238071 epoch total loss 0.541714668\n",
      "Trained batch 1584 batch loss 0.463695526 epoch total loss 0.541665375\n",
      "Trained batch 1585 batch loss 0.482671022 epoch total loss 0.541628182\n",
      "Trained batch 1586 batch loss 0.57283777 epoch total loss 0.541647851\n",
      "Trained batch 1587 batch loss 0.451738954 epoch total loss 0.541591167\n",
      "Trained batch 1588 batch loss 0.54147613 epoch total loss 0.541591108\n",
      "Trained batch 1589 batch loss 0.453209043 epoch total loss 0.541535497\n",
      "Trained batch 1590 batch loss 0.441147625 epoch total loss 0.541472375\n",
      "Trained batch 1591 batch loss 0.489597499 epoch total loss 0.541439772\n",
      "Trained batch 1592 batch loss 0.464588046 epoch total loss 0.541391492\n",
      "Trained batch 1593 batch loss 0.522307098 epoch total loss 0.541379511\n",
      "Trained batch 1594 batch loss 0.426801443 epoch total loss 0.541307628\n",
      "Trained batch 1595 batch loss 0.497210622 epoch total loss 0.54128\n",
      "Trained batch 1596 batch loss 0.567828357 epoch total loss 0.541296601\n",
      "Trained batch 1597 batch loss 0.574323118 epoch total loss 0.541317284\n",
      "Trained batch 1598 batch loss 0.576711416 epoch total loss 0.541339457\n",
      "Trained batch 1599 batch loss 0.613612056 epoch total loss 0.541384637\n",
      "Trained batch 1600 batch loss 0.623844087 epoch total loss 0.541436136\n",
      "Trained batch 1601 batch loss 0.540095925 epoch total loss 0.541435301\n",
      "Trained batch 1602 batch loss 0.550173938 epoch total loss 0.541440785\n",
      "Trained batch 1603 batch loss 0.572558045 epoch total loss 0.541460216\n",
      "Trained batch 1604 batch loss 0.602321744 epoch total loss 0.541498125\n",
      "Trained batch 1605 batch loss 0.536005139 epoch total loss 0.541494727\n",
      "Trained batch 1606 batch loss 0.59155941 epoch total loss 0.5415259\n",
      "Trained batch 1607 batch loss 0.444924533 epoch total loss 0.541465759\n",
      "Trained batch 1608 batch loss 0.469821453 epoch total loss 0.541421235\n",
      "Trained batch 1609 batch loss 0.55517453 epoch total loss 0.541429758\n",
      "Trained batch 1610 batch loss 0.546000421 epoch total loss 0.541432619\n",
      "Trained batch 1611 batch loss 0.583871424 epoch total loss 0.541458964\n",
      "Trained batch 1612 batch loss 0.512928545 epoch total loss 0.541441262\n",
      "Trained batch 1613 batch loss 0.53864 epoch total loss 0.541439533\n",
      "Trained batch 1614 batch loss 0.539141893 epoch total loss 0.541438103\n",
      "Trained batch 1615 batch loss 0.51875174 epoch total loss 0.541424036\n",
      "Trained batch 1616 batch loss 0.587537825 epoch total loss 0.541452587\n",
      "Trained batch 1617 batch loss 0.542867899 epoch total loss 0.541453421\n",
      "Trained batch 1618 batch loss 0.566942275 epoch total loss 0.541469216\n",
      "Trained batch 1619 batch loss 0.488571465 epoch total loss 0.541436553\n",
      "Trained batch 1620 batch loss 0.50583452 epoch total loss 0.541414559\n",
      "Trained batch 1621 batch loss 0.515500367 epoch total loss 0.541398585\n",
      "Trained batch 1622 batch loss 0.650307477 epoch total loss 0.541465759\n",
      "Trained batch 1623 batch loss 0.492219359 epoch total loss 0.541435421\n",
      "Trained batch 1624 batch loss 0.53317678 epoch total loss 0.541430354\n",
      "Trained batch 1625 batch loss 0.600235879 epoch total loss 0.541466534\n",
      "Trained batch 1626 batch loss 0.590749741 epoch total loss 0.541496873\n",
      "Trained batch 1627 batch loss 0.518764317 epoch total loss 0.541482866\n",
      "Trained batch 1628 batch loss 0.513691902 epoch total loss 0.541465759\n",
      "Trained batch 1629 batch loss 0.576001465 epoch total loss 0.541487\n",
      "Trained batch 1630 batch loss 0.500869215 epoch total loss 0.541462064\n",
      "Trained batch 1631 batch loss 0.493837297 epoch total loss 0.541432858\n",
      "Trained batch 1632 batch loss 0.444948494 epoch total loss 0.54137373\n",
      "Trained batch 1633 batch loss 0.560951531 epoch total loss 0.54138571\n",
      "Trained batch 1634 batch loss 0.579417944 epoch total loss 0.541409\n",
      "Trained batch 1635 batch loss 0.5560745 epoch total loss 0.541417956\n",
      "Trained batch 1636 batch loss 0.615274191 epoch total loss 0.541463137\n",
      "Trained batch 1637 batch loss 0.579606056 epoch total loss 0.541486442\n",
      "Trained batch 1638 batch loss 0.637863517 epoch total loss 0.541545272\n",
      "Trained batch 1639 batch loss 0.623853624 epoch total loss 0.541595459\n",
      "Trained batch 1640 batch loss 0.576887548 epoch total loss 0.541617036\n",
      "Trained batch 1641 batch loss 0.638672769 epoch total loss 0.541676164\n",
      "Trained batch 1642 batch loss 0.64227581 epoch total loss 0.541737437\n",
      "Trained batch 1643 batch loss 0.605629921 epoch total loss 0.541776299\n",
      "Trained batch 1644 batch loss 0.571917176 epoch total loss 0.541794658\n",
      "Trained batch 1645 batch loss 0.604260445 epoch total loss 0.541832626\n",
      "Trained batch 1646 batch loss 0.537826717 epoch total loss 0.541830182\n",
      "Trained batch 1647 batch loss 0.47864157 epoch total loss 0.541791797\n",
      "Trained batch 1648 batch loss 0.506918907 epoch total loss 0.541770637\n",
      "Trained batch 1649 batch loss 0.531389177 epoch total loss 0.541764319\n",
      "Trained batch 1650 batch loss 0.430572242 epoch total loss 0.541696906\n",
      "Trained batch 1651 batch loss 0.53868711 epoch total loss 0.541695118\n",
      "Trained batch 1652 batch loss 0.569925904 epoch total loss 0.541712224\n",
      "Trained batch 1653 batch loss 0.488407314 epoch total loss 0.54168\n",
      "Trained batch 1654 batch loss 0.573950887 epoch total loss 0.541699469\n",
      "Trained batch 1655 batch loss 0.5434829 epoch total loss 0.541700542\n",
      "Trained batch 1656 batch loss 0.448504269 epoch total loss 0.541644275\n",
      "Trained batch 1657 batch loss 0.513864458 epoch total loss 0.541627467\n",
      "Trained batch 1658 batch loss 0.549302936 epoch total loss 0.541632116\n",
      "Trained batch 1659 batch loss 0.443627656 epoch total loss 0.541573048\n",
      "Trained batch 1660 batch loss 0.624710679 epoch total loss 0.541623116\n",
      "Trained batch 1661 batch loss 0.495490134 epoch total loss 0.54159534\n",
      "Trained batch 1662 batch loss 0.531952739 epoch total loss 0.541589558\n",
      "Trained batch 1663 batch loss 0.468543112 epoch total loss 0.54154563\n",
      "Trained batch 1664 batch loss 0.4853248 epoch total loss 0.541511893\n",
      "Trained batch 1665 batch loss 0.49111554 epoch total loss 0.541481555\n",
      "Trained batch 1666 batch loss 0.590550482 epoch total loss 0.541511059\n",
      "Trained batch 1667 batch loss 0.525181115 epoch total loss 0.541501284\n",
      "Trained batch 1668 batch loss 0.460913748 epoch total loss 0.541452944\n",
      "Trained batch 1669 batch loss 0.474871725 epoch total loss 0.541413069\n",
      "Trained batch 1670 batch loss 0.5172683 epoch total loss 0.541398585\n",
      "Trained batch 1671 batch loss 0.482935339 epoch total loss 0.541363597\n",
      "Trained batch 1672 batch loss 0.528543413 epoch total loss 0.541355968\n",
      "Trained batch 1673 batch loss 0.49431631 epoch total loss 0.541327834\n",
      "Trained batch 1674 batch loss 0.462877572 epoch total loss 0.541281\n",
      "Trained batch 1675 batch loss 0.507875919 epoch total loss 0.541261\n",
      "Trained batch 1676 batch loss 0.542147279 epoch total loss 0.541261613\n",
      "Trained batch 1677 batch loss 0.58233583 epoch total loss 0.541286051\n",
      "Trained batch 1678 batch loss 0.572749138 epoch total loss 0.541304827\n",
      "Trained batch 1679 batch loss 0.579093933 epoch total loss 0.541327357\n",
      "Trained batch 1680 batch loss 0.528346658 epoch total loss 0.541319609\n",
      "Trained batch 1681 batch loss 0.549675107 epoch total loss 0.541324556\n",
      "Trained batch 1682 batch loss 0.548514128 epoch total loss 0.541328847\n",
      "Trained batch 1683 batch loss 0.653805 epoch total loss 0.541395664\n",
      "Trained batch 1684 batch loss 0.625106215 epoch total loss 0.541445434\n",
      "Trained batch 1685 batch loss 0.521216273 epoch total loss 0.541433394\n",
      "Trained batch 1686 batch loss 0.57357347 epoch total loss 0.541452467\n",
      "Trained batch 1687 batch loss 0.6514709 epoch total loss 0.541517675\n",
      "Trained batch 1688 batch loss 0.561169624 epoch total loss 0.541529298\n",
      "Trained batch 1689 batch loss 0.60433054 epoch total loss 0.541566491\n",
      "Trained batch 1690 batch loss 0.591908455 epoch total loss 0.541596293\n",
      "Trained batch 1691 batch loss 0.525923789 epoch total loss 0.541587055\n",
      "Trained batch 1692 batch loss 0.566880465 epoch total loss 0.541602\n",
      "Trained batch 1693 batch loss 0.60336411 epoch total loss 0.541638494\n",
      "Trained batch 1694 batch loss 0.549371481 epoch total loss 0.541643083\n",
      "Trained batch 1695 batch loss 0.548690557 epoch total loss 0.541647196\n",
      "Trained batch 1696 batch loss 0.52220571 epoch total loss 0.541635752\n",
      "Trained batch 1697 batch loss 0.568993747 epoch total loss 0.541651845\n",
      "Trained batch 1698 batch loss 0.605542839 epoch total loss 0.541689515\n",
      "Trained batch 1699 batch loss 0.550969064 epoch total loss 0.541694939\n",
      "Trained batch 1700 batch loss 0.572989404 epoch total loss 0.541713357\n",
      "Trained batch 1701 batch loss 0.556159 epoch total loss 0.54172188\n",
      "Trained batch 1702 batch loss 0.553032458 epoch total loss 0.541728497\n",
      "Trained batch 1703 batch loss 0.607452929 epoch total loss 0.54176712\n",
      "Trained batch 1704 batch loss 0.519339561 epoch total loss 0.541753948\n",
      "Trained batch 1705 batch loss 0.514042437 epoch total loss 0.541737676\n",
      "Trained batch 1706 batch loss 0.457783073 epoch total loss 0.541688502\n",
      "Trained batch 1707 batch loss 0.563874543 epoch total loss 0.541701496\n",
      "Trained batch 1708 batch loss 0.483820081 epoch total loss 0.541667581\n",
      "Trained batch 1709 batch loss 0.423088521 epoch total loss 0.541598201\n",
      "Trained batch 1710 batch loss 0.444810748 epoch total loss 0.541541636\n",
      "Trained batch 1711 batch loss 0.464247912 epoch total loss 0.541496456\n",
      "Trained batch 1712 batch loss 0.417857766 epoch total loss 0.541424215\n",
      "Trained batch 1713 batch loss 0.460877359 epoch total loss 0.541377187\n",
      "Trained batch 1714 batch loss 0.518736064 epoch total loss 0.541364\n",
      "Trained batch 1715 batch loss 0.505898476 epoch total loss 0.541343331\n",
      "Trained batch 1716 batch loss 0.475376219 epoch total loss 0.541304886\n",
      "Trained batch 1717 batch loss 0.551723421 epoch total loss 0.541310966\n",
      "Trained batch 1718 batch loss 0.595134258 epoch total loss 0.541342318\n",
      "Trained batch 1719 batch loss 0.475612074 epoch total loss 0.541304052\n",
      "Trained batch 1720 batch loss 0.449528486 epoch total loss 0.541250706\n",
      "Trained batch 1721 batch loss 0.578108907 epoch total loss 0.541272104\n",
      "Trained batch 1722 batch loss 0.479525983 epoch total loss 0.541236281\n",
      "Trained batch 1723 batch loss 0.522821188 epoch total loss 0.541225553\n",
      "Trained batch 1724 batch loss 0.560149 epoch total loss 0.54123652\n",
      "Trained batch 1725 batch loss 0.428737581 epoch total loss 0.541171312\n",
      "Trained batch 1726 batch loss 0.492402375 epoch total loss 0.54114306\n",
      "Trained batch 1727 batch loss 0.52787292 epoch total loss 0.541135371\n",
      "Trained batch 1728 batch loss 0.534205914 epoch total loss 0.541131377\n",
      "Trained batch 1729 batch loss 0.596846223 epoch total loss 0.541163623\n",
      "Trained batch 1730 batch loss 0.563918352 epoch total loss 0.541176736\n",
      "Trained batch 1731 batch loss 0.561259925 epoch total loss 0.541188359\n",
      "Trained batch 1732 batch loss 0.566355109 epoch total loss 0.541202903\n",
      "Trained batch 1733 batch loss 0.519308031 epoch total loss 0.541190267\n",
      "Trained batch 1734 batch loss 0.54471451 epoch total loss 0.541192293\n",
      "Trained batch 1735 batch loss 0.547325969 epoch total loss 0.54119581\n",
      "Trained batch 1736 batch loss 0.381450117 epoch total loss 0.54110378\n",
      "Trained batch 1737 batch loss 0.458656043 epoch total loss 0.541056335\n",
      "Trained batch 1738 batch loss 0.51586771 epoch total loss 0.541041851\n",
      "Trained batch 1739 batch loss 0.466982484 epoch total loss 0.540999293\n",
      "Trained batch 1740 batch loss 0.626186728 epoch total loss 0.541048229\n",
      "Trained batch 1741 batch loss 0.610550642 epoch total loss 0.541088104\n",
      "Trained batch 1742 batch loss 0.524210751 epoch total loss 0.541078448\n",
      "Trained batch 1743 batch loss 0.594254851 epoch total loss 0.541108966\n",
      "Trained batch 1744 batch loss 0.500457287 epoch total loss 0.541085601\n",
      "Trained batch 1745 batch loss 0.558481276 epoch total loss 0.541095555\n",
      "Trained batch 1746 batch loss 0.569282 epoch total loss 0.541111708\n",
      "Trained batch 1747 batch loss 0.579852521 epoch total loss 0.541133881\n",
      "Trained batch 1748 batch loss 0.566298842 epoch total loss 0.541148305\n",
      "Trained batch 1749 batch loss 0.526986599 epoch total loss 0.541140199\n",
      "Trained batch 1750 batch loss 0.537563324 epoch total loss 0.541138113\n",
      "Trained batch 1751 batch loss 0.458194286 epoch total loss 0.541090727\n",
      "Trained batch 1752 batch loss 0.436026305 epoch total loss 0.541030765\n",
      "Trained batch 1753 batch loss 0.479462087 epoch total loss 0.540995657\n",
      "Trained batch 1754 batch loss 0.458721042 epoch total loss 0.540948808\n",
      "Trained batch 1755 batch loss 0.509116292 epoch total loss 0.540930629\n",
      "Trained batch 1756 batch loss 0.502982199 epoch total loss 0.540909\n",
      "Trained batch 1757 batch loss 0.558919251 epoch total loss 0.540919244\n",
      "Trained batch 1758 batch loss 0.529957414 epoch total loss 0.540913045\n",
      "Trained batch 1759 batch loss 0.572390854 epoch total loss 0.540930927\n",
      "Trained batch 1760 batch loss 0.538597167 epoch total loss 0.540929615\n",
      "Trained batch 1761 batch loss 0.62571913 epoch total loss 0.540977716\n",
      "Trained batch 1762 batch loss 0.608447 epoch total loss 0.541016042\n",
      "Trained batch 1763 batch loss 0.670050621 epoch total loss 0.541089237\n",
      "Trained batch 1764 batch loss 0.564652383 epoch total loss 0.541102588\n",
      "Trained batch 1765 batch loss 0.529329717 epoch total loss 0.541095912\n",
      "Trained batch 1766 batch loss 0.608440518 epoch total loss 0.541134059\n",
      "Trained batch 1767 batch loss 0.583385348 epoch total loss 0.541157961\n",
      "Trained batch 1768 batch loss 0.604737163 epoch total loss 0.541193902\n",
      "Trained batch 1769 batch loss 0.607667506 epoch total loss 0.541231513\n",
      "Trained batch 1770 batch loss 0.597575366 epoch total loss 0.541263342\n",
      "Trained batch 1771 batch loss 0.589209855 epoch total loss 0.541290462\n",
      "Trained batch 1772 batch loss 0.604824245 epoch total loss 0.541326284\n",
      "Trained batch 1773 batch loss 0.556643546 epoch total loss 0.541334927\n",
      "Trained batch 1774 batch loss 0.59847796 epoch total loss 0.541367114\n",
      "Trained batch 1775 batch loss 0.672816575 epoch total loss 0.541441143\n",
      "Trained batch 1776 batch loss 0.598631382 epoch total loss 0.541473329\n",
      "Trained batch 1777 batch loss 0.521328807 epoch total loss 0.541462\n",
      "Trained batch 1778 batch loss 0.528503 epoch total loss 0.541454732\n",
      "Trained batch 1779 batch loss 0.505942941 epoch total loss 0.541434765\n",
      "Trained batch 1780 batch loss 0.515806079 epoch total loss 0.541420341\n",
      "Trained batch 1781 batch loss 0.512340128 epoch total loss 0.541404\n",
      "Trained batch 1782 batch loss 0.495477885 epoch total loss 0.54137826\n",
      "Trained batch 1783 batch loss 0.478838503 epoch total loss 0.541343153\n",
      "Trained batch 1784 batch loss 0.515198946 epoch total loss 0.54132849\n",
      "Trained batch 1785 batch loss 0.553137541 epoch total loss 0.541335106\n",
      "Trained batch 1786 batch loss 0.489200175 epoch total loss 0.541305959\n",
      "Trained batch 1787 batch loss 0.512823224 epoch total loss 0.54129\n",
      "Trained batch 1788 batch loss 0.58052808 epoch total loss 0.54131192\n",
      "Trained batch 1789 batch loss 0.577108622 epoch total loss 0.541331947\n",
      "Trained batch 1790 batch loss 0.58498472 epoch total loss 0.541356325\n",
      "Trained batch 1791 batch loss 0.545318 epoch total loss 0.541358471\n",
      "Trained batch 1792 batch loss 0.650668621 epoch total loss 0.541419506\n",
      "Trained batch 1793 batch loss 0.590106785 epoch total loss 0.541446626\n",
      "Trained batch 1794 batch loss 0.668110728 epoch total loss 0.541517258\n",
      "Trained batch 1795 batch loss 0.643914104 epoch total loss 0.541574299\n",
      "Trained batch 1796 batch loss 0.62749964 epoch total loss 0.541622162\n",
      "Trained batch 1797 batch loss 0.592200279 epoch total loss 0.541650295\n",
      "Trained batch 1798 batch loss 0.685338497 epoch total loss 0.541730225\n",
      "Trained batch 1799 batch loss 0.570227504 epoch total loss 0.54174608\n",
      "Trained batch 1800 batch loss 0.535057127 epoch total loss 0.541742325\n",
      "Trained batch 1801 batch loss 0.556288064 epoch total loss 0.541750431\n",
      "Trained batch 1802 batch loss 0.498225868 epoch total loss 0.541726291\n",
      "Trained batch 1803 batch loss 0.456985176 epoch total loss 0.541679263\n",
      "Trained batch 1804 batch loss 0.564617097 epoch total loss 0.541691959\n",
      "Trained batch 1805 batch loss 0.667906046 epoch total loss 0.541761935\n",
      "Trained batch 1806 batch loss 0.54303062 epoch total loss 0.54176259\n",
      "Trained batch 1807 batch loss 0.666561127 epoch total loss 0.541831672\n",
      "Trained batch 1808 batch loss 0.449185073 epoch total loss 0.541780412\n",
      "Trained batch 1809 batch loss 0.530499578 epoch total loss 0.541774213\n",
      "Trained batch 1810 batch loss 0.576496422 epoch total loss 0.541793346\n",
      "Trained batch 1811 batch loss 0.508027673 epoch total loss 0.54177475\n",
      "Trained batch 1812 batch loss 0.473537207 epoch total loss 0.54173708\n",
      "Trained batch 1813 batch loss 0.532611072 epoch total loss 0.541732\n",
      "Trained batch 1814 batch loss 0.560177386 epoch total loss 0.541742206\n",
      "Trained batch 1815 batch loss 0.507204592 epoch total loss 0.541723192\n",
      "Trained batch 1816 batch loss 0.458494931 epoch total loss 0.541677356\n",
      "Trained batch 1817 batch loss 0.443125337 epoch total loss 0.541623116\n",
      "Trained batch 1818 batch loss 0.556988478 epoch total loss 0.541631579\n",
      "Trained batch 1819 batch loss 0.580580711 epoch total loss 0.541653\n",
      "Trained batch 1820 batch loss 0.510593176 epoch total loss 0.541635931\n",
      "Trained batch 1821 batch loss 0.500259519 epoch total loss 0.541613162\n",
      "Trained batch 1822 batch loss 0.538002253 epoch total loss 0.541611195\n",
      "Trained batch 1823 batch loss 0.515627623 epoch total loss 0.541596949\n",
      "Trained batch 1824 batch loss 0.575050235 epoch total loss 0.541615307\n",
      "Trained batch 1825 batch loss 0.506531417 epoch total loss 0.541596055\n",
      "Trained batch 1826 batch loss 0.464528 epoch total loss 0.541553855\n",
      "Trained batch 1827 batch loss 0.508263528 epoch total loss 0.541535616\n",
      "Trained batch 1828 batch loss 0.45642513 epoch total loss 0.541489065\n",
      "Trained batch 1829 batch loss 0.431173116 epoch total loss 0.541428745\n",
      "Trained batch 1830 batch loss 0.519690335 epoch total loss 0.541416883\n",
      "Trained batch 1831 batch loss 0.488984764 epoch total loss 0.541388273\n",
      "Trained batch 1832 batch loss 0.563642502 epoch total loss 0.541400433\n",
      "Trained batch 1833 batch loss 0.552596569 epoch total loss 0.541406572\n",
      "Trained batch 1834 batch loss 0.542773426 epoch total loss 0.541407287\n",
      "Trained batch 1835 batch loss 0.564180374 epoch total loss 0.541419744\n",
      "Trained batch 1836 batch loss 0.463543236 epoch total loss 0.541377306\n",
      "Trained batch 1837 batch loss 0.568448067 epoch total loss 0.541392\n",
      "Trained batch 1838 batch loss 0.586690068 epoch total loss 0.541416645\n",
      "Trained batch 1839 batch loss 0.597227871 epoch total loss 0.541447043\n",
      "Trained batch 1840 batch loss 0.601626754 epoch total loss 0.541479707\n",
      "Trained batch 1841 batch loss 0.577318847 epoch total loss 0.541499197\n",
      "Trained batch 1842 batch loss 0.564909101 epoch total loss 0.541511893\n",
      "Trained batch 1843 batch loss 0.564506769 epoch total loss 0.541524351\n",
      "Trained batch 1844 batch loss 0.489299297 epoch total loss 0.541496038\n",
      "Trained batch 1845 batch loss 0.535200775 epoch total loss 0.541492641\n",
      "Trained batch 1846 batch loss 0.491514176 epoch total loss 0.54146558\n",
      "Trained batch 1847 batch loss 0.542591453 epoch total loss 0.541466177\n",
      "Trained batch 1848 batch loss 0.515195787 epoch total loss 0.541452\n",
      "Trained batch 1849 batch loss 0.584544182 epoch total loss 0.541475296\n",
      "Trained batch 1850 batch loss 0.536746562 epoch total loss 0.541472733\n",
      "Trained batch 1851 batch loss 0.493740499 epoch total loss 0.541446924\n",
      "Trained batch 1852 batch loss 0.452830225 epoch total loss 0.541399062\n",
      "Trained batch 1853 batch loss 0.425878555 epoch total loss 0.541336715\n",
      "Trained batch 1854 batch loss 0.39651 epoch total loss 0.541258633\n",
      "Trained batch 1855 batch loss 0.348674357 epoch total loss 0.541154802\n",
      "Trained batch 1856 batch loss 0.513566434 epoch total loss 0.54113996\n",
      "Trained batch 1857 batch loss 0.550535 epoch total loss 0.541144967\n",
      "Trained batch 1858 batch loss 0.578761935 epoch total loss 0.541165233\n",
      "Trained batch 1859 batch loss 0.61216408 epoch total loss 0.541203439\n",
      "Trained batch 1860 batch loss 0.57364428 epoch total loss 0.541220903\n",
      "Trained batch 1861 batch loss 0.600153804 epoch total loss 0.541252553\n",
      "Trained batch 1862 batch loss 0.688741386 epoch total loss 0.541331768\n",
      "Trained batch 1863 batch loss 0.640993774 epoch total loss 0.541385233\n",
      "Trained batch 1864 batch loss 0.581411898 epoch total loss 0.541406751\n",
      "Trained batch 1865 batch loss 0.536048353 epoch total loss 0.54140389\n",
      "Trained batch 1866 batch loss 0.572960675 epoch total loss 0.541420758\n",
      "Trained batch 1867 batch loss 0.484919071 epoch total loss 0.541390479\n",
      "Trained batch 1868 batch loss 0.627450228 epoch total loss 0.541436553\n",
      "Trained batch 1869 batch loss 0.733976841 epoch total loss 0.54153955\n",
      "Trained batch 1870 batch loss 0.721786618 epoch total loss 0.541636\n",
      "Trained batch 1871 batch loss 0.66601938 epoch total loss 0.541702449\n",
      "Trained batch 1872 batch loss 0.602502286 epoch total loss 0.541734934\n",
      "Trained batch 1873 batch loss 0.621495843 epoch total loss 0.541777492\n",
      "Trained batch 1874 batch loss 0.659212053 epoch total loss 0.541840196\n",
      "Trained batch 1875 batch loss 0.64782548 epoch total loss 0.541896701\n",
      "Trained batch 1876 batch loss 0.572854638 epoch total loss 0.541913211\n",
      "Trained batch 1877 batch loss 0.593794644 epoch total loss 0.541940868\n",
      "Trained batch 1878 batch loss 0.642884612 epoch total loss 0.541994631\n",
      "Trained batch 1879 batch loss 0.589822233 epoch total loss 0.542020082\n",
      "Trained batch 1880 batch loss 0.530602455 epoch total loss 0.542014\n",
      "Trained batch 1881 batch loss 0.574706674 epoch total loss 0.542031407\n",
      "Trained batch 1882 batch loss 0.594199598 epoch total loss 0.542059064\n",
      "Trained batch 1883 batch loss 0.637431622 epoch total loss 0.542109728\n",
      "Trained batch 1884 batch loss 0.59170568 epoch total loss 0.542136073\n",
      "Trained batch 1885 batch loss 0.52778244 epoch total loss 0.542128444\n",
      "Trained batch 1886 batch loss 0.655743182 epoch total loss 0.542188704\n",
      "Trained batch 1887 batch loss 0.552997112 epoch total loss 0.542194426\n",
      "Trained batch 1888 batch loss 0.566406786 epoch total loss 0.542207241\n",
      "Trained batch 1889 batch loss 0.567571938 epoch total loss 0.542220712\n",
      "Trained batch 1890 batch loss 0.561151743 epoch total loss 0.542230725\n",
      "Trained batch 1891 batch loss 0.58349663 epoch total loss 0.542252541\n",
      "Trained batch 1892 batch loss 0.48631373 epoch total loss 0.542223\n",
      "Trained batch 1893 batch loss 0.533809185 epoch total loss 0.542218506\n",
      "Trained batch 1894 batch loss 0.493218601 epoch total loss 0.542192638\n",
      "Trained batch 1895 batch loss 0.549045742 epoch total loss 0.542196274\n",
      "Trained batch 1896 batch loss 0.469575077 epoch total loss 0.542157948\n",
      "Trained batch 1897 batch loss 0.496341199 epoch total loss 0.542133808\n",
      "Trained batch 1898 batch loss 0.386574417 epoch total loss 0.542051852\n",
      "Trained batch 1899 batch loss 0.410262346 epoch total loss 0.541982472\n",
      "Trained batch 1900 batch loss 0.485183477 epoch total loss 0.54195261\n",
      "Trained batch 1901 batch loss 0.642350912 epoch total loss 0.54200542\n",
      "Trained batch 1902 batch loss 0.56530726 epoch total loss 0.542017639\n",
      "Trained batch 1903 batch loss 0.587314785 epoch total loss 0.542041481\n",
      "Trained batch 1904 batch loss 0.5217731 epoch total loss 0.542030811\n",
      "Trained batch 1905 batch loss 0.620394051 epoch total loss 0.542071879\n",
      "Trained batch 1906 batch loss 0.633981764 epoch total loss 0.542120159\n",
      "Trained batch 1907 batch loss 0.584661245 epoch total loss 0.54214251\n",
      "Trained batch 1908 batch loss 0.575997472 epoch total loss 0.542160273\n",
      "Trained batch 1909 batch loss 0.556667924 epoch total loss 0.542167842\n",
      "Trained batch 1910 batch loss 0.611901402 epoch total loss 0.54220438\n",
      "Trained batch 1911 batch loss 0.595168 epoch total loss 0.542232096\n",
      "Trained batch 1912 batch loss 0.513289213 epoch total loss 0.542216957\n",
      "Trained batch 1913 batch loss 0.552258551 epoch total loss 0.542222202\n",
      "Trained batch 1914 batch loss 0.567899346 epoch total loss 0.542235613\n",
      "Trained batch 1915 batch loss 0.532283604 epoch total loss 0.542230427\n",
      "Trained batch 1916 batch loss 0.558181047 epoch total loss 0.542238772\n",
      "Trained batch 1917 batch loss 0.589654446 epoch total loss 0.542263448\n",
      "Trained batch 1918 batch loss 0.559022129 epoch total loss 0.54227221\n",
      "Trained batch 1919 batch loss 0.555715621 epoch total loss 0.542279184\n",
      "Trained batch 1920 batch loss 0.522288322 epoch total loss 0.542268813\n",
      "Trained batch 1921 batch loss 0.5310781 epoch total loss 0.542263031\n",
      "Trained batch 1922 batch loss 0.462078094 epoch total loss 0.542221248\n",
      "Trained batch 1923 batch loss 0.505981445 epoch total loss 0.542202413\n",
      "Trained batch 1924 batch loss 0.514553487 epoch total loss 0.542188048\n",
      "Trained batch 1925 batch loss 0.511022806 epoch total loss 0.542171836\n",
      "Trained batch 1926 batch loss 0.485153496 epoch total loss 0.542142212\n",
      "Trained batch 1927 batch loss 0.47772038 epoch total loss 0.542108774\n",
      "Trained batch 1928 batch loss 0.444004089 epoch total loss 0.542057872\n",
      "Trained batch 1929 batch loss 0.492030054 epoch total loss 0.542031944\n",
      "Trained batch 1930 batch loss 0.579268515 epoch total loss 0.542051196\n",
      "Trained batch 1931 batch loss 0.593202353 epoch total loss 0.54207772\n",
      "Trained batch 1932 batch loss 0.594383538 epoch total loss 0.542104781\n",
      "Trained batch 1933 batch loss 0.541888595 epoch total loss 0.542104661\n",
      "Trained batch 1934 batch loss 0.583117485 epoch total loss 0.542125881\n",
      "Trained batch 1935 batch loss 0.560291767 epoch total loss 0.542135239\n",
      "Trained batch 1936 batch loss 0.531282246 epoch total loss 0.542129636\n",
      "Trained batch 1937 batch loss 0.533982337 epoch total loss 0.542125404\n",
      "Trained batch 1938 batch loss 0.501099169 epoch total loss 0.542104244\n",
      "Trained batch 1939 batch loss 0.55827862 epoch total loss 0.542112529\n",
      "Trained batch 1940 batch loss 0.574609935 epoch total loss 0.542129278\n",
      "Trained batch 1941 batch loss 0.515332639 epoch total loss 0.54211551\n",
      "Trained batch 1942 batch loss 0.491978437 epoch total loss 0.542089701\n",
      "Trained batch 1943 batch loss 0.505706966 epoch total loss 0.542071\n",
      "Trained batch 1944 batch loss 0.541196883 epoch total loss 0.542070508\n",
      "Trained batch 1945 batch loss 0.451510161 epoch total loss 0.542023957\n",
      "Trained batch 1946 batch loss 0.462990165 epoch total loss 0.541983366\n",
      "Trained batch 1947 batch loss 0.503264487 epoch total loss 0.541963458\n",
      "Trained batch 1948 batch loss 0.597200751 epoch total loss 0.54199183\n",
      "Trained batch 1949 batch loss 0.449768066 epoch total loss 0.541944444\n",
      "Trained batch 1950 batch loss 0.477148473 epoch total loss 0.541911244\n",
      "Trained batch 1951 batch loss 0.528632879 epoch total loss 0.541904449\n",
      "Trained batch 1952 batch loss 0.560682178 epoch total loss 0.541914105\n",
      "Trained batch 1953 batch loss 0.58260572 epoch total loss 0.541934907\n",
      "Trained batch 1954 batch loss 0.610070825 epoch total loss 0.541969836\n",
      "Trained batch 1955 batch loss 0.579165816 epoch total loss 0.54198885\n",
      "Trained batch 1956 batch loss 0.495462269 epoch total loss 0.541965127\n",
      "Trained batch 1957 batch loss 0.638019383 epoch total loss 0.542014182\n",
      "Trained batch 1958 batch loss 0.521642745 epoch total loss 0.54200381\n",
      "Trained batch 1959 batch loss 0.624316096 epoch total loss 0.542045772\n",
      "Trained batch 1960 batch loss 0.571054 epoch total loss 0.542060554\n",
      "Trained batch 1961 batch loss 0.478371888 epoch total loss 0.542028129\n",
      "Trained batch 1962 batch loss 0.639669478 epoch total loss 0.542077839\n",
      "Trained batch 1963 batch loss 0.493754774 epoch total loss 0.542053282\n",
      "Trained batch 1964 batch loss 0.666637 epoch total loss 0.542116702\n",
      "Trained batch 1965 batch loss 0.688037 epoch total loss 0.542190909\n",
      "Trained batch 1966 batch loss 0.628873706 epoch total loss 0.542235\n",
      "Trained batch 1967 batch loss 0.640417695 epoch total loss 0.542284906\n",
      "Trained batch 1968 batch loss 0.463609278 epoch total loss 0.542245\n",
      "Trained batch 1969 batch loss 0.55443722 epoch total loss 0.54225117\n",
      "Trained batch 1970 batch loss 0.549721181 epoch total loss 0.542254925\n",
      "Trained batch 1971 batch loss 0.540126264 epoch total loss 0.542253852\n",
      "Trained batch 1972 batch loss 0.581161916 epoch total loss 0.542273581\n",
      "Trained batch 1973 batch loss 0.624022782 epoch total loss 0.542315\n",
      "Trained batch 1974 batch loss 0.574974239 epoch total loss 0.542331576\n",
      "Trained batch 1975 batch loss 0.506932616 epoch total loss 0.542313635\n",
      "Trained batch 1976 batch loss 0.591343045 epoch total loss 0.542338431\n",
      "Trained batch 1977 batch loss 0.575381219 epoch total loss 0.54235518\n",
      "Trained batch 1978 batch loss 0.50293529 epoch total loss 0.542335272\n",
      "Trained batch 1979 batch loss 0.524829566 epoch total loss 0.542326391\n",
      "Trained batch 1980 batch loss 0.573295593 epoch total loss 0.542342\n",
      "Trained batch 1981 batch loss 0.558663309 epoch total loss 0.542350292\n",
      "Trained batch 1982 batch loss 0.637636 epoch total loss 0.542398393\n",
      "Trained batch 1983 batch loss 0.561706483 epoch total loss 0.542408049\n",
      "Trained batch 1984 batch loss 0.597414672 epoch total loss 0.542435825\n",
      "Trained batch 1985 batch loss 0.630138516 epoch total loss 0.54248\n",
      "Trained batch 1986 batch loss 0.598843217 epoch total loss 0.542508364\n",
      "Trained batch 1987 batch loss 0.644942939 epoch total loss 0.542559922\n",
      "Trained batch 1988 batch loss 0.593030751 epoch total loss 0.542585313\n",
      "Trained batch 1989 batch loss 0.600921392 epoch total loss 0.542614639\n",
      "Trained batch 1990 batch loss 0.626764297 epoch total loss 0.542656898\n",
      "Trained batch 1991 batch loss 0.582743526 epoch total loss 0.542677045\n",
      "Trained batch 1992 batch loss 0.541654646 epoch total loss 0.542676508\n",
      "Trained batch 1993 batch loss 0.671456456 epoch total loss 0.54274112\n",
      "Trained batch 1994 batch loss 0.531318963 epoch total loss 0.542735457\n",
      "Trained batch 1995 batch loss 0.494314611 epoch total loss 0.542711139\n",
      "Trained batch 1996 batch loss 0.541806519 epoch total loss 0.542710662\n",
      "Trained batch 1997 batch loss 0.509519041 epoch total loss 0.542694032\n",
      "Trained batch 1998 batch loss 0.49411276 epoch total loss 0.542669773\n",
      "Trained batch 1999 batch loss 0.491927564 epoch total loss 0.542644382\n",
      "Trained batch 2000 batch loss 0.537626922 epoch total loss 0.542641819\n",
      "Trained batch 2001 batch loss 0.530294716 epoch total loss 0.542635679\n",
      "Trained batch 2002 batch loss 0.615737915 epoch total loss 0.542672157\n",
      "Trained batch 2003 batch loss 0.5938586 epoch total loss 0.542697728\n",
      "Trained batch 2004 batch loss 0.502946436 epoch total loss 0.542677879\n",
      "Trained batch 2005 batch loss 0.579213738 epoch total loss 0.542696118\n",
      "Trained batch 2006 batch loss 0.542825878 epoch total loss 0.542696178\n",
      "Trained batch 2007 batch loss 0.624527 epoch total loss 0.542736948\n",
      "Trained batch 2008 batch loss 0.482850313 epoch total loss 0.542707145\n",
      "Trained batch 2009 batch loss 0.539793134 epoch total loss 0.542705715\n",
      "Trained batch 2010 batch loss 0.506351888 epoch total loss 0.542687595\n",
      "Trained batch 2011 batch loss 0.538163245 epoch total loss 0.54268539\n",
      "Trained batch 2012 batch loss 0.555889666 epoch total loss 0.542691946\n",
      "Trained batch 2013 batch loss 0.57707274 epoch total loss 0.542709053\n",
      "Trained batch 2014 batch loss 0.513786197 epoch total loss 0.542694688\n",
      "Trained batch 2015 batch loss 0.587895393 epoch total loss 0.542717099\n",
      "Trained batch 2016 batch loss 0.490402818 epoch total loss 0.542691112\n",
      "Trained batch 2017 batch loss 0.455854952 epoch total loss 0.542648077\n",
      "Trained batch 2018 batch loss 0.51607132 epoch total loss 0.542634904\n",
      "Trained batch 2019 batch loss 0.437134564 epoch total loss 0.542582631\n",
      "Trained batch 2020 batch loss 0.491027355 epoch total loss 0.54255712\n",
      "Trained batch 2021 batch loss 0.58133024 epoch total loss 0.542576253\n",
      "Trained batch 2022 batch loss 0.597210944 epoch total loss 0.542603254\n",
      "Trained batch 2023 batch loss 0.545974731 epoch total loss 0.542604923\n",
      "Trained batch 2024 batch loss 0.497692466 epoch total loss 0.54258275\n",
      "Trained batch 2025 batch loss 0.545701444 epoch total loss 0.5425843\n",
      "Trained batch 2026 batch loss 0.540767252 epoch total loss 0.542583406\n",
      "Trained batch 2027 batch loss 0.580002189 epoch total loss 0.542601824\n",
      "Trained batch 2028 batch loss 0.502078414 epoch total loss 0.542581856\n",
      "Trained batch 2029 batch loss 0.5100106 epoch total loss 0.542565763\n",
      "Trained batch 2030 batch loss 0.42446664 epoch total loss 0.542507589\n",
      "Trained batch 2031 batch loss 0.439932078 epoch total loss 0.542457104\n",
      "Trained batch 2032 batch loss 0.393199623 epoch total loss 0.542383611\n",
      "Trained batch 2033 batch loss 0.439184368 epoch total loss 0.542332888\n",
      "Trained batch 2034 batch loss 0.429091096 epoch total loss 0.542277217\n",
      "Trained batch 2035 batch loss 0.370672882 epoch total loss 0.542192876\n",
      "Trained batch 2036 batch loss 0.40326944 epoch total loss 0.542124689\n",
      "Trained batch 2037 batch loss 0.459323019 epoch total loss 0.542084038\n",
      "Trained batch 2038 batch loss 0.516846955 epoch total loss 0.5420717\n",
      "Trained batch 2039 batch loss 0.472983539 epoch total loss 0.542037785\n",
      "Trained batch 2040 batch loss 0.464658439 epoch total loss 0.541999876\n",
      "Trained batch 2041 batch loss 0.484021217 epoch total loss 0.541971445\n",
      "Trained batch 2042 batch loss 0.494926572 epoch total loss 0.541948378\n",
      "Trained batch 2043 batch loss 0.655656755 epoch total loss 0.542004\n",
      "Trained batch 2044 batch loss 0.550899088 epoch total loss 0.5420084\n",
      "Trained batch 2045 batch loss 0.554618657 epoch total loss 0.542014539\n",
      "Trained batch 2046 batch loss 0.568379939 epoch total loss 0.542027414\n",
      "Trained batch 2047 batch loss 0.642472506 epoch total loss 0.542076468\n",
      "Trained batch 2048 batch loss 0.685747206 epoch total loss 0.542146623\n",
      "Trained batch 2049 batch loss 0.618711 epoch total loss 0.542183936\n",
      "Trained batch 2050 batch loss 0.594016671 epoch total loss 0.542209208\n",
      "Trained batch 2051 batch loss 0.700977266 epoch total loss 0.542286634\n",
      "Trained batch 2052 batch loss 0.648476541 epoch total loss 0.542338371\n",
      "Trained batch 2053 batch loss 0.672602952 epoch total loss 0.542401791\n",
      "Trained batch 2054 batch loss 0.606882334 epoch total loss 0.542433202\n",
      "Trained batch 2055 batch loss 0.476067 epoch total loss 0.542400956\n",
      "Trained batch 2056 batch loss 0.474614352 epoch total loss 0.542367935\n",
      "Trained batch 2057 batch loss 0.433514476 epoch total loss 0.542315\n",
      "Trained batch 2058 batch loss 0.484455764 epoch total loss 0.542286932\n",
      "Trained batch 2059 batch loss 0.623387933 epoch total loss 0.542326331\n",
      "Trained batch 2060 batch loss 0.549689054 epoch total loss 0.542329907\n",
      "Trained batch 2061 batch loss 0.507494569 epoch total loss 0.542313\n",
      "Trained batch 2062 batch loss 0.448936611 epoch total loss 0.54226768\n",
      "Trained batch 2063 batch loss 0.482300818 epoch total loss 0.542238653\n",
      "Trained batch 2064 batch loss 0.442876816 epoch total loss 0.542190492\n",
      "Trained batch 2065 batch loss 0.488367915 epoch total loss 0.542164445\n",
      "Trained batch 2066 batch loss 0.43563813 epoch total loss 0.542112887\n",
      "Trained batch 2067 batch loss 0.496897846 epoch total loss 0.542091072\n",
      "Trained batch 2068 batch loss 0.451440454 epoch total loss 0.542047203\n",
      "Trained batch 2069 batch loss 0.471183062 epoch total loss 0.54201293\n",
      "Trained batch 2070 batch loss 0.42459026 epoch total loss 0.541956186\n",
      "Trained batch 2071 batch loss 0.462950617 epoch total loss 0.541918039\n",
      "Trained batch 2072 batch loss 0.452171743 epoch total loss 0.541874707\n",
      "Trained batch 2073 batch loss 0.438973159 epoch total loss 0.541825056\n",
      "Trained batch 2074 batch loss 0.440658271 epoch total loss 0.541776299\n",
      "Trained batch 2075 batch loss 0.555369854 epoch total loss 0.541782856\n",
      "Trained batch 2076 batch loss 0.495764613 epoch total loss 0.541760683\n",
      "Trained batch 2077 batch loss 0.598381519 epoch total loss 0.541788\n",
      "Trained batch 2078 batch loss 0.431417346 epoch total loss 0.541734815\n",
      "Trained batch 2079 batch loss 0.392644286 epoch total loss 0.54166317\n",
      "Trained batch 2080 batch loss 0.390071362 epoch total loss 0.541590214\n",
      "Trained batch 2081 batch loss 0.418777347 epoch total loss 0.541531265\n",
      "Trained batch 2082 batch loss 0.461965382 epoch total loss 0.541493\n",
      "Trained batch 2083 batch loss 0.456493109 epoch total loss 0.541452229\n",
      "Trained batch 2084 batch loss 0.437060475 epoch total loss 0.541402102\n",
      "Trained batch 2085 batch loss 0.410723776 epoch total loss 0.541339457\n",
      "Trained batch 2086 batch loss 0.442544818 epoch total loss 0.541292071\n",
      "Trained batch 2087 batch loss 0.459249794 epoch total loss 0.541252732\n",
      "Trained batch 2088 batch loss 0.535706341 epoch total loss 0.54125011\n",
      "Trained batch 2089 batch loss 0.608352184 epoch total loss 0.541282237\n",
      "Trained batch 2090 batch loss 0.639519036 epoch total loss 0.541329265\n",
      "Trained batch 2091 batch loss 0.631071091 epoch total loss 0.54137218\n",
      "Trained batch 2092 batch loss 0.700795591 epoch total loss 0.541448414\n",
      "Trained batch 2093 batch loss 0.724079788 epoch total loss 0.541535676\n",
      "Trained batch 2094 batch loss 0.669739962 epoch total loss 0.541596949\n",
      "Trained batch 2095 batch loss 0.513719618 epoch total loss 0.541583598\n",
      "Trained batch 2096 batch loss 0.606699944 epoch total loss 0.541614652\n",
      "Trained batch 2097 batch loss 0.55796206 epoch total loss 0.54162246\n",
      "Trained batch 2098 batch loss 0.528755665 epoch total loss 0.54161638\n",
      "Trained batch 2099 batch loss 0.56242007 epoch total loss 0.541626275\n",
      "Trained batch 2100 batch loss 0.563648939 epoch total loss 0.541636705\n",
      "Trained batch 2101 batch loss 0.552524149 epoch total loss 0.541641891\n",
      "Trained batch 2102 batch loss 0.577728629 epoch total loss 0.541659057\n",
      "Trained batch 2103 batch loss 0.569223583 epoch total loss 0.54167217\n",
      "Trained batch 2104 batch loss 0.582832456 epoch total loss 0.54169178\n",
      "Trained batch 2105 batch loss 0.583321631 epoch total loss 0.541711569\n",
      "Trained batch 2106 batch loss 0.508434176 epoch total loss 0.541695774\n",
      "Trained batch 2107 batch loss 0.606684148 epoch total loss 0.541726589\n",
      "Trained batch 2108 batch loss 0.563395441 epoch total loss 0.541736841\n",
      "Trained batch 2109 batch loss 0.559537411 epoch total loss 0.541745305\n",
      "Trained batch 2110 batch loss 0.584689856 epoch total loss 0.54176569\n",
      "Trained batch 2111 batch loss 0.566823065 epoch total loss 0.541777551\n",
      "Trained batch 2112 batch loss 0.49510783 epoch total loss 0.541755438\n",
      "Trained batch 2113 batch loss 0.558996618 epoch total loss 0.541763604\n",
      "Trained batch 2114 batch loss 0.467743546 epoch total loss 0.541728616\n",
      "Trained batch 2115 batch loss 0.546923518 epoch total loss 0.541731\n",
      "Trained batch 2116 batch loss 0.474974185 epoch total loss 0.541699469\n",
      "Trained batch 2117 batch loss 0.489323288 epoch total loss 0.541674733\n",
      "Trained batch 2118 batch loss 0.479536593 epoch total loss 0.541645408\n",
      "Trained batch 2119 batch loss 0.585011244 epoch total loss 0.541665852\n",
      "Trained batch 2120 batch loss 0.460603893 epoch total loss 0.541627586\n",
      "Trained batch 2121 batch loss 0.660141 epoch total loss 0.541683495\n",
      "Trained batch 2122 batch loss 0.520333886 epoch total loss 0.541673422\n",
      "Trained batch 2123 batch loss 0.475816488 epoch total loss 0.541642427\n",
      "Trained batch 2124 batch loss 0.452999115 epoch total loss 0.541600704\n",
      "Trained batch 2125 batch loss 0.404829383 epoch total loss 0.541536331\n",
      "Trained batch 2126 batch loss 0.499463707 epoch total loss 0.541516542\n",
      "Trained batch 2127 batch loss 0.527156234 epoch total loss 0.541509748\n",
      "Trained batch 2128 batch loss 0.541917 epoch total loss 0.541509926\n",
      "Trained batch 2129 batch loss 0.55050987 epoch total loss 0.541514158\n",
      "Trained batch 2130 batch loss 0.468087107 epoch total loss 0.541479707\n",
      "Trained batch 2131 batch loss 0.477802902 epoch total loss 0.541449845\n",
      "Trained batch 2132 batch loss 0.577462196 epoch total loss 0.541466773\n",
      "Trained batch 2133 batch loss 0.563944459 epoch total loss 0.541477323\n",
      "Trained batch 2134 batch loss 0.587575495 epoch total loss 0.541498899\n",
      "Trained batch 2135 batch loss 0.623080254 epoch total loss 0.541537046\n",
      "Trained batch 2136 batch loss 0.497808129 epoch total loss 0.541516602\n",
      "Trained batch 2137 batch loss 0.517691553 epoch total loss 0.541505456\n",
      "Trained batch 2138 batch loss 0.460993707 epoch total loss 0.541467786\n",
      "Trained batch 2139 batch loss 0.533225954 epoch total loss 0.541463912\n",
      "Trained batch 2140 batch loss 0.495025277 epoch total loss 0.541442215\n",
      "Trained batch 2141 batch loss 0.534055948 epoch total loss 0.541438758\n",
      "Trained batch 2142 batch loss 0.551791787 epoch total loss 0.541443527\n",
      "Trained batch 2143 batch loss 0.478546143 epoch total loss 0.541414201\n",
      "Trained batch 2144 batch loss 0.557211697 epoch total loss 0.541421592\n",
      "Trained batch 2145 batch loss 0.613021791 epoch total loss 0.541455\n",
      "Trained batch 2146 batch loss 0.603939772 epoch total loss 0.541484058\n",
      "Trained batch 2147 batch loss 0.520174921 epoch total loss 0.541474104\n",
      "Trained batch 2148 batch loss 0.456153184 epoch total loss 0.541434407\n",
      "Trained batch 2149 batch loss 0.490237743 epoch total loss 0.541410565\n",
      "Trained batch 2150 batch loss 0.503528059 epoch total loss 0.541393\n",
      "Trained batch 2151 batch loss 0.539470375 epoch total loss 0.541392\n",
      "Trained batch 2152 batch loss 0.535975337 epoch total loss 0.541389525\n",
      "Trained batch 2153 batch loss 0.572303414 epoch total loss 0.54140389\n",
      "Trained batch 2154 batch loss 0.529348969 epoch total loss 0.541398287\n",
      "Trained batch 2155 batch loss 0.502279162 epoch total loss 0.541380107\n",
      "Trained batch 2156 batch loss 0.527152181 epoch total loss 0.541373491\n",
      "Trained batch 2157 batch loss 0.544144511 epoch total loss 0.541374803\n",
      "Trained batch 2158 batch loss 0.51893 epoch total loss 0.541364431\n",
      "Trained batch 2159 batch loss 0.57014662 epoch total loss 0.541377783\n",
      "Trained batch 2160 batch loss 0.53832972 epoch total loss 0.541376352\n",
      "Trained batch 2161 batch loss 0.507680476 epoch total loss 0.541360795\n",
      "Trained batch 2162 batch loss 0.467169434 epoch total loss 0.541326463\n",
      "Trained batch 2163 batch loss 0.481238246 epoch total loss 0.541298628\n",
      "Trained batch 2164 batch loss 0.432409495 epoch total loss 0.541248322\n",
      "Trained batch 2165 batch loss 0.50182873 epoch total loss 0.541230083\n",
      "Trained batch 2166 batch loss 0.537086725 epoch total loss 0.541228235\n",
      "Trained batch 2167 batch loss 0.56459707 epoch total loss 0.541238964\n",
      "Trained batch 2168 batch loss 0.49592191 epoch total loss 0.541218102\n",
      "Trained batch 2169 batch loss 0.405297756 epoch total loss 0.541155398\n",
      "Trained batch 2170 batch loss 0.537042618 epoch total loss 0.541153491\n",
      "Trained batch 2171 batch loss 0.627618313 epoch total loss 0.541193306\n",
      "Trained batch 2172 batch loss 0.565794706 epoch total loss 0.541204631\n",
      "Trained batch 2173 batch loss 0.472672343 epoch total loss 0.5411731\n",
      "Trained batch 2174 batch loss 0.415008485 epoch total loss 0.541115046\n",
      "Trained batch 2175 batch loss 0.390226126 epoch total loss 0.541045725\n",
      "Trained batch 2176 batch loss 0.432924837 epoch total loss 0.540996075\n",
      "Trained batch 2177 batch loss 0.433296144 epoch total loss 0.540946603\n",
      "Trained batch 2178 batch loss 0.431984246 epoch total loss 0.540896595\n",
      "Trained batch 2179 batch loss 0.436830372 epoch total loss 0.540848851\n",
      "Trained batch 2180 batch loss 0.587030053 epoch total loss 0.54087\n",
      "Trained batch 2181 batch loss 0.539522052 epoch total loss 0.540869415\n",
      "Trained batch 2182 batch loss 0.562153637 epoch total loss 0.54087919\n",
      "Trained batch 2183 batch loss 0.604723871 epoch total loss 0.540908456\n",
      "Trained batch 2184 batch loss 0.680184126 epoch total loss 0.540972173\n",
      "Trained batch 2185 batch loss 0.579677582 epoch total loss 0.540989935\n",
      "Trained batch 2186 batch loss 0.569206893 epoch total loss 0.54100281\n",
      "Trained batch 2187 batch loss 0.585799038 epoch total loss 0.541023314\n",
      "Trained batch 2188 batch loss 0.47800979 epoch total loss 0.540994525\n",
      "Trained batch 2189 batch loss 0.459896922 epoch total loss 0.540957451\n",
      "Trained batch 2190 batch loss 0.47159189 epoch total loss 0.540925801\n",
      "Trained batch 2191 batch loss 0.440364271 epoch total loss 0.540879846\n",
      "Trained batch 2192 batch loss 0.494734526 epoch total loss 0.540858805\n",
      "Trained batch 2193 batch loss 0.50967 epoch total loss 0.54084456\n",
      "Trained batch 2194 batch loss 0.564621866 epoch total loss 0.540855408\n",
      "Trained batch 2195 batch loss 0.540497839 epoch total loss 0.540855229\n",
      "Trained batch 2196 batch loss 0.519184947 epoch total loss 0.540845335\n",
      "Trained batch 2197 batch loss 0.591625333 epoch total loss 0.540868521\n",
      "Trained batch 2198 batch loss 0.526007712 epoch total loss 0.540861726\n",
      "Trained batch 2199 batch loss 0.583724618 epoch total loss 0.540881217\n",
      "Trained batch 2200 batch loss 0.500008404 epoch total loss 0.54086262\n",
      "Trained batch 2201 batch loss 0.542255342 epoch total loss 0.540863276\n",
      "Trained batch 2202 batch loss 0.659203112 epoch total loss 0.540917\n",
      "Trained batch 2203 batch loss 0.661792934 epoch total loss 0.540971875\n",
      "Trained batch 2204 batch loss 0.629968047 epoch total loss 0.541012228\n",
      "Trained batch 2205 batch loss 0.547028065 epoch total loss 0.541014969\n",
      "Trained batch 2206 batch loss 0.515257537 epoch total loss 0.541003287\n",
      "Trained batch 2207 batch loss 0.488490254 epoch total loss 0.540979505\n",
      "Trained batch 2208 batch loss 0.548633873 epoch total loss 0.540982962\n",
      "Trained batch 2209 batch loss 0.504793644 epoch total loss 0.54096657\n",
      "Trained batch 2210 batch loss 0.479427218 epoch total loss 0.540938675\n",
      "Trained batch 2211 batch loss 0.473943 epoch total loss 0.540908396\n",
      "Trained batch 2212 batch loss 0.525151193 epoch total loss 0.540901303\n",
      "Trained batch 2213 batch loss 0.545230627 epoch total loss 0.54090327\n",
      "Trained batch 2214 batch loss 0.611933 epoch total loss 0.540935338\n",
      "Trained batch 2215 batch loss 0.468583018 epoch total loss 0.540902674\n",
      "Trained batch 2216 batch loss 0.545811296 epoch total loss 0.54090488\n",
      "Trained batch 2217 batch loss 0.600918531 epoch total loss 0.540932\n",
      "Trained batch 2218 batch loss 0.513438463 epoch total loss 0.540919602\n",
      "Trained batch 2219 batch loss 0.65184468 epoch total loss 0.54096961\n",
      "Trained batch 2220 batch loss 0.680330276 epoch total loss 0.541032314\n",
      "Trained batch 2221 batch loss 0.501260579 epoch total loss 0.541014433\n",
      "Trained batch 2222 batch loss 0.530502617 epoch total loss 0.541009665\n",
      "Trained batch 2223 batch loss 0.513848543 epoch total loss 0.540997446\n",
      "Trained batch 2224 batch loss 0.486613095 epoch total loss 0.540972948\n",
      "Trained batch 2225 batch loss 0.484345138 epoch total loss 0.540947556\n",
      "Trained batch 2226 batch loss 0.472964853 epoch total loss 0.540917039\n",
      "Trained batch 2227 batch loss 0.569660544 epoch total loss 0.54093\n",
      "Trained batch 2228 batch loss 0.462860942 epoch total loss 0.540894926\n",
      "Trained batch 2229 batch loss 0.439334333 epoch total loss 0.540849388\n",
      "Trained batch 2230 batch loss 0.484237254 epoch total loss 0.540824\n",
      "Trained batch 2231 batch loss 0.50691247 epoch total loss 0.540808797\n",
      "Trained batch 2232 batch loss 0.53131634 epoch total loss 0.540804565\n",
      "Trained batch 2233 batch loss 0.570990086 epoch total loss 0.540818095\n",
      "Trained batch 2234 batch loss 0.527715445 epoch total loss 0.540812254\n",
      "Trained batch 2235 batch loss 0.528682649 epoch total loss 0.54080683\n",
      "Trained batch 2236 batch loss 0.525673449 epoch total loss 0.540800035\n",
      "Trained batch 2237 batch loss 0.428064138 epoch total loss 0.540749669\n",
      "Trained batch 2238 batch loss 0.513901 epoch total loss 0.540737689\n",
      "Trained batch 2239 batch loss 0.458238423 epoch total loss 0.540700853\n",
      "Trained batch 2240 batch loss 0.48815155 epoch total loss 0.540677369\n",
      "Trained batch 2241 batch loss 0.522204101 epoch total loss 0.540669143\n",
      "Trained batch 2242 batch loss 0.553235531 epoch total loss 0.540674746\n",
      "Trained batch 2243 batch loss 0.66177696 epoch total loss 0.540728688\n",
      "Trained batch 2244 batch loss 0.66876173 epoch total loss 0.54078573\n",
      "Trained batch 2245 batch loss 0.542963147 epoch total loss 0.540786684\n",
      "Trained batch 2246 batch loss 0.512793541 epoch total loss 0.540774286\n",
      "Trained batch 2247 batch loss 0.539693773 epoch total loss 0.540773749\n",
      "Trained batch 2248 batch loss 0.550372481 epoch total loss 0.540778041\n",
      "Trained batch 2249 batch loss 0.426479757 epoch total loss 0.540727258\n",
      "Trained batch 2250 batch loss 0.377334267 epoch total loss 0.5406546\n",
      "Trained batch 2251 batch loss 0.33186084 epoch total loss 0.540561914\n",
      "Trained batch 2252 batch loss 0.320262641 epoch total loss 0.540464103\n",
      "Trained batch 2253 batch loss 0.295351058 epoch total loss 0.540355325\n",
      "Trained batch 2254 batch loss 0.381846607 epoch total loss 0.540285\n",
      "Trained batch 2255 batch loss 0.393922 epoch total loss 0.540220082\n",
      "Trained batch 2256 batch loss 0.505406141 epoch total loss 0.540204644\n",
      "Trained batch 2257 batch loss 0.570052147 epoch total loss 0.540217876\n",
      "Trained batch 2258 batch loss 0.550588608 epoch total loss 0.540222466\n",
      "Trained batch 2259 batch loss 0.567214906 epoch total loss 0.540234387\n",
      "Trained batch 2260 batch loss 0.555425763 epoch total loss 0.540241122\n",
      "Trained batch 2261 batch loss 0.673863709 epoch total loss 0.54030019\n",
      "Trained batch 2262 batch loss 0.578817427 epoch total loss 0.540317237\n",
      "Trained batch 2263 batch loss 0.492900282 epoch total loss 0.540296316\n",
      "Trained batch 2264 batch loss 0.615029514 epoch total loss 0.540329278\n",
      "Trained batch 2265 batch loss 0.480318904 epoch total loss 0.540302813\n",
      "Trained batch 2266 batch loss 0.594064176 epoch total loss 0.540326595\n",
      "Trained batch 2267 batch loss 0.650742769 epoch total loss 0.540375292\n",
      "Trained batch 2268 batch loss 0.528237522 epoch total loss 0.540369928\n",
      "Trained batch 2269 batch loss 0.44920966 epoch total loss 0.540329754\n",
      "Trained batch 2270 batch loss 0.593847215 epoch total loss 0.540353298\n",
      "Trained batch 2271 batch loss 0.429111362 epoch total loss 0.540304303\n",
      "Trained batch 2272 batch loss 0.464834869 epoch total loss 0.540271103\n",
      "Trained batch 2273 batch loss 0.493807852 epoch total loss 0.540250659\n",
      "Trained batch 2274 batch loss 0.592998266 epoch total loss 0.540273845\n",
      "Trained batch 2275 batch loss 0.654584765 epoch total loss 0.540324092\n",
      "Trained batch 2276 batch loss 0.615428209 epoch total loss 0.540357113\n",
      "Trained batch 2277 batch loss 0.638819575 epoch total loss 0.540400326\n",
      "Trained batch 2278 batch loss 0.585227 epoch total loss 0.54042\n",
      "Trained batch 2279 batch loss 0.518841267 epoch total loss 0.540410519\n",
      "Trained batch 2280 batch loss 0.556003749 epoch total loss 0.540417373\n",
      "Trained batch 2281 batch loss 0.479351044 epoch total loss 0.540390611\n",
      "Trained batch 2282 batch loss 0.47472325 epoch total loss 0.540361822\n",
      "Trained batch 2283 batch loss 0.626113594 epoch total loss 0.540399373\n",
      "Trained batch 2284 batch loss 0.585891068 epoch total loss 0.54041934\n",
      "Trained batch 2285 batch loss 0.539024353 epoch total loss 0.540418744\n",
      "Trained batch 2286 batch loss 0.493669212 epoch total loss 0.5403983\n",
      "Trained batch 2287 batch loss 0.607339859 epoch total loss 0.540427506\n",
      "Trained batch 2288 batch loss 0.504737 epoch total loss 0.540411949\n",
      "Trained batch 2289 batch loss 0.609802544 epoch total loss 0.540442288\n",
      "Trained batch 2290 batch loss 0.647272229 epoch total loss 0.540488899\n",
      "Trained batch 2291 batch loss 0.620854139 epoch total loss 0.540524\n",
      "Trained batch 2292 batch loss 0.615868688 epoch total loss 0.540556848\n",
      "Trained batch 2293 batch loss 0.576088548 epoch total loss 0.540572345\n",
      "Trained batch 2294 batch loss 0.586058855 epoch total loss 0.540592134\n",
      "Trained batch 2295 batch loss 0.443411857 epoch total loss 0.540549815\n",
      "Trained batch 2296 batch loss 0.490639567 epoch total loss 0.540528059\n",
      "Trained batch 2297 batch loss 0.466312408 epoch total loss 0.540495694\n",
      "Trained batch 2298 batch loss 0.415000439 epoch total loss 0.540441155\n",
      "Trained batch 2299 batch loss 0.510463059 epoch total loss 0.540428102\n",
      "Trained batch 2300 batch loss 0.495363891 epoch total loss 0.540408492\n",
      "Trained batch 2301 batch loss 0.462943256 epoch total loss 0.540374815\n",
      "Trained batch 2302 batch loss 0.572525263 epoch total loss 0.540388763\n",
      "Trained batch 2303 batch loss 0.614733577 epoch total loss 0.540421069\n",
      "Trained batch 2304 batch loss 0.608776331 epoch total loss 0.540450752\n",
      "Trained batch 2305 batch loss 0.673329711 epoch total loss 0.540508389\n",
      "Trained batch 2306 batch loss 0.630324066 epoch total loss 0.540547371\n",
      "Trained batch 2307 batch loss 0.51465112 epoch total loss 0.540536106\n",
      "Trained batch 2308 batch loss 0.655822933 epoch total loss 0.540586114\n",
      "Trained batch 2309 batch loss 0.56582 epoch total loss 0.540597\n",
      "Trained batch 2310 batch loss 0.493401468 epoch total loss 0.540576577\n",
      "Trained batch 2311 batch loss 0.586258709 epoch total loss 0.540596366\n",
      "Trained batch 2312 batch loss 0.47100994 epoch total loss 0.540566325\n",
      "Trained batch 2313 batch loss 0.57472986 epoch total loss 0.540581048\n",
      "Trained batch 2314 batch loss 0.54832834 epoch total loss 0.540584445\n",
      "Trained batch 2315 batch loss 0.564714611 epoch total loss 0.540594816\n",
      "Trained batch 2316 batch loss 0.450402021 epoch total loss 0.540555894\n",
      "Trained batch 2317 batch loss 0.565341234 epoch total loss 0.540566623\n",
      "Trained batch 2318 batch loss 0.624541 epoch total loss 0.540602803\n",
      "Trained batch 2319 batch loss 0.492280096 epoch total loss 0.540582\n",
      "Trained batch 2320 batch loss 0.506106317 epoch total loss 0.5405671\n",
      "Trained batch 2321 batch loss 0.525428772 epoch total loss 0.540560603\n",
      "Trained batch 2322 batch loss 0.4061656 epoch total loss 0.540502667\n",
      "Trained batch 2323 batch loss 0.440021157 epoch total loss 0.540459454\n",
      "Trained batch 2324 batch loss 0.477596968 epoch total loss 0.540432394\n",
      "Trained batch 2325 batch loss 0.442704529 epoch total loss 0.540390372\n",
      "Trained batch 2326 batch loss 0.458476067 epoch total loss 0.540355146\n",
      "Trained batch 2327 batch loss 0.405228227 epoch total loss 0.540297091\n",
      "Trained batch 2328 batch loss 0.407550931 epoch total loss 0.540240109\n",
      "Trained batch 2329 batch loss 0.480435252 epoch total loss 0.540214419\n",
      "Trained batch 2330 batch loss 0.575652719 epoch total loss 0.540229678\n",
      "Trained batch 2331 batch loss 0.635710478 epoch total loss 0.540270627\n",
      "Trained batch 2332 batch loss 0.642846823 epoch total loss 0.540314615\n",
      "Trained batch 2333 batch loss 0.720871687 epoch total loss 0.540392\n",
      "Trained batch 2334 batch loss 0.678975105 epoch total loss 0.540451348\n",
      "Trained batch 2335 batch loss 0.657226384 epoch total loss 0.540501356\n",
      "Trained batch 2336 batch loss 0.608206272 epoch total loss 0.540530324\n",
      "Trained batch 2337 batch loss 0.547855318 epoch total loss 0.540533483\n",
      "Trained batch 2338 batch loss 0.608257115 epoch total loss 0.540562451\n",
      "Trained batch 2339 batch loss 0.574641764 epoch total loss 0.540577\n",
      "Trained batch 2340 batch loss 0.620407164 epoch total loss 0.540611088\n",
      "Trained batch 2341 batch loss 0.67243278 epoch total loss 0.540667415\n",
      "Trained batch 2342 batch loss 0.585255742 epoch total loss 0.540686429\n",
      "Trained batch 2343 batch loss 0.634863138 epoch total loss 0.540726602\n",
      "Trained batch 2344 batch loss 0.610623837 epoch total loss 0.540756404\n",
      "Trained batch 2345 batch loss 0.507410467 epoch total loss 0.540742218\n",
      "Trained batch 2346 batch loss 0.612704635 epoch total loss 0.540772915\n",
      "Trained batch 2347 batch loss 0.535809457 epoch total loss 0.540770769\n",
      "Trained batch 2348 batch loss 0.662963092 epoch total loss 0.540822804\n",
      "Trained batch 2349 batch loss 0.648836195 epoch total loss 0.540868759\n",
      "Trained batch 2350 batch loss 0.533065617 epoch total loss 0.540865481\n",
      "Trained batch 2351 batch loss 0.511217535 epoch total loss 0.540852845\n",
      "Trained batch 2352 batch loss 0.594544709 epoch total loss 0.540875673\n",
      "Trained batch 2353 batch loss 0.533109903 epoch total loss 0.540872395\n",
      "Trained batch 2354 batch loss 0.539849937 epoch total loss 0.540871918\n",
      "Trained batch 2355 batch loss 0.543496132 epoch total loss 0.540873051\n",
      "Trained batch 2356 batch loss 0.537458181 epoch total loss 0.540871561\n",
      "Trained batch 2357 batch loss 0.534064889 epoch total loss 0.5408687\n",
      "Trained batch 2358 batch loss 0.618676782 epoch total loss 0.540901661\n",
      "Trained batch 2359 batch loss 0.739498556 epoch total loss 0.540985882\n",
      "Trained batch 2360 batch loss 0.584018171 epoch total loss 0.541004062\n",
      "Trained batch 2361 batch loss 0.497161031 epoch total loss 0.540985525\n",
      "Trained batch 2362 batch loss 0.423617512 epoch total loss 0.540935814\n",
      "Trained batch 2363 batch loss 0.43605572 epoch total loss 0.540891409\n",
      "Trained batch 2364 batch loss 0.414184213 epoch total loss 0.540837824\n",
      "Trained batch 2365 batch loss 0.490794808 epoch total loss 0.540816724\n",
      "Trained batch 2366 batch loss 0.445390105 epoch total loss 0.540776372\n",
      "Trained batch 2367 batch loss 0.576389134 epoch total loss 0.540791452\n",
      "Trained batch 2368 batch loss 0.630306542 epoch total loss 0.540829241\n",
      "Trained batch 2369 batch loss 0.592092574 epoch total loss 0.540850818\n",
      "Trained batch 2370 batch loss 0.456510067 epoch total loss 0.540815294\n",
      "Trained batch 2371 batch loss 0.529583097 epoch total loss 0.540810525\n",
      "Trained batch 2372 batch loss 0.511356533 epoch total loss 0.540798068\n",
      "Trained batch 2373 batch loss 0.555954337 epoch total loss 0.540804446\n",
      "Trained batch 2374 batch loss 0.563012779 epoch total loss 0.540813804\n",
      "Trained batch 2375 batch loss 0.631024 epoch total loss 0.540851772\n",
      "Trained batch 2376 batch loss 0.574723721 epoch total loss 0.540866\n",
      "Trained batch 2377 batch loss 0.626540899 epoch total loss 0.540902078\n",
      "Trained batch 2378 batch loss 0.633555651 epoch total loss 0.54094106\n",
      "Trained batch 2379 batch loss 0.571734071 epoch total loss 0.540954\n",
      "Trained batch 2380 batch loss 0.498762608 epoch total loss 0.540936291\n",
      "Trained batch 2381 batch loss 0.52369833 epoch total loss 0.540929\n",
      "Trained batch 2382 batch loss 0.686571598 epoch total loss 0.540990174\n",
      "Trained batch 2383 batch loss 0.683609068 epoch total loss 0.54105\n",
      "Trained batch 2384 batch loss 0.684938848 epoch total loss 0.541110337\n",
      "Trained batch 2385 batch loss 0.618517458 epoch total loss 0.541142821\n",
      "Trained batch 2386 batch loss 0.65914309 epoch total loss 0.541192293\n",
      "Trained batch 2387 batch loss 0.617271721 epoch total loss 0.541224182\n",
      "Trained batch 2388 batch loss 0.62578541 epoch total loss 0.541259587\n",
      "Trained batch 2389 batch loss 0.625131 epoch total loss 0.541294694\n",
      "Trained batch 2390 batch loss 0.502043366 epoch total loss 0.541278243\n",
      "Trained batch 2391 batch loss 0.59536618 epoch total loss 0.541300893\n",
      "Trained batch 2392 batch loss 0.640508056 epoch total loss 0.541342318\n",
      "Trained batch 2393 batch loss 0.454551488 epoch total loss 0.541306078\n",
      "Trained batch 2394 batch loss 0.490010619 epoch total loss 0.541284621\n",
      "Trained batch 2395 batch loss 0.533994794 epoch total loss 0.541281581\n",
      "Trained batch 2396 batch loss 0.515735209 epoch total loss 0.541270912\n",
      "Trained batch 2397 batch loss 0.509244263 epoch total loss 0.54125756\n",
      "Trained batch 2398 batch loss 0.508799255 epoch total loss 0.54124403\n",
      "Trained batch 2399 batch loss 0.504537106 epoch total loss 0.541228712\n",
      "Trained batch 2400 batch loss 0.593849063 epoch total loss 0.541250646\n",
      "Trained batch 2401 batch loss 0.571868837 epoch total loss 0.541263402\n",
      "Trained batch 2402 batch loss 0.579806685 epoch total loss 0.541279495\n",
      "Trained batch 2403 batch loss 0.526069 epoch total loss 0.541273177\n",
      "Trained batch 2404 batch loss 0.508484304 epoch total loss 0.541259587\n",
      "Trained batch 2405 batch loss 0.551051676 epoch total loss 0.54126364\n",
      "Trained batch 2406 batch loss 0.557023525 epoch total loss 0.541270137\n",
      "Trained batch 2407 batch loss 0.540015578 epoch total loss 0.54126966\n",
      "Trained batch 2408 batch loss 0.475549042 epoch total loss 0.541242361\n",
      "Trained batch 2409 batch loss 0.560923755 epoch total loss 0.541250527\n",
      "Trained batch 2410 batch loss 0.454113692 epoch total loss 0.541214406\n",
      "Trained batch 2411 batch loss 0.509580731 epoch total loss 0.541201234\n",
      "Trained batch 2412 batch loss 0.535547554 epoch total loss 0.541198909\n",
      "Trained batch 2413 batch loss 0.55232805 epoch total loss 0.541203499\n",
      "Trained batch 2414 batch loss 0.601531625 epoch total loss 0.541228533\n",
      "Trained batch 2415 batch loss 0.544399559 epoch total loss 0.541229844\n",
      "Trained batch 2416 batch loss 0.497131974 epoch total loss 0.541211605\n",
      "Trained batch 2417 batch loss 0.439423442 epoch total loss 0.541169524\n",
      "Trained batch 2418 batch loss 0.554003179 epoch total loss 0.541174829\n",
      "Trained batch 2419 batch loss 0.619939327 epoch total loss 0.541207373\n",
      "Trained batch 2420 batch loss 0.507068 epoch total loss 0.541193306\n",
      "Trained batch 2421 batch loss 0.500665963 epoch total loss 0.541176498\n",
      "Trained batch 2422 batch loss 0.52666682 epoch total loss 0.541170478\n",
      "Trained batch 2423 batch loss 0.56806469 epoch total loss 0.541181624\n",
      "Trained batch 2424 batch loss 0.544374 epoch total loss 0.541183\n",
      "Trained batch 2425 batch loss 0.512221098 epoch total loss 0.541171\n",
      "Trained batch 2426 batch loss 0.470999 epoch total loss 0.541142046\n",
      "Trained batch 2427 batch loss 0.453645557 epoch total loss 0.541106\n",
      "Trained batch 2428 batch loss 0.48368597 epoch total loss 0.541082323\n",
      "Trained batch 2429 batch loss 0.498119652 epoch total loss 0.54106468\n",
      "Trained batch 2430 batch loss 0.614747345 epoch total loss 0.541095\n",
      "Trained batch 2431 batch loss 0.665648818 epoch total loss 0.541146219\n",
      "Trained batch 2432 batch loss 0.540078759 epoch total loss 0.541145802\n",
      "Trained batch 2433 batch loss 0.556871831 epoch total loss 0.541152239\n",
      "Trained batch 2434 batch loss 0.575051844 epoch total loss 0.541166186\n",
      "Trained batch 2435 batch loss 0.555898964 epoch total loss 0.541172206\n",
      "Trained batch 2436 batch loss 0.530414343 epoch total loss 0.541167796\n",
      "Trained batch 2437 batch loss 0.596064806 epoch total loss 0.541190326\n",
      "Trained batch 2438 batch loss 0.605048537 epoch total loss 0.541216552\n",
      "Trained batch 2439 batch loss 0.597359478 epoch total loss 0.541239619\n",
      "Trained batch 2440 batch loss 0.523708045 epoch total loss 0.541232407\n",
      "Trained batch 2441 batch loss 0.47614184 epoch total loss 0.541205764\n",
      "Trained batch 2442 batch loss 0.566267192 epoch total loss 0.541216\n",
      "Trained batch 2443 batch loss 0.646333873 epoch total loss 0.54125905\n",
      "Trained batch 2444 batch loss 0.529919326 epoch total loss 0.541254401\n",
      "Trained batch 2445 batch loss 0.544261456 epoch total loss 0.541255653\n",
      "Trained batch 2446 batch loss 0.600790441 epoch total loss 0.541280031\n",
      "Trained batch 2447 batch loss 0.655039251 epoch total loss 0.541326523\n",
      "Trained batch 2448 batch loss 0.556436121 epoch total loss 0.541332662\n",
      "Trained batch 2449 batch loss 0.674571872 epoch total loss 0.541387081\n",
      "Trained batch 2450 batch loss 0.694303 epoch total loss 0.541449487\n",
      "Trained batch 2451 batch loss 0.60225457 epoch total loss 0.541474342\n",
      "Trained batch 2452 batch loss 0.617837727 epoch total loss 0.541505456\n",
      "Trained batch 2453 batch loss 0.628767431 epoch total loss 0.54154104\n",
      "Trained batch 2454 batch loss 0.588432729 epoch total loss 0.541560113\n",
      "Trained batch 2455 batch loss 0.535676718 epoch total loss 0.541557729\n",
      "Trained batch 2456 batch loss 0.466647208 epoch total loss 0.541527212\n",
      "Trained batch 2457 batch loss 0.527096808 epoch total loss 0.54152137\n",
      "Trained batch 2458 batch loss 0.562833905 epoch total loss 0.54153\n",
      "Trained batch 2459 batch loss 0.565414965 epoch total loss 0.541539729\n",
      "Trained batch 2460 batch loss 0.567530215 epoch total loss 0.541550279\n",
      "Trained batch 2461 batch loss 0.53690809 epoch total loss 0.541548371\n",
      "Trained batch 2462 batch loss 0.587177932 epoch total loss 0.541566908\n",
      "Trained batch 2463 batch loss 0.555715 epoch total loss 0.54157263\n",
      "Trained batch 2464 batch loss 0.58915031 epoch total loss 0.541591942\n",
      "Trained batch 2465 batch loss 0.510964215 epoch total loss 0.541579545\n",
      "Trained batch 2466 batch loss 0.599708 epoch total loss 0.541603088\n",
      "Trained batch 2467 batch loss 0.617547631 epoch total loss 0.541633904\n",
      "Trained batch 2468 batch loss 0.575128496 epoch total loss 0.541647434\n",
      "Trained batch 2469 batch loss 0.532002687 epoch total loss 0.5416435\n",
      "Trained batch 2470 batch loss 0.598989308 epoch total loss 0.541666746\n",
      "Trained batch 2471 batch loss 0.528447211 epoch total loss 0.541661382\n",
      "Trained batch 2472 batch loss 0.612392485 epoch total loss 0.541690052\n",
      "Trained batch 2473 batch loss 0.614264667 epoch total loss 0.541719377\n",
      "Trained batch 2474 batch loss 0.504558086 epoch total loss 0.541704357\n",
      "Trained batch 2475 batch loss 0.533134878 epoch total loss 0.54170084\n",
      "Trained batch 2476 batch loss 0.602425694 epoch total loss 0.541725397\n",
      "Trained batch 2477 batch loss 0.54093647 epoch total loss 0.541725039\n",
      "Trained batch 2478 batch loss 0.44079867 epoch total loss 0.54168433\n",
      "Trained batch 2479 batch loss 0.476291746 epoch total loss 0.541657925\n",
      "Trained batch 2480 batch loss 0.549307227 epoch total loss 0.541661\n",
      "Trained batch 2481 batch loss 0.458464622 epoch total loss 0.541627526\n",
      "Trained batch 2482 batch loss 0.526342869 epoch total loss 0.541621327\n",
      "Trained batch 2483 batch loss 0.439207286 epoch total loss 0.541580081\n",
      "Trained batch 2484 batch loss 0.506761611 epoch total loss 0.541566074\n",
      "Trained batch 2485 batch loss 0.487867773 epoch total loss 0.541544497\n",
      "Trained batch 2486 batch loss 0.492289573 epoch total loss 0.541524649\n",
      "Trained batch 2487 batch loss 0.433784664 epoch total loss 0.541481376\n",
      "Trained batch 2488 batch loss 0.46611926 epoch total loss 0.541451037\n",
      "Trained batch 2489 batch loss 0.416203052 epoch total loss 0.54140079\n",
      "Trained batch 2490 batch loss 0.472998023 epoch total loss 0.541373312\n",
      "Trained batch 2491 batch loss 0.488324225 epoch total loss 0.541352\n",
      "Trained batch 2492 batch loss 0.490188 epoch total loss 0.54133147\n",
      "Trained batch 2493 batch loss 0.501574039 epoch total loss 0.541315556\n",
      "Trained batch 2494 batch loss 0.528054237 epoch total loss 0.541310251\n",
      "Trained batch 2495 batch loss 0.513457537 epoch total loss 0.541299045\n",
      "Trained batch 2496 batch loss 0.352407306 epoch total loss 0.541223407\n",
      "Trained batch 2497 batch loss 0.559012532 epoch total loss 0.5412305\n",
      "Trained batch 2498 batch loss 0.477683693 epoch total loss 0.541205049\n",
      "Trained batch 2499 batch loss 0.564210355 epoch total loss 0.541214228\n",
      "Trained batch 2500 batch loss 0.548287511 epoch total loss 0.541217089\n",
      "Trained batch 2501 batch loss 0.574362099 epoch total loss 0.541230321\n",
      "Trained batch 2502 batch loss 0.616923451 epoch total loss 0.5412606\n",
      "Trained batch 2503 batch loss 0.501755536 epoch total loss 0.541244805\n",
      "Trained batch 2504 batch loss 0.605564713 epoch total loss 0.541270494\n",
      "Trained batch 2505 batch loss 0.631299436 epoch total loss 0.541306436\n",
      "Trained batch 2506 batch loss 0.581112862 epoch total loss 0.541322291\n",
      "Trained batch 2507 batch loss 0.553630948 epoch total loss 0.541327178\n",
      "Trained batch 2508 batch loss 0.479312748 epoch total loss 0.541302502\n",
      "Trained batch 2509 batch loss 0.520315111 epoch total loss 0.541294098\n",
      "Trained batch 2510 batch loss 0.528358042 epoch total loss 0.541289\n",
      "Trained batch 2511 batch loss 0.515027046 epoch total loss 0.541278481\n",
      "Trained batch 2512 batch loss 0.526388764 epoch total loss 0.541272521\n",
      "Trained batch 2513 batch loss 0.50639087 epoch total loss 0.541258633\n",
      "Trained batch 2514 batch loss 0.52473861 epoch total loss 0.541252077\n",
      "Trained batch 2515 batch loss 0.466474146 epoch total loss 0.541222334\n",
      "Trained batch 2516 batch loss 0.642648697 epoch total loss 0.541262686\n",
      "Trained batch 2517 batch loss 0.558891 epoch total loss 0.54126966\n",
      "Trained batch 2518 batch loss 0.510376215 epoch total loss 0.541257381\n",
      "Trained batch 2519 batch loss 0.584925354 epoch total loss 0.541274726\n",
      "Trained batch 2520 batch loss 0.54886359 epoch total loss 0.541277766\n",
      "Trained batch 2521 batch loss 0.496052593 epoch total loss 0.541259825\n",
      "Trained batch 2522 batch loss 0.557366967 epoch total loss 0.541266203\n",
      "Trained batch 2523 batch loss 0.536762893 epoch total loss 0.541264415\n",
      "Trained batch 2524 batch loss 0.585231423 epoch total loss 0.541281819\n",
      "Trained batch 2525 batch loss 0.579581559 epoch total loss 0.541297\n",
      "Trained batch 2526 batch loss 0.579874575 epoch total loss 0.541312218\n",
      "Trained batch 2527 batch loss 0.54354614 epoch total loss 0.541313171\n",
      "Trained batch 2528 batch loss 0.590736389 epoch total loss 0.541332662\n",
      "Trained batch 2529 batch loss 0.557551384 epoch total loss 0.541339099\n",
      "Trained batch 2530 batch loss 0.497229487 epoch total loss 0.541321635\n",
      "Trained batch 2531 batch loss 0.512015164 epoch total loss 0.54131\n",
      "Trained batch 2532 batch loss 0.603638947 epoch total loss 0.541334629\n",
      "Trained batch 2533 batch loss 0.607866347 epoch total loss 0.541360915\n",
      "Trained batch 2534 batch loss 0.542274475 epoch total loss 0.541361272\n",
      "Trained batch 2535 batch loss 0.520769656 epoch total loss 0.541353106\n",
      "Trained batch 2536 batch loss 0.506459 epoch total loss 0.541339397\n",
      "Trained batch 2537 batch loss 0.539988279 epoch total loss 0.541338861\n",
      "Trained batch 2538 batch loss 0.615353107 epoch total loss 0.541368\n",
      "Trained batch 2539 batch loss 0.548591733 epoch total loss 0.541370869\n",
      "Trained batch 2540 batch loss 0.562985301 epoch total loss 0.541379392\n",
      "Trained batch 2541 batch loss 0.53921622 epoch total loss 0.541378498\n",
      "Trained batch 2542 batch loss 0.592765272 epoch total loss 0.541398764\n",
      "Trained batch 2543 batch loss 0.521353602 epoch total loss 0.541390836\n",
      "Trained batch 2544 batch loss 0.503473639 epoch total loss 0.541375935\n",
      "Trained batch 2545 batch loss 0.521360159 epoch total loss 0.541368067\n",
      "Trained batch 2546 batch loss 0.426248282 epoch total loss 0.541322887\n",
      "Trained batch 2547 batch loss 0.480540395 epoch total loss 0.541299045\n",
      "Trained batch 2548 batch loss 0.432167411 epoch total loss 0.541256189\n",
      "Trained batch 2549 batch loss 0.406209081 epoch total loss 0.541203201\n",
      "Trained batch 2550 batch loss 0.454736084 epoch total loss 0.541169286\n",
      "Trained batch 2551 batch loss 0.48407355 epoch total loss 0.541146934\n",
      "Trained batch 2552 batch loss 0.466718584 epoch total loss 0.541117728\n",
      "Trained batch 2553 batch loss 0.548248529 epoch total loss 0.541120529\n",
      "Trained batch 2554 batch loss 0.516837478 epoch total loss 0.541111052\n",
      "Trained batch 2555 batch loss 0.517447591 epoch total loss 0.541101754\n",
      "Trained batch 2556 batch loss 0.452857971 epoch total loss 0.541067243\n",
      "Trained batch 2557 batch loss 0.376104414 epoch total loss 0.54100275\n",
      "Trained batch 2558 batch loss 0.452476233 epoch total loss 0.54096812\n",
      "Trained batch 2559 batch loss 0.538326 epoch total loss 0.540967107\n",
      "Trained batch 2560 batch loss 0.549139082 epoch total loss 0.540970325\n",
      "Trained batch 2561 batch loss 0.566753924 epoch total loss 0.540980399\n",
      "Trained batch 2562 batch loss 0.594913125 epoch total loss 0.541001499\n",
      "Trained batch 2563 batch loss 0.465534031 epoch total loss 0.540972054\n",
      "Trained batch 2564 batch loss 0.454521209 epoch total loss 0.540938318\n",
      "Trained batch 2565 batch loss 0.578867316 epoch total loss 0.5409531\n",
      "Trained batch 2566 batch loss 0.548540831 epoch total loss 0.54095608\n",
      "Trained batch 2567 batch loss 0.552217484 epoch total loss 0.540960491\n",
      "Trained batch 2568 batch loss 0.660997391 epoch total loss 0.541007221\n",
      "Trained batch 2569 batch loss 0.548679471 epoch total loss 0.541010201\n",
      "Trained batch 2570 batch loss 0.498880029 epoch total loss 0.54099381\n",
      "Trained batch 2571 batch loss 0.392752469 epoch total loss 0.540936172\n",
      "Trained batch 2572 batch loss 0.546442866 epoch total loss 0.540938258\n",
      "Trained batch 2573 batch loss 0.610622048 epoch total loss 0.540965319\n",
      "Trained batch 2574 batch loss 0.599629343 epoch total loss 0.540988147\n",
      "Trained batch 2575 batch loss 0.604947507 epoch total loss 0.541012943\n",
      "Trained batch 2576 batch loss 0.684939 epoch total loss 0.541068852\n",
      "Trained batch 2577 batch loss 0.493439645 epoch total loss 0.541050375\n",
      "Trained batch 2578 batch loss 0.470028788 epoch total loss 0.541022778\n",
      "Trained batch 2579 batch loss 0.45773825 epoch total loss 0.540990472\n",
      "Trained batch 2580 batch loss 0.433399796 epoch total loss 0.540948749\n",
      "Trained batch 2581 batch loss 0.466401935 epoch total loss 0.5409199\n",
      "Trained batch 2582 batch loss 0.454503119 epoch total loss 0.540886402\n",
      "Trained batch 2583 batch loss 0.480682582 epoch total loss 0.540863097\n",
      "Trained batch 2584 batch loss 0.488168895 epoch total loss 0.540842712\n",
      "Trained batch 2585 batch loss 0.436349601 epoch total loss 0.5408023\n",
      "Trained batch 2586 batch loss 0.403516561 epoch total loss 0.540749252\n",
      "Trained batch 2587 batch loss 0.418221444 epoch total loss 0.540701866\n",
      "Trained batch 2588 batch loss 0.501112044 epoch total loss 0.540686607\n",
      "Trained batch 2589 batch loss 0.492892683 epoch total loss 0.54066813\n",
      "Trained batch 2590 batch loss 0.511022806 epoch total loss 0.540656686\n",
      "Trained batch 2591 batch loss 0.47869122 epoch total loss 0.540632725\n",
      "Trained batch 2592 batch loss 0.504174232 epoch total loss 0.540618658\n",
      "Trained batch 2593 batch loss 0.547434092 epoch total loss 0.540621281\n",
      "Trained batch 2594 batch loss 0.529964387 epoch total loss 0.540617168\n",
      "Trained batch 2595 batch loss 0.476842076 epoch total loss 0.540592611\n",
      "Trained batch 2596 batch loss 0.547429323 epoch total loss 0.540595233\n",
      "Trained batch 2597 batch loss 0.567184687 epoch total loss 0.540605485\n",
      "Trained batch 2598 batch loss 0.576666534 epoch total loss 0.540619314\n",
      "Trained batch 2599 batch loss 0.57036376 epoch total loss 0.540630758\n",
      "Trained batch 2600 batch loss 0.529497623 epoch total loss 0.540626526\n",
      "Trained batch 2601 batch loss 0.556422293 epoch total loss 0.540632546\n",
      "Trained batch 2602 batch loss 0.532859683 epoch total loss 0.540629566\n",
      "Trained batch 2603 batch loss 0.45526278 epoch total loss 0.540596783\n",
      "Trained batch 2604 batch loss 0.584494233 epoch total loss 0.540613651\n",
      "Trained batch 2605 batch loss 0.532765 epoch total loss 0.540610611\n",
      "Trained batch 2606 batch loss 0.578886 epoch total loss 0.540625274\n",
      "Trained batch 2607 batch loss 0.505906165 epoch total loss 0.540612\n",
      "Trained batch 2608 batch loss 0.537598252 epoch total loss 0.54061079\n",
      "Trained batch 2609 batch loss 0.352811247 epoch total loss 0.540538788\n",
      "Trained batch 2610 batch loss 0.375015259 epoch total loss 0.540475368\n",
      "Trained batch 2611 batch loss 0.428470016 epoch total loss 0.540432513\n",
      "Trained batch 2612 batch loss 0.377571583 epoch total loss 0.540370107\n",
      "Trained batch 2613 batch loss 0.50748837 epoch total loss 0.54035753\n",
      "Trained batch 2614 batch loss 0.477483541 epoch total loss 0.540333509\n",
      "Trained batch 2615 batch loss 0.508643508 epoch total loss 0.54032141\n",
      "Trained batch 2616 batch loss 0.515797079 epoch total loss 0.540312\n",
      "Trained batch 2617 batch loss 0.517146826 epoch total loss 0.540303111\n",
      "Trained batch 2618 batch loss 0.506927848 epoch total loss 0.540290356\n",
      "Trained batch 2619 batch loss 0.614755571 epoch total loss 0.540318787\n",
      "Trained batch 2620 batch loss 0.470332474 epoch total loss 0.540292084\n",
      "Trained batch 2621 batch loss 0.512958109 epoch total loss 0.540281653\n",
      "Trained batch 2622 batch loss 0.559938908 epoch total loss 0.540289164\n",
      "Trained batch 2623 batch loss 0.623787642 epoch total loss 0.540321\n",
      "Trained batch 2624 batch loss 0.585332453 epoch total loss 0.540338159\n",
      "Trained batch 2625 batch loss 0.568326354 epoch total loss 0.540348828\n",
      "Trained batch 2626 batch loss 0.561993182 epoch total loss 0.540357053\n",
      "Trained batch 2627 batch loss 0.651718676 epoch total loss 0.540399492\n",
      "Trained batch 2628 batch loss 0.665495217 epoch total loss 0.540447056\n",
      "Trained batch 2629 batch loss 0.504570842 epoch total loss 0.540433407\n",
      "Trained batch 2630 batch loss 0.450965732 epoch total loss 0.540399373\n",
      "Trained batch 2631 batch loss 0.569428384 epoch total loss 0.540410399\n",
      "Trained batch 2632 batch loss 0.527908802 epoch total loss 0.540405691\n",
      "Trained batch 2633 batch loss 0.522730768 epoch total loss 0.540398955\n",
      "Trained batch 2634 batch loss 0.544227123 epoch total loss 0.540400386\n",
      "Trained batch 2635 batch loss 0.541143537 epoch total loss 0.540400684\n",
      "Trained batch 2636 batch loss 0.595149517 epoch total loss 0.540421426\n",
      "Trained batch 2637 batch loss 0.58842963 epoch total loss 0.540439606\n",
      "Trained batch 2638 batch loss 0.558529 epoch total loss 0.54044646\n",
      "Trained batch 2639 batch loss 0.669888735 epoch total loss 0.540495515\n",
      "Trained batch 2640 batch loss 0.540102184 epoch total loss 0.540495396\n",
      "Trained batch 2641 batch loss 0.609384179 epoch total loss 0.540521502\n",
      "Trained batch 2642 batch loss 0.565592408 epoch total loss 0.54053092\n",
      "Trained batch 2643 batch loss 0.582842946 epoch total loss 0.540546954\n",
      "Trained batch 2644 batch loss 0.487032473 epoch total loss 0.540526748\n",
      "Trained batch 2645 batch loss 0.474233449 epoch total loss 0.540501654\n",
      "Trained batch 2646 batch loss 0.485080123 epoch total loss 0.540480733\n",
      "Trained batch 2647 batch loss 0.49024573 epoch total loss 0.540461779\n",
      "Trained batch 2648 batch loss 0.539100528 epoch total loss 0.540461242\n",
      "Trained batch 2649 batch loss 0.56872493 epoch total loss 0.540471911\n",
      "Trained batch 2650 batch loss 0.489268631 epoch total loss 0.5404526\n",
      "Trained batch 2651 batch loss 0.467252493 epoch total loss 0.540425\n",
      "Trained batch 2652 batch loss 0.435537755 epoch total loss 0.540385425\n",
      "Trained batch 2653 batch loss 0.513102651 epoch total loss 0.540375113\n",
      "Trained batch 2654 batch loss 0.541505039 epoch total loss 0.54037559\n",
      "Trained batch 2655 batch loss 0.471782714 epoch total loss 0.540349722\n",
      "Trained batch 2656 batch loss 0.434339583 epoch total loss 0.540309787\n",
      "Trained batch 2657 batch loss 0.442570031 epoch total loss 0.54027307\n",
      "Trained batch 2658 batch loss 0.410982281 epoch total loss 0.540224433\n",
      "Trained batch 2659 batch loss 0.394976079 epoch total loss 0.540169835\n",
      "Trained batch 2660 batch loss 0.557425737 epoch total loss 0.540176272\n",
      "Trained batch 2661 batch loss 0.47009027 epoch total loss 0.540149927\n",
      "Trained batch 2662 batch loss 0.467639148 epoch total loss 0.540122688\n",
      "Trained batch 2663 batch loss 0.429938078 epoch total loss 0.540081322\n",
      "Trained batch 2664 batch loss 0.486386597 epoch total loss 0.540061176\n",
      "Trained batch 2665 batch loss 0.522012115 epoch total loss 0.540054381\n",
      "Trained batch 2666 batch loss 0.46843031 epoch total loss 0.540027499\n",
      "Trained batch 2667 batch loss 0.424247801 epoch total loss 0.539984047\n",
      "Trained batch 2668 batch loss 0.517523885 epoch total loss 0.539975643\n",
      "Trained batch 2669 batch loss 0.514282465 epoch total loss 0.539966047\n",
      "Trained batch 2670 batch loss 0.436419308 epoch total loss 0.539927244\n",
      "Trained batch 2671 batch loss 0.406446964 epoch total loss 0.539877295\n",
      "Trained batch 2672 batch loss 0.513370812 epoch total loss 0.539867401\n",
      "Trained batch 2673 batch loss 0.630791 epoch total loss 0.539901376\n",
      "Trained batch 2674 batch loss 0.773865402 epoch total loss 0.539988875\n",
      "Trained batch 2675 batch loss 0.604612529 epoch total loss 0.540013075\n",
      "Trained batch 2676 batch loss 0.595380545 epoch total loss 0.540033698\n",
      "Trained batch 2677 batch loss 0.548751295 epoch total loss 0.540037\n",
      "Trained batch 2678 batch loss 0.417855412 epoch total loss 0.539991319\n",
      "Trained batch 2679 batch loss 0.516965449 epoch total loss 0.539982736\n",
      "Trained batch 2680 batch loss 0.594564319 epoch total loss 0.540003121\n",
      "Trained batch 2681 batch loss 0.664644599 epoch total loss 0.540049613\n",
      "Trained batch 2682 batch loss 0.558637381 epoch total loss 0.540056527\n",
      "Trained batch 2683 batch loss 0.485124797 epoch total loss 0.540036082\n",
      "Trained batch 2684 batch loss 0.495049119 epoch total loss 0.540019274\n",
      "Trained batch 2685 batch loss 0.543542 epoch total loss 0.540020585\n",
      "Trained batch 2686 batch loss 0.549797416 epoch total loss 0.540024221\n",
      "Trained batch 2687 batch loss 0.49682 epoch total loss 0.540008187\n",
      "Trained batch 2688 batch loss 0.453375161 epoch total loss 0.539975941\n",
      "Trained batch 2689 batch loss 0.539061785 epoch total loss 0.539975584\n",
      "Trained batch 2690 batch loss 0.567317128 epoch total loss 0.539985716\n",
      "Trained batch 2691 batch loss 0.61288023 epoch total loss 0.540012836\n",
      "Trained batch 2692 batch loss 0.632950723 epoch total loss 0.540047348\n",
      "Trained batch 2693 batch loss 0.644355297 epoch total loss 0.540086091\n",
      "Trained batch 2694 batch loss 0.587492168 epoch total loss 0.540103734\n",
      "Trained batch 2695 batch loss 0.550931871 epoch total loss 0.540107727\n",
      "Trained batch 2696 batch loss 0.639066517 epoch total loss 0.540144444\n",
      "Trained batch 2697 batch loss 0.502384245 epoch total loss 0.540130436\n",
      "Trained batch 2698 batch loss 0.495470941 epoch total loss 0.540113926\n",
      "Trained batch 2699 batch loss 0.568394542 epoch total loss 0.540124357\n",
      "Trained batch 2700 batch loss 0.594794214 epoch total loss 0.540144622\n",
      "Trained batch 2701 batch loss 0.553377628 epoch total loss 0.54014951\n",
      "Trained batch 2702 batch loss 0.544718802 epoch total loss 0.540151179\n",
      "Trained batch 2703 batch loss 0.54410547 epoch total loss 0.540152669\n",
      "Trained batch 2704 batch loss 0.483889937 epoch total loss 0.540131807\n",
      "Trained batch 2705 batch loss 0.559290171 epoch total loss 0.5401389\n",
      "Trained batch 2706 batch loss 0.513431549 epoch total loss 0.540129066\n",
      "Trained batch 2707 batch loss 0.573706031 epoch total loss 0.540141463\n",
      "Trained batch 2708 batch loss 0.601240516 epoch total loss 0.540164\n",
      "Trained batch 2709 batch loss 0.662399411 epoch total loss 0.540209115\n",
      "Trained batch 2710 batch loss 0.555747449 epoch total loss 0.540214896\n",
      "Trained batch 2711 batch loss 0.512007713 epoch total loss 0.540204465\n",
      "Trained batch 2712 batch loss 0.552415669 epoch total loss 0.540208936\n",
      "Trained batch 2713 batch loss 0.562657297 epoch total loss 0.540217221\n",
      "Trained batch 2714 batch loss 0.563748658 epoch total loss 0.540225863\n",
      "Trained batch 2715 batch loss 0.560669 epoch total loss 0.540233374\n",
      "Trained batch 2716 batch loss 0.507868052 epoch total loss 0.540221453\n",
      "Trained batch 2717 batch loss 0.539386332 epoch total loss 0.540221155\n",
      "Trained batch 2718 batch loss 0.386216819 epoch total loss 0.54016453\n",
      "Trained batch 2719 batch loss 0.478789508 epoch total loss 0.54014194\n",
      "Trained batch 2720 batch loss 0.540433884 epoch total loss 0.540142\n",
      "Trained batch 2721 batch loss 0.585632563 epoch total loss 0.540158749\n",
      "Trained batch 2722 batch loss 0.56620121 epoch total loss 0.540168285\n",
      "Trained batch 2723 batch loss 0.457467765 epoch total loss 0.540137947\n",
      "Trained batch 2724 batch loss 0.503142595 epoch total loss 0.540124357\n",
      "Trained batch 2725 batch loss 0.41196385 epoch total loss 0.540077388\n",
      "Trained batch 2726 batch loss 0.535493255 epoch total loss 0.540075719\n",
      "Trained batch 2727 batch loss 0.44790715 epoch total loss 0.540041864\n",
      "Trained batch 2728 batch loss 0.492708266 epoch total loss 0.540024519\n",
      "Trained batch 2729 batch loss 0.490465522 epoch total loss 0.54000634\n",
      "Trained batch 2730 batch loss 0.582796 epoch total loss 0.540022\n",
      "Trained batch 2731 batch loss 0.54482311 epoch total loss 0.540023804\n",
      "Trained batch 2732 batch loss 0.50658834 epoch total loss 0.540011525\n",
      "Trained batch 2733 batch loss 0.616293073 epoch total loss 0.54003948\n",
      "Trained batch 2734 batch loss 0.666852176 epoch total loss 0.540085852\n",
      "Trained batch 2735 batch loss 0.480545819 epoch total loss 0.540064096\n",
      "Trained batch 2736 batch loss 0.534323514 epoch total loss 0.540062\n",
      "Trained batch 2737 batch loss 0.514478922 epoch total loss 0.540052652\n",
      "Trained batch 2738 batch loss 0.539070547 epoch total loss 0.540052295\n",
      "Trained batch 2739 batch loss 0.569870472 epoch total loss 0.540063202\n",
      "Trained batch 2740 batch loss 0.575041652 epoch total loss 0.540075958\n",
      "Trained batch 2741 batch loss 0.595673203 epoch total loss 0.540096223\n",
      "Trained batch 2742 batch loss 0.503348947 epoch total loss 0.540082812\n",
      "Trained batch 2743 batch loss 0.478552222 epoch total loss 0.540060401\n",
      "Trained batch 2744 batch loss 0.443346858 epoch total loss 0.540025115\n",
      "Trained batch 2745 batch loss 0.485598207 epoch total loss 0.540005326\n",
      "Trained batch 2746 batch loss 0.501492083 epoch total loss 0.53999126\n",
      "Trained batch 2747 batch loss 0.534498 epoch total loss 0.539989293\n",
      "Trained batch 2748 batch loss 0.61860621 epoch total loss 0.540017903\n",
      "Trained batch 2749 batch loss 0.615769863 epoch total loss 0.54004544\n",
      "Trained batch 2750 batch loss 0.621067464 epoch total loss 0.540074944\n",
      "Trained batch 2751 batch loss 0.541881323 epoch total loss 0.5400756\n",
      "Trained batch 2752 batch loss 0.477556199 epoch total loss 0.540052831\n",
      "Trained batch 2753 batch loss 0.538425088 epoch total loss 0.540052295\n",
      "Trained batch 2754 batch loss 0.594704 epoch total loss 0.540072143\n",
      "Trained batch 2755 batch loss 0.530258358 epoch total loss 0.540068567\n",
      "Trained batch 2756 batch loss 0.643854439 epoch total loss 0.540106237\n",
      "Trained batch 2757 batch loss 0.537324369 epoch total loss 0.540105224\n",
      "Trained batch 2758 batch loss 0.471627861 epoch total loss 0.540080428\n",
      "Trained batch 2759 batch loss 0.472799569 epoch total loss 0.540056\n",
      "Trained batch 2760 batch loss 0.472909123 epoch total loss 0.540031672\n",
      "Trained batch 2761 batch loss 0.560952663 epoch total loss 0.540039241\n",
      "Trained batch 2762 batch loss 0.567721 epoch total loss 0.540049255\n",
      "Trained batch 2763 batch loss 0.609743476 epoch total loss 0.540074468\n",
      "Trained batch 2764 batch loss 0.523423076 epoch total loss 0.540068448\n",
      "Trained batch 2765 batch loss 0.553546965 epoch total loss 0.540073395\n",
      "Trained batch 2766 batch loss 0.501787364 epoch total loss 0.540059566\n",
      "Trained batch 2767 batch loss 0.56603229 epoch total loss 0.540068924\n",
      "Trained batch 2768 batch loss 0.554845214 epoch total loss 0.540074229\n",
      "Trained batch 2769 batch loss 0.487492323 epoch total loss 0.540055275\n",
      "Trained batch 2770 batch loss 0.516037226 epoch total loss 0.540046573\n",
      "Trained batch 2771 batch loss 0.500880182 epoch total loss 0.540032446\n",
      "Trained batch 2772 batch loss 0.415231973 epoch total loss 0.539987445\n",
      "Trained batch 2773 batch loss 0.602863431 epoch total loss 0.540010154\n",
      "Trained batch 2774 batch loss 0.484451532 epoch total loss 0.539990127\n",
      "Trained batch 2775 batch loss 0.640247285 epoch total loss 0.540026248\n",
      "Trained batch 2776 batch loss 0.570337 epoch total loss 0.540037155\n",
      "Epoch 8 train loss 0.5400371551513672\n",
      "Validated batch 1 batch loss 0.547067165\n",
      "Validated batch 2 batch loss 0.645733297\n",
      "Validated batch 3 batch loss 0.530103743\n",
      "Validated batch 4 batch loss 0.553157628\n",
      "Validated batch 5 batch loss 0.473998904\n",
      "Validated batch 6 batch loss 0.622934163\n",
      "Validated batch 7 batch loss 0.502610326\n",
      "Validated batch 8 batch loss 0.594675064\n",
      "Validated batch 9 batch loss 0.586190164\n",
      "Validated batch 10 batch loss 0.514919102\n",
      "Validated batch 11 batch loss 0.636681795\n",
      "Validated batch 12 batch loss 0.555366278\n",
      "Validated batch 13 batch loss 0.64608103\n",
      "Validated batch 14 batch loss 0.498424292\n",
      "Validated batch 15 batch loss 0.560274\n",
      "Validated batch 16 batch loss 0.514635384\n",
      "Validated batch 17 batch loss 0.524965167\n",
      "Validated batch 18 batch loss 0.700386047\n",
      "Validated batch 19 batch loss 0.614725947\n",
      "Validated batch 20 batch loss 0.575691164\n",
      "Validated batch 21 batch loss 0.536125422\n",
      "Validated batch 22 batch loss 0.532278776\n",
      "Validated batch 23 batch loss 0.464299351\n",
      "Validated batch 24 batch loss 0.553226292\n",
      "Validated batch 25 batch loss 0.538163483\n",
      "Validated batch 26 batch loss 0.626218796\n",
      "Validated batch 27 batch loss 0.608807\n",
      "Validated batch 28 batch loss 0.556410491\n",
      "Validated batch 29 batch loss 0.669609725\n",
      "Validated batch 30 batch loss 0.627477765\n",
      "Validated batch 31 batch loss 0.657030523\n",
      "Validated batch 32 batch loss 0.572190344\n",
      "Validated batch 33 batch loss 0.491245091\n",
      "Validated batch 34 batch loss 0.643713236\n",
      "Validated batch 35 batch loss 0.703645051\n",
      "Validated batch 36 batch loss 0.559532583\n",
      "Validated batch 37 batch loss 0.563432753\n",
      "Validated batch 38 batch loss 0.593741894\n",
      "Validated batch 39 batch loss 0.622276723\n",
      "Validated batch 40 batch loss 0.524478674\n",
      "Validated batch 41 batch loss 0.634780705\n",
      "Validated batch 42 batch loss 0.527109146\n",
      "Validated batch 43 batch loss 0.427304655\n",
      "Validated batch 44 batch loss 0.487690657\n",
      "Validated batch 45 batch loss 0.533105254\n",
      "Validated batch 46 batch loss 0.591846347\n",
      "Validated batch 47 batch loss 0.535945773\n",
      "Validated batch 48 batch loss 0.569334805\n",
      "Validated batch 49 batch loss 0.561309814\n",
      "Validated batch 50 batch loss 0.5764606\n",
      "Validated batch 51 batch loss 0.612378359\n",
      "Validated batch 52 batch loss 0.497416675\n",
      "Validated batch 53 batch loss 0.559194446\n",
      "Validated batch 54 batch loss 0.550167084\n",
      "Validated batch 55 batch loss 0.582032681\n",
      "Validated batch 56 batch loss 0.574657202\n",
      "Validated batch 57 batch loss 0.573883057\n",
      "Validated batch 58 batch loss 0.637734175\n",
      "Validated batch 59 batch loss 0.603625953\n",
      "Validated batch 60 batch loss 0.622040212\n",
      "Validated batch 61 batch loss 0.622329473\n",
      "Validated batch 62 batch loss 0.589392781\n",
      "Validated batch 63 batch loss 0.642593682\n",
      "Validated batch 64 batch loss 0.537946641\n",
      "Validated batch 65 batch loss 0.540859163\n",
      "Validated batch 66 batch loss 0.557968736\n",
      "Validated batch 67 batch loss 0.562931\n",
      "Validated batch 68 batch loss 0.608162\n",
      "Validated batch 69 batch loss 0.553470552\n",
      "Validated batch 70 batch loss 0.554969\n",
      "Validated batch 71 batch loss 0.539569\n",
      "Validated batch 72 batch loss 0.611260056\n",
      "Validated batch 73 batch loss 0.605842113\n",
      "Validated batch 74 batch loss 0.639329433\n",
      "Validated batch 75 batch loss 0.662792146\n",
      "Validated batch 76 batch loss 0.587912202\n",
      "Validated batch 77 batch loss 0.545913517\n",
      "Validated batch 78 batch loss 0.571463227\n",
      "Validated batch 79 batch loss 0.608572721\n",
      "Validated batch 80 batch loss 0.693577528\n",
      "Validated batch 81 batch loss 0.4853459\n",
      "Validated batch 82 batch loss 0.556896091\n",
      "Validated batch 83 batch loss 0.573079228\n",
      "Validated batch 84 batch loss 0.692185\n",
      "Validated batch 85 batch loss 0.44952631\n",
      "Validated batch 86 batch loss 0.467838675\n",
      "Validated batch 87 batch loss 0.497557968\n",
      "Validated batch 88 batch loss 0.620483279\n",
      "Validated batch 89 batch loss 0.591061\n",
      "Validated batch 90 batch loss 0.623851955\n",
      "Validated batch 91 batch loss 0.4819749\n",
      "Validated batch 92 batch loss 0.554972768\n",
      "Validated batch 93 batch loss 0.503486872\n",
      "Validated batch 94 batch loss 0.586806059\n",
      "Validated batch 95 batch loss 0.578188062\n",
      "Validated batch 96 batch loss 0.491632342\n",
      "Validated batch 97 batch loss 0.627275527\n",
      "Validated batch 98 batch loss 0.484925866\n",
      "Validated batch 99 batch loss 0.488884479\n",
      "Validated batch 100 batch loss 0.568596065\n",
      "Validated batch 101 batch loss 0.567316771\n",
      "Validated batch 102 batch loss 0.486036\n",
      "Validated batch 103 batch loss 0.649800301\n",
      "Validated batch 104 batch loss 0.548407316\n",
      "Validated batch 105 batch loss 0.546382308\n",
      "Validated batch 106 batch loss 0.542441\n",
      "Validated batch 107 batch loss 0.511669219\n",
      "Validated batch 108 batch loss 0.457689166\n",
      "Validated batch 109 batch loss 0.510362923\n",
      "Validated batch 110 batch loss 0.516438603\n",
      "Validated batch 111 batch loss 0.607860625\n",
      "Validated batch 112 batch loss 0.470040292\n",
      "Validated batch 113 batch loss 0.531698048\n",
      "Validated batch 114 batch loss 0.482047707\n",
      "Validated batch 115 batch loss 0.657346666\n",
      "Validated batch 116 batch loss 0.448649466\n",
      "Validated batch 117 batch loss 0.514659345\n",
      "Validated batch 118 batch loss 0.595143855\n",
      "Validated batch 119 batch loss 0.51527071\n",
      "Validated batch 120 batch loss 0.551825762\n",
      "Validated batch 121 batch loss 0.612279654\n",
      "Validated batch 122 batch loss 0.594592452\n",
      "Validated batch 123 batch loss 0.569211364\n",
      "Validated batch 124 batch loss 0.601811767\n",
      "Validated batch 125 batch loss 0.510785\n",
      "Validated batch 126 batch loss 0.54194355\n",
      "Validated batch 127 batch loss 0.661493242\n",
      "Validated batch 128 batch loss 0.593139708\n",
      "Validated batch 129 batch loss 0.491125762\n",
      "Validated batch 130 batch loss 0.423632234\n",
      "Validated batch 131 batch loss 0.510453582\n",
      "Validated batch 132 batch loss 0.583779812\n",
      "Validated batch 133 batch loss 0.523510754\n",
      "Validated batch 134 batch loss 0.493710876\n",
      "Validated batch 135 batch loss 0.501984358\n",
      "Validated batch 136 batch loss 0.59293133\n",
      "Validated batch 137 batch loss 0.685536385\n",
      "Validated batch 138 batch loss 0.615221262\n",
      "Validated batch 139 batch loss 0.593855917\n",
      "Validated batch 140 batch loss 0.667256117\n",
      "Validated batch 141 batch loss 0.53925544\n",
      "Validated batch 142 batch loss 0.566202641\n",
      "Validated batch 143 batch loss 0.576233864\n",
      "Validated batch 144 batch loss 0.483469784\n",
      "Validated batch 145 batch loss 0.533821046\n",
      "Validated batch 146 batch loss 0.578902602\n",
      "Validated batch 147 batch loss 0.490953237\n",
      "Validated batch 148 batch loss 0.503006756\n",
      "Validated batch 149 batch loss 0.574188173\n",
      "Validated batch 150 batch loss 0.56934768\n",
      "Validated batch 151 batch loss 0.541715264\n",
      "Validated batch 152 batch loss 0.605542183\n",
      "Validated batch 153 batch loss 0.585461259\n",
      "Validated batch 154 batch loss 0.481007338\n",
      "Validated batch 155 batch loss 0.559998333\n",
      "Validated batch 156 batch loss 0.593969882\n",
      "Validated batch 157 batch loss 0.599414766\n",
      "Validated batch 158 batch loss 0.570119679\n",
      "Validated batch 159 batch loss 0.619757652\n",
      "Validated batch 160 batch loss 0.601229668\n",
      "Validated batch 161 batch loss 0.634456515\n",
      "Validated batch 162 batch loss 0.581937194\n",
      "Validated batch 163 batch loss 0.579875946\n",
      "Validated batch 164 batch loss 0.569735289\n",
      "Validated batch 165 batch loss 0.510722041\n",
      "Validated batch 166 batch loss 0.558543861\n",
      "Validated batch 167 batch loss 0.603338778\n",
      "Validated batch 168 batch loss 0.456502348\n",
      "Validated batch 169 batch loss 0.624827504\n",
      "Validated batch 170 batch loss 0.543512464\n",
      "Validated batch 171 batch loss 0.535725653\n",
      "Validated batch 172 batch loss 0.564481735\n",
      "Validated batch 173 batch loss 0.551447749\n",
      "Validated batch 174 batch loss 0.561644614\n",
      "Validated batch 175 batch loss 0.593687594\n",
      "Validated batch 176 batch loss 0.544370055\n",
      "Validated batch 177 batch loss 0.617408752\n",
      "Validated batch 178 batch loss 0.697681427\n",
      "Validated batch 179 batch loss 0.663869\n",
      "Validated batch 180 batch loss 0.526195288\n",
      "Validated batch 181 batch loss 0.545351744\n",
      "Validated batch 182 batch loss 0.577802181\n",
      "Validated batch 183 batch loss 0.529447317\n",
      "Validated batch 184 batch loss 0.52224493\n",
      "Validated batch 185 batch loss 0.463552475\n",
      "Validated batch 186 batch loss 0.567822039\n",
      "Validated batch 187 batch loss 0.47383225\n",
      "Validated batch 188 batch loss 0.495053709\n",
      "Validated batch 189 batch loss 0.591602385\n",
      "Validated batch 190 batch loss 0.553482234\n",
      "Validated batch 191 batch loss 0.576046467\n",
      "Validated batch 192 batch loss 0.502789497\n",
      "Validated batch 193 batch loss 0.563922942\n",
      "Validated batch 194 batch loss 0.542118907\n",
      "Validated batch 195 batch loss 0.589346409\n",
      "Validated batch 196 batch loss 0.554909647\n",
      "Validated batch 197 batch loss 0.546587944\n",
      "Validated batch 198 batch loss 0.569419861\n",
      "Validated batch 199 batch loss 0.562872887\n",
      "Validated batch 200 batch loss 0.569545507\n",
      "Validated batch 201 batch loss 0.599027693\n",
      "Validated batch 202 batch loss 0.562593818\n",
      "Validated batch 203 batch loss 0.503135145\n",
      "Validated batch 204 batch loss 0.510561049\n",
      "Validated batch 205 batch loss 0.663637638\n",
      "Validated batch 206 batch loss 0.507814407\n",
      "Validated batch 207 batch loss 0.623973966\n",
      "Validated batch 208 batch loss 0.52574122\n",
      "Validated batch 209 batch loss 0.494916946\n",
      "Validated batch 210 batch loss 0.581108093\n",
      "Validated batch 211 batch loss 0.589140654\n",
      "Validated batch 212 batch loss 0.52230823\n",
      "Validated batch 213 batch loss 0.636628449\n",
      "Validated batch 214 batch loss 0.547372878\n",
      "Validated batch 215 batch loss 0.504545867\n",
      "Validated batch 216 batch loss 0.5638026\n",
      "Validated batch 217 batch loss 0.638747633\n",
      "Validated batch 218 batch loss 0.642758965\n",
      "Validated batch 219 batch loss 0.515008092\n",
      "Validated batch 220 batch loss 0.638423\n",
      "Validated batch 221 batch loss 0.604561508\n",
      "Validated batch 222 batch loss 0.485711575\n",
      "Validated batch 223 batch loss 0.539214969\n",
      "Validated batch 224 batch loss 0.630027115\n",
      "Validated batch 225 batch loss 0.533472\n",
      "Validated batch 226 batch loss 0.594519496\n",
      "Validated batch 227 batch loss 0.582869\n",
      "Validated batch 228 batch loss 0.474512428\n",
      "Validated batch 229 batch loss 0.533626676\n",
      "Validated batch 230 batch loss 0.588734031\n",
      "Validated batch 231 batch loss 0.542393684\n",
      "Validated batch 232 batch loss 0.525855\n",
      "Validated batch 233 batch loss 0.564118683\n",
      "Validated batch 234 batch loss 0.54412812\n",
      "Validated batch 235 batch loss 0.548563\n",
      "Validated batch 236 batch loss 0.506655455\n",
      "Validated batch 237 batch loss 0.558529\n",
      "Validated batch 238 batch loss 0.57481873\n",
      "Validated batch 239 batch loss 0.588727117\n",
      "Validated batch 240 batch loss 0.641480803\n",
      "Validated batch 241 batch loss 0.672615528\n",
      "Validated batch 242 batch loss 0.581369221\n",
      "Validated batch 243 batch loss 0.550667405\n",
      "Validated batch 244 batch loss 0.557940066\n",
      "Validated batch 245 batch loss 0.506708\n",
      "Validated batch 246 batch loss 0.605714202\n",
      "Validated batch 247 batch loss 0.512706697\n",
      "Validated batch 248 batch loss 0.518924892\n",
      "Validated batch 249 batch loss 0.611470759\n",
      "Validated batch 250 batch loss 0.54706341\n",
      "Validated batch 251 batch loss 0.544247866\n",
      "Validated batch 252 batch loss 0.601566911\n",
      "Validated batch 253 batch loss 0.576438546\n",
      "Validated batch 254 batch loss 0.513907\n",
      "Validated batch 255 batch loss 0.571469903\n",
      "Validated batch 256 batch loss 0.654278398\n",
      "Validated batch 257 batch loss 0.660085201\n",
      "Validated batch 258 batch loss 0.52098614\n",
      "Validated batch 259 batch loss 0.582136154\n",
      "Validated batch 260 batch loss 0.641572237\n",
      "Validated batch 261 batch loss 0.535749733\n",
      "Validated batch 262 batch loss 0.613811\n",
      "Validated batch 263 batch loss 0.565558255\n",
      "Validated batch 264 batch loss 0.608411491\n",
      "Validated batch 265 batch loss 0.64177\n",
      "Validated batch 266 batch loss 0.428468466\n",
      "Validated batch 267 batch loss 0.508261919\n",
      "Validated batch 268 batch loss 0.608475149\n",
      "Validated batch 269 batch loss 0.60627377\n",
      "Validated batch 270 batch loss 0.532173038\n",
      "Validated batch 271 batch loss 0.510200322\n",
      "Validated batch 272 batch loss 0.583653808\n",
      "Validated batch 273 batch loss 0.619820952\n",
      "Validated batch 274 batch loss 0.529032409\n",
      "Validated batch 275 batch loss 0.564701796\n",
      "Validated batch 276 batch loss 0.520353198\n",
      "Validated batch 277 batch loss 0.622489\n",
      "Validated batch 278 batch loss 0.547086596\n",
      "Validated batch 279 batch loss 0.583459377\n",
      "Validated batch 280 batch loss 0.539748907\n",
      "Validated batch 281 batch loss 0.515211642\n",
      "Validated batch 282 batch loss 0.553160369\n",
      "Validated batch 283 batch loss 0.596373081\n",
      "Validated batch 284 batch loss 0.508563876\n",
      "Validated batch 285 batch loss 0.566907942\n",
      "Validated batch 286 batch loss 0.50965929\n",
      "Validated batch 287 batch loss 0.561111271\n",
      "Validated batch 288 batch loss 0.634107709\n",
      "Validated batch 289 batch loss 0.518453121\n",
      "Validated batch 290 batch loss 0.603183866\n",
      "Validated batch 291 batch loss 0.61068666\n",
      "Validated batch 292 batch loss 0.617772639\n",
      "Validated batch 293 batch loss 0.615777493\n",
      "Validated batch 294 batch loss 0.574799538\n",
      "Validated batch 295 batch loss 0.641835928\n",
      "Validated batch 296 batch loss 0.545184493\n",
      "Validated batch 297 batch loss 0.538535476\n",
      "Validated batch 298 batch loss 0.593164802\n",
      "Validated batch 299 batch loss 0.619360447\n",
      "Validated batch 300 batch loss 0.693841457\n",
      "Validated batch 301 batch loss 0.549349368\n",
      "Validated batch 302 batch loss 0.634744227\n",
      "Validated batch 303 batch loss 0.664726257\n",
      "Validated batch 304 batch loss 0.591829836\n",
      "Validated batch 305 batch loss 0.625980318\n",
      "Validated batch 306 batch loss 0.588100314\n",
      "Validated batch 307 batch loss 0.638390541\n",
      "Validated batch 308 batch loss 0.671847761\n",
      "Validated batch 309 batch loss 0.591562748\n",
      "Validated batch 310 batch loss 0.632216811\n",
      "Validated batch 311 batch loss 0.608734429\n",
      "Validated batch 312 batch loss 0.495748699\n",
      "Validated batch 313 batch loss 0.552334309\n",
      "Validated batch 314 batch loss 0.635846496\n",
      "Validated batch 315 batch loss 0.649564505\n",
      "Validated batch 316 batch loss 0.638501465\n",
      "Validated batch 317 batch loss 0.641716778\n",
      "Validated batch 318 batch loss 0.585689664\n",
      "Validated batch 319 batch loss 0.54196465\n",
      "Validated batch 320 batch loss 0.544031739\n",
      "Validated batch 321 batch loss 0.60009861\n",
      "Validated batch 322 batch loss 0.5632357\n",
      "Validated batch 323 batch loss 0.541283131\n",
      "Validated batch 324 batch loss 0.516894\n",
      "Validated batch 325 batch loss 0.531929255\n",
      "Validated batch 326 batch loss 0.533092797\n",
      "Validated batch 327 batch loss 0.592789769\n",
      "Validated batch 328 batch loss 0.605729103\n",
      "Validated batch 329 batch loss 0.569943964\n",
      "Validated batch 330 batch loss 0.530520737\n",
      "Validated batch 331 batch loss 0.548036933\n",
      "Validated batch 332 batch loss 0.582898676\n",
      "Validated batch 333 batch loss 0.640346646\n",
      "Validated batch 334 batch loss 0.663301051\n",
      "Validated batch 335 batch loss 0.54463315\n",
      "Validated batch 336 batch loss 0.441796362\n",
      "Validated batch 337 batch loss 0.584663749\n",
      "Validated batch 338 batch loss 0.568817675\n",
      "Validated batch 339 batch loss 0.548689902\n",
      "Validated batch 340 batch loss 0.564560831\n",
      "Validated batch 341 batch loss 0.568079472\n",
      "Validated batch 342 batch loss 0.606011152\n",
      "Validated batch 343 batch loss 0.588656902\n",
      "Validated batch 344 batch loss 0.582518458\n",
      "Validated batch 345 batch loss 0.520144105\n",
      "Validated batch 346 batch loss 0.561925054\n",
      "Validated batch 347 batch loss 0.406648397\n",
      "Validated batch 348 batch loss 0.455656171\n",
      "Validated batch 349 batch loss 0.619183421\n",
      "Validated batch 350 batch loss 0.553809345\n",
      "Validated batch 351 batch loss 0.484824419\n",
      "Validated batch 352 batch loss 0.528822958\n",
      "Validated batch 353 batch loss 0.521141291\n",
      "Validated batch 354 batch loss 0.561448932\n",
      "Validated batch 355 batch loss 0.616834342\n",
      "Validated batch 356 batch loss 0.618354201\n",
      "Validated batch 357 batch loss 0.538604915\n",
      "Validated batch 358 batch loss 0.47332707\n",
      "Validated batch 359 batch loss 0.591182768\n",
      "Validated batch 360 batch loss 0.575667739\n",
      "Validated batch 361 batch loss 0.579250276\n",
      "Validated batch 362 batch loss 0.675749958\n",
      "Validated batch 363 batch loss 0.551236928\n",
      "Validated batch 364 batch loss 0.64639\n",
      "Validated batch 365 batch loss 0.585695863\n",
      "Validated batch 366 batch loss 0.524380326\n",
      "Validated batch 367 batch loss 0.555396676\n",
      "Validated batch 368 batch loss 0.459220797\n",
      "Validated batch 369 batch loss 0.547485\n",
      "Epoch 8 val loss 0.5667731165885925\n",
      "Model /aiffel/mpii/weights/shg_model-epoch-8-loss-0.5668.h5 saved.\n",
      "Start epoch 9 with learning rate 0.0007\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 0.71712184 epoch total loss 0.71712184\n",
      "Trained batch 2 batch loss 0.615025759 epoch total loss 0.666073799\n",
      "Trained batch 3 batch loss 0.661919594 epoch total loss 0.664689064\n",
      "Trained batch 4 batch loss 0.603019834 epoch total loss 0.649271727\n",
      "Trained batch 5 batch loss 0.571038485 epoch total loss 0.63362509\n",
      "Trained batch 6 batch loss 0.53853339 epoch total loss 0.617776453\n",
      "Trained batch 7 batch loss 0.559695542 epoch total loss 0.609479249\n",
      "Trained batch 8 batch loss 0.615998387 epoch total loss 0.610294104\n",
      "Trained batch 9 batch loss 0.592482865 epoch total loss 0.60831511\n",
      "Trained batch 10 batch loss 0.607378304 epoch total loss 0.608221412\n",
      "Trained batch 11 batch loss 0.579926133 epoch total loss 0.605649114\n",
      "Trained batch 12 batch loss 0.549526274 epoch total loss 0.600972235\n",
      "Trained batch 13 batch loss 0.528236151 epoch total loss 0.595377088\n",
      "Trained batch 14 batch loss 0.561902523 epoch total loss 0.592986107\n",
      "Trained batch 15 batch loss 0.498784214 epoch total loss 0.586706\n",
      "Trained batch 16 batch loss 0.51832813 epoch total loss 0.58243233\n",
      "Trained batch 17 batch loss 0.60006243 epoch total loss 0.583469391\n",
      "Trained batch 18 batch loss 0.538621128 epoch total loss 0.580977798\n",
      "Trained batch 19 batch loss 0.531398 epoch total loss 0.578368366\n",
      "Trained batch 20 batch loss 0.594164133 epoch total loss 0.579158127\n",
      "Trained batch 21 batch loss 0.53877449 epoch total loss 0.577235103\n",
      "Trained batch 22 batch loss 0.51161921 epoch total loss 0.574252546\n",
      "Trained batch 23 batch loss 0.544683635 epoch total loss 0.572966933\n",
      "Trained batch 24 batch loss 0.551511645 epoch total loss 0.572073\n",
      "Trained batch 25 batch loss 0.569434583 epoch total loss 0.571967423\n",
      "Trained batch 26 batch loss 0.598924696 epoch total loss 0.573004246\n",
      "Trained batch 27 batch loss 0.58311069 epoch total loss 0.573378563\n",
      "Trained batch 28 batch loss 0.548690915 epoch total loss 0.572496891\n",
      "Trained batch 29 batch loss 0.500247359 epoch total loss 0.570005536\n",
      "Trained batch 30 batch loss 0.647209167 epoch total loss 0.572579\n",
      "Trained batch 31 batch loss 0.581011653 epoch total loss 0.572851\n",
      "Trained batch 32 batch loss 0.474702448 epoch total loss 0.569783866\n",
      "Trained batch 33 batch loss 0.410369307 epoch total loss 0.564953148\n",
      "Trained batch 34 batch loss 0.415085733 epoch total loss 0.560545266\n",
      "Trained batch 35 batch loss 0.483230263 epoch total loss 0.558336258\n",
      "Trained batch 36 batch loss 0.4918589 epoch total loss 0.556489706\n",
      "Trained batch 37 batch loss 0.432768822 epoch total loss 0.553145826\n",
      "Trained batch 38 batch loss 0.611905 epoch total loss 0.55469209\n",
      "Trained batch 39 batch loss 0.560664 epoch total loss 0.554845214\n",
      "Trained batch 40 batch loss 0.617344 epoch total loss 0.55640769\n",
      "Trained batch 41 batch loss 0.553037345 epoch total loss 0.556325495\n",
      "Trained batch 42 batch loss 0.471241057 epoch total loss 0.554299653\n",
      "Trained batch 43 batch loss 0.493932605 epoch total loss 0.552895784\n",
      "Trained batch 44 batch loss 0.49277693 epoch total loss 0.551529467\n",
      "Trained batch 45 batch loss 0.488533646 epoch total loss 0.550129533\n",
      "Trained batch 46 batch loss 0.439544708 epoch total loss 0.547725499\n",
      "Trained batch 47 batch loss 0.442357212 epoch total loss 0.545483649\n",
      "Trained batch 48 batch loss 0.456438422 epoch total loss 0.543628514\n",
      "Trained batch 49 batch loss 0.407621 epoch total loss 0.540852904\n",
      "Trained batch 50 batch loss 0.431843638 epoch total loss 0.538672686\n",
      "Trained batch 51 batch loss 0.506211281 epoch total loss 0.538036168\n",
      "Trained batch 52 batch loss 0.576294422 epoch total loss 0.538771927\n",
      "Trained batch 53 batch loss 0.608893037 epoch total loss 0.540095\n",
      "Trained batch 54 batch loss 0.536061466 epoch total loss 0.540020287\n",
      "Trained batch 55 batch loss 0.555394888 epoch total loss 0.540299833\n",
      "Trained batch 56 batch loss 0.577109098 epoch total loss 0.540957093\n",
      "Trained batch 57 batch loss 0.518314719 epoch total loss 0.540559888\n",
      "Trained batch 58 batch loss 0.533409 epoch total loss 0.540436566\n",
      "Trained batch 59 batch loss 0.536282301 epoch total loss 0.540366173\n",
      "Trained batch 60 batch loss 0.457271 epoch total loss 0.538981259\n",
      "Trained batch 61 batch loss 0.415969759 epoch total loss 0.536964655\n",
      "Trained batch 62 batch loss 0.512076616 epoch total loss 0.536563277\n",
      "Trained batch 63 batch loss 0.463189721 epoch total loss 0.535398602\n",
      "Trained batch 64 batch loss 0.561649442 epoch total loss 0.535808742\n",
      "Trained batch 65 batch loss 0.469388664 epoch total loss 0.53478688\n",
      "Trained batch 66 batch loss 0.463239759 epoch total loss 0.53370285\n",
      "Trained batch 67 batch loss 0.499739915 epoch total loss 0.533196\n",
      "Trained batch 68 batch loss 0.499392748 epoch total loss 0.53269887\n",
      "Trained batch 69 batch loss 0.600503802 epoch total loss 0.533681512\n",
      "Trained batch 70 batch loss 0.546887934 epoch total loss 0.533870161\n",
      "Trained batch 71 batch loss 0.561291 epoch total loss 0.534256339\n",
      "Trained batch 72 batch loss 0.583170295 epoch total loss 0.534935713\n",
      "Trained batch 73 batch loss 0.57022208 epoch total loss 0.535419106\n",
      "Trained batch 74 batch loss 0.527860641 epoch total loss 0.535317\n",
      "Trained batch 75 batch loss 0.518895268 epoch total loss 0.535098\n",
      "Trained batch 76 batch loss 0.494018078 epoch total loss 0.534557521\n",
      "Trained batch 77 batch loss 0.562881649 epoch total loss 0.534925342\n",
      "Trained batch 78 batch loss 0.534369111 epoch total loss 0.534918249\n",
      "Trained batch 79 batch loss 0.53877008 epoch total loss 0.534966946\n",
      "Trained batch 80 batch loss 0.433830887 epoch total loss 0.533702731\n",
      "Trained batch 81 batch loss 0.592039227 epoch total loss 0.534423\n",
      "Trained batch 82 batch loss 0.509040654 epoch total loss 0.534113467\n",
      "Trained batch 83 batch loss 0.539927125 epoch total loss 0.534183502\n",
      "Trained batch 84 batch loss 0.470312715 epoch total loss 0.533423126\n",
      "Trained batch 85 batch loss 0.586469889 epoch total loss 0.534047246\n",
      "Trained batch 86 batch loss 0.511597276 epoch total loss 0.533786178\n",
      "Trained batch 87 batch loss 0.587093711 epoch total loss 0.534398913\n",
      "Trained batch 88 batch loss 0.578877 epoch total loss 0.534904361\n",
      "Trained batch 89 batch loss 0.591303706 epoch total loss 0.535538077\n",
      "Trained batch 90 batch loss 0.531324267 epoch total loss 0.535491228\n",
      "Trained batch 91 batch loss 0.580381155 epoch total loss 0.535984516\n",
      "Trained batch 92 batch loss 0.606535792 epoch total loss 0.53675139\n",
      "Trained batch 93 batch loss 0.56261307 epoch total loss 0.537029445\n",
      "Trained batch 94 batch loss 0.597698331 epoch total loss 0.537674904\n",
      "Trained batch 95 batch loss 0.569800436 epoch total loss 0.538013041\n",
      "Trained batch 96 batch loss 0.559543371 epoch total loss 0.538237333\n",
      "Trained batch 97 batch loss 0.595135152 epoch total loss 0.538823903\n",
      "Trained batch 98 batch loss 0.627777278 epoch total loss 0.539731562\n",
      "Trained batch 99 batch loss 0.592248261 epoch total loss 0.540262043\n",
      "Trained batch 100 batch loss 0.425786585 epoch total loss 0.539117277\n",
      "Trained batch 101 batch loss 0.421658218 epoch total loss 0.53795433\n",
      "Trained batch 102 batch loss 0.509178877 epoch total loss 0.537672222\n",
      "Trained batch 103 batch loss 0.569547 epoch total loss 0.537981629\n",
      "Trained batch 104 batch loss 0.428483069 epoch total loss 0.536928773\n",
      "Trained batch 105 batch loss 0.447563052 epoch total loss 0.536077678\n",
      "Trained batch 106 batch loss 0.403478563 epoch total loss 0.534826696\n",
      "Trained batch 107 batch loss 0.535904169 epoch total loss 0.534836769\n",
      "Trained batch 108 batch loss 0.403509378 epoch total loss 0.533620775\n",
      "Trained batch 109 batch loss 0.391390204 epoch total loss 0.532315969\n",
      "Trained batch 110 batch loss 0.503772497 epoch total loss 0.532056451\n",
      "Trained batch 111 batch loss 0.548757732 epoch total loss 0.532206953\n",
      "Trained batch 112 batch loss 0.431560099 epoch total loss 0.531308293\n",
      "Trained batch 113 batch loss 0.381902874 epoch total loss 0.529986143\n",
      "Trained batch 114 batch loss 0.363270462 epoch total loss 0.528523743\n",
      "Trained batch 115 batch loss 0.336042464 epoch total loss 0.52685\n",
      "Trained batch 116 batch loss 0.378770709 epoch total loss 0.525573432\n",
      "Trained batch 117 batch loss 0.418248236 epoch total loss 0.524656117\n",
      "Trained batch 118 batch loss 0.366802841 epoch total loss 0.52331835\n",
      "Trained batch 119 batch loss 0.363126934 epoch total loss 0.521972239\n",
      "Trained batch 120 batch loss 0.443364233 epoch total loss 0.521317184\n",
      "Trained batch 121 batch loss 0.520861208 epoch total loss 0.521313429\n",
      "Trained batch 122 batch loss 0.55261445 epoch total loss 0.521569967\n",
      "Trained batch 123 batch loss 0.460233867 epoch total loss 0.521071315\n",
      "Trained batch 124 batch loss 0.505467594 epoch total loss 0.520945489\n",
      "Trained batch 125 batch loss 0.504326046 epoch total loss 0.520812571\n",
      "Trained batch 126 batch loss 0.483217061 epoch total loss 0.52051419\n",
      "Trained batch 127 batch loss 0.597609639 epoch total loss 0.521121204\n",
      "Trained batch 128 batch loss 0.443688929 epoch total loss 0.520516276\n",
      "Trained batch 129 batch loss 0.546795726 epoch total loss 0.52072\n",
      "Trained batch 130 batch loss 0.514230847 epoch total loss 0.520670056\n",
      "Trained batch 131 batch loss 0.573803782 epoch total loss 0.521075726\n",
      "Trained batch 132 batch loss 0.543828607 epoch total loss 0.521248102\n",
      "Trained batch 133 batch loss 0.480127513 epoch total loss 0.520938873\n",
      "Trained batch 134 batch loss 0.521786332 epoch total loss 0.520945251\n",
      "Trained batch 135 batch loss 0.506744504 epoch total loss 0.520840049\n",
      "Trained batch 136 batch loss 0.537230551 epoch total loss 0.520960569\n",
      "Trained batch 137 batch loss 0.56321311 epoch total loss 0.521268964\n",
      "Trained batch 138 batch loss 0.550926268 epoch total loss 0.521483898\n",
      "Trained batch 139 batch loss 0.496241301 epoch total loss 0.521302283\n",
      "Trained batch 140 batch loss 0.457376063 epoch total loss 0.520845652\n",
      "Trained batch 141 batch loss 0.498302758 epoch total loss 0.520685792\n",
      "Trained batch 142 batch loss 0.516477227 epoch total loss 0.520656168\n",
      "Trained batch 143 batch loss 0.515646696 epoch total loss 0.520621121\n",
      "Trained batch 144 batch loss 0.429801255 epoch total loss 0.519990444\n",
      "Trained batch 145 batch loss 0.526331663 epoch total loss 0.520034134\n",
      "Trained batch 146 batch loss 0.552414358 epoch total loss 0.520255923\n",
      "Trained batch 147 batch loss 0.431827575 epoch total loss 0.519654393\n",
      "Trained batch 148 batch loss 0.420052826 epoch total loss 0.518981397\n",
      "Trained batch 149 batch loss 0.419331729 epoch total loss 0.518312633\n",
      "Trained batch 150 batch loss 0.40585646 epoch total loss 0.517562926\n",
      "Trained batch 151 batch loss 0.412432373 epoch total loss 0.516866684\n",
      "Trained batch 152 batch loss 0.45408386 epoch total loss 0.516453624\n",
      "Trained batch 153 batch loss 0.457025886 epoch total loss 0.51606524\n",
      "Trained batch 154 batch loss 0.470734 epoch total loss 0.515770853\n",
      "Trained batch 155 batch loss 0.550608456 epoch total loss 0.515995562\n",
      "Trained batch 156 batch loss 0.42891565 epoch total loss 0.515437424\n",
      "Trained batch 157 batch loss 0.425514758 epoch total loss 0.514864624\n",
      "Trained batch 158 batch loss 0.467754841 epoch total loss 0.514566481\n",
      "Trained batch 159 batch loss 0.482734948 epoch total loss 0.514366269\n",
      "Trained batch 160 batch loss 0.475018322 epoch total loss 0.5141204\n",
      "Trained batch 161 batch loss 0.556191862 epoch total loss 0.514381707\n",
      "Trained batch 162 batch loss 0.445012093 epoch total loss 0.513953507\n",
      "Trained batch 163 batch loss 0.431120157 epoch total loss 0.513445318\n",
      "Trained batch 164 batch loss 0.453503788 epoch total loss 0.513079882\n",
      "Trained batch 165 batch loss 0.581257463 epoch total loss 0.513493061\n",
      "Trained batch 166 batch loss 0.523798585 epoch total loss 0.51355511\n",
      "Trained batch 167 batch loss 0.604445219 epoch total loss 0.514099419\n",
      "Trained batch 168 batch loss 0.563712776 epoch total loss 0.514394701\n",
      "Trained batch 169 batch loss 0.556266129 epoch total loss 0.514642477\n",
      "Trained batch 170 batch loss 0.557534218 epoch total loss 0.514894783\n",
      "Trained batch 171 batch loss 0.561979711 epoch total loss 0.515170157\n",
      "Trained batch 172 batch loss 0.520659506 epoch total loss 0.515202045\n",
      "Trained batch 173 batch loss 0.625336409 epoch total loss 0.515838683\n",
      "Trained batch 174 batch loss 0.526974559 epoch total loss 0.515902698\n",
      "Trained batch 175 batch loss 0.51869911 epoch total loss 0.515918672\n",
      "Trained batch 176 batch loss 0.606601238 epoch total loss 0.516433895\n",
      "Trained batch 177 batch loss 0.518962801 epoch total loss 0.51644814\n",
      "Trained batch 178 batch loss 0.479195058 epoch total loss 0.516238868\n",
      "Trained batch 179 batch loss 0.51915431 epoch total loss 0.5162552\n",
      "Trained batch 180 batch loss 0.463705897 epoch total loss 0.515963256\n",
      "Trained batch 181 batch loss 0.52076745 epoch total loss 0.51598978\n",
      "Trained batch 182 batch loss 0.567041576 epoch total loss 0.51627028\n",
      "Trained batch 183 batch loss 0.569788337 epoch total loss 0.5165627\n",
      "Trained batch 184 batch loss 0.53820622 epoch total loss 0.51668036\n",
      "Trained batch 185 batch loss 0.52599144 epoch total loss 0.516730666\n",
      "Trained batch 186 batch loss 0.556322157 epoch total loss 0.516943514\n",
      "Trained batch 187 batch loss 0.558352828 epoch total loss 0.517164946\n",
      "Trained batch 188 batch loss 0.462773949 epoch total loss 0.516875625\n",
      "Trained batch 189 batch loss 0.57883656 epoch total loss 0.51720345\n",
      "Trained batch 190 batch loss 0.585933089 epoch total loss 0.517565191\n",
      "Trained batch 191 batch loss 0.555962443 epoch total loss 0.517766237\n",
      "Trained batch 192 batch loss 0.517504692 epoch total loss 0.517764866\n",
      "Trained batch 193 batch loss 0.537437 epoch total loss 0.51786679\n",
      "Trained batch 194 batch loss 0.547423482 epoch total loss 0.51801914\n",
      "Trained batch 195 batch loss 0.487479627 epoch total loss 0.517862499\n",
      "Trained batch 196 batch loss 0.509337306 epoch total loss 0.517819047\n",
      "Trained batch 197 batch loss 0.573079 epoch total loss 0.518099546\n",
      "Trained batch 198 batch loss 0.499359787 epoch total loss 0.518004894\n",
      "Trained batch 199 batch loss 0.592706501 epoch total loss 0.518380284\n",
      "Trained batch 200 batch loss 0.622110605 epoch total loss 0.518898904\n",
      "Trained batch 201 batch loss 0.463773042 epoch total loss 0.518624663\n",
      "Trained batch 202 batch loss 0.472835332 epoch total loss 0.518398\n",
      "Trained batch 203 batch loss 0.456316441 epoch total loss 0.518092155\n",
      "Trained batch 204 batch loss 0.441679686 epoch total loss 0.5177176\n",
      "Trained batch 205 batch loss 0.406695634 epoch total loss 0.517176\n",
      "Trained batch 206 batch loss 0.434713185 epoch total loss 0.516775727\n",
      "Trained batch 207 batch loss 0.42021811 epoch total loss 0.516309261\n",
      "Trained batch 208 batch loss 0.427388161 epoch total loss 0.515881777\n",
      "Trained batch 209 batch loss 0.39776957 epoch total loss 0.515316606\n",
      "Trained batch 210 batch loss 0.552369356 epoch total loss 0.515493035\n",
      "Trained batch 211 batch loss 0.531560719 epoch total loss 0.51556921\n",
      "Trained batch 212 batch loss 0.494375587 epoch total loss 0.515469253\n",
      "Trained batch 213 batch loss 0.470667601 epoch total loss 0.515258908\n",
      "Trained batch 214 batch loss 0.426272273 epoch total loss 0.514843047\n",
      "Trained batch 215 batch loss 0.385435402 epoch total loss 0.514241159\n",
      "Trained batch 216 batch loss 0.460943699 epoch total loss 0.513994455\n",
      "Trained batch 217 batch loss 0.497549981 epoch total loss 0.513918638\n",
      "Trained batch 218 batch loss 0.439927667 epoch total loss 0.513579249\n",
      "Trained batch 219 batch loss 0.431387246 epoch total loss 0.513203919\n",
      "Trained batch 220 batch loss 0.49678883 epoch total loss 0.513129294\n",
      "Trained batch 221 batch loss 0.502147496 epoch total loss 0.513079584\n",
      "Trained batch 222 batch loss 0.502573252 epoch total loss 0.513032258\n",
      "Trained batch 223 batch loss 0.542092741 epoch total loss 0.513162613\n",
      "Trained batch 224 batch loss 0.44721356 epoch total loss 0.512868166\n",
      "Trained batch 225 batch loss 0.5438658 epoch total loss 0.513006\n",
      "Trained batch 226 batch loss 0.537971616 epoch total loss 0.513116419\n",
      "Trained batch 227 batch loss 0.641935945 epoch total loss 0.513683915\n",
      "Trained batch 228 batch loss 0.598443508 epoch total loss 0.514055669\n",
      "Trained batch 229 batch loss 0.563256 epoch total loss 0.514270484\n",
      "Trained batch 230 batch loss 0.563876748 epoch total loss 0.514486134\n",
      "Trained batch 231 batch loss 0.496494889 epoch total loss 0.51440829\n",
      "Trained batch 232 batch loss 0.546940327 epoch total loss 0.51454854\n",
      "Trained batch 233 batch loss 0.600117147 epoch total loss 0.514915764\n",
      "Trained batch 234 batch loss 0.588527322 epoch total loss 0.515230358\n",
      "Trained batch 235 batch loss 0.593862712 epoch total loss 0.515565\n",
      "Trained batch 236 batch loss 0.511398 epoch total loss 0.515547335\n",
      "Trained batch 237 batch loss 0.528765798 epoch total loss 0.515603065\n",
      "Trained batch 238 batch loss 0.588693619 epoch total loss 0.515910149\n",
      "Trained batch 239 batch loss 0.68514359 epoch total loss 0.516618252\n",
      "Trained batch 240 batch loss 0.582974732 epoch total loss 0.516894758\n",
      "Trained batch 241 batch loss 0.491973191 epoch total loss 0.516791344\n",
      "Trained batch 242 batch loss 0.523376703 epoch total loss 0.516818583\n",
      "Trained batch 243 batch loss 0.504917383 epoch total loss 0.516769588\n",
      "Trained batch 244 batch loss 0.525373697 epoch total loss 0.516804874\n",
      "Trained batch 245 batch loss 0.53382957 epoch total loss 0.516874373\n",
      "Trained batch 246 batch loss 0.520903468 epoch total loss 0.516890764\n",
      "Trained batch 247 batch loss 0.556686 epoch total loss 0.517051876\n",
      "Trained batch 248 batch loss 0.57599932 epoch total loss 0.517289579\n",
      "Trained batch 249 batch loss 0.558084 epoch total loss 0.517453432\n",
      "Trained batch 250 batch loss 0.595314 epoch total loss 0.517764866\n",
      "Trained batch 251 batch loss 0.62914288 epoch total loss 0.518208683\n",
      "Trained batch 252 batch loss 0.65093708 epoch total loss 0.518735349\n",
      "Trained batch 253 batch loss 0.550135374 epoch total loss 0.518859506\n",
      "Trained batch 254 batch loss 0.608406067 epoch total loss 0.519212\n",
      "Trained batch 255 batch loss 0.551002443 epoch total loss 0.519336641\n",
      "Trained batch 256 batch loss 0.479491532 epoch total loss 0.519181\n",
      "Trained batch 257 batch loss 0.496075124 epoch total loss 0.519091129\n",
      "Trained batch 258 batch loss 0.510093927 epoch total loss 0.519056261\n",
      "Trained batch 259 batch loss 0.54905057 epoch total loss 0.519172132\n",
      "Trained batch 260 batch loss 0.502651811 epoch total loss 0.519108593\n",
      "Trained batch 261 batch loss 0.558378 epoch total loss 0.519259036\n",
      "Trained batch 262 batch loss 0.564868152 epoch total loss 0.519433141\n",
      "Trained batch 263 batch loss 0.515356839 epoch total loss 0.519417584\n",
      "Trained batch 264 batch loss 0.54142946 epoch total loss 0.519501\n",
      "Trained batch 265 batch loss 0.621720195 epoch total loss 0.519886672\n",
      "Trained batch 266 batch loss 0.617036223 epoch total loss 0.52025193\n",
      "Trained batch 267 batch loss 0.640201926 epoch total loss 0.52070117\n",
      "Trained batch 268 batch loss 0.567179859 epoch total loss 0.520874619\n",
      "Trained batch 269 batch loss 0.522016168 epoch total loss 0.520878851\n",
      "Trained batch 270 batch loss 0.542512357 epoch total loss 0.52095896\n",
      "Trained batch 271 batch loss 0.58771354 epoch total loss 0.521205246\n",
      "Trained batch 272 batch loss 0.617056966 epoch total loss 0.521557629\n",
      "Trained batch 273 batch loss 0.616061866 epoch total loss 0.521903813\n",
      "Trained batch 274 batch loss 0.544686198 epoch total loss 0.521986961\n",
      "Trained batch 275 batch loss 0.600625634 epoch total loss 0.522272944\n",
      "Trained batch 276 batch loss 0.578681886 epoch total loss 0.522477329\n",
      "Trained batch 277 batch loss 0.586254478 epoch total loss 0.522707582\n",
      "Trained batch 278 batch loss 0.601614118 epoch total loss 0.522991359\n",
      "Trained batch 279 batch loss 0.55357343 epoch total loss 0.523101\n",
      "Trained batch 280 batch loss 0.562288284 epoch total loss 0.523240924\n",
      "Trained batch 281 batch loss 0.537764311 epoch total loss 0.523292601\n",
      "Trained batch 282 batch loss 0.493904 epoch total loss 0.523188353\n",
      "Trained batch 283 batch loss 0.595009148 epoch total loss 0.523442209\n",
      "Trained batch 284 batch loss 0.595749438 epoch total loss 0.52369678\n",
      "Trained batch 285 batch loss 0.601323664 epoch total loss 0.523969173\n",
      "Trained batch 286 batch loss 0.610969365 epoch total loss 0.524273336\n",
      "Trained batch 287 batch loss 0.563881934 epoch total loss 0.52441138\n",
      "Trained batch 288 batch loss 0.555683255 epoch total loss 0.52451992\n",
      "Trained batch 289 batch loss 0.542101741 epoch total loss 0.524580777\n",
      "Trained batch 290 batch loss 0.575811744 epoch total loss 0.524757385\n",
      "Trained batch 291 batch loss 0.504260957 epoch total loss 0.524686933\n",
      "Trained batch 292 batch loss 0.506874442 epoch total loss 0.524625957\n",
      "Trained batch 293 batch loss 0.377094775 epoch total loss 0.524122417\n",
      "Trained batch 294 batch loss 0.423360854 epoch total loss 0.52377969\n",
      "Trained batch 295 batch loss 0.492160797 epoch total loss 0.523672462\n",
      "Trained batch 296 batch loss 0.51230526 epoch total loss 0.523634076\n",
      "Trained batch 297 batch loss 0.535415113 epoch total loss 0.523673713\n",
      "Trained batch 298 batch loss 0.527595162 epoch total loss 0.523686886\n",
      "Trained batch 299 batch loss 0.535084367 epoch total loss 0.523725\n",
      "Trained batch 300 batch loss 0.514397502 epoch total loss 0.523693919\n",
      "Trained batch 301 batch loss 0.632305443 epoch total loss 0.524054766\n",
      "Trained batch 302 batch loss 0.559432864 epoch total loss 0.524171889\n",
      "Trained batch 303 batch loss 0.671586037 epoch total loss 0.524658382\n",
      "Trained batch 304 batch loss 0.660447359 epoch total loss 0.525105059\n",
      "Trained batch 305 batch loss 0.524702191 epoch total loss 0.525103748\n",
      "Trained batch 306 batch loss 0.611239433 epoch total loss 0.525385261\n",
      "Trained batch 307 batch loss 0.582310498 epoch total loss 0.525570631\n",
      "Trained batch 308 batch loss 0.614401042 epoch total loss 0.525859058\n",
      "Trained batch 309 batch loss 0.480954111 epoch total loss 0.525713742\n",
      "Trained batch 310 batch loss 0.493941575 epoch total loss 0.525611222\n",
      "Trained batch 311 batch loss 0.408949196 epoch total loss 0.52523613\n",
      "Trained batch 312 batch loss 0.442747205 epoch total loss 0.524971724\n",
      "Trained batch 313 batch loss 0.412468553 epoch total loss 0.524612308\n",
      "Trained batch 314 batch loss 0.485223979 epoch total loss 0.524486899\n",
      "Trained batch 315 batch loss 0.478663743 epoch total loss 0.524341464\n",
      "Trained batch 316 batch loss 0.684209406 epoch total loss 0.524847329\n",
      "Trained batch 317 batch loss 0.760123193 epoch total loss 0.525589526\n",
      "Trained batch 318 batch loss 0.554201245 epoch total loss 0.525679469\n",
      "Trained batch 319 batch loss 0.552539766 epoch total loss 0.52576369\n",
      "Trained batch 320 batch loss 0.453450471 epoch total loss 0.52553767\n",
      "Trained batch 321 batch loss 0.511988461 epoch total loss 0.52549547\n",
      "Trained batch 322 batch loss 0.468029797 epoch total loss 0.525317\n",
      "Trained batch 323 batch loss 0.502737403 epoch total loss 0.525247097\n",
      "Trained batch 324 batch loss 0.476194203 epoch total loss 0.525095701\n",
      "Trained batch 325 batch loss 0.499263942 epoch total loss 0.525016248\n",
      "Trained batch 326 batch loss 0.530519485 epoch total loss 0.525033116\n",
      "Trained batch 327 batch loss 0.518915296 epoch total loss 0.5250144\n",
      "Trained batch 328 batch loss 0.506523669 epoch total loss 0.524958074\n",
      "Trained batch 329 batch loss 0.412075937 epoch total loss 0.524615\n",
      "Trained batch 330 batch loss 0.50437 epoch total loss 0.524553597\n",
      "Trained batch 331 batch loss 0.498317093 epoch total loss 0.524474382\n",
      "Trained batch 332 batch loss 0.553024471 epoch total loss 0.524560332\n",
      "Trained batch 333 batch loss 0.489238501 epoch total loss 0.524454296\n",
      "Trained batch 334 batch loss 0.566315114 epoch total loss 0.524579644\n",
      "Trained batch 335 batch loss 0.564630687 epoch total loss 0.524699211\n",
      "Trained batch 336 batch loss 0.508801818 epoch total loss 0.524651885\n",
      "Trained batch 337 batch loss 0.553986788 epoch total loss 0.524738908\n",
      "Trained batch 338 batch loss 0.594161272 epoch total loss 0.524944305\n",
      "Trained batch 339 batch loss 0.561403871 epoch total loss 0.525051892\n",
      "Trained batch 340 batch loss 0.591514528 epoch total loss 0.525247335\n",
      "Trained batch 341 batch loss 0.422830194 epoch total loss 0.524947\n",
      "Trained batch 342 batch loss 0.436639607 epoch total loss 0.52468878\n",
      "Trained batch 343 batch loss 0.492408574 epoch total loss 0.524594665\n",
      "Trained batch 344 batch loss 0.573504508 epoch total loss 0.524736822\n",
      "Trained batch 345 batch loss 0.544090927 epoch total loss 0.524792969\n",
      "Trained batch 346 batch loss 0.476693 epoch total loss 0.524654\n",
      "Trained batch 347 batch loss 0.512635946 epoch total loss 0.524619341\n",
      "Trained batch 348 batch loss 0.528973103 epoch total loss 0.524631858\n",
      "Trained batch 349 batch loss 0.589346409 epoch total loss 0.524817288\n",
      "Trained batch 350 batch loss 0.553529441 epoch total loss 0.524899304\n",
      "Trained batch 351 batch loss 0.484999716 epoch total loss 0.524785638\n",
      "Trained batch 352 batch loss 0.506869555 epoch total loss 0.524734735\n",
      "Trained batch 353 batch loss 0.510369182 epoch total loss 0.524694\n",
      "Trained batch 354 batch loss 0.550884366 epoch total loss 0.524768054\n",
      "Trained batch 355 batch loss 0.5696401 epoch total loss 0.524894416\n",
      "Trained batch 356 batch loss 0.55267489 epoch total loss 0.524972439\n",
      "Trained batch 357 batch loss 0.504324138 epoch total loss 0.524914622\n",
      "Trained batch 358 batch loss 0.526276886 epoch total loss 0.524918437\n",
      "Trained batch 359 batch loss 0.532439828 epoch total loss 0.524939358\n",
      "Trained batch 360 batch loss 0.442182273 epoch total loss 0.524709463\n",
      "Trained batch 361 batch loss 0.484965771 epoch total loss 0.524599433\n",
      "Trained batch 362 batch loss 0.496623278 epoch total loss 0.524522126\n",
      "Trained batch 363 batch loss 0.49853611 epoch total loss 0.524450541\n",
      "Trained batch 364 batch loss 0.468653083 epoch total loss 0.524297297\n",
      "Trained batch 365 batch loss 0.417071313 epoch total loss 0.524003506\n",
      "Trained batch 366 batch loss 0.459657162 epoch total loss 0.523827672\n",
      "Trained batch 367 batch loss 0.454072356 epoch total loss 0.523637593\n",
      "Trained batch 368 batch loss 0.518475115 epoch total loss 0.523623586\n",
      "Trained batch 369 batch loss 0.591983199 epoch total loss 0.523808837\n",
      "Trained batch 370 batch loss 0.575572789 epoch total loss 0.523948729\n",
      "Trained batch 371 batch loss 0.557581723 epoch total loss 0.524039447\n",
      "Trained batch 372 batch loss 0.558047175 epoch total loss 0.524130821\n",
      "Trained batch 373 batch loss 0.475003719 epoch total loss 0.523999155\n",
      "Trained batch 374 batch loss 0.47620827 epoch total loss 0.523871362\n",
      "Trained batch 375 batch loss 0.516660929 epoch total loss 0.52385211\n",
      "Trained batch 376 batch loss 0.568841934 epoch total loss 0.523971796\n",
      "Trained batch 377 batch loss 0.531888723 epoch total loss 0.523992777\n",
      "Trained batch 378 batch loss 0.538670778 epoch total loss 0.524031639\n",
      "Trained batch 379 batch loss 0.554512262 epoch total loss 0.524112046\n",
      "Trained batch 380 batch loss 0.489297628 epoch total loss 0.524020493\n",
      "Trained batch 381 batch loss 0.475154787 epoch total loss 0.523892224\n",
      "Trained batch 382 batch loss 0.493225396 epoch total loss 0.523811936\n",
      "Trained batch 383 batch loss 0.482874453 epoch total loss 0.523705065\n",
      "Trained batch 384 batch loss 0.47753638 epoch total loss 0.523584843\n",
      "Trained batch 385 batch loss 0.512111425 epoch total loss 0.52355504\n",
      "Trained batch 386 batch loss 0.439574897 epoch total loss 0.523337483\n",
      "Trained batch 387 batch loss 0.487749815 epoch total loss 0.523245513\n",
      "Trained batch 388 batch loss 0.46648258 epoch total loss 0.523099184\n",
      "Trained batch 389 batch loss 0.479044437 epoch total loss 0.522986\n",
      "Trained batch 390 batch loss 0.533653259 epoch total loss 0.523013294\n",
      "Trained batch 391 batch loss 0.494871825 epoch total loss 0.522941351\n",
      "Trained batch 392 batch loss 0.530911565 epoch total loss 0.522961676\n",
      "Trained batch 393 batch loss 0.604925454 epoch total loss 0.523170233\n",
      "Trained batch 394 batch loss 0.566159189 epoch total loss 0.523279309\n",
      "Trained batch 395 batch loss 0.551003754 epoch total loss 0.523349524\n",
      "Trained batch 396 batch loss 0.518620968 epoch total loss 0.523337603\n",
      "Trained batch 397 batch loss 0.483272731 epoch total loss 0.523236692\n",
      "Trained batch 398 batch loss 0.446097 epoch total loss 0.523042858\n",
      "Trained batch 399 batch loss 0.443505377 epoch total loss 0.52284354\n",
      "Trained batch 400 batch loss 0.517520607 epoch total loss 0.522830188\n",
      "Trained batch 401 batch loss 0.515743494 epoch total loss 0.522812545\n",
      "Trained batch 402 batch loss 0.544645846 epoch total loss 0.522866845\n",
      "Trained batch 403 batch loss 0.527910113 epoch total loss 0.522879362\n",
      "Trained batch 404 batch loss 0.501967609 epoch total loss 0.522827625\n",
      "Trained batch 405 batch loss 0.676763058 epoch total loss 0.523207664\n",
      "Trained batch 406 batch loss 0.544231832 epoch total loss 0.523259461\n",
      "Trained batch 407 batch loss 0.573890209 epoch total loss 0.523383856\n",
      "Trained batch 408 batch loss 0.512854755 epoch total loss 0.523358047\n",
      "Trained batch 409 batch loss 0.580270052 epoch total loss 0.523497164\n",
      "Trained batch 410 batch loss 0.502409637 epoch total loss 0.523445785\n",
      "Trained batch 411 batch loss 0.45613569 epoch total loss 0.523282\n",
      "Trained batch 412 batch loss 0.48753792 epoch total loss 0.523195207\n",
      "Trained batch 413 batch loss 0.549883306 epoch total loss 0.523259819\n",
      "Trained batch 414 batch loss 0.568756521 epoch total loss 0.52336973\n",
      "Trained batch 415 batch loss 0.56136173 epoch total loss 0.523461223\n",
      "Trained batch 416 batch loss 0.548845649 epoch total loss 0.523522258\n",
      "Trained batch 417 batch loss 0.566328526 epoch total loss 0.523624897\n",
      "Trained batch 418 batch loss 0.532427847 epoch total loss 0.523646\n",
      "Trained batch 419 batch loss 0.595614672 epoch total loss 0.523817718\n",
      "Trained batch 420 batch loss 0.57172513 epoch total loss 0.523931801\n",
      "Trained batch 421 batch loss 0.547603488 epoch total loss 0.523988068\n",
      "Trained batch 422 batch loss 0.562465489 epoch total loss 0.524079263\n",
      "Trained batch 423 batch loss 0.456401795 epoch total loss 0.523919225\n",
      "Trained batch 424 batch loss 0.549023807 epoch total loss 0.523978472\n",
      "Trained batch 425 batch loss 0.472261518 epoch total loss 0.523856759\n",
      "Trained batch 426 batch loss 0.468406081 epoch total loss 0.523726583\n",
      "Trained batch 427 batch loss 0.5287323 epoch total loss 0.523738325\n",
      "Trained batch 428 batch loss 0.532934427 epoch total loss 0.523759782\n",
      "Trained batch 429 batch loss 0.599136531 epoch total loss 0.523935497\n",
      "Trained batch 430 batch loss 0.627742887 epoch total loss 0.524176896\n",
      "Trained batch 431 batch loss 0.484571248 epoch total loss 0.524085045\n",
      "Trained batch 432 batch loss 0.58317703 epoch total loss 0.524221838\n",
      "Trained batch 433 batch loss 0.460794091 epoch total loss 0.524075329\n",
      "Trained batch 434 batch loss 0.497134686 epoch total loss 0.524013281\n",
      "Trained batch 435 batch loss 0.469446599 epoch total loss 0.523887813\n",
      "Trained batch 436 batch loss 0.45961231 epoch total loss 0.523740411\n",
      "Trained batch 437 batch loss 0.447228283 epoch total loss 0.523565352\n",
      "Trained batch 438 batch loss 0.500252962 epoch total loss 0.523512125\n",
      "Trained batch 439 batch loss 0.536726117 epoch total loss 0.523542225\n",
      "Trained batch 440 batch loss 0.570326 epoch total loss 0.52364856\n",
      "Trained batch 441 batch loss 0.617245436 epoch total loss 0.523860812\n",
      "Trained batch 442 batch loss 0.585385442 epoch total loss 0.524\n",
      "Trained batch 443 batch loss 0.564227462 epoch total loss 0.524090827\n",
      "Trained batch 444 batch loss 0.631854594 epoch total loss 0.524333537\n",
      "Trained batch 445 batch loss 0.579222381 epoch total loss 0.524456859\n",
      "Trained batch 446 batch loss 0.514923215 epoch total loss 0.524435461\n",
      "Trained batch 447 batch loss 0.606264889 epoch total loss 0.524618566\n",
      "Trained batch 448 batch loss 0.594980359 epoch total loss 0.524775624\n",
      "Trained batch 449 batch loss 0.649295926 epoch total loss 0.525052905\n",
      "Trained batch 450 batch loss 0.55258745 epoch total loss 0.525114119\n",
      "Trained batch 451 batch loss 0.509052873 epoch total loss 0.525078475\n",
      "Trained batch 452 batch loss 0.513083756 epoch total loss 0.525051951\n",
      "Trained batch 453 batch loss 0.621366918 epoch total loss 0.525264561\n",
      "Trained batch 454 batch loss 0.588698268 epoch total loss 0.525404274\n",
      "Trained batch 455 batch loss 0.582517862 epoch total loss 0.525529802\n",
      "Trained batch 456 batch loss 0.586489558 epoch total loss 0.525663495\n",
      "Trained batch 457 batch loss 0.521864653 epoch total loss 0.52565515\n",
      "Trained batch 458 batch loss 0.489881 epoch total loss 0.525577068\n",
      "Trained batch 459 batch loss 0.447301894 epoch total loss 0.525406539\n",
      "Trained batch 460 batch loss 0.475515276 epoch total loss 0.525298059\n",
      "Trained batch 461 batch loss 0.463390917 epoch total loss 0.52516377\n",
      "Trained batch 462 batch loss 0.535773873 epoch total loss 0.525186718\n",
      "Trained batch 463 batch loss 0.457468241 epoch total loss 0.525040448\n",
      "Trained batch 464 batch loss 0.342975855 epoch total loss 0.52464807\n",
      "Trained batch 465 batch loss 0.471327364 epoch total loss 0.524533391\n",
      "Trained batch 466 batch loss 0.523348331 epoch total loss 0.524530888\n",
      "Trained batch 467 batch loss 0.567854404 epoch total loss 0.524623632\n",
      "Trained batch 468 batch loss 0.55663377 epoch total loss 0.524692059\n",
      "Trained batch 469 batch loss 0.515449 epoch total loss 0.524672329\n",
      "Trained batch 470 batch loss 0.49124229 epoch total loss 0.524601221\n",
      "Trained batch 471 batch loss 0.553197145 epoch total loss 0.524661899\n",
      "Trained batch 472 batch loss 0.59910655 epoch total loss 0.524819613\n",
      "Trained batch 473 batch loss 0.522731662 epoch total loss 0.524815202\n",
      "Trained batch 474 batch loss 0.63529253 epoch total loss 0.525048316\n",
      "Trained batch 475 batch loss 0.558616281 epoch total loss 0.525118947\n",
      "Trained batch 476 batch loss 0.520219803 epoch total loss 0.525108635\n",
      "Trained batch 477 batch loss 0.521608114 epoch total loss 0.525101304\n",
      "Trained batch 478 batch loss 0.536319 epoch total loss 0.525124788\n",
      "Trained batch 479 batch loss 0.52302283 epoch total loss 0.525120378\n",
      "Trained batch 480 batch loss 0.572689712 epoch total loss 0.5252195\n",
      "Trained batch 481 batch loss 0.535958946 epoch total loss 0.525241852\n",
      "Trained batch 482 batch loss 0.452891082 epoch total loss 0.525091767\n",
      "Trained batch 483 batch loss 0.457063735 epoch total loss 0.524950922\n",
      "Trained batch 484 batch loss 0.591792822 epoch total loss 0.525089\n",
      "Trained batch 485 batch loss 0.58759129 epoch total loss 0.525217891\n",
      "Trained batch 486 batch loss 0.533401 epoch total loss 0.525234699\n",
      "Trained batch 487 batch loss 0.503131032 epoch total loss 0.52518934\n",
      "Trained batch 488 batch loss 0.514137864 epoch total loss 0.52516669\n",
      "Trained batch 489 batch loss 0.531103969 epoch total loss 0.52517879\n",
      "Trained batch 490 batch loss 0.452599198 epoch total loss 0.525030732\n",
      "Trained batch 491 batch loss 0.45974955 epoch total loss 0.524897754\n",
      "Trained batch 492 batch loss 0.498009264 epoch total loss 0.524843097\n",
      "Trained batch 493 batch loss 0.582270682 epoch total loss 0.524959624\n",
      "Trained batch 494 batch loss 0.572113454 epoch total loss 0.525055051\n",
      "Trained batch 495 batch loss 0.454077 epoch total loss 0.524911642\n",
      "Trained batch 496 batch loss 0.52027905 epoch total loss 0.524902344\n",
      "Trained batch 497 batch loss 0.628719 epoch total loss 0.525111258\n",
      "Trained batch 498 batch loss 0.549508631 epoch total loss 0.525160193\n",
      "Trained batch 499 batch loss 0.537666678 epoch total loss 0.525185287\n",
      "Trained batch 500 batch loss 0.503677785 epoch total loss 0.525142252\n",
      "Trained batch 501 batch loss 0.457360238 epoch total loss 0.525007\n",
      "Trained batch 502 batch loss 0.473924905 epoch total loss 0.524905264\n",
      "Trained batch 503 batch loss 0.492038906 epoch total loss 0.524839938\n",
      "Trained batch 504 batch loss 0.504223 epoch total loss 0.524799\n",
      "Trained batch 505 batch loss 0.538510442 epoch total loss 0.524826169\n",
      "Trained batch 506 batch loss 0.548126817 epoch total loss 0.524872184\n",
      "Trained batch 507 batch loss 0.54143852 epoch total loss 0.524904847\n",
      "Trained batch 508 batch loss 0.488196254 epoch total loss 0.524832606\n",
      "Trained batch 509 batch loss 0.452380031 epoch total loss 0.52469027\n",
      "Trained batch 510 batch loss 0.533536434 epoch total loss 0.524707615\n",
      "Trained batch 511 batch loss 0.584500432 epoch total loss 0.524824619\n",
      "Trained batch 512 batch loss 0.512162745 epoch total loss 0.524799943\n",
      "Trained batch 513 batch loss 0.476987958 epoch total loss 0.524706721\n",
      "Trained batch 514 batch loss 0.475763947 epoch total loss 0.524611533\n",
      "Trained batch 515 batch loss 0.470606863 epoch total loss 0.524506688\n",
      "Trained batch 516 batch loss 0.466730446 epoch total loss 0.524394751\n",
      "Trained batch 517 batch loss 0.483999372 epoch total loss 0.524316609\n",
      "Trained batch 518 batch loss 0.531244695 epoch total loss 0.52433\n",
      "Trained batch 519 batch loss 0.539392114 epoch total loss 0.524359047\n",
      "Trained batch 520 batch loss 0.489072025 epoch total loss 0.524291158\n",
      "Trained batch 521 batch loss 0.513886809 epoch total loss 0.52427119\n",
      "Trained batch 522 batch loss 0.519062459 epoch total loss 0.524261236\n",
      "Trained batch 523 batch loss 0.568042278 epoch total loss 0.524345\n",
      "Trained batch 524 batch loss 0.549002826 epoch total loss 0.524392068\n",
      "Trained batch 525 batch loss 0.583509803 epoch total loss 0.524504602\n",
      "Trained batch 526 batch loss 0.525471509 epoch total loss 0.524506509\n",
      "Trained batch 527 batch loss 0.458329737 epoch total loss 0.524380922\n",
      "Trained batch 528 batch loss 0.556880951 epoch total loss 0.524442494\n",
      "Trained batch 529 batch loss 0.569834471 epoch total loss 0.524528265\n",
      "Trained batch 530 batch loss 0.598956704 epoch total loss 0.524668753\n",
      "Trained batch 531 batch loss 0.625658751 epoch total loss 0.524858952\n",
      "Trained batch 532 batch loss 0.476264149 epoch total loss 0.524767578\n",
      "Trained batch 533 batch loss 0.540142298 epoch total loss 0.524796426\n",
      "Trained batch 534 batch loss 0.528300524 epoch total loss 0.524803\n",
      "Trained batch 535 batch loss 0.520191789 epoch total loss 0.52479434\n",
      "Trained batch 536 batch loss 0.512836933 epoch total loss 0.524772048\n",
      "Trained batch 537 batch loss 0.556503117 epoch total loss 0.524831116\n",
      "Trained batch 538 batch loss 0.493079215 epoch total loss 0.524772108\n",
      "Trained batch 539 batch loss 0.49846077 epoch total loss 0.524723291\n",
      "Trained batch 540 batch loss 0.46748054 epoch total loss 0.524617314\n",
      "Trained batch 541 batch loss 0.46814394 epoch total loss 0.524512887\n",
      "Trained batch 542 batch loss 0.460769802 epoch total loss 0.524395287\n",
      "Trained batch 543 batch loss 0.514713407 epoch total loss 0.524377465\n",
      "Trained batch 544 batch loss 0.511882782 epoch total loss 0.524354458\n",
      "Trained batch 545 batch loss 0.54326 epoch total loss 0.524389207\n",
      "Trained batch 546 batch loss 0.571095645 epoch total loss 0.52447474\n",
      "Trained batch 547 batch loss 0.579032123 epoch total loss 0.524574518\n",
      "Trained batch 548 batch loss 0.602748334 epoch total loss 0.524717152\n",
      "Trained batch 549 batch loss 0.662978172 epoch total loss 0.524969\n",
      "Trained batch 550 batch loss 0.622914076 epoch total loss 0.52514708\n",
      "Trained batch 551 batch loss 0.616259933 epoch total loss 0.525312483\n",
      "Trained batch 552 batch loss 0.605291843 epoch total loss 0.525457382\n",
      "Trained batch 553 batch loss 0.524176121 epoch total loss 0.525455\n",
      "Trained batch 554 batch loss 0.680642724 epoch total loss 0.52573514\n",
      "Trained batch 555 batch loss 0.546060622 epoch total loss 0.525771737\n",
      "Trained batch 556 batch loss 0.42802 epoch total loss 0.525595903\n",
      "Trained batch 557 batch loss 0.462530971 epoch total loss 0.525482655\n",
      "Trained batch 558 batch loss 0.428368777 epoch total loss 0.525308669\n",
      "Trained batch 559 batch loss 0.472168237 epoch total loss 0.525213599\n",
      "Trained batch 560 batch loss 0.422933489 epoch total loss 0.525031\n",
      "Trained batch 561 batch loss 0.402415186 epoch total loss 0.524812341\n",
      "Trained batch 562 batch loss 0.389081985 epoch total loss 0.524570823\n",
      "Trained batch 563 batch loss 0.405446529 epoch total loss 0.524359286\n",
      "Trained batch 564 batch loss 0.49823305 epoch total loss 0.524312913\n",
      "Trained batch 565 batch loss 0.497562379 epoch total loss 0.524265587\n",
      "Trained batch 566 batch loss 0.559983969 epoch total loss 0.524328709\n",
      "Trained batch 567 batch loss 0.548131824 epoch total loss 0.52437067\n",
      "Trained batch 568 batch loss 0.464712024 epoch total loss 0.524265647\n",
      "Trained batch 569 batch loss 0.421966523 epoch total loss 0.524085879\n",
      "Trained batch 570 batch loss 0.399892211 epoch total loss 0.523868\n",
      "Trained batch 571 batch loss 0.419761777 epoch total loss 0.523685694\n",
      "Trained batch 572 batch loss 0.390558392 epoch total loss 0.523453\n",
      "Trained batch 573 batch loss 0.42040959 epoch total loss 0.52327317\n",
      "Trained batch 574 batch loss 0.420111477 epoch total loss 0.523093402\n",
      "Trained batch 575 batch loss 0.461514682 epoch total loss 0.522986293\n",
      "Trained batch 576 batch loss 0.68871516 epoch total loss 0.523274064\n",
      "Trained batch 577 batch loss 0.58715111 epoch total loss 0.52338475\n",
      "Trained batch 578 batch loss 0.567784071 epoch total loss 0.52346158\n",
      "Trained batch 579 batch loss 0.568862557 epoch total loss 0.52353996\n",
      "Trained batch 580 batch loss 0.508220732 epoch total loss 0.523513556\n",
      "Trained batch 581 batch loss 0.581799746 epoch total loss 0.523613811\n",
      "Trained batch 582 batch loss 0.580561221 epoch total loss 0.523711681\n",
      "Trained batch 583 batch loss 0.496328443 epoch total loss 0.523664713\n",
      "Trained batch 584 batch loss 0.475572079 epoch total loss 0.523582399\n",
      "Trained batch 585 batch loss 0.476012468 epoch total loss 0.523501098\n",
      "Trained batch 586 batch loss 0.38277632 epoch total loss 0.523260951\n",
      "Trained batch 587 batch loss 0.475079566 epoch total loss 0.523178875\n",
      "Trained batch 588 batch loss 0.565657735 epoch total loss 0.523251057\n",
      "Trained batch 589 batch loss 0.513688326 epoch total loss 0.523234844\n",
      "Trained batch 590 batch loss 0.538753211 epoch total loss 0.523261189\n",
      "Trained batch 591 batch loss 0.528247 epoch total loss 0.523269653\n",
      "Trained batch 592 batch loss 0.577039123 epoch total loss 0.523360431\n",
      "Trained batch 593 batch loss 0.493172288 epoch total loss 0.523309529\n",
      "Trained batch 594 batch loss 0.523213446 epoch total loss 0.52330935\n",
      "Trained batch 595 batch loss 0.509864509 epoch total loss 0.52328676\n",
      "Trained batch 596 batch loss 0.547605157 epoch total loss 0.523327589\n",
      "Trained batch 597 batch loss 0.62309283 epoch total loss 0.52349472\n",
      "Trained batch 598 batch loss 0.648473203 epoch total loss 0.523703694\n",
      "Trained batch 599 batch loss 0.554365 epoch total loss 0.523754835\n",
      "Trained batch 600 batch loss 0.491245359 epoch total loss 0.523700655\n",
      "Trained batch 601 batch loss 0.528963089 epoch total loss 0.523709416\n",
      "Trained batch 602 batch loss 0.576308489 epoch total loss 0.523796737\n",
      "Trained batch 603 batch loss 0.543735 epoch total loss 0.523829818\n",
      "Trained batch 604 batch loss 0.46240139 epoch total loss 0.523728132\n",
      "Trained batch 605 batch loss 0.502078772 epoch total loss 0.52369231\n",
      "Trained batch 606 batch loss 0.45747155 epoch total loss 0.523583055\n",
      "Trained batch 607 batch loss 0.468143731 epoch total loss 0.523491681\n",
      "Trained batch 608 batch loss 0.498365939 epoch total loss 0.523450375\n",
      "Trained batch 609 batch loss 0.51038 epoch total loss 0.523428857\n",
      "Trained batch 610 batch loss 0.514113665 epoch total loss 0.523413599\n",
      "Trained batch 611 batch loss 0.546725273 epoch total loss 0.523451746\n",
      "Trained batch 612 batch loss 0.534114182 epoch total loss 0.52346915\n",
      "Trained batch 613 batch loss 0.447089255 epoch total loss 0.523344576\n",
      "Trained batch 614 batch loss 0.501061559 epoch total loss 0.523308277\n",
      "Trained batch 615 batch loss 0.510456085 epoch total loss 0.523287416\n",
      "Trained batch 616 batch loss 0.462141216 epoch total loss 0.523188114\n",
      "Trained batch 617 batch loss 0.628851533 epoch total loss 0.523359358\n",
      "Trained batch 618 batch loss 0.610719502 epoch total loss 0.523500681\n",
      "Trained batch 619 batch loss 0.54610455 epoch total loss 0.523537219\n",
      "Trained batch 620 batch loss 0.626970172 epoch total loss 0.523704112\n",
      "Trained batch 621 batch loss 0.645009339 epoch total loss 0.523899436\n",
      "Trained batch 622 batch loss 0.59412086 epoch total loss 0.524012327\n",
      "Trained batch 623 batch loss 0.573624432 epoch total loss 0.524091959\n",
      "Trained batch 624 batch loss 0.589650452 epoch total loss 0.524197042\n",
      "Trained batch 625 batch loss 0.502617061 epoch total loss 0.524162531\n",
      "Trained batch 626 batch loss 0.513139844 epoch total loss 0.524144948\n",
      "Trained batch 627 batch loss 0.493314087 epoch total loss 0.524095774\n",
      "Trained batch 628 batch loss 0.56811583 epoch total loss 0.524165869\n",
      "Trained batch 629 batch loss 0.623641849 epoch total loss 0.524324\n",
      "Trained batch 630 batch loss 0.571031928 epoch total loss 0.524398148\n",
      "Trained batch 631 batch loss 0.513898909 epoch total loss 0.524381518\n",
      "Trained batch 632 batch loss 0.523027122 epoch total loss 0.524379373\n",
      "Trained batch 633 batch loss 0.52192235 epoch total loss 0.524375498\n",
      "Trained batch 634 batch loss 0.534961581 epoch total loss 0.524392188\n",
      "Trained batch 635 batch loss 0.568151474 epoch total loss 0.524461091\n",
      "Trained batch 636 batch loss 0.58761245 epoch total loss 0.524560392\n",
      "Trained batch 637 batch loss 0.465271354 epoch total loss 0.524467349\n",
      "Trained batch 638 batch loss 0.44635576 epoch total loss 0.524344921\n",
      "Trained batch 639 batch loss 0.545265079 epoch total loss 0.524377644\n",
      "Trained batch 640 batch loss 0.528342903 epoch total loss 0.524383843\n",
      "Trained batch 641 batch loss 0.624528289 epoch total loss 0.524540067\n",
      "Trained batch 642 batch loss 0.519267261 epoch total loss 0.524531841\n",
      "Trained batch 643 batch loss 0.502453685 epoch total loss 0.524497509\n",
      "Trained batch 644 batch loss 0.526624143 epoch total loss 0.524500787\n",
      "Trained batch 645 batch loss 0.456114531 epoch total loss 0.524394751\n",
      "Trained batch 646 batch loss 0.521242857 epoch total loss 0.524389863\n",
      "Trained batch 647 batch loss 0.423131049 epoch total loss 0.524233341\n",
      "Trained batch 648 batch loss 0.512388945 epoch total loss 0.524215102\n",
      "Trained batch 649 batch loss 0.563516557 epoch total loss 0.524275601\n",
      "Trained batch 650 batch loss 0.469821751 epoch total loss 0.524191856\n",
      "Trained batch 651 batch loss 0.465850472 epoch total loss 0.524102211\n",
      "Trained batch 652 batch loss 0.521607757 epoch total loss 0.524098396\n",
      "Trained batch 653 batch loss 0.554481685 epoch total loss 0.524144948\n",
      "Trained batch 654 batch loss 0.527379155 epoch total loss 0.524149835\n",
      "Trained batch 655 batch loss 0.459301472 epoch total loss 0.524050832\n",
      "Trained batch 656 batch loss 0.579283834 epoch total loss 0.524135053\n",
      "Trained batch 657 batch loss 0.52082 epoch total loss 0.52413\n",
      "Trained batch 658 batch loss 0.436349273 epoch total loss 0.523996532\n",
      "Trained batch 659 batch loss 0.50282 epoch total loss 0.523964405\n",
      "Trained batch 660 batch loss 0.529617 epoch total loss 0.523972929\n",
      "Trained batch 661 batch loss 0.513484836 epoch total loss 0.523957074\n",
      "Trained batch 662 batch loss 0.528783917 epoch total loss 0.523964345\n",
      "Trained batch 663 batch loss 0.614317298 epoch total loss 0.524100661\n",
      "Trained batch 664 batch loss 0.532568395 epoch total loss 0.524113417\n",
      "Trained batch 665 batch loss 0.425852448 epoch total loss 0.523965597\n",
      "Trained batch 666 batch loss 0.460996181 epoch total loss 0.523871064\n",
      "Trained batch 667 batch loss 0.493166864 epoch total loss 0.523825049\n",
      "Trained batch 668 batch loss 0.51933372 epoch total loss 0.523818314\n",
      "Trained batch 669 batch loss 0.56377995 epoch total loss 0.523878038\n",
      "Trained batch 670 batch loss 0.506410122 epoch total loss 0.523852\n",
      "Trained batch 671 batch loss 0.506493 epoch total loss 0.523826122\n",
      "Trained batch 672 batch loss 0.451823294 epoch total loss 0.523718953\n",
      "Trained batch 673 batch loss 0.494433701 epoch total loss 0.523675501\n",
      "Trained batch 674 batch loss 0.469944656 epoch total loss 0.52359575\n",
      "Trained batch 675 batch loss 0.499678195 epoch total loss 0.523560286\n",
      "Trained batch 676 batch loss 0.489334404 epoch total loss 0.523509681\n",
      "Trained batch 677 batch loss 0.482020885 epoch total loss 0.523448408\n",
      "Trained batch 678 batch loss 0.553903103 epoch total loss 0.52349329\n",
      "Trained batch 679 batch loss 0.56515038 epoch total loss 0.523554683\n",
      "Trained batch 680 batch loss 0.589837551 epoch total loss 0.523652136\n",
      "Trained batch 681 batch loss 0.59229511 epoch total loss 0.523752928\n",
      "Trained batch 682 batch loss 0.498876065 epoch total loss 0.52371645\n",
      "Trained batch 683 batch loss 0.524738848 epoch total loss 0.52371794\n",
      "Trained batch 684 batch loss 0.529642522 epoch total loss 0.523726642\n",
      "Trained batch 685 batch loss 0.553688407 epoch total loss 0.523770332\n",
      "Trained batch 686 batch loss 0.625081778 epoch total loss 0.523918033\n",
      "Trained batch 687 batch loss 0.589308083 epoch total loss 0.524013221\n",
      "Trained batch 688 batch loss 0.543471098 epoch total loss 0.524041474\n",
      "Trained batch 689 batch loss 0.609058857 epoch total loss 0.524164855\n",
      "Trained batch 690 batch loss 0.564414561 epoch total loss 0.524223208\n",
      "Trained batch 691 batch loss 0.591872215 epoch total loss 0.524321079\n",
      "Trained batch 692 batch loss 0.555094838 epoch total loss 0.524365544\n",
      "Trained batch 693 batch loss 0.514426529 epoch total loss 0.524351239\n",
      "Trained batch 694 batch loss 0.603684485 epoch total loss 0.524465561\n",
      "Trained batch 695 batch loss 0.491819888 epoch total loss 0.524418592\n",
      "Trained batch 696 batch loss 0.505800545 epoch total loss 0.52439183\n",
      "Trained batch 697 batch loss 0.483775675 epoch total loss 0.524333537\n",
      "Trained batch 698 batch loss 0.537300229 epoch total loss 0.524352133\n",
      "Trained batch 699 batch loss 0.569503546 epoch total loss 0.524416685\n",
      "Trained batch 700 batch loss 0.572931588 epoch total loss 0.524486\n",
      "Trained batch 701 batch loss 0.572937369 epoch total loss 0.524555087\n",
      "Trained batch 702 batch loss 0.580554128 epoch total loss 0.524634898\n",
      "Trained batch 703 batch loss 0.58665216 epoch total loss 0.524723113\n",
      "Trained batch 704 batch loss 0.619265139 epoch total loss 0.524857402\n",
      "Trained batch 705 batch loss 0.618981361 epoch total loss 0.524990916\n",
      "Trained batch 706 batch loss 0.674282968 epoch total loss 0.525202394\n",
      "Trained batch 707 batch loss 0.684126914 epoch total loss 0.525427163\n",
      "Trained batch 708 batch loss 0.651993036 epoch total loss 0.525605917\n",
      "Trained batch 709 batch loss 0.622739911 epoch total loss 0.525742948\n",
      "Trained batch 710 batch loss 0.500029385 epoch total loss 0.525706708\n",
      "Trained batch 711 batch loss 0.418213904 epoch total loss 0.525555551\n",
      "Trained batch 712 batch loss 0.44741863 epoch total loss 0.525445759\n",
      "Trained batch 713 batch loss 0.524159908 epoch total loss 0.525444\n",
      "Trained batch 714 batch loss 0.543495774 epoch total loss 0.525469244\n",
      "Trained batch 715 batch loss 0.483291507 epoch total loss 0.525410235\n",
      "Trained batch 716 batch loss 0.514137268 epoch total loss 0.525394499\n",
      "Trained batch 717 batch loss 0.433104694 epoch total loss 0.525265813\n",
      "Trained batch 718 batch loss 0.501886725 epoch total loss 0.525233209\n",
      "Trained batch 719 batch loss 0.432659894 epoch total loss 0.525104463\n",
      "Trained batch 720 batch loss 0.444460839 epoch total loss 0.524992466\n",
      "Trained batch 721 batch loss 0.440666 epoch total loss 0.524875522\n",
      "Trained batch 722 batch loss 0.430912912 epoch total loss 0.524745345\n",
      "Trained batch 723 batch loss 0.537729383 epoch total loss 0.524763286\n",
      "Trained batch 724 batch loss 0.462107927 epoch total loss 0.52467674\n",
      "Trained batch 725 batch loss 0.346542031 epoch total loss 0.52443105\n",
      "Trained batch 726 batch loss 0.449665546 epoch total loss 0.524328053\n",
      "Trained batch 727 batch loss 0.412248522 epoch total loss 0.524173915\n",
      "Trained batch 728 batch loss 0.466942132 epoch total loss 0.524095297\n",
      "Trained batch 729 batch loss 0.399547726 epoch total loss 0.52392447\n",
      "Trained batch 730 batch loss 0.583661556 epoch total loss 0.524006248\n",
      "Trained batch 731 batch loss 0.540362418 epoch total loss 0.524028659\n",
      "Trained batch 732 batch loss 0.60247767 epoch total loss 0.524135828\n",
      "Trained batch 733 batch loss 0.487652242 epoch total loss 0.524086058\n",
      "Trained batch 734 batch loss 0.358363509 epoch total loss 0.523860276\n",
      "Trained batch 735 batch loss 0.375540763 epoch total loss 0.523658454\n",
      "Trained batch 736 batch loss 0.466812044 epoch total loss 0.523581207\n",
      "Trained batch 737 batch loss 0.445148945 epoch total loss 0.523474813\n",
      "Trained batch 738 batch loss 0.463903725 epoch total loss 0.523394108\n",
      "Trained batch 739 batch loss 0.517438233 epoch total loss 0.523386\n",
      "Trained batch 740 batch loss 0.480049759 epoch total loss 0.52332741\n",
      "Trained batch 741 batch loss 0.525646925 epoch total loss 0.523330569\n",
      "Trained batch 742 batch loss 0.53011775 epoch total loss 0.523339689\n",
      "Trained batch 743 batch loss 0.48722595 epoch total loss 0.523291051\n",
      "Trained batch 744 batch loss 0.561653674 epoch total loss 0.523342609\n",
      "Trained batch 745 batch loss 0.617668569 epoch total loss 0.523469269\n",
      "Trained batch 746 batch loss 0.519750476 epoch total loss 0.523464262\n",
      "Trained batch 747 batch loss 0.492004603 epoch total loss 0.523422122\n",
      "Trained batch 748 batch loss 0.604982138 epoch total loss 0.523531199\n",
      "Trained batch 749 batch loss 0.645633221 epoch total loss 0.523694217\n",
      "Trained batch 750 batch loss 0.494917929 epoch total loss 0.523655832\n",
      "Trained batch 751 batch loss 0.441106498 epoch total loss 0.523545861\n",
      "Trained batch 752 batch loss 0.397982299 epoch total loss 0.523378909\n",
      "Trained batch 753 batch loss 0.511077344 epoch total loss 0.523362577\n",
      "Trained batch 754 batch loss 0.573978126 epoch total loss 0.523429692\n",
      "Trained batch 755 batch loss 0.564217448 epoch total loss 0.523483694\n",
      "Trained batch 756 batch loss 0.51884371 epoch total loss 0.523477554\n",
      "Trained batch 757 batch loss 0.494596064 epoch total loss 0.523439407\n",
      "Trained batch 758 batch loss 0.465890288 epoch total loss 0.523363471\n",
      "Trained batch 759 batch loss 0.503254175 epoch total loss 0.523337\n",
      "Trained batch 760 batch loss 0.441808462 epoch total loss 0.523229718\n",
      "Trained batch 761 batch loss 0.53072083 epoch total loss 0.523239553\n",
      "Trained batch 762 batch loss 0.371579409 epoch total loss 0.523040533\n",
      "Trained batch 763 batch loss 0.492698878 epoch total loss 0.523000777\n",
      "Trained batch 764 batch loss 0.535144389 epoch total loss 0.523016691\n",
      "Trained batch 765 batch loss 0.565565228 epoch total loss 0.523072302\n",
      "Trained batch 766 batch loss 0.579798937 epoch total loss 0.523146331\n",
      "Trained batch 767 batch loss 0.522315383 epoch total loss 0.523145258\n",
      "Trained batch 768 batch loss 0.565562367 epoch total loss 0.523200452\n",
      "Trained batch 769 batch loss 0.536335289 epoch total loss 0.523217559\n",
      "Trained batch 770 batch loss 0.463776022 epoch total loss 0.523140371\n",
      "Trained batch 771 batch loss 0.489102423 epoch total loss 0.523096204\n",
      "Trained batch 772 batch loss 0.430395514 epoch total loss 0.52297616\n",
      "Trained batch 773 batch loss 0.367138475 epoch total loss 0.522774518\n",
      "Trained batch 774 batch loss 0.425871432 epoch total loss 0.522649348\n",
      "Trained batch 775 batch loss 0.419167697 epoch total loss 0.522515774\n",
      "Trained batch 776 batch loss 0.35530293 epoch total loss 0.522300303\n",
      "Trained batch 777 batch loss 0.413391113 epoch total loss 0.522160172\n",
      "Trained batch 778 batch loss 0.361510456 epoch total loss 0.521953642\n",
      "Trained batch 779 batch loss 0.475349844 epoch total loss 0.521893859\n",
      "Trained batch 780 batch loss 0.440001965 epoch total loss 0.521788836\n",
      "Trained batch 781 batch loss 0.546111941 epoch total loss 0.52182\n",
      "Trained batch 782 batch loss 0.450928688 epoch total loss 0.52172935\n",
      "Trained batch 783 batch loss 0.598378539 epoch total loss 0.521827221\n",
      "Trained batch 784 batch loss 0.52409029 epoch total loss 0.521830082\n",
      "Trained batch 785 batch loss 0.528172433 epoch total loss 0.521838188\n",
      "Trained batch 786 batch loss 0.551639795 epoch total loss 0.521876097\n",
      "Trained batch 787 batch loss 0.573723793 epoch total loss 0.52194196\n",
      "Trained batch 788 batch loss 0.572504163 epoch total loss 0.522006154\n",
      "Trained batch 789 batch loss 0.546769083 epoch total loss 0.522037566\n",
      "Trained batch 790 batch loss 0.553777695 epoch total loss 0.522077739\n",
      "Trained batch 791 batch loss 0.694612503 epoch total loss 0.522295833\n",
      "Trained batch 792 batch loss 0.717875779 epoch total loss 0.522542775\n",
      "Trained batch 793 batch loss 0.541297674 epoch total loss 0.522566438\n",
      "Trained batch 794 batch loss 0.456146359 epoch total loss 0.522482753\n",
      "Trained batch 795 batch loss 0.570242703 epoch total loss 0.522542834\n",
      "Trained batch 796 batch loss 0.545930147 epoch total loss 0.522572219\n",
      "Trained batch 797 batch loss 0.630963624 epoch total loss 0.522708237\n",
      "Trained batch 798 batch loss 0.670093596 epoch total loss 0.522892952\n",
      "Trained batch 799 batch loss 0.641844153 epoch total loss 0.523041785\n",
      "Trained batch 800 batch loss 0.538241684 epoch total loss 0.523060799\n",
      "Trained batch 801 batch loss 0.575219035 epoch total loss 0.523125947\n",
      "Trained batch 802 batch loss 0.504425049 epoch total loss 0.523102582\n",
      "Trained batch 803 batch loss 0.58670491 epoch total loss 0.523181796\n",
      "Trained batch 804 batch loss 0.553966343 epoch total loss 0.523220062\n",
      "Trained batch 805 batch loss 0.539754093 epoch total loss 0.523240626\n",
      "Trained batch 806 batch loss 0.513488889 epoch total loss 0.523228526\n",
      "Trained batch 807 batch loss 0.446193 epoch total loss 0.523133099\n",
      "Trained batch 808 batch loss 0.590216756 epoch total loss 0.523216069\n",
      "Trained batch 809 batch loss 0.542811155 epoch total loss 0.523240328\n",
      "Trained batch 810 batch loss 0.567291319 epoch total loss 0.523294687\n",
      "Trained batch 811 batch loss 0.553723693 epoch total loss 0.523332238\n",
      "Trained batch 812 batch loss 0.570781 epoch total loss 0.523390651\n",
      "Trained batch 813 batch loss 0.532606184 epoch total loss 0.523402\n",
      "Trained batch 814 batch loss 0.586373925 epoch total loss 0.523479283\n",
      "Trained batch 815 batch loss 0.576560497 epoch total loss 0.523544431\n",
      "Trained batch 816 batch loss 0.47837314 epoch total loss 0.523489058\n",
      "Trained batch 817 batch loss 0.553589225 epoch total loss 0.523525894\n",
      "Trained batch 818 batch loss 0.505744278 epoch total loss 0.523504198\n",
      "Trained batch 819 batch loss 0.531543911 epoch total loss 0.523514\n",
      "Trained batch 820 batch loss 0.547653496 epoch total loss 0.523543477\n",
      "Trained batch 821 batch loss 0.556860626 epoch total loss 0.523584\n",
      "Trained batch 822 batch loss 0.558968663 epoch total loss 0.523627043\n",
      "Trained batch 823 batch loss 0.535993695 epoch total loss 0.523642063\n",
      "Trained batch 824 batch loss 0.49269402 epoch total loss 0.523604512\n",
      "Trained batch 825 batch loss 0.535152733 epoch total loss 0.523618519\n",
      "Trained batch 826 batch loss 0.577665329 epoch total loss 0.523683965\n",
      "Trained batch 827 batch loss 0.51199311 epoch total loss 0.523669839\n",
      "Trained batch 828 batch loss 0.516092598 epoch total loss 0.52366066\n",
      "Trained batch 829 batch loss 0.460476875 epoch total loss 0.523584485\n",
      "Trained batch 830 batch loss 0.436492294 epoch total loss 0.523479521\n",
      "Trained batch 831 batch loss 0.533204675 epoch total loss 0.523491204\n",
      "Trained batch 832 batch loss 0.544128895 epoch total loss 0.523516059\n",
      "Trained batch 833 batch loss 0.598017812 epoch total loss 0.523605466\n",
      "Trained batch 834 batch loss 0.522780895 epoch total loss 0.523604453\n",
      "Trained batch 835 batch loss 0.584047794 epoch total loss 0.523676872\n",
      "Trained batch 836 batch loss 0.554492712 epoch total loss 0.523713708\n",
      "Trained batch 837 batch loss 0.598 epoch total loss 0.523802459\n",
      "Trained batch 838 batch loss 0.542014241 epoch total loss 0.523824215\n",
      "Trained batch 839 batch loss 0.550734222 epoch total loss 0.523856282\n",
      "Trained batch 840 batch loss 0.516877651 epoch total loss 0.523847938\n",
      "Trained batch 841 batch loss 0.524415135 epoch total loss 0.523848653\n",
      "Trained batch 842 batch loss 0.55750984 epoch total loss 0.523888588\n",
      "Trained batch 843 batch loss 0.565991879 epoch total loss 0.523938537\n",
      "Trained batch 844 batch loss 0.567628682 epoch total loss 0.523990273\n",
      "Trained batch 845 batch loss 0.645954251 epoch total loss 0.524134636\n",
      "Trained batch 846 batch loss 0.542973876 epoch total loss 0.524156928\n",
      "Trained batch 847 batch loss 0.493589193 epoch total loss 0.524120808\n",
      "Trained batch 848 batch loss 0.508012295 epoch total loss 0.524101853\n",
      "Trained batch 849 batch loss 0.436840594 epoch total loss 0.523999035\n",
      "Trained batch 850 batch loss 0.460888416 epoch total loss 0.523924768\n",
      "Trained batch 851 batch loss 0.499959886 epoch total loss 0.523896635\n",
      "Trained batch 852 batch loss 0.519264579 epoch total loss 0.523891151\n",
      "Trained batch 853 batch loss 0.522972 epoch total loss 0.523890138\n",
      "Trained batch 854 batch loss 0.518973827 epoch total loss 0.523884356\n",
      "Trained batch 855 batch loss 0.397687793 epoch total loss 0.523736775\n",
      "Trained batch 856 batch loss 0.454872042 epoch total loss 0.523656309\n",
      "Trained batch 857 batch loss 0.440496087 epoch total loss 0.523559272\n",
      "Trained batch 858 batch loss 0.530597925 epoch total loss 0.523567438\n",
      "Trained batch 859 batch loss 0.589967191 epoch total loss 0.523644745\n",
      "Trained batch 860 batch loss 0.429663837 epoch total loss 0.52353549\n",
      "Trained batch 861 batch loss 0.421239138 epoch total loss 0.523416638\n",
      "Trained batch 862 batch loss 0.495694101 epoch total loss 0.523384511\n",
      "Trained batch 863 batch loss 0.476147681 epoch total loss 0.523329735\n",
      "Trained batch 864 batch loss 0.42849189 epoch total loss 0.52322\n",
      "Trained batch 865 batch loss 0.503362775 epoch total loss 0.523197055\n",
      "Trained batch 866 batch loss 0.443989575 epoch total loss 0.523105562\n",
      "Trained batch 867 batch loss 0.503636777 epoch total loss 0.523083091\n",
      "Trained batch 868 batch loss 0.539827704 epoch total loss 0.523102403\n",
      "Trained batch 869 batch loss 0.508466542 epoch total loss 0.523085535\n",
      "Trained batch 870 batch loss 0.462602258 epoch total loss 0.523016036\n",
      "Trained batch 871 batch loss 0.449409455 epoch total loss 0.522931516\n",
      "Trained batch 872 batch loss 0.468251199 epoch total loss 0.522868812\n",
      "Trained batch 873 batch loss 0.536101103 epoch total loss 0.522884\n",
      "Trained batch 874 batch loss 0.481358975 epoch total loss 0.522836447\n",
      "Trained batch 875 batch loss 0.456698805 epoch total loss 0.522760868\n",
      "Trained batch 876 batch loss 0.522128642 epoch total loss 0.522760153\n",
      "Trained batch 877 batch loss 0.559670091 epoch total loss 0.522802234\n",
      "Trained batch 878 batch loss 0.55431217 epoch total loss 0.522838116\n",
      "Trained batch 879 batch loss 0.51317358 epoch total loss 0.522827148\n",
      "Trained batch 880 batch loss 0.537090182 epoch total loss 0.522843361\n",
      "Trained batch 881 batch loss 0.645758033 epoch total loss 0.522982836\n",
      "Trained batch 882 batch loss 0.604037404 epoch total loss 0.523074746\n",
      "Trained batch 883 batch loss 0.616525292 epoch total loss 0.523180544\n",
      "Trained batch 884 batch loss 0.572374701 epoch total loss 0.523236215\n",
      "Trained batch 885 batch loss 0.573285699 epoch total loss 0.52329278\n",
      "Trained batch 886 batch loss 0.57943362 epoch total loss 0.52335614\n",
      "Trained batch 887 batch loss 0.550141811 epoch total loss 0.523386359\n",
      "Trained batch 888 batch loss 0.499229908 epoch total loss 0.52335912\n",
      "Trained batch 889 batch loss 0.520359576 epoch total loss 0.523355782\n",
      "Trained batch 890 batch loss 0.517336071 epoch total loss 0.523349\n",
      "Trained batch 891 batch loss 0.524539411 epoch total loss 0.523350358\n",
      "Trained batch 892 batch loss 0.428522736 epoch total loss 0.523244\n",
      "Trained batch 893 batch loss 0.459400743 epoch total loss 0.523172557\n",
      "Trained batch 894 batch loss 0.441976815 epoch total loss 0.52308172\n",
      "Trained batch 895 batch loss 0.561668694 epoch total loss 0.523124874\n",
      "Trained batch 896 batch loss 0.549721599 epoch total loss 0.523154557\n",
      "Trained batch 897 batch loss 0.514478922 epoch total loss 0.523144841\n",
      "Trained batch 898 batch loss 0.558574855 epoch total loss 0.523184299\n",
      "Trained batch 899 batch loss 0.61994648 epoch total loss 0.523291886\n",
      "Trained batch 900 batch loss 0.586672664 epoch total loss 0.523362339\n",
      "Trained batch 901 batch loss 0.581108332 epoch total loss 0.523426414\n",
      "Trained batch 902 batch loss 0.561049938 epoch total loss 0.523468137\n",
      "Trained batch 903 batch loss 0.504866242 epoch total loss 0.523447514\n",
      "Trained batch 904 batch loss 0.515340507 epoch total loss 0.523438573\n",
      "Trained batch 905 batch loss 0.551551342 epoch total loss 0.523469627\n",
      "Trained batch 906 batch loss 0.49719274 epoch total loss 0.523440599\n",
      "Trained batch 907 batch loss 0.585484564 epoch total loss 0.523509\n",
      "Trained batch 908 batch loss 0.536919892 epoch total loss 0.523523748\n",
      "Trained batch 909 batch loss 0.488478541 epoch total loss 0.523485184\n",
      "Trained batch 910 batch loss 0.443483293 epoch total loss 0.523397267\n",
      "Trained batch 911 batch loss 0.405399382 epoch total loss 0.523267746\n",
      "Trained batch 912 batch loss 0.572090685 epoch total loss 0.523321271\n",
      "Trained batch 913 batch loss 0.588420749 epoch total loss 0.523392558\n",
      "Trained batch 914 batch loss 0.427812219 epoch total loss 0.523288\n",
      "Trained batch 915 batch loss 0.5062747 epoch total loss 0.523269415\n",
      "Trained batch 916 batch loss 0.406157792 epoch total loss 0.523141563\n",
      "Trained batch 917 batch loss 0.446722329 epoch total loss 0.523058236\n",
      "Trained batch 918 batch loss 0.440704882 epoch total loss 0.522968531\n",
      "Trained batch 919 batch loss 0.443382293 epoch total loss 0.522881925\n",
      "Trained batch 920 batch loss 0.572064877 epoch total loss 0.52293539\n",
      "Trained batch 921 batch loss 0.566405356 epoch total loss 0.522982597\n",
      "Trained batch 922 batch loss 0.559703529 epoch total loss 0.523022413\n",
      "Trained batch 923 batch loss 0.497901678 epoch total loss 0.522995174\n",
      "Trained batch 924 batch loss 0.642940879 epoch total loss 0.523125\n",
      "Trained batch 925 batch loss 0.503148198 epoch total loss 0.523103416\n",
      "Trained batch 926 batch loss 0.550038934 epoch total loss 0.523132503\n",
      "Trained batch 927 batch loss 0.497787237 epoch total loss 0.523105145\n",
      "Trained batch 928 batch loss 0.693687141 epoch total loss 0.523288965\n",
      "Trained batch 929 batch loss 0.568438292 epoch total loss 0.523337543\n",
      "Trained batch 930 batch loss 0.565910578 epoch total loss 0.523383379\n",
      "Trained batch 931 batch loss 0.552647352 epoch total loss 0.523414791\n",
      "Trained batch 932 batch loss 0.53679496 epoch total loss 0.523429155\n",
      "Trained batch 933 batch loss 0.52431792 epoch total loss 0.523430109\n",
      "Trained batch 934 batch loss 0.517458379 epoch total loss 0.523423731\n",
      "Trained batch 935 batch loss 0.51153487 epoch total loss 0.523411\n",
      "Trained batch 936 batch loss 0.533017159 epoch total loss 0.523421288\n",
      "Trained batch 937 batch loss 0.536852658 epoch total loss 0.523435593\n",
      "Trained batch 938 batch loss 0.566687882 epoch total loss 0.523481727\n",
      "Trained batch 939 batch loss 0.576768279 epoch total loss 0.52353847\n",
      "Trained batch 940 batch loss 0.493733108 epoch total loss 0.523506761\n",
      "Trained batch 941 batch loss 0.439613491 epoch total loss 0.523417592\n",
      "Trained batch 942 batch loss 0.385393411 epoch total loss 0.523271084\n",
      "Trained batch 943 batch loss 0.382929623 epoch total loss 0.523122311\n",
      "Trained batch 944 batch loss 0.359401137 epoch total loss 0.522948861\n",
      "Trained batch 945 batch loss 0.416849077 epoch total loss 0.522836566\n",
      "Trained batch 946 batch loss 0.544804394 epoch total loss 0.522859812\n",
      "Trained batch 947 batch loss 0.610994697 epoch total loss 0.522952855\n",
      "Trained batch 948 batch loss 0.532453716 epoch total loss 0.522962868\n",
      "Trained batch 949 batch loss 0.599431574 epoch total loss 0.523043454\n",
      "Trained batch 950 batch loss 0.619023919 epoch total loss 0.523144484\n",
      "Trained batch 951 batch loss 0.598665953 epoch total loss 0.523223877\n",
      "Trained batch 952 batch loss 0.619531274 epoch total loss 0.523325\n",
      "Trained batch 953 batch loss 0.609610796 epoch total loss 0.523415625\n",
      "Trained batch 954 batch loss 0.531403184 epoch total loss 0.52342397\n",
      "Trained batch 955 batch loss 0.471071631 epoch total loss 0.523369133\n",
      "Trained batch 956 batch loss 0.414970309 epoch total loss 0.523255765\n",
      "Trained batch 957 batch loss 0.428344369 epoch total loss 0.523156583\n",
      "Trained batch 958 batch loss 0.423214197 epoch total loss 0.523052275\n",
      "Trained batch 959 batch loss 0.488859415 epoch total loss 0.523016632\n",
      "Trained batch 960 batch loss 0.503096759 epoch total loss 0.52299583\n",
      "Trained batch 961 batch loss 0.442361712 epoch total loss 0.522911966\n",
      "Trained batch 962 batch loss 0.483019 epoch total loss 0.522870481\n",
      "Trained batch 963 batch loss 0.493413836 epoch total loss 0.522839904\n",
      "Trained batch 964 batch loss 0.500938833 epoch total loss 0.522817194\n",
      "Trained batch 965 batch loss 0.539223909 epoch total loss 0.522834182\n",
      "Trained batch 966 batch loss 0.453307152 epoch total loss 0.522762179\n",
      "Trained batch 967 batch loss 0.442989051 epoch total loss 0.522679687\n",
      "Trained batch 968 batch loss 0.382069647 epoch total loss 0.52253443\n",
      "Trained batch 969 batch loss 0.624561131 epoch total loss 0.522639751\n",
      "Trained batch 970 batch loss 0.492838651 epoch total loss 0.522609055\n",
      "Trained batch 971 batch loss 0.538853884 epoch total loss 0.522625744\n",
      "Trained batch 972 batch loss 0.559741616 epoch total loss 0.522663951\n",
      "Trained batch 973 batch loss 0.503904283 epoch total loss 0.522644699\n",
      "Trained batch 974 batch loss 0.495661855 epoch total loss 0.522617\n",
      "Trained batch 975 batch loss 0.57727325 epoch total loss 0.522673\n",
      "Trained batch 976 batch loss 0.421694756 epoch total loss 0.522569537\n",
      "Trained batch 977 batch loss 0.464457303 epoch total loss 0.522510052\n",
      "Trained batch 978 batch loss 0.475288689 epoch total loss 0.522461772\n",
      "Trained batch 979 batch loss 0.466996551 epoch total loss 0.522405148\n",
      "Trained batch 980 batch loss 0.530048132 epoch total loss 0.522412956\n",
      "Trained batch 981 batch loss 0.458903521 epoch total loss 0.522348225\n",
      "Trained batch 982 batch loss 0.459050298 epoch total loss 0.522283733\n",
      "Trained batch 983 batch loss 0.499764115 epoch total loss 0.522260845\n",
      "Trained batch 984 batch loss 0.471325845 epoch total loss 0.522209048\n",
      "Trained batch 985 batch loss 0.556035578 epoch total loss 0.522243381\n",
      "Trained batch 986 batch loss 0.554967165 epoch total loss 0.52227658\n",
      "Trained batch 987 batch loss 0.589375556 epoch total loss 0.52234453\n",
      "Trained batch 988 batch loss 0.636054575 epoch total loss 0.522459626\n",
      "Trained batch 989 batch loss 0.642139256 epoch total loss 0.522580683\n",
      "Trained batch 990 batch loss 0.664290845 epoch total loss 0.522723794\n",
      "Trained batch 991 batch loss 0.595505953 epoch total loss 0.522797287\n",
      "Trained batch 992 batch loss 0.554238558 epoch total loss 0.522829\n",
      "Trained batch 993 batch loss 0.493934393 epoch total loss 0.522799909\n",
      "Trained batch 994 batch loss 0.460920364 epoch total loss 0.522737682\n",
      "Trained batch 995 batch loss 0.547372699 epoch total loss 0.522762418\n",
      "Trained batch 996 batch loss 0.490138739 epoch total loss 0.522729635\n",
      "Trained batch 997 batch loss 0.470400304 epoch total loss 0.522677183\n",
      "Trained batch 998 batch loss 0.609985769 epoch total loss 0.522764623\n",
      "Trained batch 999 batch loss 0.563647687 epoch total loss 0.522805572\n",
      "Trained batch 1000 batch loss 0.516774535 epoch total loss 0.522799551\n",
      "Trained batch 1001 batch loss 0.48870784 epoch total loss 0.522765517\n",
      "Trained batch 1002 batch loss 0.581035137 epoch total loss 0.522823691\n",
      "Trained batch 1003 batch loss 0.603450656 epoch total loss 0.522904038\n",
      "Trained batch 1004 batch loss 0.571549773 epoch total loss 0.522952497\n",
      "Trained batch 1005 batch loss 0.542830467 epoch total loss 0.522972286\n",
      "Trained batch 1006 batch loss 0.519589305 epoch total loss 0.522968948\n",
      "Trained batch 1007 batch loss 0.619195879 epoch total loss 0.523064494\n",
      "Trained batch 1008 batch loss 0.469689578 epoch total loss 0.523011506\n",
      "Trained batch 1009 batch loss 0.527467489 epoch total loss 0.523015916\n",
      "Trained batch 1010 batch loss 0.506437182 epoch total loss 0.522999525\n",
      "Trained batch 1011 batch loss 0.480133891 epoch total loss 0.522957146\n",
      "Trained batch 1012 batch loss 0.443122089 epoch total loss 0.52287823\n",
      "Trained batch 1013 batch loss 0.486312836 epoch total loss 0.522842169\n",
      "Trained batch 1014 batch loss 0.479710758 epoch total loss 0.522799611\n",
      "Trained batch 1015 batch loss 0.457313657 epoch total loss 0.522735119\n",
      "Trained batch 1016 batch loss 0.503572762 epoch total loss 0.522716284\n",
      "Trained batch 1017 batch loss 0.519027412 epoch total loss 0.522712708\n",
      "Trained batch 1018 batch loss 0.490438402 epoch total loss 0.522681\n",
      "Trained batch 1019 batch loss 0.582636714 epoch total loss 0.522739828\n",
      "Trained batch 1020 batch loss 0.5067572 epoch total loss 0.522724152\n",
      "Trained batch 1021 batch loss 0.55108285 epoch total loss 0.522751927\n",
      "Trained batch 1022 batch loss 0.385516703 epoch total loss 0.522617638\n",
      "Trained batch 1023 batch loss 0.441335768 epoch total loss 0.522538185\n",
      "Trained batch 1024 batch loss 0.493380368 epoch total loss 0.522509754\n",
      "Trained batch 1025 batch loss 0.460086048 epoch total loss 0.522448838\n",
      "Trained batch 1026 batch loss 0.47574687 epoch total loss 0.522403359\n",
      "Trained batch 1027 batch loss 0.545744717 epoch total loss 0.522426069\n",
      "Trained batch 1028 batch loss 0.625549853 epoch total loss 0.522526383\n",
      "Trained batch 1029 batch loss 0.680898428 epoch total loss 0.522680283\n",
      "Trained batch 1030 batch loss 0.52401638 epoch total loss 0.522681534\n",
      "Trained batch 1031 batch loss 0.528041184 epoch total loss 0.52268672\n",
      "Trained batch 1032 batch loss 0.540714443 epoch total loss 0.522704184\n",
      "Trained batch 1033 batch loss 0.481142968 epoch total loss 0.522663951\n",
      "Trained batch 1034 batch loss 0.5122751 epoch total loss 0.522653878\n",
      "Trained batch 1035 batch loss 0.345869213 epoch total loss 0.52248311\n",
      "Trained batch 1036 batch loss 0.391927719 epoch total loss 0.522357047\n",
      "Trained batch 1037 batch loss 0.313340902 epoch total loss 0.522155523\n",
      "Trained batch 1038 batch loss 0.302938044 epoch total loss 0.521944344\n",
      "Trained batch 1039 batch loss 0.352517813 epoch total loss 0.521781266\n",
      "Trained batch 1040 batch loss 0.351660907 epoch total loss 0.521617711\n",
      "Trained batch 1041 batch loss 0.598517358 epoch total loss 0.521691561\n",
      "Trained batch 1042 batch loss 0.534712911 epoch total loss 0.521704078\n",
      "Trained batch 1043 batch loss 0.720442772 epoch total loss 0.521894634\n",
      "Trained batch 1044 batch loss 0.713173568 epoch total loss 0.522077918\n",
      "Trained batch 1045 batch loss 0.659303 epoch total loss 0.522209227\n",
      "Trained batch 1046 batch loss 0.660191059 epoch total loss 0.522341132\n",
      "Trained batch 1047 batch loss 0.547398806 epoch total loss 0.522365093\n",
      "Trained batch 1048 batch loss 0.533712506 epoch total loss 0.522375882\n",
      "Trained batch 1049 batch loss 0.519554317 epoch total loss 0.522373199\n",
      "Trained batch 1050 batch loss 0.499903828 epoch total loss 0.522351801\n",
      "Trained batch 1051 batch loss 0.524347186 epoch total loss 0.522353649\n",
      "Trained batch 1052 batch loss 0.489609063 epoch total loss 0.522322536\n",
      "Trained batch 1053 batch loss 0.552745521 epoch total loss 0.522351444\n",
      "Trained batch 1054 batch loss 0.554270864 epoch total loss 0.522381723\n",
      "Trained batch 1055 batch loss 0.610242665 epoch total loss 0.522465\n",
      "Trained batch 1056 batch loss 0.572148919 epoch total loss 0.522512\n",
      "Trained batch 1057 batch loss 0.575520933 epoch total loss 0.522562146\n",
      "Trained batch 1058 batch loss 0.524494171 epoch total loss 0.522564\n",
      "Trained batch 1059 batch loss 0.593829751 epoch total loss 0.522631228\n",
      "Trained batch 1060 batch loss 0.61966157 epoch total loss 0.52272284\n",
      "Trained batch 1061 batch loss 0.637241125 epoch total loss 0.522830784\n",
      "Trained batch 1062 batch loss 0.606638 epoch total loss 0.522909701\n",
      "Trained batch 1063 batch loss 0.60590744 epoch total loss 0.522987723\n",
      "Trained batch 1064 batch loss 0.711596608 epoch total loss 0.523165047\n",
      "Trained batch 1065 batch loss 0.60837841 epoch total loss 0.523245037\n",
      "Trained batch 1066 batch loss 0.595920682 epoch total loss 0.523313224\n",
      "Trained batch 1067 batch loss 0.589730799 epoch total loss 0.523375511\n",
      "Trained batch 1068 batch loss 0.53831327 epoch total loss 0.523389518\n",
      "Trained batch 1069 batch loss 0.568118215 epoch total loss 0.523431361\n",
      "Trained batch 1070 batch loss 0.544020593 epoch total loss 0.523450553\n",
      "Trained batch 1071 batch loss 0.448910475 epoch total loss 0.523381\n",
      "Trained batch 1072 batch loss 0.581711531 epoch total loss 0.523435414\n",
      "Trained batch 1073 batch loss 0.486705303 epoch total loss 0.523401141\n",
      "Trained batch 1074 batch loss 0.613719642 epoch total loss 0.523485243\n",
      "Trained batch 1075 batch loss 0.582988 epoch total loss 0.523540616\n",
      "Trained batch 1076 batch loss 0.555958 epoch total loss 0.523570716\n",
      "Trained batch 1077 batch loss 0.545896888 epoch total loss 0.523591459\n",
      "Trained batch 1078 batch loss 0.540463686 epoch total loss 0.523607135\n",
      "Trained batch 1079 batch loss 0.533785 epoch total loss 0.523616612\n",
      "Trained batch 1080 batch loss 0.534275174 epoch total loss 0.523626506\n",
      "Trained batch 1081 batch loss 0.483834207 epoch total loss 0.523589671\n",
      "Trained batch 1082 batch loss 0.527551413 epoch total loss 0.523593307\n",
      "Trained batch 1083 batch loss 0.539242148 epoch total loss 0.523607731\n",
      "Trained batch 1084 batch loss 0.521646738 epoch total loss 0.523605943\n",
      "Trained batch 1085 batch loss 0.513776243 epoch total loss 0.523596942\n",
      "Trained batch 1086 batch loss 0.561721623 epoch total loss 0.523632\n",
      "Trained batch 1087 batch loss 0.602820456 epoch total loss 0.523704886\n",
      "Trained batch 1088 batch loss 0.512674332 epoch total loss 0.523694754\n",
      "Trained batch 1089 batch loss 0.553341389 epoch total loss 0.523722\n",
      "Trained batch 1090 batch loss 0.587017655 epoch total loss 0.523780107\n",
      "Trained batch 1091 batch loss 0.521687865 epoch total loss 0.523778141\n",
      "Trained batch 1092 batch loss 0.582846701 epoch total loss 0.523832202\n",
      "Trained batch 1093 batch loss 0.599783778 epoch total loss 0.523901701\n",
      "Trained batch 1094 batch loss 0.565278053 epoch total loss 0.52393955\n",
      "Trained batch 1095 batch loss 0.674286604 epoch total loss 0.524076879\n",
      "Trained batch 1096 batch loss 0.614567339 epoch total loss 0.524159431\n",
      "Trained batch 1097 batch loss 0.546935737 epoch total loss 0.524180233\n",
      "Trained batch 1098 batch loss 0.622709811 epoch total loss 0.524269938\n",
      "Trained batch 1099 batch loss 0.55154264 epoch total loss 0.524294734\n",
      "Trained batch 1100 batch loss 0.503171086 epoch total loss 0.524275541\n",
      "Trained batch 1101 batch loss 0.537019372 epoch total loss 0.524287105\n",
      "Trained batch 1102 batch loss 0.499267876 epoch total loss 0.524264395\n",
      "Trained batch 1103 batch loss 0.511178136 epoch total loss 0.524252534\n",
      "Trained batch 1104 batch loss 0.513774276 epoch total loss 0.524243057\n",
      "Trained batch 1105 batch loss 0.47181648 epoch total loss 0.524195611\n",
      "Trained batch 1106 batch loss 0.582603931 epoch total loss 0.524248421\n",
      "Trained batch 1107 batch loss 0.660368681 epoch total loss 0.524371326\n",
      "Trained batch 1108 batch loss 0.578344703 epoch total loss 0.524420083\n",
      "Trained batch 1109 batch loss 0.609350443 epoch total loss 0.524496675\n",
      "Trained batch 1110 batch loss 0.627726614 epoch total loss 0.524589717\n",
      "Trained batch 1111 batch loss 0.663188219 epoch total loss 0.52471447\n",
      "Trained batch 1112 batch loss 0.706672 epoch total loss 0.524878085\n",
      "Trained batch 1113 batch loss 0.603678226 epoch total loss 0.524948895\n",
      "Trained batch 1114 batch loss 0.462733418 epoch total loss 0.524893045\n",
      "Trained batch 1115 batch loss 0.510671377 epoch total loss 0.52488029\n",
      "Trained batch 1116 batch loss 0.479170322 epoch total loss 0.524839342\n",
      "Trained batch 1117 batch loss 0.560198426 epoch total loss 0.524871\n",
      "Trained batch 1118 batch loss 0.609370291 epoch total loss 0.52494657\n",
      "Trained batch 1119 batch loss 0.54818064 epoch total loss 0.524967313\n",
      "Trained batch 1120 batch loss 0.476158261 epoch total loss 0.524923682\n",
      "Trained batch 1121 batch loss 0.535861254 epoch total loss 0.524933517\n",
      "Trained batch 1122 batch loss 0.663015366 epoch total loss 0.525056541\n",
      "Trained batch 1123 batch loss 0.570153415 epoch total loss 0.525096714\n",
      "Trained batch 1124 batch loss 0.546192825 epoch total loss 0.52511549\n",
      "Trained batch 1125 batch loss 0.60799861 epoch total loss 0.525189102\n",
      "Trained batch 1126 batch loss 0.414918184 epoch total loss 0.525091171\n",
      "Trained batch 1127 batch loss 0.426274389 epoch total loss 0.525003493\n",
      "Trained batch 1128 batch loss 0.402092308 epoch total loss 0.524894536\n",
      "Trained batch 1129 batch loss 0.556206703 epoch total loss 0.524922311\n",
      "Trained batch 1130 batch loss 0.580074966 epoch total loss 0.524971128\n",
      "Trained batch 1131 batch loss 0.604325 epoch total loss 0.525041223\n",
      "Trained batch 1132 batch loss 0.549176931 epoch total loss 0.525062561\n",
      "Trained batch 1133 batch loss 0.627567768 epoch total loss 0.525153041\n",
      "Trained batch 1134 batch loss 0.409166545 epoch total loss 0.525050759\n",
      "Trained batch 1135 batch loss 0.494346678 epoch total loss 0.525023699\n",
      "Trained batch 1136 batch loss 0.577992797 epoch total loss 0.525070369\n",
      "Trained batch 1137 batch loss 0.483946502 epoch total loss 0.525034189\n",
      "Trained batch 1138 batch loss 0.51600343 epoch total loss 0.525026262\n",
      "Trained batch 1139 batch loss 0.593222916 epoch total loss 0.525086105\n",
      "Trained batch 1140 batch loss 0.562974095 epoch total loss 0.525119364\n",
      "Trained batch 1141 batch loss 0.536294699 epoch total loss 0.525129139\n",
      "Trained batch 1142 batch loss 0.546163857 epoch total loss 0.525147557\n",
      "Trained batch 1143 batch loss 0.527385 epoch total loss 0.525149524\n",
      "Trained batch 1144 batch loss 0.63082844 epoch total loss 0.525241852\n",
      "Trained batch 1145 batch loss 0.504901409 epoch total loss 0.52522409\n",
      "Trained batch 1146 batch loss 0.537030399 epoch total loss 0.525234401\n",
      "Trained batch 1147 batch loss 0.583009362 epoch total loss 0.525284767\n",
      "Trained batch 1148 batch loss 0.621315897 epoch total loss 0.525368452\n",
      "Trained batch 1149 batch loss 0.597704887 epoch total loss 0.525431395\n",
      "Trained batch 1150 batch loss 0.588269234 epoch total loss 0.525486052\n",
      "Trained batch 1151 batch loss 0.616230667 epoch total loss 0.525564849\n",
      "Trained batch 1152 batch loss 0.555725574 epoch total loss 0.525591075\n",
      "Trained batch 1153 batch loss 0.632523298 epoch total loss 0.525683761\n",
      "Trained batch 1154 batch loss 0.562559903 epoch total loss 0.525715768\n",
      "Trained batch 1155 batch loss 0.588588595 epoch total loss 0.525770128\n",
      "Trained batch 1156 batch loss 0.58071053 epoch total loss 0.525817633\n",
      "Trained batch 1157 batch loss 0.681913495 epoch total loss 0.525952578\n",
      "Trained batch 1158 batch loss 0.496219099 epoch total loss 0.525926888\n",
      "Trained batch 1159 batch loss 0.488467693 epoch total loss 0.525894523\n",
      "Trained batch 1160 batch loss 0.491191506 epoch total loss 0.525864661\n",
      "Trained batch 1161 batch loss 0.487042665 epoch total loss 0.525831223\n",
      "Trained batch 1162 batch loss 0.476489514 epoch total loss 0.525788784\n",
      "Trained batch 1163 batch loss 0.568053603 epoch total loss 0.525825143\n",
      "Trained batch 1164 batch loss 0.524799526 epoch total loss 0.525824189\n",
      "Trained batch 1165 batch loss 0.43982169 epoch total loss 0.525750399\n",
      "Trained batch 1166 batch loss 0.526179492 epoch total loss 0.525750756\n",
      "Trained batch 1167 batch loss 0.458706856 epoch total loss 0.525693297\n",
      "Trained batch 1168 batch loss 0.461079031 epoch total loss 0.525637925\n",
      "Trained batch 1169 batch loss 0.586933613 epoch total loss 0.525690377\n",
      "Trained batch 1170 batch loss 0.551010966 epoch total loss 0.525712\n",
      "Trained batch 1171 batch loss 0.550942719 epoch total loss 0.52573359\n",
      "Trained batch 1172 batch loss 0.489478827 epoch total loss 0.525702655\n",
      "Trained batch 1173 batch loss 0.495614529 epoch total loss 0.525677\n",
      "Trained batch 1174 batch loss 0.528053105 epoch total loss 0.525679052\n",
      "Trained batch 1175 batch loss 0.579095244 epoch total loss 0.52572453\n",
      "Trained batch 1176 batch loss 0.682184 epoch total loss 0.525857568\n",
      "Trained batch 1177 batch loss 0.638570487 epoch total loss 0.525953293\n",
      "Trained batch 1178 batch loss 0.642529845 epoch total loss 0.526052296\n",
      "Trained batch 1179 batch loss 0.570997953 epoch total loss 0.526090384\n",
      "Trained batch 1180 batch loss 0.582899094 epoch total loss 0.526138484\n",
      "Trained batch 1181 batch loss 0.51489085 epoch total loss 0.526129\n",
      "Trained batch 1182 batch loss 0.550315917 epoch total loss 0.526149452\n",
      "Trained batch 1183 batch loss 0.561134696 epoch total loss 0.526179\n",
      "Trained batch 1184 batch loss 0.458583981 epoch total loss 0.526121914\n",
      "Trained batch 1185 batch loss 0.56452018 epoch total loss 0.526154339\n",
      "Trained batch 1186 batch loss 0.551727891 epoch total loss 0.526175916\n",
      "Trained batch 1187 batch loss 0.48679325 epoch total loss 0.526142716\n",
      "Trained batch 1188 batch loss 0.4758811 epoch total loss 0.526100457\n",
      "Trained batch 1189 batch loss 0.512722552 epoch total loss 0.526089191\n",
      "Trained batch 1190 batch loss 0.592986882 epoch total loss 0.526145339\n",
      "Trained batch 1191 batch loss 0.456953138 epoch total loss 0.526087284\n",
      "Trained batch 1192 batch loss 0.524746418 epoch total loss 0.526086152\n",
      "Trained batch 1193 batch loss 0.472907543 epoch total loss 0.526041567\n",
      "Trained batch 1194 batch loss 0.448496163 epoch total loss 0.525976598\n",
      "Trained batch 1195 batch loss 0.462369382 epoch total loss 0.525923371\n",
      "Trained batch 1196 batch loss 0.444296688 epoch total loss 0.525855064\n",
      "Trained batch 1197 batch loss 0.407256544 epoch total loss 0.525755942\n",
      "Trained batch 1198 batch loss 0.40542531 epoch total loss 0.525655508\n",
      "Trained batch 1199 batch loss 0.438551188 epoch total loss 0.52558285\n",
      "Trained batch 1200 batch loss 0.375002444 epoch total loss 0.525457382\n",
      "Trained batch 1201 batch loss 0.439749956 epoch total loss 0.525386\n",
      "Trained batch 1202 batch loss 0.513674259 epoch total loss 0.52537626\n",
      "Trained batch 1203 batch loss 0.670991778 epoch total loss 0.525497317\n",
      "Trained batch 1204 batch loss 0.594209492 epoch total loss 0.525554419\n",
      "Trained batch 1205 batch loss 0.700148165 epoch total loss 0.525699317\n",
      "Trained batch 1206 batch loss 0.528155386 epoch total loss 0.525701344\n",
      "Trained batch 1207 batch loss 0.6814 epoch total loss 0.525830328\n",
      "Trained batch 1208 batch loss 0.698129594 epoch total loss 0.525972962\n",
      "Trained batch 1209 batch loss 0.5529 epoch total loss 0.525995195\n",
      "Trained batch 1210 batch loss 0.580195665 epoch total loss 0.52604\n",
      "Trained batch 1211 batch loss 0.543514311 epoch total loss 0.526054442\n",
      "Trained batch 1212 batch loss 0.647279501 epoch total loss 0.526154459\n",
      "Trained batch 1213 batch loss 0.650699854 epoch total loss 0.526257157\n",
      "Trained batch 1214 batch loss 0.609775841 epoch total loss 0.526325941\n",
      "Trained batch 1215 batch loss 0.53136903 epoch total loss 0.526330113\n",
      "Trained batch 1216 batch loss 0.494645864 epoch total loss 0.526304066\n",
      "Trained batch 1217 batch loss 0.477720231 epoch total loss 0.526264131\n",
      "Trained batch 1218 batch loss 0.588848293 epoch total loss 0.52631551\n",
      "Trained batch 1219 batch loss 0.528208673 epoch total loss 0.52631706\n",
      "Trained batch 1220 batch loss 0.52183485 epoch total loss 0.526313424\n",
      "Trained batch 1221 batch loss 0.53594 epoch total loss 0.526321292\n",
      "Trained batch 1222 batch loss 0.4526124 epoch total loss 0.526261\n",
      "Trained batch 1223 batch loss 0.478734732 epoch total loss 0.526222169\n",
      "Trained batch 1224 batch loss 0.507037222 epoch total loss 0.526206493\n",
      "Trained batch 1225 batch loss 0.53460139 epoch total loss 0.526213348\n",
      "Trained batch 1226 batch loss 0.517196238 epoch total loss 0.526206\n",
      "Trained batch 1227 batch loss 0.486477673 epoch total loss 0.526173592\n",
      "Trained batch 1228 batch loss 0.442937374 epoch total loss 0.526105821\n",
      "Trained batch 1229 batch loss 0.444950134 epoch total loss 0.526039779\n",
      "Trained batch 1230 batch loss 0.509282231 epoch total loss 0.52602613\n",
      "Trained batch 1231 batch loss 0.469726831 epoch total loss 0.525980413\n",
      "Trained batch 1232 batch loss 0.509497583 epoch total loss 0.525967062\n",
      "Trained batch 1233 batch loss 0.414565891 epoch total loss 0.525876701\n",
      "Trained batch 1234 batch loss 0.533922255 epoch total loss 0.525883198\n",
      "Trained batch 1235 batch loss 0.574073553 epoch total loss 0.525922239\n",
      "Trained batch 1236 batch loss 0.578416228 epoch total loss 0.525964737\n",
      "Trained batch 1237 batch loss 0.470479488 epoch total loss 0.525919855\n",
      "Trained batch 1238 batch loss 0.519296885 epoch total loss 0.52591449\n",
      "Trained batch 1239 batch loss 0.44681704 epoch total loss 0.525850654\n",
      "Trained batch 1240 batch loss 0.525585115 epoch total loss 0.525850475\n",
      "Trained batch 1241 batch loss 0.533072531 epoch total loss 0.525856256\n",
      "Trained batch 1242 batch loss 0.520462751 epoch total loss 0.525851905\n",
      "Trained batch 1243 batch loss 0.474307179 epoch total loss 0.52581048\n",
      "Trained batch 1244 batch loss 0.535447955 epoch total loss 0.525818229\n",
      "Trained batch 1245 batch loss 0.521166265 epoch total loss 0.525814474\n",
      "Trained batch 1246 batch loss 0.557293832 epoch total loss 0.525839746\n",
      "Trained batch 1247 batch loss 0.520875454 epoch total loss 0.525835812\n",
      "Trained batch 1248 batch loss 0.627721131 epoch total loss 0.52591747\n",
      "Trained batch 1249 batch loss 0.573791802 epoch total loss 0.525955796\n",
      "Trained batch 1250 batch loss 0.526663 epoch total loss 0.525956333\n",
      "Trained batch 1251 batch loss 0.486720711 epoch total loss 0.525925\n",
      "Trained batch 1252 batch loss 0.562521 epoch total loss 0.525954187\n",
      "Trained batch 1253 batch loss 0.456719667 epoch total loss 0.525898933\n",
      "Trained batch 1254 batch loss 0.456807762 epoch total loss 0.525843799\n",
      "Trained batch 1255 batch loss 0.554368377 epoch total loss 0.525866568\n",
      "Trained batch 1256 batch loss 0.487862587 epoch total loss 0.525836289\n",
      "Trained batch 1257 batch loss 0.525246322 epoch total loss 0.525835812\n",
      "Trained batch 1258 batch loss 0.570071 epoch total loss 0.525871\n",
      "Trained batch 1259 batch loss 0.609597087 epoch total loss 0.525937498\n",
      "Trained batch 1260 batch loss 0.521248102 epoch total loss 0.525933802\n",
      "Trained batch 1261 batch loss 0.532547176 epoch total loss 0.525939047\n",
      "Trained batch 1262 batch loss 0.52312547 epoch total loss 0.525936782\n",
      "Trained batch 1263 batch loss 0.491686285 epoch total loss 0.525909662\n",
      "Trained batch 1264 batch loss 0.554452062 epoch total loss 0.525932252\n",
      "Trained batch 1265 batch loss 0.587684333 epoch total loss 0.525981069\n",
      "Trained batch 1266 batch loss 0.538682282 epoch total loss 0.525991142\n",
      "Trained batch 1267 batch loss 0.487649351 epoch total loss 0.525960922\n",
      "Trained batch 1268 batch loss 0.557161033 epoch total loss 0.525985539\n",
      "Trained batch 1269 batch loss 0.431591481 epoch total loss 0.525911152\n",
      "Trained batch 1270 batch loss 0.499173731 epoch total loss 0.525890052\n",
      "Trained batch 1271 batch loss 0.47438103 epoch total loss 0.525849521\n",
      "Trained batch 1272 batch loss 0.497787237 epoch total loss 0.525827467\n",
      "Trained batch 1273 batch loss 0.595682561 epoch total loss 0.525882363\n",
      "Trained batch 1274 batch loss 0.617865622 epoch total loss 0.525954545\n",
      "Trained batch 1275 batch loss 0.530745327 epoch total loss 0.5259583\n",
      "Trained batch 1276 batch loss 0.56254214 epoch total loss 0.525987\n",
      "Trained batch 1277 batch loss 0.508582 epoch total loss 0.52597338\n",
      "Trained batch 1278 batch loss 0.507842124 epoch total loss 0.525959194\n",
      "Trained batch 1279 batch loss 0.550134182 epoch total loss 0.525978088\n",
      "Trained batch 1280 batch loss 0.539259434 epoch total loss 0.52598846\n",
      "Trained batch 1281 batch loss 0.557871819 epoch total loss 0.526013315\n",
      "Trained batch 1282 batch loss 0.609985232 epoch total loss 0.52607882\n",
      "Trained batch 1283 batch loss 0.537912309 epoch total loss 0.526088059\n",
      "Trained batch 1284 batch loss 0.435453 epoch total loss 0.526017427\n",
      "Trained batch 1285 batch loss 0.465668917 epoch total loss 0.525970459\n",
      "Trained batch 1286 batch loss 0.518967628 epoch total loss 0.525965035\n",
      "Trained batch 1287 batch loss 0.604856074 epoch total loss 0.526026368\n",
      "Trained batch 1288 batch loss 0.543592334 epoch total loss 0.526039958\n",
      "Trained batch 1289 batch loss 0.54781872 epoch total loss 0.526056826\n",
      "Trained batch 1290 batch loss 0.462312818 epoch total loss 0.526007473\n",
      "Trained batch 1291 batch loss 0.537547052 epoch total loss 0.526016414\n",
      "Trained batch 1292 batch loss 0.538838 epoch total loss 0.526026309\n",
      "Trained batch 1293 batch loss 0.564469159 epoch total loss 0.526056\n",
      "Trained batch 1294 batch loss 0.516035199 epoch total loss 0.526048303\n",
      "Trained batch 1295 batch loss 0.503420472 epoch total loss 0.526030838\n",
      "Trained batch 1296 batch loss 0.482267559 epoch total loss 0.525997043\n",
      "Trained batch 1297 batch loss 0.439744264 epoch total loss 0.525930524\n",
      "Trained batch 1298 batch loss 0.463128179 epoch total loss 0.525882125\n",
      "Trained batch 1299 batch loss 0.626753 epoch total loss 0.52595979\n",
      "Trained batch 1300 batch loss 0.57225728 epoch total loss 0.525995433\n",
      "Trained batch 1301 batch loss 0.569508731 epoch total loss 0.526028872\n",
      "Trained batch 1302 batch loss 0.479972601 epoch total loss 0.525993526\n",
      "Trained batch 1303 batch loss 0.489478648 epoch total loss 0.525965512\n",
      "Trained batch 1304 batch loss 0.554659 epoch total loss 0.525987566\n",
      "Trained batch 1305 batch loss 0.502299249 epoch total loss 0.525969386\n",
      "Trained batch 1306 batch loss 0.548536479 epoch total loss 0.525986671\n",
      "Trained batch 1307 batch loss 0.522637606 epoch total loss 0.525984108\n",
      "Trained batch 1308 batch loss 0.57491982 epoch total loss 0.526021481\n",
      "Trained batch 1309 batch loss 0.50677 epoch total loss 0.526006818\n",
      "Trained batch 1310 batch loss 0.519146383 epoch total loss 0.526001573\n",
      "Trained batch 1311 batch loss 0.545986533 epoch total loss 0.526016831\n",
      "Trained batch 1312 batch loss 0.539324045 epoch total loss 0.526026964\n",
      "Trained batch 1313 batch loss 0.5978899 epoch total loss 0.526081681\n",
      "Trained batch 1314 batch loss 0.484828472 epoch total loss 0.52605027\n",
      "Trained batch 1315 batch loss 0.473228246 epoch total loss 0.526010096\n",
      "Trained batch 1316 batch loss 0.537531614 epoch total loss 0.526018858\n",
      "Trained batch 1317 batch loss 0.499562383 epoch total loss 0.525998771\n",
      "Trained batch 1318 batch loss 0.613645315 epoch total loss 0.52606523\n",
      "Trained batch 1319 batch loss 0.533275247 epoch total loss 0.526070714\n",
      "Trained batch 1320 batch loss 0.565726221 epoch total loss 0.526100755\n",
      "Trained batch 1321 batch loss 0.542878032 epoch total loss 0.52611351\n",
      "Trained batch 1322 batch loss 0.607741714 epoch total loss 0.526175201\n",
      "Trained batch 1323 batch loss 0.532870114 epoch total loss 0.526180327\n",
      "Trained batch 1324 batch loss 0.493607461 epoch total loss 0.52615571\n",
      "Trained batch 1325 batch loss 0.537649095 epoch total loss 0.526164353\n",
      "Trained batch 1326 batch loss 0.493051827 epoch total loss 0.526139379\n",
      "Trained batch 1327 batch loss 0.503651142 epoch total loss 0.526122451\n",
      "Trained batch 1328 batch loss 0.474732101 epoch total loss 0.526083767\n",
      "Trained batch 1329 batch loss 0.500953138 epoch total loss 0.526064873\n",
      "Trained batch 1330 batch loss 0.561371088 epoch total loss 0.526091456\n",
      "Trained batch 1331 batch loss 0.550833702 epoch total loss 0.526110053\n",
      "Trained batch 1332 batch loss 0.561219752 epoch total loss 0.526136398\n",
      "Trained batch 1333 batch loss 0.572362304 epoch total loss 0.526171088\n",
      "Trained batch 1334 batch loss 0.557689369 epoch total loss 0.526194692\n",
      "Trained batch 1335 batch loss 0.597432375 epoch total loss 0.526248038\n",
      "Trained batch 1336 batch loss 0.615608573 epoch total loss 0.526314914\n",
      "Trained batch 1337 batch loss 0.649278164 epoch total loss 0.526406884\n",
      "Trained batch 1338 batch loss 0.576776 epoch total loss 0.526444554\n",
      "Trained batch 1339 batch loss 0.573875964 epoch total loss 0.526479959\n",
      "Trained batch 1340 batch loss 0.606965542 epoch total loss 0.526540041\n",
      "Trained batch 1341 batch loss 0.60628587 epoch total loss 0.526599467\n",
      "Trained batch 1342 batch loss 0.568268836 epoch total loss 0.52663058\n",
      "Trained batch 1343 batch loss 0.511355698 epoch total loss 0.526619196\n",
      "Trained batch 1344 batch loss 0.460418701 epoch total loss 0.526569903\n",
      "Trained batch 1345 batch loss 0.439391047 epoch total loss 0.526505113\n",
      "Trained batch 1346 batch loss 0.426828593 epoch total loss 0.526431\n",
      "Trained batch 1347 batch loss 0.421249092 epoch total loss 0.526352942\n",
      "Trained batch 1348 batch loss 0.393199623 epoch total loss 0.526254177\n",
      "Trained batch 1349 batch loss 0.406244844 epoch total loss 0.526165187\n",
      "Trained batch 1350 batch loss 0.425066829 epoch total loss 0.526090324\n",
      "Trained batch 1351 batch loss 0.54345 epoch total loss 0.526103139\n",
      "Trained batch 1352 batch loss 0.640320837 epoch total loss 0.526187658\n",
      "Trained batch 1353 batch loss 0.549869776 epoch total loss 0.526205122\n",
      "Trained batch 1354 batch loss 0.666366875 epoch total loss 0.526308656\n",
      "Trained batch 1355 batch loss 0.706080556 epoch total loss 0.526441336\n",
      "Trained batch 1356 batch loss 0.695542932 epoch total loss 0.526566\n",
      "Trained batch 1357 batch loss 0.549413919 epoch total loss 0.526582897\n",
      "Trained batch 1358 batch loss 0.580425739 epoch total loss 0.526622593\n",
      "Trained batch 1359 batch loss 0.585021079 epoch total loss 0.526665509\n",
      "Trained batch 1360 batch loss 0.415280223 epoch total loss 0.526583612\n",
      "Trained batch 1361 batch loss 0.504947066 epoch total loss 0.526567757\n",
      "Trained batch 1362 batch loss 0.556388617 epoch total loss 0.526589632\n",
      "Trained batch 1363 batch loss 0.580041051 epoch total loss 0.526628852\n",
      "Trained batch 1364 batch loss 0.524823904 epoch total loss 0.526627541\n",
      "Trained batch 1365 batch loss 0.614366829 epoch total loss 0.526691794\n",
      "Trained batch 1366 batch loss 0.55420959 epoch total loss 0.526711941\n",
      "Trained batch 1367 batch loss 0.579578519 epoch total loss 0.526750624\n",
      "Trained batch 1368 batch loss 0.535788119 epoch total loss 0.52675724\n",
      "Trained batch 1369 batch loss 0.492518902 epoch total loss 0.526732206\n",
      "Trained batch 1370 batch loss 0.526095569 epoch total loss 0.52673173\n",
      "Trained batch 1371 batch loss 0.563293338 epoch total loss 0.526758432\n",
      "Trained batch 1372 batch loss 0.630339622 epoch total loss 0.526833892\n",
      "Trained batch 1373 batch loss 0.564588368 epoch total loss 0.52686137\n",
      "Trained batch 1374 batch loss 0.524731159 epoch total loss 0.52685982\n",
      "Trained batch 1375 batch loss 0.480824083 epoch total loss 0.526826322\n",
      "Trained batch 1376 batch loss 0.522082448 epoch total loss 0.526822925\n",
      "Trained batch 1377 batch loss 0.447055936 epoch total loss 0.526765\n",
      "Trained batch 1378 batch loss 0.485588044 epoch total loss 0.526735127\n",
      "Trained batch 1379 batch loss 0.44180727 epoch total loss 0.526673555\n",
      "Trained batch 1380 batch loss 0.451856405 epoch total loss 0.526619315\n",
      "Trained batch 1381 batch loss 0.49418509 epoch total loss 0.526595831\n",
      "Trained batch 1382 batch loss 0.567025959 epoch total loss 0.526625097\n",
      "Trained batch 1383 batch loss 0.596670449 epoch total loss 0.526675761\n",
      "Trained batch 1384 batch loss 0.543445826 epoch total loss 0.52668786\n",
      "Trained batch 1385 batch loss 0.474771887 epoch total loss 0.526650429\n",
      "Trained batch 1386 batch loss 0.465461463 epoch total loss 0.526606262\n",
      "Trained batch 1387 batch loss 0.375038266 epoch total loss 0.526497\n",
      "Trained batch 1388 batch loss 0.39643985 epoch total loss 0.526403308\n",
      "Trained batch 1389 batch loss 0.508623779 epoch total loss 0.526390493\n",
      "Trained batch 1390 batch loss 0.585501969 epoch total loss 0.526433\n",
      "Trained batch 1391 batch loss 0.586146653 epoch total loss 0.526475906\n",
      "Trained batch 1392 batch loss 0.424796581 epoch total loss 0.526402891\n",
      "Trained batch 1393 batch loss 0.489741504 epoch total loss 0.526376545\n",
      "Trained batch 1394 batch loss 0.58404094 epoch total loss 0.526417911\n",
      "Trained batch 1395 batch loss 0.496283 epoch total loss 0.526396334\n",
      "Trained batch 1396 batch loss 0.554608703 epoch total loss 0.52641654\n",
      "Trained batch 1397 batch loss 0.591216922 epoch total loss 0.526462913\n",
      "Trained batch 1398 batch loss 0.573108 epoch total loss 0.526496291\n",
      "Trained batch 1399 batch loss 0.402776062 epoch total loss 0.526407838\n",
      "Trained batch 1400 batch loss 0.436531305 epoch total loss 0.526343644\n",
      "Trained batch 1401 batch loss 0.66543889 epoch total loss 0.526442945\n",
      "Trained batch 1402 batch loss 0.571015716 epoch total loss 0.526474774\n",
      "Trained batch 1403 batch loss 0.605308294 epoch total loss 0.526530921\n",
      "Trained batch 1404 batch loss 0.580116093 epoch total loss 0.526569128\n",
      "Trained batch 1405 batch loss 0.627893806 epoch total loss 0.52664119\n",
      "Trained batch 1406 batch loss 0.563586056 epoch total loss 0.526667476\n",
      "Trained batch 1407 batch loss 0.403409243 epoch total loss 0.526579857\n",
      "Trained batch 1408 batch loss 0.420677334 epoch total loss 0.526504636\n",
      "Trained batch 1409 batch loss 0.436651617 epoch total loss 0.526440859\n",
      "Trained batch 1410 batch loss 0.418278724 epoch total loss 0.526364148\n",
      "Trained batch 1411 batch loss 0.386734962 epoch total loss 0.526265204\n",
      "Trained batch 1412 batch loss 0.485036373 epoch total loss 0.526236\n",
      "Trained batch 1413 batch loss 0.464027 epoch total loss 0.526192\n",
      "Trained batch 1414 batch loss 0.489869475 epoch total loss 0.52616632\n",
      "Trained batch 1415 batch loss 0.460790634 epoch total loss 0.526120126\n",
      "Trained batch 1416 batch loss 0.389744699 epoch total loss 0.526023805\n",
      "Trained batch 1417 batch loss 0.524610639 epoch total loss 0.526022792\n",
      "Trained batch 1418 batch loss 0.492349893 epoch total loss 0.525999069\n",
      "Trained batch 1419 batch loss 0.465669513 epoch total loss 0.525956571\n",
      "Trained batch 1420 batch loss 0.503526 epoch total loss 0.525940776\n",
      "Trained batch 1421 batch loss 0.496311218 epoch total loss 0.52592\n",
      "Trained batch 1422 batch loss 0.479928464 epoch total loss 0.525887609\n",
      "Trained batch 1423 batch loss 0.478768915 epoch total loss 0.525854528\n",
      "Trained batch 1424 batch loss 0.482989669 epoch total loss 0.525824368\n",
      "Trained batch 1425 batch loss 0.50516814 epoch total loss 0.525809884\n",
      "Trained batch 1426 batch loss 0.563032389 epoch total loss 0.525836\n",
      "Trained batch 1427 batch loss 0.543872237 epoch total loss 0.525848687\n",
      "Trained batch 1428 batch loss 0.572675943 epoch total loss 0.525881469\n",
      "Trained batch 1429 batch loss 0.541768551 epoch total loss 0.525892556\n",
      "Trained batch 1430 batch loss 0.513937175 epoch total loss 0.525884211\n",
      "Trained batch 1431 batch loss 0.581836283 epoch total loss 0.525923312\n",
      "Trained batch 1432 batch loss 0.578173637 epoch total loss 0.52595979\n",
      "Trained batch 1433 batch loss 0.61865294 epoch total loss 0.526024461\n",
      "Trained batch 1434 batch loss 0.605095446 epoch total loss 0.526079655\n",
      "Trained batch 1435 batch loss 0.572360277 epoch total loss 0.526111901\n",
      "Trained batch 1436 batch loss 0.546533048 epoch total loss 0.526126087\n",
      "Trained batch 1437 batch loss 0.598971188 epoch total loss 0.52617681\n",
      "Trained batch 1438 batch loss 0.536273897 epoch total loss 0.526183844\n",
      "Trained batch 1439 batch loss 0.536133766 epoch total loss 0.526190758\n",
      "Trained batch 1440 batch loss 0.60134691 epoch total loss 0.526242912\n",
      "Trained batch 1441 batch loss 0.516741574 epoch total loss 0.526236296\n",
      "Trained batch 1442 batch loss 0.570961 epoch total loss 0.52626735\n",
      "Trained batch 1443 batch loss 0.531139255 epoch total loss 0.526270688\n",
      "Trained batch 1444 batch loss 0.546476185 epoch total loss 0.526284695\n",
      "Trained batch 1445 batch loss 0.602909207 epoch total loss 0.526337683\n",
      "Trained batch 1446 batch loss 0.573319852 epoch total loss 0.526370168\n",
      "Trained batch 1447 batch loss 0.526167035 epoch total loss 0.526370049\n",
      "Trained batch 1448 batch loss 0.601933 epoch total loss 0.526422262\n",
      "Trained batch 1449 batch loss 0.607482612 epoch total loss 0.526478171\n",
      "Trained batch 1450 batch loss 0.557682037 epoch total loss 0.526499689\n",
      "Trained batch 1451 batch loss 0.596792161 epoch total loss 0.526548147\n",
      "Trained batch 1452 batch loss 0.63533771 epoch total loss 0.52662307\n",
      "Trained batch 1453 batch loss 0.569929 epoch total loss 0.526652873\n",
      "Trained batch 1454 batch loss 0.581776 epoch total loss 0.526690781\n",
      "Trained batch 1455 batch loss 0.544405639 epoch total loss 0.526703\n",
      "Trained batch 1456 batch loss 0.609160364 epoch total loss 0.526759624\n",
      "Trained batch 1457 batch loss 0.530502915 epoch total loss 0.526762187\n",
      "Trained batch 1458 batch loss 0.537683904 epoch total loss 0.526769638\n",
      "Trained batch 1459 batch loss 0.497299612 epoch total loss 0.526749492\n",
      "Trained batch 1460 batch loss 0.491787553 epoch total loss 0.526725471\n",
      "Trained batch 1461 batch loss 0.597995043 epoch total loss 0.526774287\n",
      "Trained batch 1462 batch loss 0.561010718 epoch total loss 0.526797712\n",
      "Trained batch 1463 batch loss 0.51569289 epoch total loss 0.526790142\n",
      "Trained batch 1464 batch loss 0.576973319 epoch total loss 0.526824415\n",
      "Trained batch 1465 batch loss 0.578721583 epoch total loss 0.52685982\n",
      "Trained batch 1466 batch loss 0.498610526 epoch total loss 0.526840568\n",
      "Trained batch 1467 batch loss 0.574436426 epoch total loss 0.526873052\n",
      "Trained batch 1468 batch loss 0.546817362 epoch total loss 0.526886582\n",
      "Trained batch 1469 batch loss 0.587525666 epoch total loss 0.526927888\n",
      "Trained batch 1470 batch loss 0.516798079 epoch total loss 0.526921\n",
      "Trained batch 1471 batch loss 0.564392447 epoch total loss 0.526946485\n",
      "Trained batch 1472 batch loss 0.548779607 epoch total loss 0.526961267\n",
      "Trained batch 1473 batch loss 0.564743161 epoch total loss 0.526986957\n",
      "Trained batch 1474 batch loss 0.551714778 epoch total loss 0.527003706\n",
      "Trained batch 1475 batch loss 0.535762072 epoch total loss 0.527009666\n",
      "Trained batch 1476 batch loss 0.42268151 epoch total loss 0.526939\n",
      "Trained batch 1477 batch loss 0.457658589 epoch total loss 0.526892066\n",
      "Trained batch 1478 batch loss 0.497505248 epoch total loss 0.526872158\n",
      "Trained batch 1479 batch loss 0.578096688 epoch total loss 0.526906788\n",
      "Trained batch 1480 batch loss 0.565680623 epoch total loss 0.526933\n",
      "Trained batch 1481 batch loss 0.576426506 epoch total loss 0.526966393\n",
      "Trained batch 1482 batch loss 0.562703848 epoch total loss 0.526990533\n",
      "Trained batch 1483 batch loss 0.587977171 epoch total loss 0.5270316\n",
      "Trained batch 1484 batch loss 0.497087508 epoch total loss 0.527011454\n",
      "Trained batch 1485 batch loss 0.560263753 epoch total loss 0.527033806\n",
      "Trained batch 1486 batch loss 0.591683805 epoch total loss 0.527077317\n",
      "Trained batch 1487 batch loss 0.618165493 epoch total loss 0.527138591\n",
      "Trained batch 1488 batch loss 0.517284393 epoch total loss 0.527131915\n",
      "Trained batch 1489 batch loss 0.519611478 epoch total loss 0.527126849\n",
      "Trained batch 1490 batch loss 0.473448157 epoch total loss 0.527090847\n",
      "Trained batch 1491 batch loss 0.461673468 epoch total loss 0.527047\n",
      "Trained batch 1492 batch loss 0.440590739 epoch total loss 0.526989043\n",
      "Trained batch 1493 batch loss 0.467151225 epoch total loss 0.526949\n",
      "Trained batch 1494 batch loss 0.475794941 epoch total loss 0.526914716\n",
      "Trained batch 1495 batch loss 0.515159965 epoch total loss 0.526906848\n",
      "Trained batch 1496 batch loss 0.524094939 epoch total loss 0.526904941\n",
      "Trained batch 1497 batch loss 0.53667289 epoch total loss 0.526911497\n",
      "Trained batch 1498 batch loss 0.526934743 epoch total loss 0.526911497\n",
      "Trained batch 1499 batch loss 0.554324806 epoch total loss 0.526929796\n",
      "Trained batch 1500 batch loss 0.541633189 epoch total loss 0.526939571\n",
      "Trained batch 1501 batch loss 0.610693872 epoch total loss 0.526995361\n",
      "Trained batch 1502 batch loss 0.63352567 epoch total loss 0.52706635\n",
      "Trained batch 1503 batch loss 0.594153523 epoch total loss 0.527111\n",
      "Trained batch 1504 batch loss 0.532295763 epoch total loss 0.527114451\n",
      "Trained batch 1505 batch loss 0.474551588 epoch total loss 0.527079523\n",
      "Trained batch 1506 batch loss 0.620430112 epoch total loss 0.527141452\n",
      "Trained batch 1507 batch loss 0.600675106 epoch total loss 0.527190268\n",
      "Trained batch 1508 batch loss 0.588478923 epoch total loss 0.527230918\n",
      "Trained batch 1509 batch loss 0.604388118 epoch total loss 0.527282\n",
      "Trained batch 1510 batch loss 0.589811 epoch total loss 0.527323425\n",
      "Trained batch 1511 batch loss 0.591068208 epoch total loss 0.527365625\n",
      "Trained batch 1512 batch loss 0.582011342 epoch total loss 0.527401745\n",
      "Trained batch 1513 batch loss 0.581466794 epoch total loss 0.527437508\n",
      "Trained batch 1514 batch loss 0.613538504 epoch total loss 0.527494371\n",
      "Trained batch 1515 batch loss 0.676191688 epoch total loss 0.52759254\n",
      "Trained batch 1516 batch loss 0.626332819 epoch total loss 0.527657688\n",
      "Trained batch 1517 batch loss 0.50748992 epoch total loss 0.527644396\n",
      "Trained batch 1518 batch loss 0.493432581 epoch total loss 0.527621806\n",
      "Trained batch 1519 batch loss 0.509534121 epoch total loss 0.527609885\n",
      "Trained batch 1520 batch loss 0.515687764 epoch total loss 0.527602077\n",
      "Trained batch 1521 batch loss 0.470817804 epoch total loss 0.527564764\n",
      "Trained batch 1522 batch loss 0.437968969 epoch total loss 0.527505875\n",
      "Trained batch 1523 batch loss 0.487728477 epoch total loss 0.527479768\n",
      "Trained batch 1524 batch loss 0.509996653 epoch total loss 0.527468324\n",
      "Trained batch 1525 batch loss 0.597377241 epoch total loss 0.5275141\n",
      "Trained batch 1526 batch loss 0.515800118 epoch total loss 0.527506471\n",
      "Trained batch 1527 batch loss 0.490483403 epoch total loss 0.527482212\n",
      "Trained batch 1528 batch loss 0.555347919 epoch total loss 0.527500451\n",
      "Trained batch 1529 batch loss 0.545003593 epoch total loss 0.527511895\n",
      "Trained batch 1530 batch loss 0.536610723 epoch total loss 0.527517855\n",
      "Trained batch 1531 batch loss 0.63846153 epoch total loss 0.527590334\n",
      "Trained batch 1532 batch loss 0.530607879 epoch total loss 0.527592301\n",
      "Trained batch 1533 batch loss 0.686061621 epoch total loss 0.527695656\n",
      "Trained batch 1534 batch loss 0.563815117 epoch total loss 0.5277192\n",
      "Trained batch 1535 batch loss 0.593105316 epoch total loss 0.527761757\n",
      "Trained batch 1536 batch loss 0.740996778 epoch total loss 0.527900577\n",
      "Trained batch 1537 batch loss 0.623394907 epoch total loss 0.527962744\n",
      "Trained batch 1538 batch loss 0.580979466 epoch total loss 0.527997196\n",
      "Trained batch 1539 batch loss 0.553888 epoch total loss 0.528014\n",
      "Trained batch 1540 batch loss 0.60167855 epoch total loss 0.528061867\n",
      "Trained batch 1541 batch loss 0.533679903 epoch total loss 0.528065503\n",
      "Trained batch 1542 batch loss 0.430194765 epoch total loss 0.528002\n",
      "Trained batch 1543 batch loss 0.517954707 epoch total loss 0.527995527\n",
      "Trained batch 1544 batch loss 0.613118649 epoch total loss 0.528050661\n",
      "Trained batch 1545 batch loss 0.563119709 epoch total loss 0.528073311\n",
      "Trained batch 1546 batch loss 0.733657718 epoch total loss 0.528206289\n",
      "Trained batch 1547 batch loss 0.523402095 epoch total loss 0.528203189\n",
      "Trained batch 1548 batch loss 0.465456098 epoch total loss 0.528162658\n",
      "Trained batch 1549 batch loss 0.519056082 epoch total loss 0.528156757\n",
      "Trained batch 1550 batch loss 0.543961704 epoch total loss 0.52816695\n",
      "Trained batch 1551 batch loss 0.475692898 epoch total loss 0.528133094\n",
      "Trained batch 1552 batch loss 0.487992585 epoch total loss 0.528107226\n",
      "Trained batch 1553 batch loss 0.560010493 epoch total loss 0.528127789\n",
      "Trained batch 1554 batch loss 0.459135056 epoch total loss 0.528083384\n",
      "Trained batch 1555 batch loss 0.50552386 epoch total loss 0.5280689\n",
      "Trained batch 1556 batch loss 0.451921582 epoch total loss 0.528019905\n",
      "Trained batch 1557 batch loss 0.475028813 epoch total loss 0.527985871\n",
      "Trained batch 1558 batch loss 0.534622371 epoch total loss 0.527990162\n",
      "Trained batch 1559 batch loss 0.526162267 epoch total loss 0.527989\n",
      "Trained batch 1560 batch loss 0.496573359 epoch total loss 0.527968824\n",
      "Trained batch 1561 batch loss 0.54852879 epoch total loss 0.527982\n",
      "Trained batch 1562 batch loss 0.569054186 epoch total loss 0.528008282\n",
      "Trained batch 1563 batch loss 0.590231836 epoch total loss 0.528048098\n",
      "Trained batch 1564 batch loss 0.633278847 epoch total loss 0.528115392\n",
      "Trained batch 1565 batch loss 0.624648035 epoch total loss 0.528177083\n",
      "Trained batch 1566 batch loss 0.564299 epoch total loss 0.52820009\n",
      "Trained batch 1567 batch loss 0.557191908 epoch total loss 0.528218627\n",
      "Trained batch 1568 batch loss 0.595483243 epoch total loss 0.528261483\n",
      "Trained batch 1569 batch loss 0.572514355 epoch total loss 0.528289676\n",
      "Trained batch 1570 batch loss 0.451723099 epoch total loss 0.528240919\n",
      "Trained batch 1571 batch loss 0.550344586 epoch total loss 0.528255\n",
      "Trained batch 1572 batch loss 0.558464944 epoch total loss 0.528274238\n",
      "Trained batch 1573 batch loss 0.503704548 epoch total loss 0.528258622\n",
      "Trained batch 1574 batch loss 0.590916336 epoch total loss 0.528298438\n",
      "Trained batch 1575 batch loss 0.51718235 epoch total loss 0.528291404\n",
      "Trained batch 1576 batch loss 0.490697891 epoch total loss 0.528267562\n",
      "Trained batch 1577 batch loss 0.503638327 epoch total loss 0.528251946\n",
      "Trained batch 1578 batch loss 0.590361416 epoch total loss 0.528291285\n",
      "Trained batch 1579 batch loss 0.535767555 epoch total loss 0.528296053\n",
      "Trained batch 1580 batch loss 0.568668127 epoch total loss 0.528321564\n",
      "Trained batch 1581 batch loss 0.613795161 epoch total loss 0.528375626\n",
      "Trained batch 1582 batch loss 0.572012722 epoch total loss 0.528403223\n",
      "Trained batch 1583 batch loss 0.50750947 epoch total loss 0.52839005\n",
      "Trained batch 1584 batch loss 0.518312335 epoch total loss 0.528383672\n",
      "Trained batch 1585 batch loss 0.551549911 epoch total loss 0.528398275\n",
      "Trained batch 1586 batch loss 0.522011042 epoch total loss 0.528394282\n",
      "Trained batch 1587 batch loss 0.559081435 epoch total loss 0.528413594\n",
      "Trained batch 1588 batch loss 0.5232535 epoch total loss 0.528410375\n",
      "Trained batch 1589 batch loss 0.497228295 epoch total loss 0.528390765\n",
      "Trained batch 1590 batch loss 0.382195741 epoch total loss 0.528298795\n",
      "Trained batch 1591 batch loss 0.504402578 epoch total loss 0.528283775\n",
      "Trained batch 1592 batch loss 0.495281726 epoch total loss 0.528263092\n",
      "Trained batch 1593 batch loss 0.478582561 epoch total loss 0.528231859\n",
      "Trained batch 1594 batch loss 0.516545177 epoch total loss 0.528224528\n",
      "Trained batch 1595 batch loss 0.496786594 epoch total loss 0.528204799\n",
      "Trained batch 1596 batch loss 0.494074702 epoch total loss 0.52818346\n",
      "Trained batch 1597 batch loss 0.475019902 epoch total loss 0.528150141\n",
      "Trained batch 1598 batch loss 0.426781446 epoch total loss 0.528086722\n",
      "Trained batch 1599 batch loss 0.490732193 epoch total loss 0.528063357\n",
      "Trained batch 1600 batch loss 0.440031618 epoch total loss 0.528008282\n",
      "Trained batch 1601 batch loss 0.495149195 epoch total loss 0.527987778\n",
      "Trained batch 1602 batch loss 0.489724129 epoch total loss 0.527963936\n",
      "Trained batch 1603 batch loss 0.520490885 epoch total loss 0.527959287\n",
      "Trained batch 1604 batch loss 0.55333221 epoch total loss 0.527975082\n",
      "Trained batch 1605 batch loss 0.612054348 epoch total loss 0.528027475\n",
      "Trained batch 1606 batch loss 0.700714469 epoch total loss 0.528135061\n",
      "Trained batch 1607 batch loss 0.628676832 epoch total loss 0.528197587\n",
      "Trained batch 1608 batch loss 0.624591768 epoch total loss 0.528257549\n",
      "Trained batch 1609 batch loss 0.541390717 epoch total loss 0.528265715\n",
      "Trained batch 1610 batch loss 0.539333344 epoch total loss 0.528272569\n",
      "Trained batch 1611 batch loss 0.510859072 epoch total loss 0.528261721\n",
      "Trained batch 1612 batch loss 0.408948272 epoch total loss 0.528187692\n",
      "Trained batch 1613 batch loss 0.447076261 epoch total loss 0.528137445\n",
      "Trained batch 1614 batch loss 0.552712321 epoch total loss 0.528152645\n",
      "Trained batch 1615 batch loss 0.620099306 epoch total loss 0.528209627\n",
      "Trained batch 1616 batch loss 0.63569963 epoch total loss 0.528276145\n",
      "Trained batch 1617 batch loss 0.654307842 epoch total loss 0.528354049\n",
      "Trained batch 1618 batch loss 0.665666401 epoch total loss 0.528438926\n",
      "Trained batch 1619 batch loss 0.565964162 epoch total loss 0.528462112\n",
      "Trained batch 1620 batch loss 0.585475087 epoch total loss 0.528497279\n",
      "Trained batch 1621 batch loss 0.619244337 epoch total loss 0.528553247\n",
      "Trained batch 1622 batch loss 0.552678287 epoch total loss 0.528568149\n",
      "Trained batch 1623 batch loss 0.512339771 epoch total loss 0.528558135\n",
      "Trained batch 1624 batch loss 0.610173345 epoch total loss 0.528608382\n",
      "Trained batch 1625 batch loss 0.559436619 epoch total loss 0.528627396\n",
      "Trained batch 1626 batch loss 0.57828331 epoch total loss 0.528657913\n",
      "Trained batch 1627 batch loss 0.548294306 epoch total loss 0.528669953\n",
      "Trained batch 1628 batch loss 0.532036 epoch total loss 0.52867204\n",
      "Trained batch 1629 batch loss 0.593374848 epoch total loss 0.528711796\n",
      "Trained batch 1630 batch loss 0.51979 epoch total loss 0.528706312\n",
      "Trained batch 1631 batch loss 0.511727929 epoch total loss 0.528695881\n",
      "Trained batch 1632 batch loss 0.568521202 epoch total loss 0.528720319\n",
      "Trained batch 1633 batch loss 0.528761685 epoch total loss 0.528720319\n",
      "Trained batch 1634 batch loss 0.579016387 epoch total loss 0.528751135\n",
      "Trained batch 1635 batch loss 0.561000049 epoch total loss 0.528770804\n",
      "Trained batch 1636 batch loss 0.578289092 epoch total loss 0.528801084\n",
      "Trained batch 1637 batch loss 0.544851303 epoch total loss 0.528810918\n",
      "Trained batch 1638 batch loss 0.507713497 epoch total loss 0.528798\n",
      "Trained batch 1639 batch loss 0.561850429 epoch total loss 0.52881819\n",
      "Trained batch 1640 batch loss 0.559253454 epoch total loss 0.528836727\n",
      "Trained batch 1641 batch loss 0.579646707 epoch total loss 0.528867722\n",
      "Trained batch 1642 batch loss 0.545841157 epoch total loss 0.528878033\n",
      "Trained batch 1643 batch loss 0.640244126 epoch total loss 0.528945804\n",
      "Trained batch 1644 batch loss 0.572472394 epoch total loss 0.528972268\n",
      "Trained batch 1645 batch loss 0.582819879 epoch total loss 0.529005\n",
      "Trained batch 1646 batch loss 0.551209867 epoch total loss 0.529018521\n",
      "Trained batch 1647 batch loss 0.58060348 epoch total loss 0.529049873\n",
      "Trained batch 1648 batch loss 0.568614244 epoch total loss 0.529073834\n",
      "Trained batch 1649 batch loss 0.57658267 epoch total loss 0.529102683\n",
      "Trained batch 1650 batch loss 0.525661051 epoch total loss 0.529100537\n",
      "Trained batch 1651 batch loss 0.517265081 epoch total loss 0.529093385\n",
      "Trained batch 1652 batch loss 0.564417839 epoch total loss 0.529114783\n",
      "Trained batch 1653 batch loss 0.571969807 epoch total loss 0.529140711\n",
      "Trained batch 1654 batch loss 0.445360929 epoch total loss 0.529090047\n",
      "Trained batch 1655 batch loss 0.49931109 epoch total loss 0.529072046\n",
      "Trained batch 1656 batch loss 0.502907157 epoch total loss 0.529056251\n",
      "Trained batch 1657 batch loss 0.432267398 epoch total loss 0.528997838\n",
      "Trained batch 1658 batch loss 0.52594 epoch total loss 0.528996\n",
      "Trained batch 1659 batch loss 0.438363135 epoch total loss 0.528941393\n",
      "Trained batch 1660 batch loss 0.441593826 epoch total loss 0.528888762\n",
      "Trained batch 1661 batch loss 0.44915095 epoch total loss 0.528840721\n",
      "Trained batch 1662 batch loss 0.465265691 epoch total loss 0.528802514\n",
      "Trained batch 1663 batch loss 0.365035772 epoch total loss 0.528704047\n",
      "Trained batch 1664 batch loss 0.49993372 epoch total loss 0.528686762\n",
      "Trained batch 1665 batch loss 0.45431444 epoch total loss 0.528642058\n",
      "Trained batch 1666 batch loss 0.503433764 epoch total loss 0.528626919\n",
      "Trained batch 1667 batch loss 0.445873559 epoch total loss 0.528577268\n",
      "Trained batch 1668 batch loss 0.544583499 epoch total loss 0.528586864\n",
      "Trained batch 1669 batch loss 0.552295625 epoch total loss 0.52860105\n",
      "Trained batch 1670 batch loss 0.531961322 epoch total loss 0.528603077\n",
      "Trained batch 1671 batch loss 0.409511209 epoch total loss 0.52853179\n",
      "Trained batch 1672 batch loss 0.45906 epoch total loss 0.528490245\n",
      "Trained batch 1673 batch loss 0.44300583 epoch total loss 0.528439164\n",
      "Trained batch 1674 batch loss 0.521244287 epoch total loss 0.528434813\n",
      "Trained batch 1675 batch loss 0.507522047 epoch total loss 0.528422356\n",
      "Trained batch 1676 batch loss 0.538747549 epoch total loss 0.528428495\n",
      "Trained batch 1677 batch loss 0.559337437 epoch total loss 0.528446913\n",
      "Trained batch 1678 batch loss 0.480817467 epoch total loss 0.528418541\n",
      "Trained batch 1679 batch loss 0.549246728 epoch total loss 0.528430939\n",
      "Trained batch 1680 batch loss 0.598519087 epoch total loss 0.528472662\n",
      "Trained batch 1681 batch loss 0.627550244 epoch total loss 0.528531611\n",
      "Trained batch 1682 batch loss 0.589682102 epoch total loss 0.52856797\n",
      "Trained batch 1683 batch loss 0.478473425 epoch total loss 0.528538167\n",
      "Trained batch 1684 batch loss 0.520625 epoch total loss 0.528533518\n",
      "Trained batch 1685 batch loss 0.517020702 epoch total loss 0.528526664\n",
      "Trained batch 1686 batch loss 0.562602401 epoch total loss 0.52854687\n",
      "Trained batch 1687 batch loss 0.488528 epoch total loss 0.528523147\n",
      "Trained batch 1688 batch loss 0.440799534 epoch total loss 0.528471172\n",
      "Trained batch 1689 batch loss 0.591334343 epoch total loss 0.528508425\n",
      "Trained batch 1690 batch loss 0.496511519 epoch total loss 0.52848947\n",
      "Trained batch 1691 batch loss 0.455401808 epoch total loss 0.528446257\n",
      "Trained batch 1692 batch loss 0.496395111 epoch total loss 0.528427303\n",
      "Trained batch 1693 batch loss 0.499720275 epoch total loss 0.528410316\n",
      "Trained batch 1694 batch loss 0.52105093 epoch total loss 0.528405964\n",
      "Trained batch 1695 batch loss 0.406049073 epoch total loss 0.528333843\n",
      "Trained batch 1696 batch loss 0.501722336 epoch total loss 0.528318107\n",
      "Trained batch 1697 batch loss 0.490576565 epoch total loss 0.528295875\n",
      "Trained batch 1698 batch loss 0.469355315 epoch total loss 0.528261185\n",
      "Trained batch 1699 batch loss 0.45937553 epoch total loss 0.528220594\n",
      "Trained batch 1700 batch loss 0.508531749 epoch total loss 0.528209031\n",
      "Trained batch 1701 batch loss 0.512032211 epoch total loss 0.528199553\n",
      "Trained batch 1702 batch loss 0.484056592 epoch total loss 0.528173625\n",
      "Trained batch 1703 batch loss 0.526189268 epoch total loss 0.528172433\n",
      "Trained batch 1704 batch loss 0.53376168 epoch total loss 0.528175712\n",
      "Trained batch 1705 batch loss 0.569133043 epoch total loss 0.528199732\n",
      "Trained batch 1706 batch loss 0.515769482 epoch total loss 0.528192461\n",
      "Trained batch 1707 batch loss 0.505159318 epoch total loss 0.528179\n",
      "Trained batch 1708 batch loss 0.506709933 epoch total loss 0.528166413\n",
      "Trained batch 1709 batch loss 0.483022809 epoch total loss 0.52814\n",
      "Trained batch 1710 batch loss 0.41491735 epoch total loss 0.528073788\n",
      "Trained batch 1711 batch loss 0.417154253 epoch total loss 0.528008938\n",
      "Trained batch 1712 batch loss 0.335262865 epoch total loss 0.527896404\n",
      "Trained batch 1713 batch loss 0.500195146 epoch total loss 0.527880192\n",
      "Trained batch 1714 batch loss 0.430392712 epoch total loss 0.527823329\n",
      "Trained batch 1715 batch loss 0.579030573 epoch total loss 0.527853191\n",
      "Trained batch 1716 batch loss 0.626850486 epoch total loss 0.527910888\n",
      "Trained batch 1717 batch loss 0.503128111 epoch total loss 0.527896464\n",
      "Trained batch 1718 batch loss 0.475984871 epoch total loss 0.527866244\n",
      "Trained batch 1719 batch loss 0.509659052 epoch total loss 0.527855635\n",
      "Trained batch 1720 batch loss 0.582127929 epoch total loss 0.527887225\n",
      "Trained batch 1721 batch loss 0.544887602 epoch total loss 0.52789706\n",
      "Trained batch 1722 batch loss 0.487749189 epoch total loss 0.527873755\n",
      "Trained batch 1723 batch loss 0.523825049 epoch total loss 0.52787137\n",
      "Trained batch 1724 batch loss 0.472648919 epoch total loss 0.527839363\n",
      "Trained batch 1725 batch loss 0.398956984 epoch total loss 0.527764678\n",
      "Trained batch 1726 batch loss 0.393840581 epoch total loss 0.527687073\n",
      "Trained batch 1727 batch loss 0.497301936 epoch total loss 0.527669489\n",
      "Trained batch 1728 batch loss 0.490077376 epoch total loss 0.527647734\n",
      "Trained batch 1729 batch loss 0.487434626 epoch total loss 0.527624488\n",
      "Trained batch 1730 batch loss 0.539437115 epoch total loss 0.527631283\n",
      "Trained batch 1731 batch loss 0.491904497 epoch total loss 0.52761066\n",
      "Trained batch 1732 batch loss 0.543485403 epoch total loss 0.527619779\n",
      "Trained batch 1733 batch loss 0.472256422 epoch total loss 0.527587831\n",
      "Trained batch 1734 batch loss 0.577744484 epoch total loss 0.527616739\n",
      "Trained batch 1735 batch loss 0.526991248 epoch total loss 0.527616382\n",
      "Trained batch 1736 batch loss 0.563650906 epoch total loss 0.527637124\n",
      "Trained batch 1737 batch loss 0.465289652 epoch total loss 0.527601242\n",
      "Trained batch 1738 batch loss 0.567301869 epoch total loss 0.527624071\n",
      "Trained batch 1739 batch loss 0.591152668 epoch total loss 0.527660608\n",
      "Trained batch 1740 batch loss 0.575671732 epoch total loss 0.527688205\n",
      "Trained batch 1741 batch loss 0.530320406 epoch total loss 0.527689755\n",
      "Trained batch 1742 batch loss 0.588285625 epoch total loss 0.527724504\n",
      "Trained batch 1743 batch loss 0.620890141 epoch total loss 0.52777797\n",
      "Trained batch 1744 batch loss 0.519291461 epoch total loss 0.527773082\n",
      "Trained batch 1745 batch loss 0.56019038 epoch total loss 0.527791679\n",
      "Trained batch 1746 batch loss 0.559579849 epoch total loss 0.527809858\n",
      "Trained batch 1747 batch loss 0.575276852 epoch total loss 0.527837038\n",
      "Trained batch 1748 batch loss 0.54655993 epoch total loss 0.527847767\n",
      "Trained batch 1749 batch loss 0.503246844 epoch total loss 0.5278337\n",
      "Trained batch 1750 batch loss 0.502480388 epoch total loss 0.527819216\n",
      "Trained batch 1751 batch loss 0.492627382 epoch total loss 0.52779907\n",
      "Trained batch 1752 batch loss 0.440890074 epoch total loss 0.527749479\n",
      "Trained batch 1753 batch loss 0.521620095 epoch total loss 0.527746\n",
      "Trained batch 1754 batch loss 0.581960499 epoch total loss 0.527776897\n",
      "Trained batch 1755 batch loss 0.618436337 epoch total loss 0.527828574\n",
      "Trained batch 1756 batch loss 0.599474 epoch total loss 0.527869344\n",
      "Trained batch 1757 batch loss 0.633639 epoch total loss 0.527929604\n",
      "Trained batch 1758 batch loss 0.515102327 epoch total loss 0.527922273\n",
      "Trained batch 1759 batch loss 0.519186378 epoch total loss 0.527917266\n",
      "Trained batch 1760 batch loss 0.594511509 epoch total loss 0.527955115\n",
      "Trained batch 1761 batch loss 0.525977552 epoch total loss 0.527954\n",
      "Trained batch 1762 batch loss 0.552471936 epoch total loss 0.52796793\n",
      "Trained batch 1763 batch loss 0.500109 epoch total loss 0.527952135\n",
      "Trained batch 1764 batch loss 0.565147698 epoch total loss 0.527973175\n",
      "Trained batch 1765 batch loss 0.469305694 epoch total loss 0.52794\n",
      "Trained batch 1766 batch loss 0.517620504 epoch total loss 0.527934134\n",
      "Trained batch 1767 batch loss 0.551555157 epoch total loss 0.527947485\n",
      "Trained batch 1768 batch loss 0.606246412 epoch total loss 0.527991772\n",
      "Trained batch 1769 batch loss 0.626007378 epoch total loss 0.528047204\n",
      "Trained batch 1770 batch loss 0.582437694 epoch total loss 0.52807796\n",
      "Trained batch 1771 batch loss 0.624685466 epoch total loss 0.528132498\n",
      "Trained batch 1772 batch loss 0.566827893 epoch total loss 0.528154373\n",
      "Trained batch 1773 batch loss 0.566094637 epoch total loss 0.528175771\n",
      "Trained batch 1774 batch loss 0.487237811 epoch total loss 0.528152704\n",
      "Trained batch 1775 batch loss 0.427552432 epoch total loss 0.528096\n",
      "Trained batch 1776 batch loss 0.444258243 epoch total loss 0.528048813\n",
      "Trained batch 1777 batch loss 0.472502381 epoch total loss 0.528017521\n",
      "Trained batch 1778 batch loss 0.449772328 epoch total loss 0.527973533\n",
      "Trained batch 1779 batch loss 0.449642569 epoch total loss 0.527929485\n",
      "Trained batch 1780 batch loss 0.489062399 epoch total loss 0.52790767\n",
      "Trained batch 1781 batch loss 0.532678604 epoch total loss 0.527910352\n",
      "Trained batch 1782 batch loss 0.57658869 epoch total loss 0.527937651\n",
      "Trained batch 1783 batch loss 0.518849671 epoch total loss 0.527932584\n",
      "Trained batch 1784 batch loss 0.601933658 epoch total loss 0.527974069\n",
      "Trained batch 1785 batch loss 0.568829298 epoch total loss 0.527996957\n",
      "Trained batch 1786 batch loss 0.481993705 epoch total loss 0.527971208\n",
      "Trained batch 1787 batch loss 0.594554245 epoch total loss 0.528008461\n",
      "Trained batch 1788 batch loss 0.666027308 epoch total loss 0.528085649\n",
      "Trained batch 1789 batch loss 0.666325212 epoch total loss 0.528162897\n",
      "Trained batch 1790 batch loss 0.527453065 epoch total loss 0.528162479\n",
      "Trained batch 1791 batch loss 0.711542904 epoch total loss 0.52826488\n",
      "Trained batch 1792 batch loss 0.531995177 epoch total loss 0.528266966\n",
      "Trained batch 1793 batch loss 0.553793907 epoch total loss 0.528281212\n",
      "Trained batch 1794 batch loss 0.538446426 epoch total loss 0.528286874\n",
      "Trained batch 1795 batch loss 0.546277165 epoch total loss 0.528296888\n",
      "Trained batch 1796 batch loss 0.465144277 epoch total loss 0.528261721\n",
      "Trained batch 1797 batch loss 0.374997616 epoch total loss 0.528176427\n",
      "Trained batch 1798 batch loss 0.358025312 epoch total loss 0.528081834\n",
      "Trained batch 1799 batch loss 0.46165207 epoch total loss 0.528044879\n",
      "Trained batch 1800 batch loss 0.486801326 epoch total loss 0.528022\n",
      "Trained batch 1801 batch loss 0.500264406 epoch total loss 0.528006554\n",
      "Trained batch 1802 batch loss 0.531819403 epoch total loss 0.52800864\n",
      "Trained batch 1803 batch loss 0.453027338 epoch total loss 0.527967095\n",
      "Trained batch 1804 batch loss 0.3739402 epoch total loss 0.527881682\n",
      "Trained batch 1805 batch loss 0.35921526 epoch total loss 0.527788222\n",
      "Trained batch 1806 batch loss 0.379185379 epoch total loss 0.527705967\n",
      "Trained batch 1807 batch loss 0.373533636 epoch total loss 0.527620673\n",
      "Trained batch 1808 batch loss 0.398927569 epoch total loss 0.527549446\n",
      "Trained batch 1809 batch loss 0.414434254 epoch total loss 0.52748692\n",
      "Trained batch 1810 batch loss 0.467328191 epoch total loss 0.527453721\n",
      "Trained batch 1811 batch loss 0.409995824 epoch total loss 0.527388871\n",
      "Trained batch 1812 batch loss 0.518296838 epoch total loss 0.527383804\n",
      "Trained batch 1813 batch loss 0.458779186 epoch total loss 0.527346\n",
      "Trained batch 1814 batch loss 0.443682909 epoch total loss 0.527299881\n",
      "Trained batch 1815 batch loss 0.550339699 epoch total loss 0.527312577\n",
      "Trained batch 1816 batch loss 0.5354352 epoch total loss 0.527317047\n",
      "Trained batch 1817 batch loss 0.459676087 epoch total loss 0.527279854\n",
      "Trained batch 1818 batch loss 0.485749483 epoch total loss 0.527257\n",
      "Trained batch 1819 batch loss 0.52774483 epoch total loss 0.527257264\n",
      "Trained batch 1820 batch loss 0.526404917 epoch total loss 0.527256846\n",
      "Trained batch 1821 batch loss 0.51988703 epoch total loss 0.527252793\n",
      "Trained batch 1822 batch loss 0.549074113 epoch total loss 0.527264774\n",
      "Trained batch 1823 batch loss 0.596956253 epoch total loss 0.527303\n",
      "Trained batch 1824 batch loss 0.626024663 epoch total loss 0.527357161\n",
      "Trained batch 1825 batch loss 0.59545821 epoch total loss 0.527394474\n",
      "Trained batch 1826 batch loss 0.530713499 epoch total loss 0.527396262\n",
      "Trained batch 1827 batch loss 0.572155118 epoch total loss 0.527420759\n",
      "Trained batch 1828 batch loss 0.630424738 epoch total loss 0.527477086\n",
      "Trained batch 1829 batch loss 0.563487172 epoch total loss 0.527496815\n",
      "Trained batch 1830 batch loss 0.531242251 epoch total loss 0.527498841\n",
      "Trained batch 1831 batch loss 0.496115804 epoch total loss 0.527481675\n",
      "Trained batch 1832 batch loss 0.552090764 epoch total loss 0.527495086\n",
      "Trained batch 1833 batch loss 0.577334166 epoch total loss 0.527522266\n",
      "Trained batch 1834 batch loss 0.528050244 epoch total loss 0.527522624\n",
      "Trained batch 1835 batch loss 0.553464174 epoch total loss 0.52753675\n",
      "Trained batch 1836 batch loss 0.508910537 epoch total loss 0.527526617\n",
      "Trained batch 1837 batch loss 0.496754825 epoch total loss 0.527509868\n",
      "Trained batch 1838 batch loss 0.504481435 epoch total loss 0.527497292\n",
      "Trained batch 1839 batch loss 0.459782183 epoch total loss 0.527460456\n",
      "Trained batch 1840 batch loss 0.482079417 epoch total loss 0.52743578\n",
      "Trained batch 1841 batch loss 0.494734645 epoch total loss 0.527418\n",
      "Trained batch 1842 batch loss 0.534540892 epoch total loss 0.527421892\n",
      "Trained batch 1843 batch loss 0.531823695 epoch total loss 0.527424276\n",
      "Trained batch 1844 batch loss 0.594329298 epoch total loss 0.527460575\n",
      "Trained batch 1845 batch loss 0.604787111 epoch total loss 0.527502477\n",
      "Trained batch 1846 batch loss 0.618405938 epoch total loss 0.527551711\n",
      "Trained batch 1847 batch loss 0.569041669 epoch total loss 0.527574182\n",
      "Trained batch 1848 batch loss 0.581097543 epoch total loss 0.527603149\n",
      "Trained batch 1849 batch loss 0.479487 epoch total loss 0.527577102\n",
      "Trained batch 1850 batch loss 0.527737558 epoch total loss 0.527577221\n",
      "Trained batch 1851 batch loss 0.51719588 epoch total loss 0.527571619\n",
      "Trained batch 1852 batch loss 0.500668406 epoch total loss 0.527557075\n",
      "Trained batch 1853 batch loss 0.53347528 epoch total loss 0.527560234\n",
      "Trained batch 1854 batch loss 0.518096387 epoch total loss 0.527555108\n",
      "Trained batch 1855 batch loss 0.529228926 epoch total loss 0.527556062\n",
      "Trained batch 1856 batch loss 0.500310779 epoch total loss 0.527541339\n",
      "Trained batch 1857 batch loss 0.427995741 epoch total loss 0.527487755\n",
      "Trained batch 1858 batch loss 0.507265091 epoch total loss 0.527476847\n",
      "Trained batch 1859 batch loss 0.546031892 epoch total loss 0.527486861\n",
      "Trained batch 1860 batch loss 0.571253121 epoch total loss 0.527510345\n",
      "Trained batch 1861 batch loss 0.527583539 epoch total loss 0.527510405\n",
      "Trained batch 1862 batch loss 0.614862502 epoch total loss 0.527557313\n",
      "Trained batch 1863 batch loss 0.561089575 epoch total loss 0.527575314\n",
      "Trained batch 1864 batch loss 0.55640465 epoch total loss 0.527590752\n",
      "Trained batch 1865 batch loss 0.509161472 epoch total loss 0.527580917\n",
      "Trained batch 1866 batch loss 0.533733964 epoch total loss 0.527584195\n",
      "Trained batch 1867 batch loss 0.567828357 epoch total loss 0.527605712\n",
      "Trained batch 1868 batch loss 0.556354582 epoch total loss 0.52762109\n",
      "Trained batch 1869 batch loss 0.528151035 epoch total loss 0.527621388\n",
      "Trained batch 1870 batch loss 0.557376 epoch total loss 0.527637303\n",
      "Trained batch 1871 batch loss 0.511722744 epoch total loss 0.527628779\n",
      "Trained batch 1872 batch loss 0.518550396 epoch total loss 0.527623951\n",
      "Trained batch 1873 batch loss 0.536483169 epoch total loss 0.52762866\n",
      "Trained batch 1874 batch loss 0.630138338 epoch total loss 0.527683377\n",
      "Trained batch 1875 batch loss 0.58007288 epoch total loss 0.527711332\n",
      "Trained batch 1876 batch loss 0.595616102 epoch total loss 0.527747512\n",
      "Trained batch 1877 batch loss 0.552395582 epoch total loss 0.527760684\n",
      "Trained batch 1878 batch loss 0.442522138 epoch total loss 0.527715266\n",
      "Trained batch 1879 batch loss 0.452144891 epoch total loss 0.527675033\n",
      "Trained batch 1880 batch loss 0.480327606 epoch total loss 0.527649879\n",
      "Trained batch 1881 batch loss 0.527705669 epoch total loss 0.527649879\n",
      "Trained batch 1882 batch loss 0.453600764 epoch total loss 0.52761054\n",
      "Trained batch 1883 batch loss 0.487053216 epoch total loss 0.527589\n",
      "Trained batch 1884 batch loss 0.408159286 epoch total loss 0.527525604\n",
      "Trained batch 1885 batch loss 0.409085631 epoch total loss 0.52746278\n",
      "Trained batch 1886 batch loss 0.412749708 epoch total loss 0.527401924\n",
      "Trained batch 1887 batch loss 0.531619549 epoch total loss 0.527404189\n",
      "Trained batch 1888 batch loss 0.507214963 epoch total loss 0.52739346\n",
      "Trained batch 1889 batch loss 0.471635401 epoch total loss 0.527363956\n",
      "Trained batch 1890 batch loss 0.500742 epoch total loss 0.52734983\n",
      "Trained batch 1891 batch loss 0.527495205 epoch total loss 0.527349889\n",
      "Trained batch 1892 batch loss 0.513802469 epoch total loss 0.527342737\n",
      "Trained batch 1893 batch loss 0.48417154 epoch total loss 0.527319968\n",
      "Trained batch 1894 batch loss 0.535628676 epoch total loss 0.527324378\n",
      "Trained batch 1895 batch loss 0.467835 epoch total loss 0.527292967\n",
      "Trained batch 1896 batch loss 0.565868497 epoch total loss 0.527313292\n",
      "Trained batch 1897 batch loss 0.495717049 epoch total loss 0.527296662\n",
      "Trained batch 1898 batch loss 0.413310885 epoch total loss 0.527236581\n",
      "Trained batch 1899 batch loss 0.457707226 epoch total loss 0.5272\n",
      "Trained batch 1900 batch loss 0.442429602 epoch total loss 0.527155399\n",
      "Trained batch 1901 batch loss 0.591683805 epoch total loss 0.527189314\n",
      "Trained batch 1902 batch loss 0.578324914 epoch total loss 0.527216196\n",
      "Trained batch 1903 batch loss 0.539817035 epoch total loss 0.527222812\n",
      "Trained batch 1904 batch loss 0.555058181 epoch total loss 0.527237415\n",
      "Trained batch 1905 batch loss 0.580357909 epoch total loss 0.52726531\n",
      "Trained batch 1906 batch loss 0.550024748 epoch total loss 0.527277291\n",
      "Trained batch 1907 batch loss 0.544482172 epoch total loss 0.527286291\n",
      "Trained batch 1908 batch loss 0.509804845 epoch total loss 0.527277172\n",
      "Trained batch 1909 batch loss 0.529161811 epoch total loss 0.527278125\n",
      "Trained batch 1910 batch loss 0.580383241 epoch total loss 0.527305961\n",
      "Trained batch 1911 batch loss 0.500004351 epoch total loss 0.527291656\n",
      "Trained batch 1912 batch loss 0.549594343 epoch total loss 0.527303338\n",
      "Trained batch 1913 batch loss 0.561720371 epoch total loss 0.527321339\n",
      "Trained batch 1914 batch loss 0.55334425 epoch total loss 0.527334929\n",
      "Trained batch 1915 batch loss 0.64857018 epoch total loss 0.527398229\n",
      "Trained batch 1916 batch loss 0.606600761 epoch total loss 0.527439594\n",
      "Trained batch 1917 batch loss 0.487731189 epoch total loss 0.527418852\n",
      "Trained batch 1918 batch loss 0.537335634 epoch total loss 0.527424037\n",
      "Trained batch 1919 batch loss 0.493805975 epoch total loss 0.527406514\n",
      "Trained batch 1920 batch loss 0.58106029 epoch total loss 0.527434468\n",
      "Trained batch 1921 batch loss 0.509203374 epoch total loss 0.527425\n",
      "Trained batch 1922 batch loss 0.552872837 epoch total loss 0.527438223\n",
      "Trained batch 1923 batch loss 0.554855764 epoch total loss 0.527452469\n",
      "Trained batch 1924 batch loss 0.599836648 epoch total loss 0.527490139\n",
      "Trained batch 1925 batch loss 0.674701 epoch total loss 0.527566612\n",
      "Trained batch 1926 batch loss 0.577572286 epoch total loss 0.52759254\n",
      "Trained batch 1927 batch loss 0.518629789 epoch total loss 0.527587891\n",
      "Trained batch 1928 batch loss 0.457138956 epoch total loss 0.527551353\n",
      "Trained batch 1929 batch loss 0.493300676 epoch total loss 0.527533591\n",
      "Trained batch 1930 batch loss 0.407544613 epoch total loss 0.527471423\n",
      "Trained batch 1931 batch loss 0.468199223 epoch total loss 0.527440727\n",
      "Trained batch 1932 batch loss 0.525075853 epoch total loss 0.527439475\n",
      "Trained batch 1933 batch loss 0.486810744 epoch total loss 0.527418494\n",
      "Trained batch 1934 batch loss 0.579353809 epoch total loss 0.527445316\n",
      "Trained batch 1935 batch loss 0.546087 epoch total loss 0.527455\n",
      "Trained batch 1936 batch loss 0.449493468 epoch total loss 0.527414739\n",
      "Trained batch 1937 batch loss 0.460764229 epoch total loss 0.527380288\n",
      "Trained batch 1938 batch loss 0.564487517 epoch total loss 0.52739948\n",
      "Trained batch 1939 batch loss 0.512945473 epoch total loss 0.527392\n",
      "Trained batch 1940 batch loss 0.576756835 epoch total loss 0.527417481\n",
      "Trained batch 1941 batch loss 0.631860077 epoch total loss 0.527471244\n",
      "Trained batch 1942 batch loss 0.508441 epoch total loss 0.527461469\n",
      "Trained batch 1943 batch loss 0.671889544 epoch total loss 0.527535796\n",
      "Trained batch 1944 batch loss 0.611430526 epoch total loss 0.52757895\n",
      "Trained batch 1945 batch loss 0.549932778 epoch total loss 0.527590454\n",
      "Trained batch 1946 batch loss 0.543330312 epoch total loss 0.52759856\n",
      "Trained batch 1947 batch loss 0.528625607 epoch total loss 0.527599096\n",
      "Trained batch 1948 batch loss 0.647227407 epoch total loss 0.527660489\n",
      "Trained batch 1949 batch loss 0.679199696 epoch total loss 0.527738273\n",
      "Trained batch 1950 batch loss 0.65871042 epoch total loss 0.527805448\n",
      "Trained batch 1951 batch loss 0.601424813 epoch total loss 0.527843177\n",
      "Trained batch 1952 batch loss 0.612903535 epoch total loss 0.527886748\n",
      "Trained batch 1953 batch loss 0.623585641 epoch total loss 0.527935743\n",
      "Trained batch 1954 batch loss 0.63188 epoch total loss 0.527988911\n",
      "Trained batch 1955 batch loss 0.536741674 epoch total loss 0.527993381\n",
      "Trained batch 1956 batch loss 0.596078694 epoch total loss 0.52802819\n",
      "Trained batch 1957 batch loss 0.526143193 epoch total loss 0.528027177\n",
      "Trained batch 1958 batch loss 0.437796891 epoch total loss 0.527981102\n",
      "Trained batch 1959 batch loss 0.46610406 epoch total loss 0.527949512\n",
      "Trained batch 1960 batch loss 0.355886519 epoch total loss 0.527861655\n",
      "Trained batch 1961 batch loss 0.578457832 epoch total loss 0.527887523\n",
      "Trained batch 1962 batch loss 0.42884019 epoch total loss 0.527837038\n",
      "Trained batch 1963 batch loss 0.476301342 epoch total loss 0.527810752\n",
      "Trained batch 1964 batch loss 0.488537937 epoch total loss 0.527790785\n",
      "Trained batch 1965 batch loss 0.463793814 epoch total loss 0.527758181\n",
      "Trained batch 1966 batch loss 0.43687883 epoch total loss 0.527711928\n",
      "Trained batch 1967 batch loss 0.447745621 epoch total loss 0.527671278\n",
      "Trained batch 1968 batch loss 0.565264165 epoch total loss 0.527690411\n",
      "Trained batch 1969 batch loss 0.501597345 epoch total loss 0.527677178\n",
      "Trained batch 1970 batch loss 0.584378362 epoch total loss 0.527705908\n",
      "Trained batch 1971 batch loss 0.528378 epoch total loss 0.527706265\n",
      "Trained batch 1972 batch loss 0.442625284 epoch total loss 0.527663112\n",
      "Trained batch 1973 batch loss 0.467712641 epoch total loss 0.527632773\n",
      "Trained batch 1974 batch loss 0.520485401 epoch total loss 0.527629137\n",
      "Trained batch 1975 batch loss 0.580666423 epoch total loss 0.527656\n",
      "Trained batch 1976 batch loss 0.516344845 epoch total loss 0.527650297\n",
      "Trained batch 1977 batch loss 0.541599035 epoch total loss 0.52765733\n",
      "Trained batch 1978 batch loss 0.584674478 epoch total loss 0.527686179\n",
      "Trained batch 1979 batch loss 0.680903196 epoch total loss 0.527763605\n",
      "Trained batch 1980 batch loss 0.669566751 epoch total loss 0.52783525\n",
      "Trained batch 1981 batch loss 0.665715814 epoch total loss 0.527904868\n",
      "Trained batch 1982 batch loss 0.690110207 epoch total loss 0.527986705\n",
      "Trained batch 1983 batch loss 0.648728609 epoch total loss 0.528047562\n",
      "Trained batch 1984 batch loss 0.61130774 epoch total loss 0.528089523\n",
      "Trained batch 1985 batch loss 0.558968246 epoch total loss 0.52810508\n",
      "Trained batch 1986 batch loss 0.431715906 epoch total loss 0.528056562\n",
      "Trained batch 1987 batch loss 0.460616738 epoch total loss 0.528022587\n",
      "Trained batch 1988 batch loss 0.516899049 epoch total loss 0.528017\n",
      "Trained batch 1989 batch loss 0.521652341 epoch total loss 0.528013766\n",
      "Trained batch 1990 batch loss 0.641601443 epoch total loss 0.528070807\n",
      "Trained batch 1991 batch loss 0.590162873 epoch total loss 0.52810204\n",
      "Trained batch 1992 batch loss 0.631729186 epoch total loss 0.528154075\n",
      "Trained batch 1993 batch loss 0.599522114 epoch total loss 0.528189838\n",
      "Trained batch 1994 batch loss 0.55973196 epoch total loss 0.528205633\n",
      "Trained batch 1995 batch loss 0.510086536 epoch total loss 0.528196573\n",
      "Trained batch 1996 batch loss 0.592047274 epoch total loss 0.528228581\n",
      "Trained batch 1997 batch loss 0.582222641 epoch total loss 0.528255641\n",
      "Trained batch 1998 batch loss 0.567657173 epoch total loss 0.528275371\n",
      "Trained batch 1999 batch loss 0.489697725 epoch total loss 0.528256059\n",
      "Trained batch 2000 batch loss 0.539989412 epoch total loss 0.52826196\n",
      "Trained batch 2001 batch loss 0.592952311 epoch total loss 0.528294265\n",
      "Trained batch 2002 batch loss 0.535679817 epoch total loss 0.528297961\n",
      "Trained batch 2003 batch loss 0.471355438 epoch total loss 0.52826947\n",
      "Trained batch 2004 batch loss 0.563967824 epoch total loss 0.528287292\n",
      "Trained batch 2005 batch loss 0.51144433 epoch total loss 0.528278887\n",
      "Trained batch 2006 batch loss 0.421211809 epoch total loss 0.528225541\n",
      "Trained batch 2007 batch loss 0.473493546 epoch total loss 0.528198302\n",
      "Trained batch 2008 batch loss 0.466196775 epoch total loss 0.528167427\n",
      "Trained batch 2009 batch loss 0.559451699 epoch total loss 0.528183\n",
      "Trained batch 2010 batch loss 0.478613973 epoch total loss 0.528158367\n",
      "Trained batch 2011 batch loss 0.442870796 epoch total loss 0.528115928\n",
      "Trained batch 2012 batch loss 0.497676969 epoch total loss 0.528100789\n",
      "Trained batch 2013 batch loss 0.479242682 epoch total loss 0.52807653\n",
      "Trained batch 2014 batch loss 0.523798227 epoch total loss 0.528074443\n",
      "Trained batch 2015 batch loss 0.501174033 epoch total loss 0.528061092\n",
      "Trained batch 2016 batch loss 0.410320491 epoch total loss 0.528002679\n",
      "Trained batch 2017 batch loss 0.447622687 epoch total loss 0.527962804\n",
      "Trained batch 2018 batch loss 0.488211334 epoch total loss 0.527943075\n",
      "Trained batch 2019 batch loss 0.407973647 epoch total loss 0.527883649\n",
      "Trained batch 2020 batch loss 0.485644698 epoch total loss 0.527862728\n",
      "Trained batch 2021 batch loss 0.532179773 epoch total loss 0.527864873\n",
      "Trained batch 2022 batch loss 0.470764458 epoch total loss 0.52783668\n",
      "Trained batch 2023 batch loss 0.596791804 epoch total loss 0.527870774\n",
      "Trained batch 2024 batch loss 0.627814054 epoch total loss 0.527920127\n",
      "Trained batch 2025 batch loss 0.510068357 epoch total loss 0.527911305\n",
      "Trained batch 2026 batch loss 0.584880948 epoch total loss 0.527939379\n",
      "Trained batch 2027 batch loss 0.474734604 epoch total loss 0.527913153\n",
      "Trained batch 2028 batch loss 0.488291949 epoch total loss 0.527893603\n",
      "Trained batch 2029 batch loss 0.577323258 epoch total loss 0.527917922\n",
      "Trained batch 2030 batch loss 0.512648106 epoch total loss 0.527910471\n",
      "Trained batch 2031 batch loss 0.454497695 epoch total loss 0.527874291\n",
      "Trained batch 2032 batch loss 0.525844336 epoch total loss 0.527873278\n",
      "Trained batch 2033 batch loss 0.520552456 epoch total loss 0.527869701\n",
      "Trained batch 2034 batch loss 0.427613258 epoch total loss 0.527820408\n",
      "Trained batch 2035 batch loss 0.481386 epoch total loss 0.52779758\n",
      "Trained batch 2036 batch loss 0.506557584 epoch total loss 0.527787209\n",
      "Trained batch 2037 batch loss 0.585165858 epoch total loss 0.527815342\n",
      "Trained batch 2038 batch loss 0.538823545 epoch total loss 0.527820766\n",
      "Trained batch 2039 batch loss 0.543506145 epoch total loss 0.527828455\n",
      "Trained batch 2040 batch loss 0.566552639 epoch total loss 0.527847409\n",
      "Trained batch 2041 batch loss 0.3591353 epoch total loss 0.527764738\n",
      "Trained batch 2042 batch loss 0.55761373 epoch total loss 0.527779341\n",
      "Trained batch 2043 batch loss 0.501372516 epoch total loss 0.527766407\n",
      "Trained batch 2044 batch loss 0.578149319 epoch total loss 0.527791083\n",
      "Trained batch 2045 batch loss 0.547230661 epoch total loss 0.52780056\n",
      "Trained batch 2046 batch loss 0.587242544 epoch total loss 0.527829647\n",
      "Trained batch 2047 batch loss 0.567664 epoch total loss 0.527849078\n",
      "Trained batch 2048 batch loss 0.644117832 epoch total loss 0.527905881\n",
      "Trained batch 2049 batch loss 0.61797142 epoch total loss 0.52794981\n",
      "Trained batch 2050 batch loss 0.534918427 epoch total loss 0.527953207\n",
      "Trained batch 2051 batch loss 0.532405615 epoch total loss 0.527955353\n",
      "Trained batch 2052 batch loss 0.53518182 epoch total loss 0.52795887\n",
      "Trained batch 2053 batch loss 0.548589408 epoch total loss 0.527968884\n",
      "Trained batch 2054 batch loss 0.51990962 epoch total loss 0.52796495\n",
      "Trained batch 2055 batch loss 0.621492743 epoch total loss 0.528010488\n",
      "Trained batch 2056 batch loss 0.549357653 epoch total loss 0.528020859\n",
      "Trained batch 2057 batch loss 0.506391943 epoch total loss 0.528010309\n",
      "Trained batch 2058 batch loss 0.630807042 epoch total loss 0.528060257\n",
      "Trained batch 2059 batch loss 0.580243766 epoch total loss 0.528085589\n",
      "Trained batch 2060 batch loss 0.616517127 epoch total loss 0.528128564\n",
      "Trained batch 2061 batch loss 0.581975698 epoch total loss 0.528154731\n",
      "Trained batch 2062 batch loss 0.545817733 epoch total loss 0.528163254\n",
      "Trained batch 2063 batch loss 0.498013437 epoch total loss 0.528148651\n",
      "Trained batch 2064 batch loss 0.577626288 epoch total loss 0.528172612\n",
      "Trained batch 2065 batch loss 0.620026886 epoch total loss 0.528217077\n",
      "Trained batch 2066 batch loss 0.616840124 epoch total loss 0.52826\n",
      "Trained batch 2067 batch loss 0.546079278 epoch total loss 0.528268576\n",
      "Trained batch 2068 batch loss 0.543559 epoch total loss 0.528275967\n",
      "Trained batch 2069 batch loss 0.500718713 epoch total loss 0.528262675\n",
      "Trained batch 2070 batch loss 0.535109 epoch total loss 0.528266\n",
      "Trained batch 2071 batch loss 0.561433733 epoch total loss 0.528282\n",
      "Trained batch 2072 batch loss 0.492714047 epoch total loss 0.528264821\n",
      "Trained batch 2073 batch loss 0.594055533 epoch total loss 0.52829659\n",
      "Trained batch 2074 batch loss 0.566609502 epoch total loss 0.528315067\n",
      "Trained batch 2075 batch loss 0.564256072 epoch total loss 0.528332353\n",
      "Trained batch 2076 batch loss 0.51922816 epoch total loss 0.528328\n",
      "Trained batch 2077 batch loss 0.541496217 epoch total loss 0.528334379\n",
      "Trained batch 2078 batch loss 0.545190096 epoch total loss 0.528342485\n",
      "Trained batch 2079 batch loss 0.515990853 epoch total loss 0.528336525\n",
      "Trained batch 2080 batch loss 0.473680377 epoch total loss 0.528310239\n",
      "Trained batch 2081 batch loss 0.483418286 epoch total loss 0.528288662\n",
      "Trained batch 2082 batch loss 0.501088262 epoch total loss 0.528275549\n",
      "Trained batch 2083 batch loss 0.513300657 epoch total loss 0.528268397\n",
      "Trained batch 2084 batch loss 0.540396929 epoch total loss 0.528274238\n",
      "Trained batch 2085 batch loss 0.525467873 epoch total loss 0.528272867\n",
      "Trained batch 2086 batch loss 0.515591323 epoch total loss 0.528266847\n",
      "Trained batch 2087 batch loss 0.454917192 epoch total loss 0.52823168\n",
      "Trained batch 2088 batch loss 0.48235932 epoch total loss 0.528209686\n",
      "Trained batch 2089 batch loss 0.419926584 epoch total loss 0.52815789\n",
      "Trained batch 2090 batch loss 0.51181525 epoch total loss 0.528150082\n",
      "Trained batch 2091 batch loss 0.401946783 epoch total loss 0.528089702\n",
      "Trained batch 2092 batch loss 0.414011955 epoch total loss 0.528035223\n",
      "Trained batch 2093 batch loss 0.386946321 epoch total loss 0.527967811\n",
      "Trained batch 2094 batch loss 0.498168886 epoch total loss 0.527953565\n",
      "Trained batch 2095 batch loss 0.532404661 epoch total loss 0.527955651\n",
      "Trained batch 2096 batch loss 0.573402286 epoch total loss 0.527977347\n",
      "Trained batch 2097 batch loss 0.570492327 epoch total loss 0.527997613\n",
      "Trained batch 2098 batch loss 0.629584074 epoch total loss 0.528046\n",
      "Trained batch 2099 batch loss 0.588834763 epoch total loss 0.528075\n",
      "Trained batch 2100 batch loss 0.613594294 epoch total loss 0.528115749\n",
      "Trained batch 2101 batch loss 0.636327505 epoch total loss 0.528167248\n",
      "Trained batch 2102 batch loss 0.647996664 epoch total loss 0.52822423\n",
      "Trained batch 2103 batch loss 0.627411485 epoch total loss 0.528271437\n",
      "Trained batch 2104 batch loss 0.537476897 epoch total loss 0.528275788\n",
      "Trained batch 2105 batch loss 0.576966226 epoch total loss 0.528299\n",
      "Trained batch 2106 batch loss 0.550554097 epoch total loss 0.528309524\n",
      "Trained batch 2107 batch loss 0.591557622 epoch total loss 0.528339565\n",
      "Trained batch 2108 batch loss 0.56997478 epoch total loss 0.528359294\n",
      "Trained batch 2109 batch loss 0.527202606 epoch total loss 0.528358757\n",
      "Trained batch 2110 batch loss 0.471090645 epoch total loss 0.528331578\n",
      "Trained batch 2111 batch loss 0.466220558 epoch total loss 0.528302133\n",
      "Trained batch 2112 batch loss 0.550654292 epoch total loss 0.528312743\n",
      "Trained batch 2113 batch loss 0.495184571 epoch total loss 0.528297067\n",
      "Trained batch 2114 batch loss 0.522370815 epoch total loss 0.528294265\n",
      "Trained batch 2115 batch loss 0.624153495 epoch total loss 0.528339565\n",
      "Trained batch 2116 batch loss 0.498183459 epoch total loss 0.528325319\n",
      "Trained batch 2117 batch loss 0.502779782 epoch total loss 0.528313279\n",
      "Trained batch 2118 batch loss 0.492431074 epoch total loss 0.528296351\n",
      "Trained batch 2119 batch loss 0.594433904 epoch total loss 0.528327584\n",
      "Trained batch 2120 batch loss 0.572288394 epoch total loss 0.528348267\n",
      "Trained batch 2121 batch loss 0.502896547 epoch total loss 0.528336287\n",
      "Trained batch 2122 batch loss 0.601672471 epoch total loss 0.528370857\n",
      "Trained batch 2123 batch loss 0.552049279 epoch total loss 0.528382\n",
      "Trained batch 2124 batch loss 0.563018322 epoch total loss 0.528398275\n",
      "Trained batch 2125 batch loss 0.517554343 epoch total loss 0.528393209\n",
      "Trained batch 2126 batch loss 0.506363094 epoch total loss 0.528382838\n",
      "Trained batch 2127 batch loss 0.534696221 epoch total loss 0.528385818\n",
      "Trained batch 2128 batch loss 0.541157544 epoch total loss 0.528391778\n",
      "Trained batch 2129 batch loss 0.62245822 epoch total loss 0.528435946\n",
      "Trained batch 2130 batch loss 0.606068909 epoch total loss 0.528472424\n",
      "Trained batch 2131 batch loss 0.571913123 epoch total loss 0.528492808\n",
      "Trained batch 2132 batch loss 0.530820966 epoch total loss 0.528493881\n",
      "Trained batch 2133 batch loss 0.571250319 epoch total loss 0.528513908\n",
      "Trained batch 2134 batch loss 0.513554573 epoch total loss 0.528506935\n",
      "Trained batch 2135 batch loss 0.575016141 epoch total loss 0.52852869\n",
      "Trained batch 2136 batch loss 0.593705595 epoch total loss 0.528559268\n",
      "Trained batch 2137 batch loss 0.554658711 epoch total loss 0.528571486\n",
      "Trained batch 2138 batch loss 0.545711458 epoch total loss 0.528579473\n",
      "Trained batch 2139 batch loss 0.538994968 epoch total loss 0.528584301\n",
      "Trained batch 2140 batch loss 0.624588251 epoch total loss 0.528629184\n",
      "Trained batch 2141 batch loss 0.524994493 epoch total loss 0.528627515\n",
      "Trained batch 2142 batch loss 0.544783354 epoch total loss 0.528635085\n",
      "Trained batch 2143 batch loss 0.654672 epoch total loss 0.528693855\n",
      "Trained batch 2144 batch loss 0.593567133 epoch total loss 0.528724134\n",
      "Trained batch 2145 batch loss 0.63739723 epoch total loss 0.528774858\n",
      "Trained batch 2146 batch loss 0.604316533 epoch total loss 0.528810084\n",
      "Trained batch 2147 batch loss 0.493353546 epoch total loss 0.528793573\n",
      "Trained batch 2148 batch loss 0.460730731 epoch total loss 0.528761864\n",
      "Trained batch 2149 batch loss 0.587450147 epoch total loss 0.528789163\n",
      "Trained batch 2150 batch loss 0.566989601 epoch total loss 0.528806925\n",
      "Trained batch 2151 batch loss 0.572174251 epoch total loss 0.528827071\n",
      "Trained batch 2152 batch loss 0.54847008 epoch total loss 0.528836191\n",
      "Trained batch 2153 batch loss 0.531391203 epoch total loss 0.528837383\n",
      "Trained batch 2154 batch loss 0.676284552 epoch total loss 0.528905869\n",
      "Trained batch 2155 batch loss 0.55347 epoch total loss 0.528917253\n",
      "Trained batch 2156 batch loss 0.557179868 epoch total loss 0.528930306\n",
      "Trained batch 2157 batch loss 0.554288208 epoch total loss 0.528942108\n",
      "Trained batch 2158 batch loss 0.485398233 epoch total loss 0.528921902\n",
      "Trained batch 2159 batch loss 0.539394617 epoch total loss 0.52892679\n",
      "Trained batch 2160 batch loss 0.519757032 epoch total loss 0.528922498\n",
      "Trained batch 2161 batch loss 0.636819065 epoch total loss 0.528972447\n",
      "Trained batch 2162 batch loss 0.46832481 epoch total loss 0.528944433\n",
      "Trained batch 2163 batch loss 0.54077208 epoch total loss 0.528949916\n",
      "Trained batch 2164 batch loss 0.60429734 epoch total loss 0.528984725\n",
      "Trained batch 2165 batch loss 0.556627572 epoch total loss 0.528997481\n",
      "Trained batch 2166 batch loss 0.5354352 epoch total loss 0.529000461\n",
      "Trained batch 2167 batch loss 0.560085654 epoch total loss 0.529014766\n",
      "Trained batch 2168 batch loss 0.544194818 epoch total loss 0.52902174\n",
      "Trained batch 2169 batch loss 0.493462443 epoch total loss 0.529005349\n",
      "Trained batch 2170 batch loss 0.513973534 epoch total loss 0.528998375\n",
      "Trained batch 2171 batch loss 0.5378865 epoch total loss 0.529002488\n",
      "Trained batch 2172 batch loss 0.531112552 epoch total loss 0.529003441\n",
      "Trained batch 2173 batch loss 0.569275618 epoch total loss 0.529022\n",
      "Trained batch 2174 batch loss 0.474798679 epoch total loss 0.528997064\n",
      "Trained batch 2175 batch loss 0.394792289 epoch total loss 0.528935373\n",
      "Trained batch 2176 batch loss 0.399454296 epoch total loss 0.528875828\n",
      "Trained batch 2177 batch loss 0.428391308 epoch total loss 0.528829694\n",
      "Trained batch 2178 batch loss 0.425863296 epoch total loss 0.528782427\n",
      "Trained batch 2179 batch loss 0.494143307 epoch total loss 0.528766513\n",
      "Trained batch 2180 batch loss 0.513705134 epoch total loss 0.528759599\n",
      "Trained batch 2181 batch loss 0.468760192 epoch total loss 0.528732061\n",
      "Trained batch 2182 batch loss 0.492514461 epoch total loss 0.528715491\n",
      "Trained batch 2183 batch loss 0.463646322 epoch total loss 0.528685689\n",
      "Trained batch 2184 batch loss 0.476669908 epoch total loss 0.528661847\n",
      "Trained batch 2185 batch loss 0.57514739 epoch total loss 0.528683186\n",
      "Trained batch 2186 batch loss 0.429569185 epoch total loss 0.528637826\n",
      "Trained batch 2187 batch loss 0.54556644 epoch total loss 0.528645575\n",
      "Trained batch 2188 batch loss 0.575733423 epoch total loss 0.528667033\n",
      "Trained batch 2189 batch loss 0.542108119 epoch total loss 0.528673172\n",
      "Trained batch 2190 batch loss 0.643102586 epoch total loss 0.528725445\n",
      "Trained batch 2191 batch loss 0.574219882 epoch total loss 0.528746188\n",
      "Trained batch 2192 batch loss 0.611839354 epoch total loss 0.528784096\n",
      "Trained batch 2193 batch loss 0.610690057 epoch total loss 0.528821468\n",
      "Trained batch 2194 batch loss 0.572797894 epoch total loss 0.528841496\n",
      "Trained batch 2195 batch loss 0.584279299 epoch total loss 0.528866708\n",
      "Trained batch 2196 batch loss 0.507183731 epoch total loss 0.528856874\n",
      "Trained batch 2197 batch loss 0.480632901 epoch total loss 0.528834879\n",
      "Trained batch 2198 batch loss 0.475081801 epoch total loss 0.528810441\n",
      "Trained batch 2199 batch loss 0.466408372 epoch total loss 0.52878207\n",
      "Trained batch 2200 batch loss 0.512250423 epoch total loss 0.5287745\n",
      "Trained batch 2201 batch loss 0.547027588 epoch total loss 0.528782785\n",
      "Trained batch 2202 batch loss 0.548339128 epoch total loss 0.528791666\n",
      "Trained batch 2203 batch loss 0.592678249 epoch total loss 0.528820693\n",
      "Trained batch 2204 batch loss 0.615469337 epoch total loss 0.52886\n",
      "Trained batch 2205 batch loss 0.5996387 epoch total loss 0.5288921\n",
      "Trained batch 2206 batch loss 0.569533587 epoch total loss 0.528910518\n",
      "Trained batch 2207 batch loss 0.518721282 epoch total loss 0.528905869\n",
      "Trained batch 2208 batch loss 0.564045608 epoch total loss 0.528921843\n",
      "Trained batch 2209 batch loss 0.57938695 epoch total loss 0.528944671\n",
      "Trained batch 2210 batch loss 0.479828 epoch total loss 0.528922439\n",
      "Trained batch 2211 batch loss 0.538857877 epoch total loss 0.528926909\n",
      "Trained batch 2212 batch loss 0.450264335 epoch total loss 0.528891385\n",
      "Trained batch 2213 batch loss 0.429334223 epoch total loss 0.528846383\n",
      "Trained batch 2214 batch loss 0.517153263 epoch total loss 0.528841138\n",
      "Trained batch 2215 batch loss 0.519535422 epoch total loss 0.528836906\n",
      "Trained batch 2216 batch loss 0.595167339 epoch total loss 0.528866887\n",
      "Trained batch 2217 batch loss 0.577707529 epoch total loss 0.528888941\n",
      "Trained batch 2218 batch loss 0.595393181 epoch total loss 0.528918862\n",
      "Trained batch 2219 batch loss 0.6100685 epoch total loss 0.52895546\n",
      "Trained batch 2220 batch loss 0.581699133 epoch total loss 0.528979242\n",
      "Trained batch 2221 batch loss 0.612605929 epoch total loss 0.529016852\n",
      "Trained batch 2222 batch loss 0.597261429 epoch total loss 0.529047549\n",
      "Trained batch 2223 batch loss 0.57129246 epoch total loss 0.529066563\n",
      "Trained batch 2224 batch loss 0.500341773 epoch total loss 0.529053688\n",
      "Trained batch 2225 batch loss 0.529125094 epoch total loss 0.529053748\n",
      "Trained batch 2226 batch loss 0.607820451 epoch total loss 0.529089093\n",
      "Trained batch 2227 batch loss 0.56787163 epoch total loss 0.529106498\n",
      "Trained batch 2228 batch loss 0.516644359 epoch total loss 0.529100895\n",
      "Trained batch 2229 batch loss 0.499920249 epoch total loss 0.529087782\n",
      "Trained batch 2230 batch loss 0.514846802 epoch total loss 0.529081404\n",
      "Trained batch 2231 batch loss 0.455862761 epoch total loss 0.529048562\n",
      "Trained batch 2232 batch loss 0.499232382 epoch total loss 0.529035211\n",
      "Trained batch 2233 batch loss 0.443181872 epoch total loss 0.528996825\n",
      "Trained batch 2234 batch loss 0.562926412 epoch total loss 0.529011965\n",
      "Trained batch 2235 batch loss 0.55753231 epoch total loss 0.52902472\n",
      "Trained batch 2236 batch loss 0.571053267 epoch total loss 0.529043496\n",
      "Trained batch 2237 batch loss 0.589801192 epoch total loss 0.529070675\n",
      "Trained batch 2238 batch loss 0.615342855 epoch total loss 0.52910924\n",
      "Trained batch 2239 batch loss 0.538919449 epoch total loss 0.52911365\n",
      "Trained batch 2240 batch loss 0.486548424 epoch total loss 0.529094636\n",
      "Trained batch 2241 batch loss 0.496016026 epoch total loss 0.529079854\n",
      "Trained batch 2242 batch loss 0.461160362 epoch total loss 0.529049575\n",
      "Trained batch 2243 batch loss 0.564785838 epoch total loss 0.529065549\n",
      "Trained batch 2244 batch loss 0.501738191 epoch total loss 0.52905333\n",
      "Trained batch 2245 batch loss 0.413187057 epoch total loss 0.529001713\n",
      "Trained batch 2246 batch loss 0.46202895 epoch total loss 0.52897191\n",
      "Trained batch 2247 batch loss 0.424930513 epoch total loss 0.528925598\n",
      "Trained batch 2248 batch loss 0.494945109 epoch total loss 0.528910518\n",
      "Trained batch 2249 batch loss 0.466627479 epoch total loss 0.528882861\n",
      "Trained batch 2250 batch loss 0.539798856 epoch total loss 0.528887689\n",
      "Trained batch 2251 batch loss 0.492716759 epoch total loss 0.528871596\n",
      "Trained batch 2252 batch loss 0.47742182 epoch total loss 0.528848767\n",
      "Trained batch 2253 batch loss 0.454146415 epoch total loss 0.528815567\n",
      "Trained batch 2254 batch loss 0.520108402 epoch total loss 0.528811753\n",
      "Trained batch 2255 batch loss 0.561270714 epoch total loss 0.528826118\n",
      "Trained batch 2256 batch loss 0.476993829 epoch total loss 0.52880317\n",
      "Trained batch 2257 batch loss 0.564731896 epoch total loss 0.528819084\n",
      "Trained batch 2258 batch loss 0.525900424 epoch total loss 0.528817773\n",
      "Trained batch 2259 batch loss 0.5011186 epoch total loss 0.528805494\n",
      "Trained batch 2260 batch loss 0.631154537 epoch total loss 0.528850794\n",
      "Trained batch 2261 batch loss 0.528531432 epoch total loss 0.528850675\n",
      "Trained batch 2262 batch loss 0.559963584 epoch total loss 0.528864384\n",
      "Trained batch 2263 batch loss 0.576973617 epoch total loss 0.528885663\n",
      "Trained batch 2264 batch loss 0.558887482 epoch total loss 0.528898895\n",
      "Trained batch 2265 batch loss 0.513833761 epoch total loss 0.528892219\n",
      "Trained batch 2266 batch loss 0.450551659 epoch total loss 0.528857648\n",
      "Trained batch 2267 batch loss 0.469524503 epoch total loss 0.528831482\n",
      "Trained batch 2268 batch loss 0.438725412 epoch total loss 0.528791726\n",
      "Trained batch 2269 batch loss 0.384996295 epoch total loss 0.528728366\n",
      "Trained batch 2270 batch loss 0.445536613 epoch total loss 0.528691769\n",
      "Trained batch 2271 batch loss 0.605858505 epoch total loss 0.528725684\n",
      "Trained batch 2272 batch loss 0.62708056 epoch total loss 0.528769\n",
      "Trained batch 2273 batch loss 0.559441626 epoch total loss 0.528782487\n",
      "Trained batch 2274 batch loss 0.612806082 epoch total loss 0.528819442\n",
      "Trained batch 2275 batch loss 0.565381169 epoch total loss 0.528835535\n",
      "Trained batch 2276 batch loss 0.581537783 epoch total loss 0.528858662\n",
      "Trained batch 2277 batch loss 0.602626741 epoch total loss 0.528891087\n",
      "Trained batch 2278 batch loss 0.644897223 epoch total loss 0.528942049\n",
      "Trained batch 2279 batch loss 0.573692143 epoch total loss 0.528961658\n",
      "Trained batch 2280 batch loss 0.641790509 epoch total loss 0.52901119\n",
      "Trained batch 2281 batch loss 0.573042274 epoch total loss 0.529030442\n",
      "Trained batch 2282 batch loss 0.507149339 epoch total loss 0.529020905\n",
      "Trained batch 2283 batch loss 0.525656879 epoch total loss 0.529019415\n",
      "Trained batch 2284 batch loss 0.515376627 epoch total loss 0.529013455\n",
      "Trained batch 2285 batch loss 0.628053427 epoch total loss 0.529056787\n",
      "Trained batch 2286 batch loss 0.543153763 epoch total loss 0.529063\n",
      "Trained batch 2287 batch loss 0.469534427 epoch total loss 0.529036939\n",
      "Trained batch 2288 batch loss 0.584653735 epoch total loss 0.529061198\n",
      "Trained batch 2289 batch loss 0.655757248 epoch total loss 0.529116571\n",
      "Trained batch 2290 batch loss 0.544194937 epoch total loss 0.529123127\n",
      "Trained batch 2291 batch loss 0.501470447 epoch total loss 0.529111087\n",
      "Trained batch 2292 batch loss 0.576787472 epoch total loss 0.529131889\n",
      "Trained batch 2293 batch loss 0.630659163 epoch total loss 0.529176116\n",
      "Trained batch 2294 batch loss 0.580278277 epoch total loss 0.529198408\n",
      "Trained batch 2295 batch loss 0.645772278 epoch total loss 0.529249191\n",
      "Trained batch 2296 batch loss 0.529176474 epoch total loss 0.529249191\n",
      "Trained batch 2297 batch loss 0.557145357 epoch total loss 0.529261351\n",
      "Trained batch 2298 batch loss 0.610857129 epoch total loss 0.529296815\n",
      "Trained batch 2299 batch loss 0.521947563 epoch total loss 0.529293656\n",
      "Trained batch 2300 batch loss 0.534685135 epoch total loss 0.529296\n",
      "Trained batch 2301 batch loss 0.598774552 epoch total loss 0.529326141\n",
      "Trained batch 2302 batch loss 0.541574717 epoch total loss 0.529331505\n",
      "Trained batch 2303 batch loss 0.555548549 epoch total loss 0.52934289\n",
      "Trained batch 2304 batch loss 0.574078441 epoch total loss 0.529362321\n",
      "Trained batch 2305 batch loss 0.454081297 epoch total loss 0.529329658\n",
      "Trained batch 2306 batch loss 0.513207555 epoch total loss 0.529322684\n",
      "Trained batch 2307 batch loss 0.61201638 epoch total loss 0.529358506\n",
      "Trained batch 2308 batch loss 0.562567532 epoch total loss 0.529372931\n",
      "Trained batch 2309 batch loss 0.551478446 epoch total loss 0.529382527\n",
      "Trained batch 2310 batch loss 0.540352046 epoch total loss 0.529387295\n",
      "Trained batch 2311 batch loss 0.591416895 epoch total loss 0.529414117\n",
      "Trained batch 2312 batch loss 0.606398642 epoch total loss 0.529447436\n",
      "Trained batch 2313 batch loss 0.641462207 epoch total loss 0.529495895\n",
      "Trained batch 2314 batch loss 0.637836218 epoch total loss 0.529542685\n",
      "Trained batch 2315 batch loss 0.491722941 epoch total loss 0.529526353\n",
      "Trained batch 2316 batch loss 0.514960289 epoch total loss 0.529520094\n",
      "Trained batch 2317 batch loss 0.485956401 epoch total loss 0.529501319\n",
      "Trained batch 2318 batch loss 0.582695127 epoch total loss 0.529524207\n",
      "Trained batch 2319 batch loss 0.537368298 epoch total loss 0.529527605\n",
      "Trained batch 2320 batch loss 0.657914162 epoch total loss 0.529583\n",
      "Trained batch 2321 batch loss 0.542357683 epoch total loss 0.529588461\n",
      "Trained batch 2322 batch loss 0.493841767 epoch total loss 0.529573083\n",
      "Trained batch 2323 batch loss 0.597974956 epoch total loss 0.529602528\n",
      "Trained batch 2324 batch loss 0.490743339 epoch total loss 0.529585838\n",
      "Trained batch 2325 batch loss 0.570708454 epoch total loss 0.529603481\n",
      "Trained batch 2326 batch loss 0.570532084 epoch total loss 0.529621124\n",
      "Trained batch 2327 batch loss 0.513254046 epoch total loss 0.529614091\n",
      "Trained batch 2328 batch loss 0.550605476 epoch total loss 0.529623151\n",
      "Trained batch 2329 batch loss 0.609776258 epoch total loss 0.529657543\n",
      "Trained batch 2330 batch loss 0.583823383 epoch total loss 0.529680789\n",
      "Trained batch 2331 batch loss 0.582005799 epoch total loss 0.529703259\n",
      "Trained batch 2332 batch loss 0.684636295 epoch total loss 0.529769719\n",
      "Trained batch 2333 batch loss 0.59234941 epoch total loss 0.529796541\n",
      "Trained batch 2334 batch loss 0.555056214 epoch total loss 0.529807389\n",
      "Trained batch 2335 batch loss 0.554540396 epoch total loss 0.529818\n",
      "Trained batch 2336 batch loss 0.591315687 epoch total loss 0.529844284\n",
      "Trained batch 2337 batch loss 0.520952225 epoch total loss 0.529840529\n",
      "Trained batch 2338 batch loss 0.604511797 epoch total loss 0.529872477\n",
      "Trained batch 2339 batch loss 0.63206917 epoch total loss 0.529916167\n",
      "Trained batch 2340 batch loss 0.571970642 epoch total loss 0.529934168\n",
      "Trained batch 2341 batch loss 0.518159449 epoch total loss 0.529929101\n",
      "Trained batch 2342 batch loss 0.587528 epoch total loss 0.529953718\n",
      "Trained batch 2343 batch loss 0.6216048 epoch total loss 0.529992819\n",
      "Trained batch 2344 batch loss 0.650731146 epoch total loss 0.530044377\n",
      "Trained batch 2345 batch loss 0.655972958 epoch total loss 0.530098081\n",
      "Trained batch 2346 batch loss 0.535290897 epoch total loss 0.530100286\n",
      "Trained batch 2347 batch loss 0.549020588 epoch total loss 0.530108333\n",
      "Trained batch 2348 batch loss 0.588189125 epoch total loss 0.530133069\n",
      "Trained batch 2349 batch loss 0.579941 epoch total loss 0.530154288\n",
      "Trained batch 2350 batch loss 0.5068295 epoch total loss 0.530144334\n",
      "Trained batch 2351 batch loss 0.462471902 epoch total loss 0.530115604\n",
      "Trained batch 2352 batch loss 0.51297617 epoch total loss 0.530108273\n",
      "Trained batch 2353 batch loss 0.588354349 epoch total loss 0.530133069\n",
      "Trained batch 2354 batch loss 0.569809437 epoch total loss 0.530149937\n",
      "Trained batch 2355 batch loss 0.62396431 epoch total loss 0.530189753\n",
      "Trained batch 2356 batch loss 0.581796885 epoch total loss 0.530211687\n",
      "Trained batch 2357 batch loss 0.425405 epoch total loss 0.530167222\n",
      "Trained batch 2358 batch loss 0.49046579 epoch total loss 0.530150354\n",
      "Trained batch 2359 batch loss 0.486414641 epoch total loss 0.530131876\n",
      "Trained batch 2360 batch loss 0.430807412 epoch total loss 0.530089736\n",
      "Trained batch 2361 batch loss 0.440658599 epoch total loss 0.530051887\n",
      "Trained batch 2362 batch loss 0.476179659 epoch total loss 0.530029118\n",
      "Trained batch 2363 batch loss 0.468798846 epoch total loss 0.53000313\n",
      "Trained batch 2364 batch loss 0.532938838 epoch total loss 0.530004382\n",
      "Trained batch 2365 batch loss 0.548629701 epoch total loss 0.53001225\n",
      "Trained batch 2366 batch loss 0.454045385 epoch total loss 0.529980183\n",
      "Trained batch 2367 batch loss 0.4388524 epoch total loss 0.529941678\n",
      "Trained batch 2368 batch loss 0.47346735 epoch total loss 0.529917836\n",
      "Trained batch 2369 batch loss 0.481583178 epoch total loss 0.529897451\n",
      "Trained batch 2370 batch loss 0.500301123 epoch total loss 0.529884934\n",
      "Trained batch 2371 batch loss 0.532558322 epoch total loss 0.529886067\n",
      "Trained batch 2372 batch loss 0.560324609 epoch total loss 0.529898882\n",
      "Trained batch 2373 batch loss 0.538633525 epoch total loss 0.529902518\n",
      "Trained batch 2374 batch loss 0.556988 epoch total loss 0.529913962\n",
      "Trained batch 2375 batch loss 0.531230092 epoch total loss 0.529914498\n",
      "Trained batch 2376 batch loss 0.456090331 epoch total loss 0.529883444\n",
      "Trained batch 2377 batch loss 0.462583482 epoch total loss 0.529855072\n",
      "Trained batch 2378 batch loss 0.501243889 epoch total loss 0.529843032\n",
      "Trained batch 2379 batch loss 0.563079894 epoch total loss 0.529857039\n",
      "Trained batch 2380 batch loss 0.564141 epoch total loss 0.529871404\n",
      "Trained batch 2381 batch loss 0.601034641 epoch total loss 0.529901326\n",
      "Trained batch 2382 batch loss 0.601948619 epoch total loss 0.529931545\n",
      "Trained batch 2383 batch loss 0.555765 epoch total loss 0.529942393\n",
      "Trained batch 2384 batch loss 0.602027595 epoch total loss 0.529972672\n",
      "Trained batch 2385 batch loss 0.519393504 epoch total loss 0.529968262\n",
      "Trained batch 2386 batch loss 0.452249825 epoch total loss 0.529935658\n",
      "Trained batch 2387 batch loss 0.544507444 epoch total loss 0.529941797\n",
      "Trained batch 2388 batch loss 0.511338949 epoch total loss 0.529934\n",
      "Trained batch 2389 batch loss 0.562728882 epoch total loss 0.529947758\n",
      "Trained batch 2390 batch loss 0.446392477 epoch total loss 0.52991277\n",
      "Trained batch 2391 batch loss 0.446967185 epoch total loss 0.529878139\n",
      "Trained batch 2392 batch loss 0.456813157 epoch total loss 0.529847562\n",
      "Trained batch 2393 batch loss 0.51895088 epoch total loss 0.529843032\n",
      "Trained batch 2394 batch loss 0.465879291 epoch total loss 0.52981627\n",
      "Trained batch 2395 batch loss 0.471976221 epoch total loss 0.52979207\n",
      "Trained batch 2396 batch loss 0.480986536 epoch total loss 0.529771686\n",
      "Trained batch 2397 batch loss 0.432031065 epoch total loss 0.529730916\n",
      "Trained batch 2398 batch loss 0.457779437 epoch total loss 0.529700935\n",
      "Trained batch 2399 batch loss 0.386179507 epoch total loss 0.529641092\n",
      "Trained batch 2400 batch loss 0.521637857 epoch total loss 0.529637754\n",
      "Trained batch 2401 batch loss 0.421919197 epoch total loss 0.529592872\n",
      "Trained batch 2402 batch loss 0.475986898 epoch total loss 0.52957052\n",
      "Trained batch 2403 batch loss 0.43506 epoch total loss 0.52953124\n",
      "Trained batch 2404 batch loss 0.464108229 epoch total loss 0.529504\n",
      "Trained batch 2405 batch loss 0.507539511 epoch total loss 0.529494882\n",
      "Trained batch 2406 batch loss 0.459961355 epoch total loss 0.529466\n",
      "Trained batch 2407 batch loss 0.433065087 epoch total loss 0.529425919\n",
      "Trained batch 2408 batch loss 0.459890366 epoch total loss 0.52939707\n",
      "Trained batch 2409 batch loss 0.529421449 epoch total loss 0.52939707\n",
      "Trained batch 2410 batch loss 0.449304551 epoch total loss 0.529363811\n",
      "Trained batch 2411 batch loss 0.402166814 epoch total loss 0.529311121\n",
      "Trained batch 2412 batch loss 0.553415596 epoch total loss 0.529321134\n",
      "Trained batch 2413 batch loss 0.604919553 epoch total loss 0.529352486\n",
      "Trained batch 2414 batch loss 0.653508127 epoch total loss 0.529403925\n",
      "Trained batch 2415 batch loss 0.651041448 epoch total loss 0.529454291\n",
      "Trained batch 2416 batch loss 0.614038229 epoch total loss 0.529489279\n",
      "Trained batch 2417 batch loss 0.43399021 epoch total loss 0.529449761\n",
      "Trained batch 2418 batch loss 0.554414511 epoch total loss 0.529460073\n",
      "Trained batch 2419 batch loss 0.531017303 epoch total loss 0.529460728\n",
      "Trained batch 2420 batch loss 0.576303542 epoch total loss 0.5294801\n",
      "Trained batch 2421 batch loss 0.561825514 epoch total loss 0.529493392\n",
      "Trained batch 2422 batch loss 0.596707523 epoch total loss 0.529521167\n",
      "Trained batch 2423 batch loss 0.525425255 epoch total loss 0.529519439\n",
      "Trained batch 2424 batch loss 0.520703554 epoch total loss 0.529515862\n",
      "Trained batch 2425 batch loss 0.468561828 epoch total loss 0.52949065\n",
      "Trained batch 2426 batch loss 0.544826 epoch total loss 0.529496968\n",
      "Trained batch 2427 batch loss 0.422342598 epoch total loss 0.52945286\n",
      "Trained batch 2428 batch loss 0.514141619 epoch total loss 0.529446542\n",
      "Trained batch 2429 batch loss 0.508356214 epoch total loss 0.52943784\n",
      "Trained batch 2430 batch loss 0.652677059 epoch total loss 0.529488564\n",
      "Trained batch 2431 batch loss 0.48649776 epoch total loss 0.529470861\n",
      "Trained batch 2432 batch loss 0.63709712 epoch total loss 0.529515088\n",
      "Trained batch 2433 batch loss 0.55302012 epoch total loss 0.529524744\n",
      "Trained batch 2434 batch loss 0.594992757 epoch total loss 0.529551625\n",
      "Trained batch 2435 batch loss 0.597884119 epoch total loss 0.529579699\n",
      "Trained batch 2436 batch loss 0.586915851 epoch total loss 0.529603243\n",
      "Trained batch 2437 batch loss 0.567076623 epoch total loss 0.529618621\n",
      "Trained batch 2438 batch loss 0.508503616 epoch total loss 0.52961\n",
      "Trained batch 2439 batch loss 0.57904458 epoch total loss 0.529630244\n",
      "Trained batch 2440 batch loss 0.575451255 epoch total loss 0.529649\n",
      "Trained batch 2441 batch loss 0.567973673 epoch total loss 0.529664755\n",
      "Trained batch 2442 batch loss 0.578043 epoch total loss 0.529684544\n",
      "Trained batch 2443 batch loss 0.518105209 epoch total loss 0.529679775\n",
      "Trained batch 2444 batch loss 0.640789032 epoch total loss 0.529725194\n",
      "Trained batch 2445 batch loss 0.550357044 epoch total loss 0.529733658\n",
      "Trained batch 2446 batch loss 0.501843631 epoch total loss 0.529722273\n",
      "Trained batch 2447 batch loss 0.470089912 epoch total loss 0.529697895\n",
      "Trained batch 2448 batch loss 0.500693619 epoch total loss 0.529686093\n",
      "Trained batch 2449 batch loss 0.48681426 epoch total loss 0.52966857\n",
      "Trained batch 2450 batch loss 0.494754732 epoch total loss 0.529654324\n",
      "Trained batch 2451 batch loss 0.466807693 epoch total loss 0.529628694\n",
      "Trained batch 2452 batch loss 0.562455177 epoch total loss 0.529642105\n",
      "Trained batch 2453 batch loss 0.502317846 epoch total loss 0.529630959\n",
      "Trained batch 2454 batch loss 0.51208204 epoch total loss 0.529623806\n",
      "Trained batch 2455 batch loss 0.469715059 epoch total loss 0.529599369\n",
      "Trained batch 2456 batch loss 0.510878861 epoch total loss 0.529591739\n",
      "Trained batch 2457 batch loss 0.529004633 epoch total loss 0.52959156\n",
      "Trained batch 2458 batch loss 0.481633335 epoch total loss 0.52957207\n",
      "Trained batch 2459 batch loss 0.399173677 epoch total loss 0.529519\n",
      "Trained batch 2460 batch loss 0.47337994 epoch total loss 0.529496193\n",
      "Trained batch 2461 batch loss 0.409615546 epoch total loss 0.529447496\n",
      "Trained batch 2462 batch loss 0.391974449 epoch total loss 0.529391646\n",
      "Trained batch 2463 batch loss 0.540876091 epoch total loss 0.529396355\n",
      "Trained batch 2464 batch loss 0.572930217 epoch total loss 0.529414\n",
      "Trained batch 2465 batch loss 0.540921628 epoch total loss 0.529418647\n",
      "Trained batch 2466 batch loss 0.511448622 epoch total loss 0.529411376\n",
      "Trained batch 2467 batch loss 0.54200834 epoch total loss 0.529416442\n",
      "Trained batch 2468 batch loss 0.492490739 epoch total loss 0.529401481\n",
      "Trained batch 2469 batch loss 0.531911731 epoch total loss 0.529402494\n",
      "Trained batch 2470 batch loss 0.54863131 epoch total loss 0.529410243\n",
      "Trained batch 2471 batch loss 0.620888233 epoch total loss 0.529447258\n",
      "Trained batch 2472 batch loss 0.571921289 epoch total loss 0.529464424\n",
      "Trained batch 2473 batch loss 0.519622445 epoch total loss 0.52946043\n",
      "Trained batch 2474 batch loss 0.517317891 epoch total loss 0.529455543\n",
      "Trained batch 2475 batch loss 0.582918823 epoch total loss 0.529477119\n",
      "Trained batch 2476 batch loss 0.759774446 epoch total loss 0.529570162\n",
      "Trained batch 2477 batch loss 0.719435751 epoch total loss 0.529646814\n",
      "Trained batch 2478 batch loss 0.647152424 epoch total loss 0.5296942\n",
      "Trained batch 2479 batch loss 0.597970963 epoch total loss 0.529721797\n",
      "Trained batch 2480 batch loss 0.640444 epoch total loss 0.52976644\n",
      "Trained batch 2481 batch loss 0.684406519 epoch total loss 0.529828787\n",
      "Trained batch 2482 batch loss 0.633889556 epoch total loss 0.529870749\n",
      "Trained batch 2483 batch loss 0.632918417 epoch total loss 0.529912233\n",
      "Trained batch 2484 batch loss 0.567444265 epoch total loss 0.529927373\n",
      "Trained batch 2485 batch loss 0.563106298 epoch total loss 0.529940724\n",
      "Trained batch 2486 batch loss 0.512449145 epoch total loss 0.529933691\n",
      "Trained batch 2487 batch loss 0.598168 epoch total loss 0.529961109\n",
      "Trained batch 2488 batch loss 0.58616066 epoch total loss 0.529983699\n",
      "Trained batch 2489 batch loss 0.561460316 epoch total loss 0.529996336\n",
      "Trained batch 2490 batch loss 0.499828517 epoch total loss 0.529984236\n",
      "Trained batch 2491 batch loss 0.562211931 epoch total loss 0.52999717\n",
      "Trained batch 2492 batch loss 0.638909698 epoch total loss 0.53004092\n",
      "Trained batch 2493 batch loss 0.549059033 epoch total loss 0.530048549\n",
      "Trained batch 2494 batch loss 0.583047688 epoch total loss 0.530069768\n",
      "Trained batch 2495 batch loss 0.546646535 epoch total loss 0.530076385\n",
      "Trained batch 2496 batch loss 0.546242476 epoch total loss 0.530082881\n",
      "Trained batch 2497 batch loss 0.584944665 epoch total loss 0.530104876\n",
      "Trained batch 2498 batch loss 0.520007551 epoch total loss 0.530100822\n",
      "Trained batch 2499 batch loss 0.581211805 epoch total loss 0.530121267\n",
      "Trained batch 2500 batch loss 0.490133882 epoch total loss 0.530105293\n",
      "Trained batch 2501 batch loss 0.548386812 epoch total loss 0.530112565\n",
      "Trained batch 2502 batch loss 0.526559472 epoch total loss 0.530111194\n",
      "Trained batch 2503 batch loss 0.460775495 epoch total loss 0.530083477\n",
      "Trained batch 2504 batch loss 0.446205795 epoch total loss 0.53005\n",
      "Trained batch 2505 batch loss 0.400037825 epoch total loss 0.529998064\n",
      "Trained batch 2506 batch loss 0.4180682 epoch total loss 0.52995342\n",
      "Trained batch 2507 batch loss 0.420722812 epoch total loss 0.529909849\n",
      "Trained batch 2508 batch loss 0.586843133 epoch total loss 0.529932559\n",
      "Trained batch 2509 batch loss 0.588938355 epoch total loss 0.529956102\n",
      "Trained batch 2510 batch loss 0.551525772 epoch total loss 0.529964685\n",
      "Trained batch 2511 batch loss 0.573809206 epoch total loss 0.52998215\n",
      "Trained batch 2512 batch loss 0.559706807 epoch total loss 0.529993951\n",
      "Trained batch 2513 batch loss 0.655769169 epoch total loss 0.530044\n",
      "Trained batch 2514 batch loss 0.598167717 epoch total loss 0.53007108\n",
      "Trained batch 2515 batch loss 0.571431279 epoch total loss 0.530087531\n",
      "Trained batch 2516 batch loss 0.510957599 epoch total loss 0.530079961\n",
      "Trained batch 2517 batch loss 0.682244539 epoch total loss 0.5301404\n",
      "Trained batch 2518 batch loss 0.593286216 epoch total loss 0.530165493\n",
      "Trained batch 2519 batch loss 0.556929708 epoch total loss 0.530176103\n",
      "Trained batch 2520 batch loss 0.505097687 epoch total loss 0.530166149\n",
      "Trained batch 2521 batch loss 0.590807676 epoch total loss 0.530190229\n",
      "Trained batch 2522 batch loss 0.663206637 epoch total loss 0.53024292\n",
      "Trained batch 2523 batch loss 0.499465048 epoch total loss 0.530230761\n",
      "Trained batch 2524 batch loss 0.536316633 epoch total loss 0.530233204\n",
      "Trained batch 2525 batch loss 0.496259332 epoch total loss 0.530219734\n",
      "Trained batch 2526 batch loss 0.475462943 epoch total loss 0.530198038\n",
      "Trained batch 2527 batch loss 0.521876574 epoch total loss 0.530194759\n",
      "Trained batch 2528 batch loss 0.474067777 epoch total loss 0.530172586\n",
      "Trained batch 2529 batch loss 0.560133636 epoch total loss 0.530184448\n",
      "Trained batch 2530 batch loss 0.468634188 epoch total loss 0.530160129\n",
      "Trained batch 2531 batch loss 0.540162683 epoch total loss 0.530164063\n",
      "Trained batch 2532 batch loss 0.60068655 epoch total loss 0.530191898\n",
      "Trained batch 2533 batch loss 0.558847189 epoch total loss 0.530203223\n",
      "Trained batch 2534 batch loss 0.501371205 epoch total loss 0.530191839\n",
      "Trained batch 2535 batch loss 0.526463687 epoch total loss 0.530190349\n",
      "Trained batch 2536 batch loss 0.554813743 epoch total loss 0.530200064\n",
      "Trained batch 2537 batch loss 0.499199748 epoch total loss 0.530187845\n",
      "Trained batch 2538 batch loss 0.573439598 epoch total loss 0.530204892\n",
      "Trained batch 2539 batch loss 0.468557477 epoch total loss 0.530180573\n",
      "Trained batch 2540 batch loss 0.472299218 epoch total loss 0.530157804\n",
      "Trained batch 2541 batch loss 0.491655231 epoch total loss 0.530142665\n",
      "Trained batch 2542 batch loss 0.471469522 epoch total loss 0.530119598\n",
      "Trained batch 2543 batch loss 0.530964851 epoch total loss 0.530119896\n",
      "Trained batch 2544 batch loss 0.550256073 epoch total loss 0.530127883\n",
      "Trained batch 2545 batch loss 0.554893851 epoch total loss 0.530137599\n",
      "Trained batch 2546 batch loss 0.503567934 epoch total loss 0.530127168\n",
      "Trained batch 2547 batch loss 0.481200218 epoch total loss 0.530108\n",
      "Trained batch 2548 batch loss 0.538668752 epoch total loss 0.530111313\n",
      "Trained batch 2549 batch loss 0.645042419 epoch total loss 0.530156374\n",
      "Trained batch 2550 batch loss 0.443651259 epoch total loss 0.530122459\n",
      "Trained batch 2551 batch loss 0.531617463 epoch total loss 0.530123055\n",
      "Trained batch 2552 batch loss 0.55226779 epoch total loss 0.530131698\n",
      "Trained batch 2553 batch loss 0.524240732 epoch total loss 0.530129433\n",
      "Trained batch 2554 batch loss 0.491308928 epoch total loss 0.530114233\n",
      "Trained batch 2555 batch loss 0.519658864 epoch total loss 0.530110121\n",
      "Trained batch 2556 batch loss 0.473069102 epoch total loss 0.530087829\n",
      "Trained batch 2557 batch loss 0.519515276 epoch total loss 0.530083656\n",
      "Trained batch 2558 batch loss 0.397374839 epoch total loss 0.5300318\n",
      "Trained batch 2559 batch loss 0.459016979 epoch total loss 0.530004\n",
      "Trained batch 2560 batch loss 0.555321336 epoch total loss 0.530013919\n",
      "Trained batch 2561 batch loss 0.542143464 epoch total loss 0.530018628\n",
      "Trained batch 2562 batch loss 0.488732517 epoch total loss 0.530002534\n",
      "Trained batch 2563 batch loss 0.511116564 epoch total loss 0.529995143\n",
      "Trained batch 2564 batch loss 0.647031128 epoch total loss 0.530040741\n",
      "Trained batch 2565 batch loss 0.60572648 epoch total loss 0.530070245\n",
      "Trained batch 2566 batch loss 0.560347855 epoch total loss 0.530082047\n",
      "Trained batch 2567 batch loss 0.481491238 epoch total loss 0.530063093\n",
      "Trained batch 2568 batch loss 0.521657407 epoch total loss 0.530059814\n",
      "Trained batch 2569 batch loss 0.596504331 epoch total loss 0.530085683\n",
      "Trained batch 2570 batch loss 0.470206738 epoch total loss 0.530062377\n",
      "Trained batch 2571 batch loss 0.490655363 epoch total loss 0.530047059\n",
      "Trained batch 2572 batch loss 0.497759879 epoch total loss 0.530034542\n",
      "Trained batch 2573 batch loss 0.53658545 epoch total loss 0.530037105\n",
      "Trained batch 2574 batch loss 0.458185077 epoch total loss 0.530009151\n",
      "Trained batch 2575 batch loss 0.512883902 epoch total loss 0.530002534\n",
      "Trained batch 2576 batch loss 0.578146756 epoch total loss 0.530021191\n",
      "Trained batch 2577 batch loss 0.509782 epoch total loss 0.530013323\n",
      "Trained batch 2578 batch loss 0.712946713 epoch total loss 0.530084252\n",
      "Trained batch 2579 batch loss 0.535040915 epoch total loss 0.530086219\n",
      "Trained batch 2580 batch loss 0.599252522 epoch total loss 0.530113\n",
      "Trained batch 2581 batch loss 0.57266283 epoch total loss 0.530129492\n",
      "Trained batch 2582 batch loss 0.644299 epoch total loss 0.530173659\n",
      "Trained batch 2583 batch loss 0.585793376 epoch total loss 0.530195236\n",
      "Trained batch 2584 batch loss 0.610514164 epoch total loss 0.53022629\n",
      "Trained batch 2585 batch loss 0.519853354 epoch total loss 0.530222297\n",
      "Trained batch 2586 batch loss 0.553120255 epoch total loss 0.530231118\n",
      "Trained batch 2587 batch loss 0.606813073 epoch total loss 0.530260742\n",
      "Trained batch 2588 batch loss 0.591117501 epoch total loss 0.530284226\n",
      "Trained batch 2589 batch loss 0.578664482 epoch total loss 0.530302882\n",
      "Trained batch 2590 batch loss 0.621600509 epoch total loss 0.530338168\n",
      "Trained batch 2591 batch loss 0.527363956 epoch total loss 0.530337\n",
      "Trained batch 2592 batch loss 0.541955113 epoch total loss 0.530341506\n",
      "Trained batch 2593 batch loss 0.543397546 epoch total loss 0.530346572\n",
      "Trained batch 2594 batch loss 0.57384187 epoch total loss 0.530363321\n",
      "Trained batch 2595 batch loss 0.511589 epoch total loss 0.530356109\n",
      "Trained batch 2596 batch loss 0.511117399 epoch total loss 0.530348659\n",
      "Trained batch 2597 batch loss 0.426115155 epoch total loss 0.530308545\n",
      "Trained batch 2598 batch loss 0.536567688 epoch total loss 0.530311\n",
      "Trained batch 2599 batch loss 0.469924927 epoch total loss 0.530287743\n",
      "Trained batch 2600 batch loss 0.372122 epoch total loss 0.530226886\n",
      "Trained batch 2601 batch loss 0.483837217 epoch total loss 0.530209064\n",
      "Trained batch 2602 batch loss 0.538060069 epoch total loss 0.530212104\n",
      "Trained batch 2603 batch loss 0.472168207 epoch total loss 0.530189812\n",
      "Trained batch 2604 batch loss 0.500423908 epoch total loss 0.530178368\n",
      "Trained batch 2605 batch loss 0.505457759 epoch total loss 0.530168891\n",
      "Trained batch 2606 batch loss 0.587520599 epoch total loss 0.530190885\n",
      "Trained batch 2607 batch loss 0.669902682 epoch total loss 0.530244529\n",
      "Trained batch 2608 batch loss 0.630663872 epoch total loss 0.530283\n",
      "Trained batch 2609 batch loss 0.592783332 epoch total loss 0.530306935\n",
      "Trained batch 2610 batch loss 0.464242131 epoch total loss 0.530281603\n",
      "Trained batch 2611 batch loss 0.515968263 epoch total loss 0.53027612\n",
      "Trained batch 2612 batch loss 0.423587799 epoch total loss 0.530235291\n",
      "Trained batch 2613 batch loss 0.379645288 epoch total loss 0.530177653\n",
      "Trained batch 2614 batch loss 0.370715976 epoch total loss 0.530116677\n",
      "Trained batch 2615 batch loss 0.445974588 epoch total loss 0.530084491\n",
      "Trained batch 2616 batch loss 0.454030693 epoch total loss 0.530055404\n",
      "Trained batch 2617 batch loss 0.533113658 epoch total loss 0.530056536\n",
      "Trained batch 2618 batch loss 0.547807515 epoch total loss 0.530063331\n",
      "Trained batch 2619 batch loss 0.564006686 epoch total loss 0.530076265\n",
      "Trained batch 2620 batch loss 0.591706097 epoch total loss 0.530099809\n",
      "Trained batch 2621 batch loss 0.547787666 epoch total loss 0.530106544\n",
      "Trained batch 2622 batch loss 0.522728443 epoch total loss 0.530103683\n",
      "Trained batch 2623 batch loss 0.592774928 epoch total loss 0.530127585\n",
      "Trained batch 2624 batch loss 0.536134124 epoch total loss 0.53012985\n",
      "Trained batch 2625 batch loss 0.524692595 epoch total loss 0.530127764\n",
      "Trained batch 2626 batch loss 0.558120608 epoch total loss 0.530138433\n",
      "Trained batch 2627 batch loss 0.49263978 epoch total loss 0.530124187\n",
      "Trained batch 2628 batch loss 0.545311689 epoch total loss 0.530129969\n",
      "Trained batch 2629 batch loss 0.485964268 epoch total loss 0.530113161\n",
      "Trained batch 2630 batch loss 0.505793214 epoch total loss 0.530103862\n",
      "Trained batch 2631 batch loss 0.556302 epoch total loss 0.530113816\n",
      "Trained batch 2632 batch loss 0.528899431 epoch total loss 0.530113399\n",
      "Trained batch 2633 batch loss 0.502587438 epoch total loss 0.530102909\n",
      "Trained batch 2634 batch loss 0.610879719 epoch total loss 0.530133545\n",
      "Trained batch 2635 batch loss 0.639793873 epoch total loss 0.530175149\n",
      "Trained batch 2636 batch loss 0.56118989 epoch total loss 0.530186951\n",
      "Trained batch 2637 batch loss 0.53880161 epoch total loss 0.530190229\n",
      "Trained batch 2638 batch loss 0.428109318 epoch total loss 0.530151486\n",
      "Trained batch 2639 batch loss 0.476465404 epoch total loss 0.530131161\n",
      "Trained batch 2640 batch loss 0.431069434 epoch total loss 0.53009361\n",
      "Trained batch 2641 batch loss 0.475920945 epoch total loss 0.530073106\n",
      "Trained batch 2642 batch loss 0.469945192 epoch total loss 0.530050337\n",
      "Trained batch 2643 batch loss 0.441378713 epoch total loss 0.53001684\n",
      "Trained batch 2644 batch loss 0.4108181 epoch total loss 0.529971719\n",
      "Trained batch 2645 batch loss 0.604964674 epoch total loss 0.530000091\n",
      "Trained batch 2646 batch loss 0.588990033 epoch total loss 0.530022383\n",
      "Trained batch 2647 batch loss 0.488802612 epoch total loss 0.530006766\n",
      "Trained batch 2648 batch loss 0.422340751 epoch total loss 0.529966116\n",
      "Trained batch 2649 batch loss 0.479853272 epoch total loss 0.529947221\n",
      "Trained batch 2650 batch loss 0.429122448 epoch total loss 0.529909134\n",
      "Trained batch 2651 batch loss 0.520292 epoch total loss 0.529905498\n",
      "Trained batch 2652 batch loss 0.494134456 epoch total loss 0.529892\n",
      "Trained batch 2653 batch loss 0.466427088 epoch total loss 0.529868126\n",
      "Trained batch 2654 batch loss 0.561973393 epoch total loss 0.529880226\n",
      "Trained batch 2655 batch loss 0.533328652 epoch total loss 0.529881537\n",
      "Trained batch 2656 batch loss 0.554509103 epoch total loss 0.529890835\n",
      "Trained batch 2657 batch loss 0.550052524 epoch total loss 0.529898405\n",
      "Trained batch 2658 batch loss 0.478268445 epoch total loss 0.529879\n",
      "Trained batch 2659 batch loss 0.674285114 epoch total loss 0.529933274\n",
      "Trained batch 2660 batch loss 0.582672596 epoch total loss 0.529953122\n",
      "Trained batch 2661 batch loss 0.581832767 epoch total loss 0.529972613\n",
      "Trained batch 2662 batch loss 0.604916692 epoch total loss 0.530000746\n",
      "Trained batch 2663 batch loss 0.576044321 epoch total loss 0.530018032\n",
      "Trained batch 2664 batch loss 0.505739093 epoch total loss 0.530008912\n",
      "Trained batch 2665 batch loss 0.553488791 epoch total loss 0.530017734\n",
      "Trained batch 2666 batch loss 0.581721544 epoch total loss 0.530037105\n",
      "Trained batch 2667 batch loss 0.561114132 epoch total loss 0.530048728\n",
      "Trained batch 2668 batch loss 0.480968356 epoch total loss 0.53003037\n",
      "Trained batch 2669 batch loss 0.557370901 epoch total loss 0.530040622\n",
      "Trained batch 2670 batch loss 0.521991253 epoch total loss 0.530037582\n",
      "Trained batch 2671 batch loss 0.491566777 epoch total loss 0.530023158\n",
      "Trained batch 2672 batch loss 0.601087034 epoch total loss 0.530049741\n",
      "Trained batch 2673 batch loss 0.570057392 epoch total loss 0.530064762\n",
      "Trained batch 2674 batch loss 0.574501514 epoch total loss 0.530081332\n",
      "Trained batch 2675 batch loss 0.611860871 epoch total loss 0.530111909\n",
      "Trained batch 2676 batch loss 0.607496738 epoch total loss 0.530140817\n",
      "Trained batch 2677 batch loss 0.551812947 epoch total loss 0.530148923\n",
      "Trained batch 2678 batch loss 0.497361153 epoch total loss 0.530136645\n",
      "Trained batch 2679 batch loss 0.562922597 epoch total loss 0.530148864\n",
      "Trained batch 2680 batch loss 0.513482153 epoch total loss 0.530142605\n",
      "Trained batch 2681 batch loss 0.526718915 epoch total loss 0.530141354\n",
      "Trained batch 2682 batch loss 0.664352536 epoch total loss 0.530191362\n",
      "Trained batch 2683 batch loss 0.579562366 epoch total loss 0.53020978\n",
      "Trained batch 2684 batch loss 0.692116797 epoch total loss 0.5302701\n",
      "Trained batch 2685 batch loss 0.618791 epoch total loss 0.530303061\n",
      "Trained batch 2686 batch loss 0.524518073 epoch total loss 0.530300915\n",
      "Trained batch 2687 batch loss 0.486025214 epoch total loss 0.530284464\n",
      "Trained batch 2688 batch loss 0.478834033 epoch total loss 0.530265331\n",
      "Trained batch 2689 batch loss 0.572490513 epoch total loss 0.530281067\n",
      "Trained batch 2690 batch loss 0.61530745 epoch total loss 0.530312717\n",
      "Trained batch 2691 batch loss 0.598244965 epoch total loss 0.53033793\n",
      "Trained batch 2692 batch loss 0.510007381 epoch total loss 0.53033042\n",
      "Trained batch 2693 batch loss 0.472993 epoch total loss 0.530309141\n",
      "Trained batch 2694 batch loss 0.567432642 epoch total loss 0.53032285\n",
      "Trained batch 2695 batch loss 0.599068165 epoch total loss 0.53034842\n",
      "Trained batch 2696 batch loss 0.551221132 epoch total loss 0.530356169\n",
      "Trained batch 2697 batch loss 0.637225032 epoch total loss 0.530395806\n",
      "Trained batch 2698 batch loss 0.623250961 epoch total loss 0.530430198\n",
      "Trained batch 2699 batch loss 0.656538963 epoch total loss 0.530476928\n",
      "Trained batch 2700 batch loss 0.63998729 epoch total loss 0.530517459\n",
      "Trained batch 2701 batch loss 0.601812303 epoch total loss 0.530543864\n",
      "Trained batch 2702 batch loss 0.682747543 epoch total loss 0.53060019\n",
      "Trained batch 2703 batch loss 0.600574911 epoch total loss 0.530626118\n",
      "Trained batch 2704 batch loss 0.543736279 epoch total loss 0.530630946\n",
      "Trained batch 2705 batch loss 0.499538958 epoch total loss 0.530619442\n",
      "Trained batch 2706 batch loss 0.552983522 epoch total loss 0.530627668\n",
      "Trained batch 2707 batch loss 0.532388 epoch total loss 0.530628324\n",
      "Trained batch 2708 batch loss 0.517944753 epoch total loss 0.530623674\n",
      "Trained batch 2709 batch loss 0.498131514 epoch total loss 0.530611694\n",
      "Trained batch 2710 batch loss 0.58370918 epoch total loss 0.530631244\n",
      "Trained batch 2711 batch loss 0.541163743 epoch total loss 0.530635118\n",
      "Trained batch 2712 batch loss 0.604486167 epoch total loss 0.530662358\n",
      "Trained batch 2713 batch loss 0.546402574 epoch total loss 0.530668199\n",
      "Trained batch 2714 batch loss 0.644397676 epoch total loss 0.530710101\n",
      "Trained batch 2715 batch loss 0.584092319 epoch total loss 0.530729771\n",
      "Trained batch 2716 batch loss 0.660871506 epoch total loss 0.530777693\n",
      "Trained batch 2717 batch loss 0.63874042 epoch total loss 0.530817449\n",
      "Trained batch 2718 batch loss 0.560981691 epoch total loss 0.530828536\n",
      "Trained batch 2719 batch loss 0.588336289 epoch total loss 0.530849695\n",
      "Trained batch 2720 batch loss 0.587167442 epoch total loss 0.530870438\n",
      "Trained batch 2721 batch loss 0.674903393 epoch total loss 0.530923367\n",
      "Trained batch 2722 batch loss 0.630103707 epoch total loss 0.530959785\n",
      "Trained batch 2723 batch loss 0.669074 epoch total loss 0.531010509\n",
      "Trained batch 2724 batch loss 0.650501907 epoch total loss 0.531054378\n",
      "Trained batch 2725 batch loss 0.637005508 epoch total loss 0.53109324\n",
      "Trained batch 2726 batch loss 0.655995846 epoch total loss 0.531139076\n",
      "Trained batch 2727 batch loss 0.645706952 epoch total loss 0.531181097\n",
      "Trained batch 2728 batch loss 0.620543301 epoch total loss 0.53121382\n",
      "Trained batch 2729 batch loss 0.57064414 epoch total loss 0.531228304\n",
      "Trained batch 2730 batch loss 0.628164589 epoch total loss 0.531263828\n",
      "Trained batch 2731 batch loss 0.540392 epoch total loss 0.531267166\n",
      "Trained batch 2732 batch loss 0.641403317 epoch total loss 0.531307459\n",
      "Trained batch 2733 batch loss 0.57891351 epoch total loss 0.531324863\n",
      "Trained batch 2734 batch loss 0.544032812 epoch total loss 0.531329513\n",
      "Trained batch 2735 batch loss 0.574371338 epoch total loss 0.531345248\n",
      "Trained batch 2736 batch loss 0.556439638 epoch total loss 0.531354427\n",
      "Trained batch 2737 batch loss 0.562039137 epoch total loss 0.531365633\n",
      "Trained batch 2738 batch loss 0.552485228 epoch total loss 0.531373322\n",
      "Trained batch 2739 batch loss 0.596218 epoch total loss 0.531397\n",
      "Trained batch 2740 batch loss 0.499722511 epoch total loss 0.531385422\n",
      "Trained batch 2741 batch loss 0.521602809 epoch total loss 0.531381845\n",
      "Trained batch 2742 batch loss 0.498881578 epoch total loss 0.531370044\n",
      "Trained batch 2743 batch loss 0.493379146 epoch total loss 0.531356156\n",
      "Trained batch 2744 batch loss 0.572118819 epoch total loss 0.531371057\n",
      "Trained batch 2745 batch loss 0.485084713 epoch total loss 0.531354189\n",
      "Trained batch 2746 batch loss 0.551436901 epoch total loss 0.531361461\n",
      "Trained batch 2747 batch loss 0.455946296 epoch total loss 0.531334043\n",
      "Trained batch 2748 batch loss 0.51660651 epoch total loss 0.531328678\n",
      "Trained batch 2749 batch loss 0.521976411 epoch total loss 0.531325281\n",
      "Trained batch 2750 batch loss 0.570592701 epoch total loss 0.531339526\n",
      "Trained batch 2751 batch loss 0.532910049 epoch total loss 0.531340122\n",
      "Trained batch 2752 batch loss 0.531981 epoch total loss 0.531340361\n",
      "Trained batch 2753 batch loss 0.443221301 epoch total loss 0.531308353\n",
      "Trained batch 2754 batch loss 0.50540036 epoch total loss 0.531298935\n",
      "Trained batch 2755 batch loss 0.57082206 epoch total loss 0.5313133\n",
      "Trained batch 2756 batch loss 0.453167796 epoch total loss 0.531284928\n",
      "Trained batch 2757 batch loss 0.527090371 epoch total loss 0.531283379\n",
      "Trained batch 2758 batch loss 0.494050235 epoch total loss 0.531269848\n",
      "Trained batch 2759 batch loss 0.441908419 epoch total loss 0.531237483\n",
      "Trained batch 2760 batch loss 0.497885168 epoch total loss 0.531225383\n",
      "Trained batch 2761 batch loss 0.569258094 epoch total loss 0.531239152\n",
      "Trained batch 2762 batch loss 0.545248747 epoch total loss 0.531244278\n",
      "Trained batch 2763 batch loss 0.505034924 epoch total loss 0.531234741\n",
      "Trained batch 2764 batch loss 0.533801377 epoch total loss 0.531235695\n",
      "Trained batch 2765 batch loss 0.508044362 epoch total loss 0.531227291\n",
      "Trained batch 2766 batch loss 0.546503246 epoch total loss 0.531232834\n",
      "Trained batch 2767 batch loss 0.434215546 epoch total loss 0.531197786\n",
      "Trained batch 2768 batch loss 0.529963374 epoch total loss 0.531197309\n",
      "Trained batch 2769 batch loss 0.498958528 epoch total loss 0.531185627\n",
      "Trained batch 2770 batch loss 0.544010758 epoch total loss 0.531190276\n",
      "Trained batch 2771 batch loss 0.457472056 epoch total loss 0.531163692\n",
      "Trained batch 2772 batch loss 0.548410356 epoch total loss 0.531169951\n",
      "Trained batch 2773 batch loss 0.52279824 epoch total loss 0.531166911\n",
      "Trained batch 2774 batch loss 0.599113 epoch total loss 0.531191409\n",
      "Trained batch 2775 batch loss 0.606107354 epoch total loss 0.53121841\n",
      "Trained batch 2776 batch loss 0.55323565 epoch total loss 0.531226337\n",
      "Epoch 9 train loss 0.5312263369560242\n",
      "Validated batch 1 batch loss 0.548341393\n",
      "Validated batch 2 batch loss 0.579575062\n",
      "Validated batch 3 batch loss 0.549794376\n",
      "Validated batch 4 batch loss 0.569054127\n",
      "Validated batch 5 batch loss 0.617290258\n",
      "Validated batch 6 batch loss 0.471080899\n",
      "Validated batch 7 batch loss 0.556000233\n",
      "Validated batch 8 batch loss 0.589504123\n",
      "Validated batch 9 batch loss 0.547002435\n",
      "Validated batch 10 batch loss 0.565334558\n",
      "Validated batch 11 batch loss 0.572004497\n",
      "Validated batch 12 batch loss 0.610842943\n",
      "Validated batch 13 batch loss 0.580882609\n",
      "Validated batch 14 batch loss 0.628861189\n",
      "Validated batch 15 batch loss 0.612777293\n",
      "Validated batch 16 batch loss 0.570357382\n",
      "Validated batch 17 batch loss 0.627708554\n",
      "Validated batch 18 batch loss 0.546456456\n",
      "Validated batch 19 batch loss 0.552816749\n",
      "Validated batch 20 batch loss 0.527421713\n",
      "Validated batch 21 batch loss 0.533602417\n",
      "Validated batch 22 batch loss 0.604011297\n",
      "Validated batch 23 batch loss 0.533136189\n",
      "Validated batch 24 batch loss 0.588142872\n",
      "Validated batch 25 batch loss 0.527036548\n",
      "Validated batch 26 batch loss 0.594291806\n",
      "Validated batch 27 batch loss 0.596986771\n",
      "Validated batch 28 batch loss 0.64677906\n",
      "Validated batch 29 batch loss 0.636450768\n",
      "Validated batch 30 batch loss 0.588851\n",
      "Validated batch 31 batch loss 0.550348401\n",
      "Validated batch 32 batch loss 0.562394381\n",
      "Validated batch 33 batch loss 0.618415833\n",
      "Validated batch 34 batch loss 0.693922341\n",
      "Validated batch 35 batch loss 0.502314627\n",
      "Validated batch 36 batch loss 0.543044\n",
      "Validated batch 37 batch loss 0.570729136\n",
      "Validated batch 38 batch loss 0.636067212\n",
      "Validated batch 39 batch loss 0.434935182\n",
      "Validated batch 40 batch loss 0.463827789\n",
      "Validated batch 41 batch loss 0.507557154\n",
      "Validated batch 42 batch loss 0.614526212\n",
      "Validated batch 43 batch loss 0.575761259\n",
      "Validated batch 44 batch loss 0.620648265\n",
      "Validated batch 45 batch loss 0.457160622\n",
      "Validated batch 46 batch loss 0.563589\n",
      "Validated batch 47 batch loss 0.485824496\n",
      "Validated batch 48 batch loss 0.593871832\n",
      "Validated batch 49 batch loss 0.591187775\n",
      "Validated batch 50 batch loss 0.472605318\n",
      "Validated batch 51 batch loss 0.620627403\n",
      "Validated batch 52 batch loss 0.456533104\n",
      "Validated batch 53 batch loss 0.53004384\n",
      "Validated batch 54 batch loss 0.567864418\n",
      "Validated batch 55 batch loss 0.578065515\n",
      "Validated batch 56 batch loss 0.464464307\n",
      "Validated batch 57 batch loss 0.688436031\n",
      "Validated batch 58 batch loss 0.514616609\n",
      "Validated batch 59 batch loss 0.517772138\n",
      "Validated batch 60 batch loss 0.559838533\n",
      "Validated batch 61 batch loss 0.489869118\n",
      "Validated batch 62 batch loss 0.471682072\n",
      "Validated batch 63 batch loss 0.493691742\n",
      "Validated batch 64 batch loss 0.55193615\n",
      "Validated batch 65 batch loss 0.595517\n",
      "Validated batch 66 batch loss 0.466164947\n",
      "Validated batch 67 batch loss 0.498493433\n",
      "Validated batch 68 batch loss 0.518619776\n",
      "Validated batch 69 batch loss 0.593393087\n",
      "Validated batch 70 batch loss 0.456349373\n",
      "Validated batch 71 batch loss 0.470607191\n",
      "Validated batch 72 batch loss 0.589961469\n",
      "Validated batch 73 batch loss 0.532713294\n",
      "Validated batch 74 batch loss 0.556034744\n",
      "Validated batch 75 batch loss 0.656097591\n",
      "Validated batch 76 batch loss 0.601734936\n",
      "Validated batch 77 batch loss 0.587196946\n",
      "Validated batch 78 batch loss 0.55003047\n",
      "Validated batch 79 batch loss 0.541713357\n",
      "Validated batch 80 batch loss 0.568405807\n",
      "Validated batch 81 batch loss 0.633645356\n",
      "Validated batch 82 batch loss 0.572479486\n",
      "Validated batch 83 batch loss 0.452497959\n",
      "Validated batch 84 batch loss 0.458999813\n",
      "Validated batch 85 batch loss 0.450945526\n",
      "Validated batch 86 batch loss 0.575675786\n",
      "Validated batch 87 batch loss 0.505414426\n",
      "Validated batch 88 batch loss 0.451887935\n",
      "Validated batch 89 batch loss 0.531245887\n",
      "Validated batch 90 batch loss 0.565316558\n",
      "Validated batch 91 batch loss 0.680087864\n",
      "Validated batch 92 batch loss 0.602471709\n",
      "Validated batch 93 batch loss 0.538155138\n",
      "Validated batch 94 batch loss 0.590676785\n",
      "Validated batch 95 batch loss 0.564700425\n",
      "Validated batch 96 batch loss 0.531919062\n",
      "Validated batch 97 batch loss 0.54183042\n",
      "Validated batch 98 batch loss 0.544460058\n",
      "Validated batch 99 batch loss 0.518994689\n",
      "Validated batch 100 batch loss 0.623353422\n",
      "Validated batch 101 batch loss 0.607373059\n",
      "Validated batch 102 batch loss 0.596573114\n",
      "Validated batch 103 batch loss 0.718010306\n",
      "Validated batch 104 batch loss 0.59214437\n",
      "Validated batch 105 batch loss 0.531904638\n",
      "Validated batch 106 batch loss 0.565074801\n",
      "Validated batch 107 batch loss 0.56106329\n",
      "Validated batch 108 batch loss 0.605525\n",
      "Validated batch 109 batch loss 0.54987824\n",
      "Validated batch 110 batch loss 0.580916\n",
      "Validated batch 111 batch loss 0.583789527\n",
      "Validated batch 112 batch loss 0.617415309\n",
      "Validated batch 113 batch loss 0.583693\n",
      "Validated batch 114 batch loss 0.614125729\n",
      "Validated batch 115 batch loss 0.535256267\n",
      "Validated batch 116 batch loss 0.515612721\n",
      "Validated batch 117 batch loss 0.596596241\n",
      "Validated batch 118 batch loss 0.743027866\n",
      "Validated batch 119 batch loss 0.614775181\n",
      "Validated batch 120 batch loss 0.47565189\n",
      "Validated batch 121 batch loss 0.619922578\n",
      "Validated batch 122 batch loss 0.625451565\n",
      "Validated batch 123 batch loss 0.524285674\n",
      "Validated batch 124 batch loss 0.588800311\n",
      "Validated batch 125 batch loss 0.614293039\n",
      "Validated batch 126 batch loss 0.597624779\n",
      "Validated batch 127 batch loss 0.585832775\n",
      "Validated batch 128 batch loss 0.440702707\n",
      "Validated batch 129 batch loss 0.593561351\n",
      "Validated batch 130 batch loss 0.595323801\n",
      "Validated batch 131 batch loss 0.561894357\n",
      "Validated batch 132 batch loss 0.516838551\n",
      "Validated batch 133 batch loss 0.570946574\n",
      "Validated batch 134 batch loss 0.52880758\n",
      "Validated batch 135 batch loss 0.584975243\n",
      "Validated batch 136 batch loss 0.563153684\n",
      "Validated batch 137 batch loss 0.51420325\n",
      "Validated batch 138 batch loss 0.572128117\n",
      "Validated batch 139 batch loss 0.652427912\n",
      "Validated batch 140 batch loss 0.665162742\n",
      "Validated batch 141 batch loss 0.541457951\n",
      "Validated batch 142 batch loss 0.583683193\n",
      "Validated batch 143 batch loss 0.576530337\n",
      "Validated batch 144 batch loss 0.483983696\n",
      "Validated batch 145 batch loss 0.528761864\n",
      "Validated batch 146 batch loss 0.604600966\n",
      "Validated batch 147 batch loss 0.486081123\n",
      "Validated batch 148 batch loss 0.535034418\n",
      "Validated batch 149 batch loss 0.587124169\n",
      "Validated batch 150 batch loss 0.557057858\n",
      "Validated batch 151 batch loss 0.547490239\n",
      "Validated batch 152 batch loss 0.592252493\n",
      "Validated batch 153 batch loss 0.565258443\n",
      "Validated batch 154 batch loss 0.480775386\n",
      "Validated batch 155 batch loss 0.523813784\n",
      "Validated batch 156 batch loss 0.578611255\n",
      "Validated batch 157 batch loss 0.570569754\n",
      "Validated batch 158 batch loss 0.5332582\n",
      "Validated batch 159 batch loss 0.607513547\n",
      "Validated batch 160 batch loss 0.583084464\n",
      "Validated batch 161 batch loss 0.636647582\n",
      "Validated batch 162 batch loss 0.581502676\n",
      "Validated batch 163 batch loss 0.572227538\n",
      "Validated batch 164 batch loss 0.555318356\n",
      "Validated batch 165 batch loss 0.506337523\n",
      "Validated batch 166 batch loss 0.534190118\n",
      "Validated batch 167 batch loss 0.60870856\n",
      "Validated batch 168 batch loss 0.436289102\n",
      "Validated batch 169 batch loss 0.617346823\n",
      "Validated batch 170 batch loss 0.505030751\n",
      "Validated batch 171 batch loss 0.518179715\n",
      "Validated batch 172 batch loss 0.555518866\n",
      "Validated batch 173 batch loss 0.554437935\n",
      "Validated batch 174 batch loss 0.562124252\n",
      "Validated batch 175 batch loss 0.575185299\n",
      "Validated batch 176 batch loss 0.533162892\n",
      "Validated batch 177 batch loss 0.642217398\n",
      "Validated batch 178 batch loss 0.727951467\n",
      "Validated batch 179 batch loss 0.678192616\n",
      "Validated batch 180 batch loss 0.515831709\n",
      "Validated batch 181 batch loss 0.517473102\n",
      "Validated batch 182 batch loss 0.581879616\n",
      "Validated batch 183 batch loss 0.51679045\n",
      "Validated batch 184 batch loss 0.508763492\n",
      "Validated batch 185 batch loss 0.4833408\n",
      "Validated batch 186 batch loss 0.524698079\n",
      "Validated batch 187 batch loss 0.541792095\n",
      "Validated batch 188 batch loss 0.493604869\n",
      "Validated batch 189 batch loss 0.514618814\n",
      "Validated batch 190 batch loss 0.546097577\n",
      "Validated batch 191 batch loss 0.564504504\n",
      "Validated batch 192 batch loss 0.491372705\n",
      "Validated batch 193 batch loss 0.573011\n",
      "Validated batch 194 batch loss 0.465313017\n",
      "Validated batch 195 batch loss 0.548864365\n",
      "Validated batch 196 batch loss 0.626922\n",
      "Validated batch 197 batch loss 0.506192446\n",
      "Validated batch 198 batch loss 0.606093884\n",
      "Validated batch 199 batch loss 0.581824124\n",
      "Validated batch 200 batch loss 0.632112384\n",
      "Validated batch 201 batch loss 0.583606064\n",
      "Validated batch 202 batch loss 0.575691164\n",
      "Validated batch 203 batch loss 0.604808509\n",
      "Validated batch 204 batch loss 0.496329039\n",
      "Validated batch 205 batch loss 0.539911747\n",
      "Validated batch 206 batch loss 0.582034111\n",
      "Validated batch 207 batch loss 0.645637751\n",
      "Validated batch 208 batch loss 0.681824207\n",
      "Validated batch 209 batch loss 0.572145641\n",
      "Validated batch 210 batch loss 0.61946255\n",
      "Validated batch 211 batch loss 0.646818\n",
      "Validated batch 212 batch loss 0.610624909\n",
      "Validated batch 213 batch loss 0.613872588\n",
      "Validated batch 214 batch loss 0.625279963\n",
      "Validated batch 215 batch loss 0.635830164\n",
      "Validated batch 216 batch loss 0.63736254\n",
      "Validated batch 217 batch loss 0.66065824\n",
      "Validated batch 218 batch loss 0.586039424\n",
      "Validated batch 219 batch loss 0.624634802\n",
      "Validated batch 220 batch loss 0.474032849\n",
      "Validated batch 221 batch loss 0.558329105\n",
      "Validated batch 222 batch loss 0.6037907\n",
      "Validated batch 223 batch loss 0.648911774\n",
      "Validated batch 224 batch loss 0.600885868\n",
      "Validated batch 225 batch loss 0.640020072\n",
      "Validated batch 226 batch loss 0.552918196\n",
      "Validated batch 227 batch loss 0.504781544\n",
      "Validated batch 228 batch loss 0.575516403\n",
      "Validated batch 229 batch loss 0.605275\n",
      "Validated batch 230 batch loss 0.529527247\n",
      "Validated batch 231 batch loss 0.574674726\n",
      "Validated batch 232 batch loss 0.474150896\n",
      "Validated batch 233 batch loss 0.510691643\n",
      "Validated batch 234 batch loss 0.554447412\n",
      "Validated batch 235 batch loss 0.608720422\n",
      "Validated batch 236 batch loss 0.570273757\n",
      "Validated batch 237 batch loss 0.510041\n",
      "Validated batch 238 batch loss 0.495555133\n",
      "Validated batch 239 batch loss 0.567506254\n",
      "Validated batch 240 batch loss 0.570334494\n",
      "Validated batch 241 batch loss 0.672943413\n",
      "Validated batch 242 batch loss 0.62060219\n",
      "Validated batch 243 batch loss 0.510455787\n",
      "Validated batch 244 batch loss 0.473619163\n",
      "Validated batch 245 batch loss 0.550302207\n",
      "Validated batch 246 batch loss 0.525598407\n",
      "Validated batch 247 batch loss 0.56137532\n",
      "Validated batch 248 batch loss 0.520838261\n",
      "Validated batch 249 batch loss 0.566632152\n",
      "Validated batch 250 batch loss 0.583788514\n",
      "Validated batch 251 batch loss 0.582742035\n",
      "Validated batch 252 batch loss 0.564554155\n",
      "Validated batch 253 batch loss 0.541338444\n",
      "Validated batch 254 batch loss 0.504092097\n",
      "Validated batch 255 batch loss 0.35912\n",
      "Validated batch 256 batch loss 0.496810734\n",
      "Validated batch 257 batch loss 0.557167888\n",
      "Validated batch 258 batch loss 0.5223611\n",
      "Validated batch 259 batch loss 0.515204966\n",
      "Validated batch 260 batch loss 0.483452618\n",
      "Validated batch 261 batch loss 0.525050521\n",
      "Validated batch 262 batch loss 0.553739429\n",
      "Validated batch 263 batch loss 0.659547925\n",
      "Validated batch 264 batch loss 0.552516937\n",
      "Validated batch 265 batch loss 0.529190063\n",
      "Validated batch 266 batch loss 0.451915205\n",
      "Validated batch 267 batch loss 0.567343473\n",
      "Validated batch 268 batch loss 0.525339723\n",
      "Validated batch 269 batch loss 0.595716357\n",
      "Validated batch 270 batch loss 0.610837817\n",
      "Validated batch 271 batch loss 0.554727614\n",
      "Validated batch 272 batch loss 0.56151253\n",
      "Validated batch 273 batch loss 0.559931695\n",
      "Validated batch 274 batch loss 0.556361318\n",
      "Validated batch 275 batch loss 0.490118802\n",
      "Validated batch 276 batch loss 0.445863336\n",
      "Validated batch 277 batch loss 0.548796117\n",
      "Validated batch 278 batch loss 0.547964\n",
      "Validated batch 279 batch loss 0.517522633\n",
      "Validated batch 280 batch loss 0.447879404\n",
      "Validated batch 281 batch loss 0.558992743\n",
      "Validated batch 282 batch loss 0.542874515\n",
      "Validated batch 283 batch loss 0.531353772\n",
      "Validated batch 284 batch loss 0.543430209\n",
      "Validated batch 285 batch loss 0.498969227\n",
      "Validated batch 286 batch loss 0.529602289\n",
      "Validated batch 287 batch loss 0.560050726\n",
      "Validated batch 288 batch loss 0.585526347\n",
      "Validated batch 289 batch loss 0.578225851\n",
      "Validated batch 290 batch loss 0.492148757\n",
      "Validated batch 291 batch loss 0.644656837\n",
      "Validated batch 292 batch loss 0.513326824\n",
      "Validated batch 293 batch loss 0.650464892\n",
      "Validated batch 294 batch loss 0.55254364\n",
      "Validated batch 295 batch loss 0.465711474\n",
      "Validated batch 296 batch loss 0.477411509\n",
      "Validated batch 297 batch loss 0.689577758\n",
      "Validated batch 298 batch loss 0.514686108\n",
      "Validated batch 299 batch loss 0.574842274\n",
      "Validated batch 300 batch loss 0.552115142\n",
      "Validated batch 301 batch loss 0.505241454\n",
      "Validated batch 302 batch loss 0.523455799\n",
      "Validated batch 303 batch loss 0.63200748\n",
      "Validated batch 304 batch loss 0.53431946\n",
      "Validated batch 305 batch loss 0.588581324\n",
      "Validated batch 306 batch loss 0.538345098\n",
      "Validated batch 307 batch loss 0.552551806\n",
      "Validated batch 308 batch loss 0.486434937\n",
      "Validated batch 309 batch loss 0.639271498\n",
      "Validated batch 310 batch loss 0.630822182\n",
      "Validated batch 311 batch loss 0.569704771\n",
      "Validated batch 312 batch loss 0.551683\n",
      "Validated batch 313 batch loss 0.629983664\n",
      "Validated batch 314 batch loss 0.550912201\n",
      "Validated batch 315 batch loss 0.459064871\n",
      "Validated batch 316 batch loss 0.593870938\n",
      "Validated batch 317 batch loss 0.554187179\n",
      "Validated batch 318 batch loss 0.571299613\n",
      "Validated batch 319 batch loss 0.556772709\n",
      "Validated batch 320 batch loss 0.545103669\n",
      "Validated batch 321 batch loss 0.481277734\n",
      "Validated batch 322 batch loss 0.60094595\n",
      "Validated batch 323 batch loss 0.522445738\n",
      "Validated batch 324 batch loss 0.52628988\n",
      "Validated batch 325 batch loss 0.647850633\n",
      "Validated batch 326 batch loss 0.499605596\n",
      "Validated batch 327 batch loss 0.550442636\n",
      "Validated batch 328 batch loss 0.524697304\n",
      "Validated batch 329 batch loss 0.616333902\n",
      "Validated batch 330 batch loss 0.488341451\n",
      "Validated batch 331 batch loss 0.589696884\n",
      "Validated batch 332 batch loss 0.554406404\n",
      "Validated batch 333 batch loss 0.514276326\n",
      "Validated batch 334 batch loss 0.621614814\n",
      "Validated batch 335 batch loss 0.582056046\n",
      "Validated batch 336 batch loss 0.646868527\n",
      "Validated batch 337 batch loss 0.508959532\n",
      "Validated batch 338 batch loss 0.567795932\n",
      "Validated batch 339 batch loss 0.49264285\n",
      "Validated batch 340 batch loss 0.566864908\n",
      "Validated batch 341 batch loss 0.717888653\n",
      "Validated batch 342 batch loss 0.581808448\n",
      "Validated batch 343 batch loss 0.583222091\n",
      "Validated batch 344 batch loss 0.517970622\n",
      "Validated batch 345 batch loss 0.504268348\n",
      "Validated batch 346 batch loss 0.448046356\n",
      "Validated batch 347 batch loss 0.562239766\n",
      "Validated batch 348 batch loss 0.572087884\n",
      "Validated batch 349 batch loss 0.612943769\n",
      "Validated batch 350 batch loss 0.614386201\n",
      "Validated batch 351 batch loss 0.570598722\n",
      "Validated batch 352 batch loss 0.667004585\n",
      "Validated batch 353 batch loss 0.591049969\n",
      "Validated batch 354 batch loss 0.659409821\n",
      "Validated batch 355 batch loss 0.604375899\n",
      "Validated batch 356 batch loss 0.520070791\n",
      "Validated batch 357 batch loss 0.64181459\n",
      "Validated batch 358 batch loss 0.654653907\n",
      "Validated batch 359 batch loss 0.572446942\n",
      "Validated batch 360 batch loss 0.542907476\n",
      "Validated batch 361 batch loss 0.677126765\n",
      "Validated batch 362 batch loss 0.531340182\n",
      "Validated batch 363 batch loss 0.547958791\n",
      "Validated batch 364 batch loss 0.620652\n",
      "Validated batch 365 batch loss 0.498999\n",
      "Validated batch 366 batch loss 0.421268284\n",
      "Validated batch 367 batch loss 0.485213578\n",
      "Validated batch 368 batch loss 0.517892\n",
      "Validated batch 369 batch loss 0.626513839\n",
      "Epoch 9 val loss 0.5609627366065979\n",
      "Model /aiffel/mpii/weights/shg_model-epoch-9-loss-0.5610.h5 saved.\n",
      "Start epoch 10 with learning rate 0.0007\n",
      "Start distributed training...\n",
      "Trained batch 1 batch loss 0.590373397 epoch total loss 0.590373397\n",
      "Trained batch 2 batch loss 0.49258098 epoch total loss 0.541477203\n",
      "Trained batch 3 batch loss 0.542650521 epoch total loss 0.541868269\n",
      "Trained batch 4 batch loss 0.549528 epoch total loss 0.543783188\n",
      "Trained batch 5 batch loss 0.565618575 epoch total loss 0.548150241\n",
      "Trained batch 6 batch loss 0.566723585 epoch total loss 0.551245809\n",
      "Trained batch 7 batch loss 0.625006497 epoch total loss 0.561783\n",
      "Trained batch 8 batch loss 0.494338393 epoch total loss 0.553352475\n",
      "Trained batch 9 batch loss 0.551049769 epoch total loss 0.553096592\n",
      "Trained batch 10 batch loss 0.534286439 epoch total loss 0.551215589\n",
      "Trained batch 11 batch loss 0.534345329 epoch total loss 0.549681902\n",
      "Trained batch 12 batch loss 0.555303097 epoch total loss 0.550150335\n",
      "Trained batch 13 batch loss 0.542019129 epoch total loss 0.549524903\n",
      "Trained batch 14 batch loss 0.531500638 epoch total loss 0.548237443\n",
      "Trained batch 15 batch loss 0.521699786 epoch total loss 0.546468318\n",
      "Trained batch 16 batch loss 0.611170411 epoch total loss 0.550512195\n",
      "Trained batch 17 batch loss 0.667969108 epoch total loss 0.557421386\n",
      "Trained batch 18 batch loss 0.538621902 epoch total loss 0.556377\n",
      "Trained batch 19 batch loss 0.455724061 epoch total loss 0.551079452\n",
      "Trained batch 20 batch loss 0.386493325 epoch total loss 0.542850137\n",
      "Trained batch 21 batch loss 0.38971445 epoch total loss 0.535558\n",
      "Trained batch 22 batch loss 0.447692156 epoch total loss 0.531564057\n",
      "Trained batch 23 batch loss 0.542302489 epoch total loss 0.53203094\n",
      "Trained batch 24 batch loss 0.503860831 epoch total loss 0.530857146\n",
      "Trained batch 25 batch loss 0.514889181 epoch total loss 0.530218422\n",
      "Trained batch 26 batch loss 0.512783766 epoch total loss 0.52954787\n",
      "Trained batch 27 batch loss 0.496110469 epoch total loss 0.528309464\n",
      "Trained batch 28 batch loss 0.420024931 epoch total loss 0.524442136\n",
      "Trained batch 29 batch loss 0.522971869 epoch total loss 0.524391472\n",
      "Trained batch 30 batch loss 0.549790323 epoch total loss 0.525238097\n",
      "Trained batch 31 batch loss 0.5631302 epoch total loss 0.526460409\n",
      "Trained batch 32 batch loss 0.50180006 epoch total loss 0.525689781\n",
      "Trained batch 33 batch loss 0.547390401 epoch total loss 0.526347339\n",
      "Trained batch 34 batch loss 0.55124861 epoch total loss 0.527079761\n",
      "Trained batch 35 batch loss 0.608695626 epoch total loss 0.529411614\n",
      "Trained batch 36 batch loss 0.55308497 epoch total loss 0.530069232\n",
      "Trained batch 37 batch loss 0.522865534 epoch total loss 0.529874563\n",
      "Trained batch 38 batch loss 0.589802 epoch total loss 0.531451583\n",
      "Trained batch 39 batch loss 0.56182605 epoch total loss 0.532230437\n",
      "Trained batch 40 batch loss 0.643132567 epoch total loss 0.535003\n",
      "Trained batch 41 batch loss 0.668020248 epoch total loss 0.538247347\n",
      "Trained batch 42 batch loss 0.550403416 epoch total loss 0.538536727\n",
      "Trained batch 43 batch loss 0.640064836 epoch total loss 0.540897846\n",
      "Trained batch 44 batch loss 0.539675474 epoch total loss 0.54087007\n",
      "Trained batch 45 batch loss 0.526523471 epoch total loss 0.540551245\n",
      "Trained batch 46 batch loss 0.547652364 epoch total loss 0.540705621\n",
      "Trained batch 47 batch loss 0.508041 epoch total loss 0.540010631\n",
      "Trained batch 48 batch loss 0.523403943 epoch total loss 0.539664686\n",
      "Trained batch 49 batch loss 0.4889732 epoch total loss 0.538630128\n",
      "Trained batch 50 batch loss 0.430118144 epoch total loss 0.536459923\n",
      "Trained batch 51 batch loss 0.367552578 epoch total loss 0.533148\n",
      "Trained batch 52 batch loss 0.401537329 epoch total loss 0.530617\n",
      "Trained batch 53 batch loss 0.430449069 epoch total loss 0.528727055\n",
      "Trained batch 54 batch loss 0.433372617 epoch total loss 0.526961207\n",
      "Trained batch 55 batch loss 0.459281236 epoch total loss 0.525730669\n",
      "Trained batch 56 batch loss 0.409368604 epoch total loss 0.523652792\n",
      "Trained batch 57 batch loss 0.494913906 epoch total loss 0.523148596\n",
      "Trained batch 58 batch loss 0.507242143 epoch total loss 0.522874355\n",
      "Trained batch 59 batch loss 0.537740231 epoch total loss 0.523126304\n",
      "Trained batch 60 batch loss 0.52380991 epoch total loss 0.523137689\n",
      "Trained batch 61 batch loss 0.399242193 epoch total loss 0.521106601\n",
      "Trained batch 62 batch loss 0.514819682 epoch total loss 0.521005213\n",
      "Trained batch 63 batch loss 0.509149194 epoch total loss 0.520817\n",
      "Trained batch 64 batch loss 0.582737446 epoch total loss 0.521784484\n",
      "Trained batch 65 batch loss 0.551862955 epoch total loss 0.522247255\n",
      "Trained batch 66 batch loss 0.591227889 epoch total loss 0.523292422\n",
      "Trained batch 67 batch loss 0.580109477 epoch total loss 0.524140418\n",
      "Trained batch 68 batch loss 0.573873162 epoch total loss 0.524871767\n",
      "Trained batch 69 batch loss 0.598294 epoch total loss 0.525935829\n",
      "Trained batch 70 batch loss 0.465385079 epoch total loss 0.525070846\n",
      "Trained batch 71 batch loss 0.482592314 epoch total loss 0.524472594\n",
      "Trained batch 72 batch loss 0.466133296 epoch total loss 0.523662329\n",
      "Trained batch 73 batch loss 0.467748523 epoch total loss 0.522896349\n",
      "Trained batch 74 batch loss 0.500241935 epoch total loss 0.52259016\n",
      "Trained batch 75 batch loss 0.535618484 epoch total loss 0.522763848\n",
      "Trained batch 76 batch loss 0.537689567 epoch total loss 0.522960246\n",
      "Trained batch 77 batch loss 0.576338708 epoch total loss 0.523653507\n",
      "Trained batch 78 batch loss 0.515529394 epoch total loss 0.523549378\n",
      "Trained batch 79 batch loss 0.659015715 epoch total loss 0.525264084\n",
      "Trained batch 80 batch loss 0.621852279 epoch total loss 0.526471496\n",
      "Trained batch 81 batch loss 0.473481625 epoch total loss 0.525817275\n",
      "Trained batch 82 batch loss 0.525698 epoch total loss 0.525815845\n",
      "Trained batch 83 batch loss 0.587340713 epoch total loss 0.526557088\n",
      "Trained batch 84 batch loss 0.512142181 epoch total loss 0.526385486\n",
      "Trained batch 85 batch loss 0.471447468 epoch total loss 0.525739133\n",
      "Trained batch 86 batch loss 0.494633943 epoch total loss 0.525377452\n",
      "Trained batch 87 batch loss 0.468292117 epoch total loss 0.524721324\n",
      "Trained batch 88 batch loss 0.535686135 epoch total loss 0.524845898\n",
      "Trained batch 89 batch loss 0.451728195 epoch total loss 0.524024367\n",
      "Trained batch 90 batch loss 0.456212819 epoch total loss 0.523270905\n",
      "Trained batch 91 batch loss 0.556048512 epoch total loss 0.523631096\n",
      "Trained batch 92 batch loss 0.491260111 epoch total loss 0.52327925\n",
      "Trained batch 93 batch loss 0.551934183 epoch total loss 0.523587346\n",
      "Trained batch 94 batch loss 0.552342653 epoch total loss 0.523893237\n",
      "Trained batch 95 batch loss 0.568002224 epoch total loss 0.524357498\n",
      "Trained batch 96 batch loss 0.583499789 epoch total loss 0.524973571\n",
      "Trained batch 97 batch loss 0.554931641 epoch total loss 0.525282443\n",
      "Trained batch 98 batch loss 0.543670774 epoch total loss 0.525470078\n",
      "Trained batch 99 batch loss 0.528420269 epoch total loss 0.52549988\n",
      "Trained batch 100 batch loss 0.521884918 epoch total loss 0.5254637\n",
      "Trained batch 101 batch loss 0.531243086 epoch total loss 0.525520921\n",
      "Trained batch 102 batch loss 0.622749 epoch total loss 0.526474178\n",
      "Trained batch 103 batch loss 0.561462522 epoch total loss 0.526813865\n",
      "Trained batch 104 batch loss 0.546788394 epoch total loss 0.527005911\n",
      "Trained batch 105 batch loss 0.526058793 epoch total loss 0.526996851\n",
      "Trained batch 106 batch loss 0.512106121 epoch total loss 0.526856422\n",
      "Trained batch 107 batch loss 0.563063741 epoch total loss 0.527194798\n",
      "Trained batch 108 batch loss 0.53965795 epoch total loss 0.527310193\n",
      "Trained batch 109 batch loss 0.642384052 epoch total loss 0.52836591\n",
      "Trained batch 110 batch loss 0.534850597 epoch total loss 0.528424859\n",
      "Trained batch 111 batch loss 0.554361343 epoch total loss 0.528658569\n",
      "Trained batch 112 batch loss 0.522161841 epoch total loss 0.528600574\n",
      "Trained batch 113 batch loss 0.533425808 epoch total loss 0.52864325\n",
      "Trained batch 114 batch loss 0.573227048 epoch total loss 0.529034317\n",
      "Trained batch 115 batch loss 0.544858813 epoch total loss 0.529171944\n",
      "Trained batch 116 batch loss 0.466794938 epoch total loss 0.528634191\n",
      "Trained batch 117 batch loss 0.532556832 epoch total loss 0.528667688\n",
      "Trained batch 118 batch loss 0.533386 epoch total loss 0.528707683\n",
      "Trained batch 119 batch loss 0.565758944 epoch total loss 0.529019058\n",
      "Trained batch 120 batch loss 0.573820114 epoch total loss 0.529392362\n",
      "Trained batch 121 batch loss 0.522221923 epoch total loss 0.529333115\n",
      "Trained batch 122 batch loss 0.44796 epoch total loss 0.528666139\n",
      "Trained batch 123 batch loss 0.507797956 epoch total loss 0.528496444\n",
      "Trained batch 124 batch loss 0.550401509 epoch total loss 0.528673112\n",
      "Trained batch 125 batch loss 0.530624449 epoch total loss 0.528688729\n",
      "Trained batch 126 batch loss 0.488051921 epoch total loss 0.528366208\n",
      "Trained batch 127 batch loss 0.504652858 epoch total loss 0.528179526\n",
      "Trained batch 128 batch loss 0.500130236 epoch total loss 0.52796036\n",
      "Trained batch 129 batch loss 0.454513729 epoch total loss 0.527391\n",
      "Trained batch 130 batch loss 0.467898101 epoch total loss 0.526933372\n",
      "Trained batch 131 batch loss 0.467710257 epoch total loss 0.526481271\n",
      "Trained batch 132 batch loss 0.450379938 epoch total loss 0.525904715\n",
      "Trained batch 133 batch loss 0.504898667 epoch total loss 0.525746822\n",
      "Trained batch 134 batch loss 0.523998141 epoch total loss 0.525733709\n",
      "Trained batch 135 batch loss 0.47938183 epoch total loss 0.525390387\n",
      "Trained batch 136 batch loss 0.445432305 epoch total loss 0.524802506\n",
      "Trained batch 137 batch loss 0.438234955 epoch total loss 0.524170578\n",
      "Trained batch 138 batch loss 0.489500105 epoch total loss 0.523919344\n",
      "Trained batch 139 batch loss 0.360803276 epoch total loss 0.522745848\n",
      "Trained batch 140 batch loss 0.372091889 epoch total loss 0.521669745\n",
      "Trained batch 141 batch loss 0.381061971 epoch total loss 0.52067256\n",
      "Trained batch 142 batch loss 0.432696968 epoch total loss 0.520053\n",
      "Trained batch 143 batch loss 0.47731024 epoch total loss 0.519754112\n",
      "Trained batch 144 batch loss 0.549568594 epoch total loss 0.519961119\n",
      "Trained batch 145 batch loss 0.535130143 epoch total loss 0.520065784\n",
      "Trained batch 146 batch loss 0.597947598 epoch total loss 0.520599186\n",
      "Trained batch 147 batch loss 0.582843661 epoch total loss 0.521022618\n",
      "Trained batch 148 batch loss 0.633229613 epoch total loss 0.521780789\n",
      "Trained batch 149 batch loss 0.620010138 epoch total loss 0.522440076\n",
      "Trained batch 150 batch loss 0.533942938 epoch total loss 0.522516727\n",
      "Trained batch 151 batch loss 0.588814 epoch total loss 0.522955775\n",
      "Trained batch 152 batch loss 0.581332266 epoch total loss 0.523339808\n",
      "Trained batch 153 batch loss 0.58864969 epoch total loss 0.523766637\n",
      "Trained batch 154 batch loss 0.512468576 epoch total loss 0.523693264\n",
      "Trained batch 155 batch loss 0.588369071 epoch total loss 0.524110556\n",
      "Trained batch 156 batch loss 0.52415514 epoch total loss 0.524110854\n",
      "Trained batch 157 batch loss 0.456996679 epoch total loss 0.523683369\n",
      "Trained batch 158 batch loss 0.478079081 epoch total loss 0.523394704\n",
      "Trained batch 159 batch loss 0.453796029 epoch total loss 0.522956967\n",
      "Trained batch 160 batch loss 0.519399226 epoch total loss 0.522934794\n",
      "Trained batch 161 batch loss 0.426950306 epoch total loss 0.522338569\n",
      "Trained batch 162 batch loss 0.572110951 epoch total loss 0.522645831\n",
      "Trained batch 163 batch loss 0.603869915 epoch total loss 0.523144126\n",
      "Trained batch 164 batch loss 0.424868554 epoch total loss 0.522544861\n",
      "Trained batch 165 batch loss 0.518376589 epoch total loss 0.522519588\n",
      "Trained batch 166 batch loss 0.505797863 epoch total loss 0.522418857\n",
      "Trained batch 167 batch loss 0.528930724 epoch total loss 0.522457898\n",
      "Trained batch 168 batch loss 0.553238273 epoch total loss 0.522641063\n",
      "Trained batch 169 batch loss 0.513816118 epoch total loss 0.522588849\n",
      "Trained batch 170 batch loss 0.460416079 epoch total loss 0.522223175\n",
      "Trained batch 171 batch loss 0.542953193 epoch total loss 0.52234441\n",
      "Trained batch 172 batch loss 0.568692088 epoch total loss 0.522613883\n",
      "Trained batch 173 batch loss 0.539315343 epoch total loss 0.522710443\n",
      "Trained batch 174 batch loss 0.518223166 epoch total loss 0.522684634\n",
      "Trained batch 175 batch loss 0.601076365 epoch total loss 0.523132563\n",
      "Trained batch 176 batch loss 0.469295472 epoch total loss 0.522826672\n",
      "Trained batch 177 batch loss 0.440458983 epoch total loss 0.522361338\n",
      "Trained batch 178 batch loss 0.474913925 epoch total loss 0.522094786\n",
      "Trained batch 179 batch loss 0.577228606 epoch total loss 0.522402823\n",
      "Trained batch 180 batch loss 0.443623811 epoch total loss 0.521965146\n",
      "Trained batch 181 batch loss 0.495400578 epoch total loss 0.521818399\n",
      "Trained batch 182 batch loss 0.424238145 epoch total loss 0.521282256\n",
      "Trained batch 183 batch loss 0.553553 epoch total loss 0.521458566\n",
      "Trained batch 184 batch loss 0.449575603 epoch total loss 0.521067917\n",
      "Trained batch 185 batch loss 0.598301709 epoch total loss 0.521485388\n",
      "Trained batch 186 batch loss 0.513763487 epoch total loss 0.521443903\n",
      "Trained batch 187 batch loss 0.480313659 epoch total loss 0.521223962\n",
      "Trained batch 188 batch loss 0.477293432 epoch total loss 0.520990312\n",
      "Trained batch 189 batch loss 0.42703259 epoch total loss 0.52049315\n",
      "Trained batch 190 batch loss 0.480192959 epoch total loss 0.520281076\n",
      "Trained batch 191 batch loss 0.520358086 epoch total loss 0.520281434\n",
      "Trained batch 192 batch loss 0.494083822 epoch total loss 0.520145\n",
      "Trained batch 193 batch loss 0.445977479 epoch total loss 0.519760728\n",
      "Trained batch 194 batch loss 0.530704439 epoch total loss 0.519817114\n",
      "Trained batch 195 batch loss 0.44750759 epoch total loss 0.519446313\n",
      "Trained batch 196 batch loss 0.472926021 epoch total loss 0.519208968\n",
      "Trained batch 197 batch loss 0.468255341 epoch total loss 0.518950284\n",
      "Trained batch 198 batch loss 0.465327948 epoch total loss 0.51867944\n",
      "Trained batch 199 batch loss 0.469408125 epoch total loss 0.518431842\n",
      "Trained batch 200 batch loss 0.511021435 epoch total loss 0.518394828\n",
      "Trained batch 201 batch loss 0.531851411 epoch total loss 0.518461764\n",
      "Trained batch 202 batch loss 0.574954569 epoch total loss 0.518741429\n",
      "Trained batch 203 batch loss 0.536860168 epoch total loss 0.518830657\n",
      "Trained batch 204 batch loss 0.549682856 epoch total loss 0.518981874\n",
      "Trained batch 205 batch loss 0.514338076 epoch total loss 0.518959224\n",
      "Trained batch 206 batch loss 0.528169215 epoch total loss 0.519003928\n",
      "Trained batch 207 batch loss 0.525412381 epoch total loss 0.519034922\n",
      "Trained batch 208 batch loss 0.508656859 epoch total loss 0.518985033\n",
      "Trained batch 209 batch loss 0.610575378 epoch total loss 0.519423246\n",
      "Trained batch 210 batch loss 0.556125641 epoch total loss 0.519598\n",
      "Trained batch 211 batch loss 0.584583104 epoch total loss 0.519906\n",
      "Trained batch 212 batch loss 0.579041243 epoch total loss 0.520184934\n",
      "Trained batch 213 batch loss 0.571390867 epoch total loss 0.52042532\n",
      "Trained batch 214 batch loss 0.533132374 epoch total loss 0.520484686\n",
      "Trained batch 215 batch loss 0.56508857 epoch total loss 0.52069217\n",
      "Trained batch 216 batch loss 0.59546566 epoch total loss 0.521038353\n",
      "Trained batch 217 batch loss 0.445799589 epoch total loss 0.520691633\n",
      "Trained batch 218 batch loss 0.578822613 epoch total loss 0.520958245\n",
      "Trained batch 219 batch loss 0.549960256 epoch total loss 0.521090686\n",
      "Trained batch 220 batch loss 0.5465343 epoch total loss 0.521206319\n",
      "Trained batch 221 batch loss 0.539234102 epoch total loss 0.521287858\n",
      "Trained batch 222 batch loss 0.580830514 epoch total loss 0.521556079\n",
      "Trained batch 223 batch loss 0.457157046 epoch total loss 0.521267295\n",
      "Trained batch 224 batch loss 0.493520141 epoch total loss 0.521143436\n",
      "Trained batch 225 batch loss 0.600438416 epoch total loss 0.521495879\n",
      "Trained batch 226 batch loss 0.568732083 epoch total loss 0.521704912\n",
      "Trained batch 227 batch loss 0.704672098 epoch total loss 0.522510886\n",
      "Trained batch 228 batch loss 0.59677 epoch total loss 0.522836626\n",
      "Trained batch 229 batch loss 0.601769745 epoch total loss 0.523181319\n",
      "Trained batch 230 batch loss 0.565855265 epoch total loss 0.523366868\n",
      "Trained batch 231 batch loss 0.54920578 epoch total loss 0.523478746\n",
      "Trained batch 232 batch loss 0.569368482 epoch total loss 0.523676515\n",
      "Trained batch 233 batch loss 0.542383552 epoch total loss 0.523756802\n",
      "Trained batch 234 batch loss 0.52056 epoch total loss 0.523743153\n",
      "Trained batch 235 batch loss 0.494057059 epoch total loss 0.523616791\n",
      "Trained batch 236 batch loss 0.496785969 epoch total loss 0.523503125\n",
      "Trained batch 237 batch loss 0.501398563 epoch total loss 0.523409843\n",
      "Trained batch 238 batch loss 0.527501225 epoch total loss 0.523427069\n",
      "Trained batch 239 batch loss 0.554086506 epoch total loss 0.523555338\n",
      "Trained batch 240 batch loss 0.593801558 epoch total loss 0.523848\n",
      "Trained batch 241 batch loss 0.583574831 epoch total loss 0.524095833\n",
      "Trained batch 242 batch loss 0.664881647 epoch total loss 0.524677575\n",
      "Trained batch 243 batch loss 0.651126087 epoch total loss 0.525197923\n",
      "Trained batch 244 batch loss 0.660266101 epoch total loss 0.525751531\n",
      "Trained batch 245 batch loss 0.712748051 epoch total loss 0.526514769\n",
      "Trained batch 246 batch loss 0.455949783 epoch total loss 0.526227951\n",
      "Trained batch 247 batch loss 0.470433086 epoch total loss 0.526002049\n",
      "Trained batch 248 batch loss 0.498877704 epoch total loss 0.525892615\n",
      "Trained batch 249 batch loss 0.529587269 epoch total loss 0.525907457\n",
      "Trained batch 250 batch loss 0.527201891 epoch total loss 0.525912642\n",
      "Trained batch 251 batch loss 0.597719908 epoch total loss 0.526198745\n",
      "Trained batch 252 batch loss 0.490084767 epoch total loss 0.526055396\n",
      "Trained batch 253 batch loss 0.520661116 epoch total loss 0.526034117\n",
      "Trained batch 254 batch loss 0.637675 epoch total loss 0.526473641\n",
      "Trained batch 255 batch loss 0.560247421 epoch total loss 0.526606083\n",
      "Trained batch 256 batch loss 0.628817856 epoch total loss 0.527005315\n",
      "Trained batch 257 batch loss 0.5307042 epoch total loss 0.52701968\n",
      "Trained batch 258 batch loss 0.438965976 epoch total loss 0.526678383\n",
      "Trained batch 259 batch loss 0.436843544 epoch total loss 0.526331544\n",
      "Trained batch 260 batch loss 0.515726745 epoch total loss 0.526290774\n",
      "Trained batch 261 batch loss 0.439794391 epoch total loss 0.525959373\n",
      "Trained batch 262 batch loss 0.438988388 epoch total loss 0.525627434\n",
      "Trained batch 263 batch loss 0.452470481 epoch total loss 0.525349259\n",
      "Trained batch 264 batch loss 0.511686146 epoch total loss 0.525297523\n",
      "Trained batch 265 batch loss 0.560236931 epoch total loss 0.525429368\n",
      "Trained batch 266 batch loss 0.491938531 epoch total loss 0.525303483\n",
      "Trained batch 267 batch loss 0.451696724 epoch total loss 0.525027812\n",
      "Trained batch 268 batch loss 0.543386638 epoch total loss 0.525096238\n",
      "Trained batch 269 batch loss 0.550641358 epoch total loss 0.525191247\n",
      "Trained batch 270 batch loss 0.583004475 epoch total loss 0.525405347\n",
      "Trained batch 271 batch loss 0.640136778 epoch total loss 0.525828719\n",
      "Trained batch 272 batch loss 0.571670234 epoch total loss 0.525997281\n",
      "Trained batch 273 batch loss 0.506304264 epoch total loss 0.525925159\n",
      "Trained batch 274 batch loss 0.604476333 epoch total loss 0.526211798\n",
      "Trained batch 275 batch loss 0.485811889 epoch total loss 0.526064873\n",
      "Trained batch 276 batch loss 0.565679431 epoch total loss 0.526208401\n",
      "Trained batch 277 batch loss 0.536972702 epoch total loss 0.526247263\n",
      "Trained batch 278 batch loss 0.646562278 epoch total loss 0.526680052\n",
      "Trained batch 279 batch loss 0.597362757 epoch total loss 0.526933432\n",
      "Trained batch 280 batch loss 0.544285119 epoch total loss 0.526995361\n",
      "Trained batch 281 batch loss 0.514933467 epoch total loss 0.526952446\n",
      "Trained batch 282 batch loss 0.605984747 epoch total loss 0.527232707\n",
      "Trained batch 283 batch loss 0.539298236 epoch total loss 0.527275324\n",
      "Trained batch 284 batch loss 0.548655391 epoch total loss 0.527350605\n",
      "Trained batch 285 batch loss 0.498898774 epoch total loss 0.527250826\n",
      "Trained batch 286 batch loss 0.513324797 epoch total loss 0.52720207\n",
      "Trained batch 287 batch loss 0.460837096 epoch total loss 0.526970863\n",
      "Trained batch 288 batch loss 0.514324307 epoch total loss 0.526926935\n",
      "Trained batch 289 batch loss 0.513563633 epoch total loss 0.526880682\n",
      "Trained batch 290 batch loss 0.575480938 epoch total loss 0.52704829\n",
      "Trained batch 291 batch loss 0.586783826 epoch total loss 0.527253568\n",
      "Trained batch 292 batch loss 0.506945372 epoch total loss 0.527184\n",
      "Trained batch 293 batch loss 0.642584383 epoch total loss 0.527577817\n",
      "Trained batch 294 batch loss 0.615288258 epoch total loss 0.527876198\n",
      "Trained batch 295 batch loss 0.596408725 epoch total loss 0.528108478\n",
      "Trained batch 296 batch loss 0.561232507 epoch total loss 0.528220415\n",
      "Trained batch 297 batch loss 0.575954854 epoch total loss 0.528381169\n",
      "Trained batch 298 batch loss 0.585224688 epoch total loss 0.528571904\n",
      "Trained batch 299 batch loss 0.499958783 epoch total loss 0.528476179\n",
      "Trained batch 300 batch loss 0.438819975 epoch total loss 0.528177321\n",
      "Trained batch 301 batch loss 0.464875132 epoch total loss 0.527967036\n",
      "Trained batch 302 batch loss 0.556939662 epoch total loss 0.528063\n",
      "Trained batch 303 batch loss 0.546481 epoch total loss 0.528123736\n",
      "Trained batch 304 batch loss 0.648517 epoch total loss 0.528519809\n",
      "Trained batch 305 batch loss 0.68159914 epoch total loss 0.52902168\n",
      "Trained batch 306 batch loss 0.688621759 epoch total loss 0.529543281\n",
      "Trained batch 307 batch loss 0.628787816 epoch total loss 0.529866517\n",
      "Trained batch 308 batch loss 0.65401566 epoch total loss 0.530269623\n",
      "Trained batch 309 batch loss 0.598618805 epoch total loss 0.530490816\n",
      "Trained batch 310 batch loss 0.519938171 epoch total loss 0.530456781\n",
      "Trained batch 311 batch loss 0.477088511 epoch total loss 0.53028518\n",
      "Trained batch 312 batch loss 0.504562199 epoch total loss 0.530202746\n",
      "Trained batch 313 batch loss 0.484835923 epoch total loss 0.530057788\n",
      "Trained batch 314 batch loss 0.416336983 epoch total loss 0.52969557\n",
      "Trained batch 315 batch loss 0.536097705 epoch total loss 0.529715955\n",
      "Trained batch 316 batch loss 0.538569212 epoch total loss 0.529743969\n",
      "Trained batch 317 batch loss 0.606561422 epoch total loss 0.529986322\n",
      "Trained batch 318 batch loss 0.603458643 epoch total loss 0.53021735\n",
      "Trained batch 319 batch loss 0.534858346 epoch total loss 0.530231893\n",
      "Trained batch 320 batch loss 0.555600882 epoch total loss 0.530311167\n",
      "Trained batch 321 batch loss 0.573230863 epoch total loss 0.53044486\n",
      "Trained batch 322 batch loss 0.607444882 epoch total loss 0.530684\n",
      "Trained batch 323 batch loss 0.649793506 epoch total loss 0.531052768\n",
      "Trained batch 324 batch loss 0.585624 epoch total loss 0.531221151\n",
      "Trained batch 325 batch loss 0.614892662 epoch total loss 0.531478643\n",
      "Trained batch 326 batch loss 0.618786514 epoch total loss 0.531746447\n",
      "Trained batch 327 batch loss 0.644498825 epoch total loss 0.53209126\n",
      "Trained batch 328 batch loss 0.646385789 epoch total loss 0.532439768\n",
      "Trained batch 329 batch loss 0.593648732 epoch total loss 0.532625794\n",
      "Trained batch 330 batch loss 0.534195244 epoch total loss 0.532630563\n",
      "Trained batch 331 batch loss 0.568571746 epoch total loss 0.532739162\n",
      "Trained batch 332 batch loss 0.523913741 epoch total loss 0.532712519\n",
      "Trained batch 333 batch loss 0.511987686 epoch total loss 0.532650352\n",
      "Trained batch 334 batch loss 0.493549109 epoch total loss 0.532533228\n",
      "Trained batch 335 batch loss 0.565706611 epoch total loss 0.532632291\n",
      "Trained batch 336 batch loss 0.537600517 epoch total loss 0.532647\n",
      "Trained batch 337 batch loss 0.572117805 epoch total loss 0.532764137\n",
      "Trained batch 338 batch loss 0.552864 epoch total loss 0.532823622\n",
      "Trained batch 339 batch loss 0.518493176 epoch total loss 0.532781303\n",
      "Trained batch 340 batch loss 0.515009463 epoch total loss 0.532729089\n",
      "Trained batch 341 batch loss 0.507886231 epoch total loss 0.532656252\n",
      "Trained batch 342 batch loss 0.572753191 epoch total loss 0.532773495\n",
      "Trained batch 343 batch loss 0.514353573 epoch total loss 0.532719791\n",
      "Trained batch 344 batch loss 0.484820664 epoch total loss 0.532580554\n",
      "Trained batch 345 batch loss 0.531488419 epoch total loss 0.532577395\n",
      "Trained batch 346 batch loss 0.527219176 epoch total loss 0.532561898\n",
      "Trained batch 347 batch loss 0.540291846 epoch total loss 0.53258419\n",
      "Trained batch 348 batch loss 0.517843723 epoch total loss 0.532541811\n",
      "Trained batch 349 batch loss 0.47598 epoch total loss 0.532379746\n",
      "Trained batch 350 batch loss 0.45858106 epoch total loss 0.532168925\n",
      "Trained batch 351 batch loss 0.497158587 epoch total loss 0.532069206\n",
      "Trained batch 352 batch loss 0.523379326 epoch total loss 0.53204447\n",
      "Trained batch 353 batch loss 0.445133954 epoch total loss 0.531798303\n",
      "Trained batch 354 batch loss 0.469830364 epoch total loss 0.531623244\n",
      "Trained batch 355 batch loss 0.487708777 epoch total loss 0.531499505\n",
      "Trained batch 356 batch loss 0.541450739 epoch total loss 0.53152746\n",
      "Trained batch 357 batch loss 0.471045911 epoch total loss 0.531358063\n",
      "Trained batch 358 batch loss 0.45248127 epoch total loss 0.531137705\n",
      "Trained batch 359 batch loss 0.556996524 epoch total loss 0.531209767\n",
      "Trained batch 360 batch loss 0.469891191 epoch total loss 0.531039417\n",
      "Trained batch 361 batch loss 0.575420678 epoch total loss 0.531162381\n",
      "Trained batch 362 batch loss 0.454062253 epoch total loss 0.530949354\n",
      "Trained batch 363 batch loss 0.533557236 epoch total loss 0.530956566\n",
      "Trained batch 364 batch loss 0.494297177 epoch total loss 0.530855834\n",
      "Trained batch 365 batch loss 0.481125712 epoch total loss 0.530719578\n",
      "Trained batch 366 batch loss 0.284629077 epoch total loss 0.530047178\n",
      "Trained batch 367 batch loss 0.378319412 epoch total loss 0.52963376\n",
      "Trained batch 368 batch loss 0.424736977 epoch total loss 0.529348731\n",
      "Trained batch 369 batch loss 0.49334836 epoch total loss 0.529251158\n",
      "Trained batch 370 batch loss 0.560993314 epoch total loss 0.529337\n",
      "Trained batch 371 batch loss 0.621987224 epoch total loss 0.529586732\n",
      "Trained batch 372 batch loss 0.511733 epoch total loss 0.529538691\n",
      "Trained batch 373 batch loss 0.492803276 epoch total loss 0.529440224\n",
      "Trained batch 374 batch loss 0.51550287 epoch total loss 0.529403\n",
      "Trained batch 375 batch loss 0.490689754 epoch total loss 0.529299736\n",
      "Trained batch 376 batch loss 0.509526491 epoch total loss 0.529247105\n",
      "Trained batch 377 batch loss 0.496091753 epoch total loss 0.529159188\n",
      "Trained batch 378 batch loss 0.532366157 epoch total loss 0.529167652\n",
      "Trained batch 379 batch loss 0.472460747 epoch total loss 0.529018044\n",
      "Trained batch 380 batch loss 0.405192792 epoch total loss 0.528692186\n",
      "Trained batch 381 batch loss 0.393361628 epoch total loss 0.528337\n",
      "Trained batch 382 batch loss 0.424010932 epoch total loss 0.528063893\n",
      "Trained batch 383 batch loss 0.498142034 epoch total loss 0.527985752\n",
      "Trained batch 384 batch loss 0.47226429 epoch total loss 0.527840614\n",
      "Trained batch 385 batch loss 0.49545303 epoch total loss 0.527756512\n",
      "Trained batch 386 batch loss 0.443422288 epoch total loss 0.527538\n",
      "Trained batch 387 batch loss 0.573666453 epoch total loss 0.527657211\n",
      "Trained batch 388 batch loss 0.500758111 epoch total loss 0.527587891\n",
      "Trained batch 389 batch loss 0.465809524 epoch total loss 0.527429044\n",
      "Trained batch 390 batch loss 0.583906651 epoch total loss 0.527573884\n",
      "Trained batch 391 batch loss 0.548914492 epoch total loss 0.527628481\n",
      "Trained batch 392 batch loss 0.544699609 epoch total loss 0.527672\n",
      "Trained batch 393 batch loss 0.495872259 epoch total loss 0.52759105\n",
      "Trained batch 394 batch loss 0.436023057 epoch total loss 0.527358651\n",
      "Trained batch 395 batch loss 0.634772837 epoch total loss 0.527630568\n",
      "Trained batch 396 batch loss 0.54524982 epoch total loss 0.527675033\n",
      "Trained batch 397 batch loss 0.480514795 epoch total loss 0.527556241\n",
      "Trained batch 398 batch loss 0.606223047 epoch total loss 0.52775389\n",
      "Trained batch 399 batch loss 0.599670172 epoch total loss 0.527934134\n",
      "Trained batch 400 batch loss 0.618821502 epoch total loss 0.528161347\n",
      "Trained batch 401 batch loss 0.652252 epoch total loss 0.528470814\n",
      "Trained batch 402 batch loss 0.63372606 epoch total loss 0.528732657\n",
      "Trained batch 403 batch loss 0.629075348 epoch total loss 0.528981626\n",
      "Trained batch 404 batch loss 0.552570224 epoch total loss 0.52904\n",
      "Trained batch 405 batch loss 0.563395858 epoch total loss 0.529124856\n",
      "Trained batch 406 batch loss 0.461413056 epoch total loss 0.528958082\n",
      "Trained batch 407 batch loss 0.571192086 epoch total loss 0.529061854\n",
      "Trained batch 408 batch loss 0.527679086 epoch total loss 0.529058456\n",
      "Trained batch 409 batch loss 0.494556695 epoch total loss 0.528974116\n",
      "Trained batch 410 batch loss 0.518731654 epoch total loss 0.528949142\n",
      "Trained batch 411 batch loss 0.569113851 epoch total loss 0.529046834\n",
      "Trained batch 412 batch loss 0.517659962 epoch total loss 0.529019177\n",
      "Trained batch 413 batch loss 0.585383892 epoch total loss 0.529155672\n",
      "Trained batch 414 batch loss 0.565665364 epoch total loss 0.529243827\n",
      "Trained batch 415 batch loss 0.573861 epoch total loss 0.529351354\n",
      "Trained batch 416 batch loss 0.655549347 epoch total loss 0.529654741\n",
      "Trained batch 417 batch loss 0.573400676 epoch total loss 0.529759586\n",
      "Trained batch 418 batch loss 0.575929165 epoch total loss 0.529870093\n",
      "Trained batch 419 batch loss 0.608003139 epoch total loss 0.530056536\n",
      "Trained batch 420 batch loss 0.584201574 epoch total loss 0.530185461\n",
      "Trained batch 421 batch loss 0.609367251 epoch total loss 0.530373514\n",
      "Trained batch 422 batch loss 0.590562224 epoch total loss 0.530516148\n",
      "Trained batch 423 batch loss 0.618523955 epoch total loss 0.530724227\n",
      "Trained batch 424 batch loss 0.587423146 epoch total loss 0.530857921\n",
      "Trained batch 425 batch loss 0.662645161 epoch total loss 0.531168\n",
      "Trained batch 426 batch loss 0.64202112 epoch total loss 0.531428218\n",
      "Trained batch 427 batch loss 0.584388793 epoch total loss 0.531552255\n",
      "Trained batch 428 batch loss 0.580745101 epoch total loss 0.531667173\n",
      "Trained batch 429 batch loss 0.598822892 epoch total loss 0.531823695\n",
      "Trained batch 430 batch loss 0.535412848 epoch total loss 0.531832099\n",
      "Trained batch 431 batch loss 0.589796901 epoch total loss 0.531966567\n",
      "Trained batch 432 batch loss 0.574876726 epoch total loss 0.532065868\n",
      "Trained batch 433 batch loss 0.512223661 epoch total loss 0.532020032\n",
      "Trained batch 434 batch loss 0.506717205 epoch total loss 0.531961739\n",
      "Trained batch 435 batch loss 0.426480591 epoch total loss 0.531719267\n",
      "Trained batch 436 batch loss 0.424809784 epoch total loss 0.531474054\n",
      "Trained batch 437 batch loss 0.421930611 epoch total loss 0.531223416\n",
      "Trained batch 438 batch loss 0.513148069 epoch total loss 0.53118211\n",
      "Trained batch 439 batch loss 0.519356251 epoch total loss 0.531155229\n",
      "Trained batch 440 batch loss 0.625263214 epoch total loss 0.53136909\n",
      "Trained batch 441 batch loss 0.63924545 epoch total loss 0.531613708\n",
      "Trained batch 442 batch loss 0.572009563 epoch total loss 0.531705081\n",
      "Trained batch 443 batch loss 0.567016244 epoch total loss 0.531784832\n",
      "Trained batch 444 batch loss 0.626464844 epoch total loss 0.531998038\n",
      "Trained batch 445 batch loss 0.547653675 epoch total loss 0.532033265\n",
      "Trained batch 446 batch loss 0.575701118 epoch total loss 0.532131135\n",
      "Trained batch 447 batch loss 0.633772731 epoch total loss 0.532358527\n",
      "Trained batch 448 batch loss 0.632953644 epoch total loss 0.532583058\n",
      "Trained batch 449 batch loss 0.599518538 epoch total loss 0.532732129\n",
      "Trained batch 450 batch loss 0.559835196 epoch total loss 0.53279233\n",
      "Trained batch 451 batch loss 0.509432495 epoch total loss 0.532740533\n",
      "Trained batch 452 batch loss 0.496514052 epoch total loss 0.532660425\n",
      "Trained batch 453 batch loss 0.566269338 epoch total loss 0.532734632\n",
      "Trained batch 454 batch loss 0.585820794 epoch total loss 0.532851517\n",
      "Trained batch 455 batch loss 0.618611157 epoch total loss 0.53304\n",
      "Trained batch 456 batch loss 0.512330174 epoch total loss 0.532994568\n",
      "Trained batch 457 batch loss 0.609763801 epoch total loss 0.533162534\n",
      "Trained batch 458 batch loss 0.496475458 epoch total loss 0.533082426\n",
      "Trained batch 459 batch loss 0.46338594 epoch total loss 0.532930613\n",
      "Trained batch 460 batch loss 0.434587449 epoch total loss 0.532716811\n",
      "Trained batch 461 batch loss 0.545823634 epoch total loss 0.532745242\n",
      "Trained batch 462 batch loss 0.365436912 epoch total loss 0.532383084\n",
      "Trained batch 463 batch loss 0.405055374 epoch total loss 0.532108068\n",
      "Trained batch 464 batch loss 0.405179471 epoch total loss 0.531834543\n",
      "Trained batch 465 batch loss 0.514367104 epoch total loss 0.531797\n",
      "Trained batch 466 batch loss 0.466428369 epoch total loss 0.531656742\n",
      "Trained batch 467 batch loss 0.533157706 epoch total loss 0.531659901\n",
      "Trained batch 468 batch loss 0.52910006 epoch total loss 0.531654477\n",
      "Trained batch 469 batch loss 0.499163747 epoch total loss 0.531585157\n",
      "Trained batch 470 batch loss 0.518909216 epoch total loss 0.531558216\n",
      "Trained batch 471 batch loss 0.5798527 epoch total loss 0.531660736\n",
      "Trained batch 472 batch loss 0.573245347 epoch total loss 0.531748831\n",
      "Trained batch 473 batch loss 0.545014679 epoch total loss 0.531776845\n",
      "Trained batch 474 batch loss 0.605003357 epoch total loss 0.531931341\n",
      "Trained batch 475 batch loss 0.597625852 epoch total loss 0.532069683\n",
      "Trained batch 476 batch loss 0.50462842 epoch total loss 0.532012\n",
      "Trained batch 477 batch loss 0.56373769 epoch total loss 0.532078505\n",
      "Trained batch 478 batch loss 0.605118811 epoch total loss 0.532231331\n",
      "Trained batch 479 batch loss 0.586208642 epoch total loss 0.532344043\n",
      "Trained batch 480 batch loss 0.655142546 epoch total loss 0.532599807\n",
      "Trained batch 481 batch loss 0.425513983 epoch total loss 0.532377183\n",
      "Trained batch 482 batch loss 0.477722824 epoch total loss 0.532263815\n",
      "Trained batch 483 batch loss 0.499960244 epoch total loss 0.532196939\n",
      "Trained batch 484 batch loss 0.520462513 epoch total loss 0.53217274\n",
      "Trained batch 485 batch loss 0.591862 epoch total loss 0.532295763\n",
      "Trained batch 486 batch loss 0.571338832 epoch total loss 0.532376111\n",
      "Trained batch 487 batch loss 0.542618334 epoch total loss 0.532397211\n",
      "Trained batch 488 batch loss 0.512894809 epoch total loss 0.532357275\n",
      "Trained batch 489 batch loss 0.525479615 epoch total loss 0.532343209\n",
      "Trained batch 490 batch loss 0.560506165 epoch total loss 0.532400727\n",
      "Trained batch 491 batch loss 0.491598904 epoch total loss 0.532317638\n",
      "Trained batch 492 batch loss 0.509833336 epoch total loss 0.532271922\n",
      "Trained batch 493 batch loss 0.556075811 epoch total loss 0.532320142\n",
      "Trained batch 494 batch loss 0.615274906 epoch total loss 0.532488048\n",
      "Trained batch 495 batch loss 0.523945153 epoch total loss 0.532470822\n",
      "Trained batch 496 batch loss 0.521760046 epoch total loss 0.532449245\n",
      "Trained batch 497 batch loss 0.489459366 epoch total loss 0.532362759\n",
      "Trained batch 498 batch loss 0.506306589 epoch total loss 0.532310426\n",
      "Trained batch 499 batch loss 0.545379758 epoch total loss 0.532336652\n",
      "Trained batch 500 batch loss 0.551359415 epoch total loss 0.53237468\n",
      "Trained batch 501 batch loss 0.536522448 epoch total loss 0.532382965\n",
      "Trained batch 502 batch loss 0.547698796 epoch total loss 0.532413483\n",
      "Trained batch 503 batch loss 0.587616205 epoch total loss 0.532523215\n",
      "Trained batch 504 batch loss 0.512459159 epoch total loss 0.532483399\n",
      "Trained batch 505 batch loss 0.444932133 epoch total loss 0.532310069\n",
      "Trained batch 506 batch loss 0.544096828 epoch total loss 0.532333374\n",
      "Trained batch 507 batch loss 0.412784338 epoch total loss 0.532097578\n",
      "Trained batch 508 batch loss 0.463354886 epoch total loss 0.531962216\n",
      "Trained batch 509 batch loss 0.463656843 epoch total loss 0.531828046\n",
      "Trained batch 510 batch loss 0.429625064 epoch total loss 0.531627655\n",
      "Trained batch 511 batch loss 0.42721343 epoch total loss 0.53142333\n",
      "Trained batch 512 batch loss 0.451588929 epoch total loss 0.531267405\n",
      "Trained batch 513 batch loss 0.451803088 epoch total loss 0.531112552\n",
      "Trained batch 514 batch loss 0.435667276 epoch total loss 0.530926824\n",
      "Trained batch 515 batch loss 0.518010259 epoch total loss 0.53090173\n",
      "Trained batch 516 batch loss 0.51229465 epoch total loss 0.530865669\n",
      "Trained batch 517 batch loss 0.428597212 epoch total loss 0.530667841\n",
      "Trained batch 518 batch loss 0.39008 epoch total loss 0.530396461\n",
      "Trained batch 519 batch loss 0.474217266 epoch total loss 0.530288219\n",
      "Trained batch 520 batch loss 0.47553888 epoch total loss 0.530182898\n",
      "Trained batch 521 batch loss 0.645535529 epoch total loss 0.53040427\n",
      "Trained batch 522 batch loss 0.508228481 epoch total loss 0.530361831\n",
      "Trained batch 523 batch loss 0.553225398 epoch total loss 0.530405521\n",
      "Trained batch 524 batch loss 0.52621758 epoch total loss 0.530397534\n",
      "Trained batch 525 batch loss 0.559313238 epoch total loss 0.530452669\n",
      "Trained batch 526 batch loss 0.528077304 epoch total loss 0.530448139\n",
      "Trained batch 527 batch loss 0.413420647 epoch total loss 0.530226052\n",
      "Trained batch 528 batch loss 0.426837981 epoch total loss 0.530030251\n",
      "Trained batch 529 batch loss 0.446657658 epoch total loss 0.529872656\n",
      "Trained batch 530 batch loss 0.469470263 epoch total loss 0.529758751\n",
      "Trained batch 531 batch loss 0.425772 epoch total loss 0.529562891\n",
      "Trained batch 532 batch loss 0.496440172 epoch total loss 0.529500604\n",
      "Trained batch 533 batch loss 0.617021382 epoch total loss 0.529664874\n",
      "Trained batch 534 batch loss 0.750283837 epoch total loss 0.530078\n",
      "Trained batch 535 batch loss 0.602652907 epoch total loss 0.530213654\n",
      "Trained batch 536 batch loss 0.566388369 epoch total loss 0.530281126\n",
      "Trained batch 537 batch loss 0.516431749 epoch total loss 0.530255318\n",
      "Trained batch 538 batch loss 0.461410284 epoch total loss 0.530127287\n",
      "Trained batch 539 batch loss 0.43993327 epoch total loss 0.52996\n",
      "Trained batch 540 batch loss 0.515223503 epoch total loss 0.529932737\n",
      "Trained batch 541 batch loss 0.51242727 epoch total loss 0.529900372\n",
      "Trained batch 542 batch loss 0.573664606 epoch total loss 0.529981077\n",
      "Trained batch 543 batch loss 0.469771028 epoch total loss 0.529870212\n",
      "Trained batch 544 batch loss 0.521882653 epoch total loss 0.52985549\n",
      "Trained batch 545 batch loss 0.446596056 epoch total loss 0.529702723\n",
      "Trained batch 546 batch loss 0.432441056 epoch total loss 0.529524565\n",
      "Trained batch 547 batch loss 0.458141595 epoch total loss 0.52939409\n",
      "Trained batch 548 batch loss 0.473191857 epoch total loss 0.529291511\n",
      "Trained batch 549 batch loss 0.563655 epoch total loss 0.529354155\n",
      "Trained batch 550 batch loss 0.543300152 epoch total loss 0.529379487\n",
      "Trained batch 551 batch loss 0.490233481 epoch total loss 0.529308438\n",
      "Trained batch 552 batch loss 0.516548753 epoch total loss 0.529285312\n",
      "Trained batch 553 batch loss 0.562445402 epoch total loss 0.529345274\n",
      "Trained batch 554 batch loss 0.492656112 epoch total loss 0.529279053\n",
      "Trained batch 555 batch loss 0.506401837 epoch total loss 0.529237807\n",
      "Trained batch 556 batch loss 0.565772891 epoch total loss 0.529303491\n",
      "Trained batch 557 batch loss 0.5078426 epoch total loss 0.529265\n",
      "Trained batch 558 batch loss 0.444317341 epoch total loss 0.529112756\n",
      "Trained batch 559 batch loss 0.462904572 epoch total loss 0.528994262\n",
      "Trained batch 560 batch loss 0.481619805 epoch total loss 0.528909683\n",
      "Trained batch 561 batch loss 0.583882093 epoch total loss 0.529007673\n",
      "Trained batch 562 batch loss 0.489871621 epoch total loss 0.528938055\n",
      "Trained batch 563 batch loss 0.507461965 epoch total loss 0.528899908\n",
      "Trained batch 564 batch loss 0.5129776 epoch total loss 0.528871655\n",
      "Trained batch 565 batch loss 0.529638946 epoch total loss 0.528873\n",
      "Trained batch 566 batch loss 0.506675184 epoch total loss 0.528833807\n",
      "Trained batch 567 batch loss 0.398946 epoch total loss 0.528604746\n",
      "Trained batch 568 batch loss 0.499930382 epoch total loss 0.52855432\n",
      "Trained batch 569 batch loss 0.569966555 epoch total loss 0.528627098\n",
      "Trained batch 570 batch loss 0.59399873 epoch total loss 0.528741777\n",
      "Trained batch 571 batch loss 0.548966825 epoch total loss 0.528777242\n",
      "Trained batch 572 batch loss 0.502512 epoch total loss 0.528731287\n",
      "Trained batch 573 batch loss 0.378009617 epoch total loss 0.528468251\n",
      "Trained batch 574 batch loss 0.546158373 epoch total loss 0.528499126\n",
      "Trained batch 575 batch loss 0.528452814 epoch total loss 0.528499\n",
      "Trained batch 576 batch loss 0.510431051 epoch total loss 0.528467655\n",
      "Trained batch 577 batch loss 0.425052881 epoch total loss 0.528288424\n",
      "Trained batch 578 batch loss 0.459128082 epoch total loss 0.528168797\n",
      "Trained batch 579 batch loss 0.509175122 epoch total loss 0.528136\n",
      "Trained batch 580 batch loss 0.501859546 epoch total loss 0.528090715\n",
      "Trained batch 581 batch loss 0.465610921 epoch total loss 0.527983129\n",
      "Trained batch 582 batch loss 0.492948234 epoch total loss 0.527922928\n",
      "Trained batch 583 batch loss 0.48221314 epoch total loss 0.527844548\n",
      "Trained batch 584 batch loss 0.560032785 epoch total loss 0.527899623\n",
      "Trained batch 585 batch loss 0.494034469 epoch total loss 0.527841806\n",
      "Trained batch 586 batch loss 0.560678482 epoch total loss 0.527897835\n",
      "Trained batch 587 batch loss 0.485167921 epoch total loss 0.527825\n",
      "Trained batch 588 batch loss 0.439377785 epoch total loss 0.527674615\n",
      "Trained batch 589 batch loss 0.533947527 epoch total loss 0.527685225\n",
      "Trained batch 590 batch loss 0.481197238 epoch total loss 0.527606487\n",
      "Trained batch 591 batch loss 0.562787771 epoch total loss 0.527666\n",
      "Trained batch 592 batch loss 0.493639052 epoch total loss 0.527608514\n",
      "Trained batch 593 batch loss 0.478626341 epoch total loss 0.527525902\n",
      "Trained batch 594 batch loss 0.453605831 epoch total loss 0.527401507\n",
      "Trained batch 595 batch loss 0.476070255 epoch total loss 0.527315259\n",
      "Trained batch 596 batch loss 0.614196777 epoch total loss 0.527461\n",
      "Trained batch 597 batch loss 0.592344522 epoch total loss 0.527569711\n",
      "Trained batch 598 batch loss 0.534966767 epoch total loss 0.527582049\n",
      "Trained batch 599 batch loss 0.508668125 epoch total loss 0.527550519\n",
      "Trained batch 600 batch loss 0.558033764 epoch total loss 0.527601302\n",
      "Trained batch 601 batch loss 0.517588 epoch total loss 0.527584612\n",
      "Trained batch 602 batch loss 0.557242453 epoch total loss 0.527633905\n",
      "Trained batch 603 batch loss 0.451946765 epoch total loss 0.527508378\n",
      "Trained batch 604 batch loss 0.480567396 epoch total loss 0.527430654\n",
      "Trained batch 605 batch loss 0.484220207 epoch total loss 0.527359247\n",
      "Trained batch 606 batch loss 0.508298874 epoch total loss 0.527327776\n",
      "Trained batch 607 batch loss 0.529533625 epoch total loss 0.527331412\n",
      "Trained batch 608 batch loss 0.551836729 epoch total loss 0.527371764\n",
      "Trained batch 609 batch loss 0.558056772 epoch total loss 0.52742213\n",
      "Trained batch 610 batch loss 0.562669098 epoch total loss 0.527479947\n",
      "Trained batch 611 batch loss 0.525144339 epoch total loss 0.527476132\n",
      "Trained batch 612 batch loss 0.610592484 epoch total loss 0.527611911\n",
      "Trained batch 613 batch loss 0.532886863 epoch total loss 0.527620554\n",
      "Trained batch 614 batch loss 0.524740577 epoch total loss 0.527615845\n",
      "Trained batch 615 batch loss 0.586293876 epoch total loss 0.527711272\n",
      "Trained batch 616 batch loss 0.527978539 epoch total loss 0.527711749\n",
      "Trained batch 617 batch loss 0.574689865 epoch total loss 0.527787864\n",
      "Trained batch 618 batch loss 0.585737288 epoch total loss 0.527881622\n",
      "Trained batch 619 batch loss 0.561768234 epoch total loss 0.527936339\n",
      "Trained batch 620 batch loss 0.502266705 epoch total loss 0.527894914\n",
      "Trained batch 621 batch loss 0.449574292 epoch total loss 0.52776885\n",
      "Trained batch 622 batch loss 0.487674862 epoch total loss 0.527704358\n",
      "Trained batch 623 batch loss 0.48393473 epoch total loss 0.527634144\n",
      "Trained batch 624 batch loss 0.46467644 epoch total loss 0.527533233\n",
      "Trained batch 625 batch loss 0.472206861 epoch total loss 0.52744472\n",
      "Trained batch 626 batch loss 0.392617613 epoch total loss 0.527229309\n",
      "Trained batch 627 batch loss 0.434707105 epoch total loss 0.527081728\n",
      "Trained batch 628 batch loss 0.387456775 epoch total loss 0.526859403\n",
      "Trained batch 629 batch loss 0.455580413 epoch total loss 0.526746035\n",
      "Trained batch 630 batch loss 0.491214693 epoch total loss 0.526689649\n",
      "Trained batch 631 batch loss 0.476223737 epoch total loss 0.526609659\n",
      "Trained batch 632 batch loss 0.466168493 epoch total loss 0.526514053\n",
      "Trained batch 633 batch loss 0.41745016 epoch total loss 0.526341736\n",
      "Trained batch 634 batch loss 0.380364627 epoch total loss 0.526111484\n",
      "Trained batch 635 batch loss 0.332 epoch total loss 0.525805831\n",
      "Trained batch 636 batch loss 0.336836159 epoch total loss 0.525508642\n",
      "Trained batch 637 batch loss 0.406076461 epoch total loss 0.525321186\n",
      "Trained batch 638 batch loss 0.407173574 epoch total loss 0.525135934\n",
      "Trained batch 639 batch loss 0.351161063 epoch total loss 0.52486372\n",
      "Trained batch 640 batch loss 0.436248541 epoch total loss 0.524725258\n",
      "Trained batch 641 batch loss 0.406277925 epoch total loss 0.524540484\n",
      "Trained batch 642 batch loss 0.497102112 epoch total loss 0.524497747\n",
      "Trained batch 643 batch loss 0.503420949 epoch total loss 0.524464965\n",
      "Trained batch 644 batch loss 0.496456474 epoch total loss 0.524421453\n",
      "Trained batch 645 batch loss 0.527527153 epoch total loss 0.524426281\n",
      "Trained batch 646 batch loss 0.504437864 epoch total loss 0.524395287\n",
      "Trained batch 647 batch loss 0.526485384 epoch total loss 0.524398565\n",
      "Trained batch 648 batch loss 0.485715747 epoch total loss 0.524338841\n",
      "Trained batch 649 batch loss 0.471636772 epoch total loss 0.52425766\n",
      "Trained batch 650 batch loss 0.514858663 epoch total loss 0.524243236\n",
      "Trained batch 651 batch loss 0.558231235 epoch total loss 0.52429539\n",
      "Trained batch 652 batch loss 0.572892368 epoch total loss 0.524369955\n",
      "Trained batch 653 batch loss 0.516003549 epoch total loss 0.52435714\n",
      "Trained batch 654 batch loss 0.520109475 epoch total loss 0.524350643\n",
      "Trained batch 655 batch loss 0.481398433 epoch total loss 0.524285078\n",
      "Trained batch 656 batch loss 0.530281901 epoch total loss 0.524294198\n",
      "Trained batch 657 batch loss 0.640823245 epoch total loss 0.524471521\n",
      "Trained batch 658 batch loss 0.56078577 epoch total loss 0.524526715\n",
      "Trained batch 659 batch loss 0.582038164 epoch total loss 0.524614\n",
      "Trained batch 660 batch loss 0.567634583 epoch total loss 0.524679124\n",
      "Trained batch 661 batch loss 0.494332969 epoch total loss 0.524633229\n",
      "Trained batch 662 batch loss 0.499637574 epoch total loss 0.524595439\n",
      "Trained batch 663 batch loss 0.509497166 epoch total loss 0.52457267\n",
      "Trained batch 664 batch loss 0.49675113 epoch total loss 0.524530828\n",
      "Trained batch 665 batch loss 0.483014226 epoch total loss 0.524468362\n",
      "Trained batch 666 batch loss 0.494858861 epoch total loss 0.524423897\n",
      "Trained batch 667 batch loss 0.424096167 epoch total loss 0.524273515\n",
      "Trained batch 668 batch loss 0.485195696 epoch total loss 0.524215043\n",
      "Trained batch 669 batch loss 0.507782638 epoch total loss 0.524190426\n",
      "Trained batch 670 batch loss 0.494524151 epoch total loss 0.524146199\n",
      "Trained batch 671 batch loss 0.536891 epoch total loss 0.524165213\n",
      "Trained batch 672 batch loss 0.553230286 epoch total loss 0.524208426\n",
      "Trained batch 673 batch loss 0.514972329 epoch total loss 0.524194717\n",
      "Trained batch 674 batch loss 0.494849294 epoch total loss 0.524151206\n",
      "Trained batch 675 batch loss 0.425918907 epoch total loss 0.524005651\n",
      "Trained batch 676 batch loss 0.449157417 epoch total loss 0.523894966\n",
      "Trained batch 677 batch loss 0.488531411 epoch total loss 0.523842692\n",
      "Trained batch 678 batch loss 0.449545383 epoch total loss 0.523733139\n",
      "Trained batch 679 batch loss 0.477127671 epoch total loss 0.523664534\n",
      "Trained batch 680 batch loss 0.516600728 epoch total loss 0.523654103\n",
      "Trained batch 681 batch loss 0.612619221 epoch total loss 0.523784757\n",
      "Trained batch 682 batch loss 0.633627534 epoch total loss 0.523945808\n",
      "Trained batch 683 batch loss 0.55801934 epoch total loss 0.523995697\n",
      "Trained batch 684 batch loss 0.508392394 epoch total loss 0.523972869\n",
      "Trained batch 685 batch loss 0.534701288 epoch total loss 0.523988545\n",
      "Trained batch 686 batch loss 0.534800291 epoch total loss 0.524004281\n",
      "Trained batch 687 batch loss 0.463700265 epoch total loss 0.523916543\n",
      "Trained batch 688 batch loss 0.390565783 epoch total loss 0.523722708\n",
      "Trained batch 689 batch loss 0.317867219 epoch total loss 0.52342397\n",
      "Trained batch 690 batch loss 0.258448184 epoch total loss 0.523039937\n",
      "Trained batch 691 batch loss 0.284596622 epoch total loss 0.522694886\n",
      "Trained batch 692 batch loss 0.345695019 epoch total loss 0.522439122\n",
      "Trained batch 693 batch loss 0.365438968 epoch total loss 0.522212565\n",
      "Trained batch 694 batch loss 0.407119542 epoch total loss 0.522046685\n",
      "Trained batch 695 batch loss 0.517056465 epoch total loss 0.522039533\n",
      "Trained batch 696 batch loss 0.5511868 epoch total loss 0.522081375\n",
      "Trained batch 697 batch loss 0.537321 epoch total loss 0.52210325\n",
      "Trained batch 698 batch loss 0.50036025 epoch total loss 0.522072136\n",
      "Trained batch 699 batch loss 0.517730355 epoch total loss 0.522065938\n",
      "Trained batch 700 batch loss 0.470999956 epoch total loss 0.521993\n",
      "Trained batch 701 batch loss 0.47034964 epoch total loss 0.52191931\n",
      "Trained batch 702 batch loss 0.50992775 epoch total loss 0.521902204\n",
      "Trained batch 703 batch loss 0.53061837 epoch total loss 0.521914601\n",
      "Trained batch 704 batch loss 0.542121947 epoch total loss 0.521943271\n",
      "Trained batch 705 batch loss 0.495812833 epoch total loss 0.521906197\n",
      "Trained batch 706 batch loss 0.552713275 epoch total loss 0.521949828\n",
      "Trained batch 707 batch loss 0.692980349 epoch total loss 0.522191763\n",
      "Trained batch 708 batch loss 0.61738342 epoch total loss 0.522326171\n",
      "Trained batch 709 batch loss 0.536657393 epoch total loss 0.522346377\n",
      "Trained batch 710 batch loss 0.574654341 epoch total loss 0.522420049\n",
      "Trained batch 711 batch loss 0.551084459 epoch total loss 0.522460401\n",
      "Trained batch 712 batch loss 0.506969273 epoch total loss 0.522438586\n",
      "Trained batch 713 batch loss 0.533260286 epoch total loss 0.522453785\n",
      "Trained batch 714 batch loss 0.556389034 epoch total loss 0.522501349\n",
      "Trained batch 715 batch loss 0.569947839 epoch total loss 0.522567689\n",
      "Trained batch 716 batch loss 0.597742856 epoch total loss 0.522672713\n",
      "Trained batch 717 batch loss 0.532360554 epoch total loss 0.522686183\n",
      "Trained batch 718 batch loss 0.523626447 epoch total loss 0.522687495\n",
      "Trained batch 719 batch loss 0.516010702 epoch total loss 0.522678196\n",
      "Trained batch 720 batch loss 0.491907716 epoch total loss 0.52263546\n",
      "Trained batch 721 batch loss 0.603671074 epoch total loss 0.522747874\n",
      "Trained batch 722 batch loss 0.504871607 epoch total loss 0.522723138\n",
      "Trained batch 723 batch loss 0.465416104 epoch total loss 0.522643864\n",
      "Trained batch 724 batch loss 0.477575272 epoch total loss 0.522581637\n",
      "Trained batch 725 batch loss 0.475885361 epoch total loss 0.522517204\n",
      "Trained batch 726 batch loss 0.475928545 epoch total loss 0.522453\n",
      "Trained batch 727 batch loss 0.531614125 epoch total loss 0.522465646\n",
      "Trained batch 728 batch loss 0.563763738 epoch total loss 0.52252233\n",
      "Trained batch 729 batch loss 0.487289548 epoch total loss 0.522474051\n",
      "Trained batch 730 batch loss 0.502932429 epoch total loss 0.522447288\n",
      "Trained batch 731 batch loss 0.510359 epoch total loss 0.522430718\n",
      "Trained batch 732 batch loss 0.478261143 epoch total loss 0.522370398\n",
      "Trained batch 733 batch loss 0.486423075 epoch total loss 0.522321343\n",
      "Trained batch 734 batch loss 0.449266076 epoch total loss 0.522221804\n",
      "Trained batch 735 batch loss 0.461305439 epoch total loss 0.522138953\n",
      "Trained batch 736 batch loss 0.48569122 epoch total loss 0.522089422\n",
      "Trained batch 737 batch loss 0.437882304 epoch total loss 0.52197516\n",
      "Trained batch 738 batch loss 0.519482315 epoch total loss 0.521971762\n",
      "Trained batch 739 batch loss 0.442825437 epoch total loss 0.521864712\n",
      "Trained batch 740 batch loss 0.482426077 epoch total loss 0.521811426\n",
      "Trained batch 741 batch loss 0.492269844 epoch total loss 0.52177155\n",
      "Trained batch 742 batch loss 0.513742805 epoch total loss 0.521760702\n",
      "Trained batch 743 batch loss 0.491547853 epoch total loss 0.521720052\n",
      "Trained batch 744 batch loss 0.603117347 epoch total loss 0.521829486\n",
      "Trained batch 745 batch loss 0.544175744 epoch total loss 0.521859467\n",
      "Trained batch 746 batch loss 0.460655153 epoch total loss 0.521777451\n",
      "Trained batch 747 batch loss 0.556245208 epoch total loss 0.521823585\n",
      "Trained batch 748 batch loss 0.559583366 epoch total loss 0.52187407\n",
      "Trained batch 749 batch loss 0.564305544 epoch total loss 0.521930695\n",
      "Trained batch 750 batch loss 0.455931723 epoch total loss 0.521842718\n",
      "Trained batch 751 batch loss 0.482455462 epoch total loss 0.521790266\n",
      "Trained batch 752 batch loss 0.478807777 epoch total loss 0.521733105\n",
      "Trained batch 753 batch loss 0.506325901 epoch total loss 0.521712601\n",
      "Trained batch 754 batch loss 0.509087086 epoch total loss 0.521695912\n",
      "Trained batch 755 batch loss 0.561152875 epoch total loss 0.521748185\n",
      "Trained batch 756 batch loss 0.529287934 epoch total loss 0.521758139\n",
      "Trained batch 757 batch loss 0.494977355 epoch total loss 0.521722734\n",
      "Trained batch 758 batch loss 0.478922904 epoch total loss 0.521666288\n",
      "Trained batch 759 batch loss 0.472688019 epoch total loss 0.521601737\n",
      "Trained batch 760 batch loss 0.450095892 epoch total loss 0.52150768\n",
      "Trained batch 761 batch loss 0.449649543 epoch total loss 0.521413207\n",
      "Trained batch 762 batch loss 0.411601901 epoch total loss 0.521269083\n",
      "Trained batch 763 batch loss 0.500611901 epoch total loss 0.521242\n",
      "Trained batch 764 batch loss 0.477058679 epoch total loss 0.521184206\n",
      "Trained batch 765 batch loss 0.510388613 epoch total loss 0.52117008\n",
      "Trained batch 766 batch loss 0.59419328 epoch total loss 0.521265388\n",
      "Trained batch 767 batch loss 0.54511553 epoch total loss 0.521296501\n",
      "Trained batch 768 batch loss 0.593762159 epoch total loss 0.521390855\n",
      "Trained batch 769 batch loss 0.494286239 epoch total loss 0.521355569\n",
      "Trained batch 770 batch loss 0.44020319 epoch total loss 0.521250248\n",
      "Trained batch 771 batch loss 0.461849689 epoch total loss 0.521173179\n",
      "Trained batch 772 batch loss 0.471402586 epoch total loss 0.521108687\n",
      "Trained batch 773 batch loss 0.402426124 epoch total loss 0.520955205\n",
      "Trained batch 774 batch loss 0.327768147 epoch total loss 0.520705581\n",
      "Trained batch 775 batch loss 0.471061707 epoch total loss 0.520641565\n",
      "Trained batch 776 batch loss 0.375054419 epoch total loss 0.52045393\n",
      "Trained batch 777 batch loss 0.364867389 epoch total loss 0.520253718\n",
      "Trained batch 778 batch loss 0.378551126 epoch total loss 0.520071566\n",
      "Trained batch 779 batch loss 0.392602026 epoch total loss 0.519907892\n",
      "Trained batch 780 batch loss 0.51289165 epoch total loss 0.519898891\n",
      "Trained batch 781 batch loss 0.46617806 epoch total loss 0.519830108\n",
      "Trained batch 782 batch loss 0.472405314 epoch total loss 0.51976949\n",
      "Trained batch 783 batch loss 0.536623836 epoch total loss 0.519791\n",
      "Trained batch 784 batch loss 0.544197 epoch total loss 0.519822121\n",
      "Trained batch 785 batch loss 0.590146959 epoch total loss 0.519911706\n",
      "Trained batch 786 batch loss 0.680905402 epoch total loss 0.520116568\n",
      "Trained batch 787 batch loss 0.498840928 epoch total loss 0.520089507\n",
      "Trained batch 788 batch loss 0.521228552 epoch total loss 0.520091\n",
      "Trained batch 789 batch loss 0.478322297 epoch total loss 0.520038068\n",
      "Trained batch 790 batch loss 0.590233564 epoch total loss 0.520126939\n",
      "Trained batch 791 batch loss 0.642892957 epoch total loss 0.52028209\n",
      "Trained batch 792 batch loss 0.53262955 epoch total loss 0.520297706\n",
      "Trained batch 793 batch loss 0.504850507 epoch total loss 0.520278215\n",
      "Trained batch 794 batch loss 0.491809309 epoch total loss 0.520242393\n",
      "Trained batch 795 batch loss 0.585122406 epoch total loss 0.520324\n",
      "Trained batch 796 batch loss 0.603422463 epoch total loss 0.52042836\n",
      "Trained batch 797 batch loss 0.63830173 epoch total loss 0.520576298\n",
      "Trained batch 798 batch loss 0.589874804 epoch total loss 0.520663142\n",
      "Trained batch 799 batch loss 0.587526798 epoch total loss 0.520746768\n",
      "Trained batch 800 batch loss 0.590707183 epoch total loss 0.520834208\n",
      "Trained batch 801 batch loss 0.618103325 epoch total loss 0.520955682\n",
      "Trained batch 802 batch loss 0.580663323 epoch total loss 0.521030128\n",
      "Trained batch 803 batch loss 0.465177506 epoch total loss 0.520960569\n",
      "Trained batch 804 batch loss 0.505759776 epoch total loss 0.520941675\n",
      "Trained batch 805 batch loss 0.655851781 epoch total loss 0.521109283\n",
      "Trained batch 806 batch loss 0.575969517 epoch total loss 0.521177292\n",
      "Trained batch 807 batch loss 0.527413249 epoch total loss 0.52118504\n",
      "Trained batch 808 batch loss 0.464608163 epoch total loss 0.521115\n",
      "Trained batch 809 batch loss 0.595695138 epoch total loss 0.521207213\n",
      "Trained batch 810 batch loss 0.61450994 epoch total loss 0.52132237\n",
      "Trained batch 811 batch loss 0.541715145 epoch total loss 0.521347523\n",
      "Trained batch 812 batch loss 0.526640773 epoch total loss 0.521354\n",
      "Trained batch 813 batch loss 0.526336 epoch total loss 0.521360159\n",
      "Trained batch 814 batch loss 0.529391348 epoch total loss 0.521370053\n",
      "Trained batch 815 batch loss 0.503702641 epoch total loss 0.521348357\n",
      "Trained batch 816 batch loss 0.598090887 epoch total loss 0.521442354\n",
      "Trained batch 817 batch loss 0.568298519 epoch total loss 0.521499753\n",
      "Trained batch 818 batch loss 0.499185979 epoch total loss 0.521472454\n",
      "Trained batch 819 batch loss 0.615561306 epoch total loss 0.521587312\n",
      "Trained batch 820 batch loss 0.603986442 epoch total loss 0.521687806\n",
      "Trained batch 821 batch loss 0.54106617 epoch total loss 0.521711409\n",
      "Trained batch 822 batch loss 0.507799625 epoch total loss 0.521694481\n",
      "Trained batch 823 batch loss 0.533657432 epoch total loss 0.521709\n",
      "Trained batch 824 batch loss 0.439515918 epoch total loss 0.521609306\n",
      "Trained batch 825 batch loss 0.523535788 epoch total loss 0.521611631\n",
      "Trained batch 826 batch loss 0.489723235 epoch total loss 0.521573\n",
      "Trained batch 827 batch loss 0.529922664 epoch total loss 0.52158314\n",
      "Trained batch 828 batch loss 0.558377683 epoch total loss 0.521627545\n",
      "Trained batch 829 batch loss 0.56662029 epoch total loss 0.521681845\n",
      "Trained batch 830 batch loss 0.586357236 epoch total loss 0.521759748\n",
      "Trained batch 831 batch loss 0.641711175 epoch total loss 0.521904111\n",
      "Trained batch 832 batch loss 0.530616343 epoch total loss 0.521914601\n",
      "Trained batch 833 batch loss 0.521289229 epoch total loss 0.521913826\n",
      "Trained batch 834 batch loss 0.636657357 epoch total loss 0.522051454\n",
      "Trained batch 835 batch loss 0.529743791 epoch total loss 0.522060692\n",
      "Trained batch 836 batch loss 0.580908716 epoch total loss 0.522131\n",
      "Trained batch 837 batch loss 0.581618965 epoch total loss 0.522202075\n",
      "Trained batch 838 batch loss 0.558512866 epoch total loss 0.522245407\n",
      "Trained batch 839 batch loss 0.547303498 epoch total loss 0.522275269\n",
      "Trained batch 840 batch loss 0.57135427 epoch total loss 0.522333682\n",
      "Trained batch 841 batch loss 0.602141798 epoch total loss 0.522428632\n",
      "Trained batch 842 batch loss 0.545719922 epoch total loss 0.522456288\n",
      "Trained batch 843 batch loss 0.585001051 epoch total loss 0.522530437\n",
      "Trained batch 844 batch loss 0.536228776 epoch total loss 0.522546649\n",
      "Trained batch 845 batch loss 0.548396051 epoch total loss 0.522577286\n",
      "Trained batch 846 batch loss 0.580825 epoch total loss 0.52264607\n",
      "Trained batch 847 batch loss 0.589137316 epoch total loss 0.522724628\n",
      "Trained batch 848 batch loss 0.555111945 epoch total loss 0.522762775\n",
      "Trained batch 849 batch loss 0.654443 epoch total loss 0.522917926\n",
      "Trained batch 850 batch loss 0.558316946 epoch total loss 0.52295953\n",
      "Trained batch 851 batch loss 0.596019566 epoch total loss 0.523045421\n",
      "Trained batch 852 batch loss 0.523697853 epoch total loss 0.523046196\n",
      "Trained batch 853 batch loss 0.563370824 epoch total loss 0.523093462\n",
      "Trained batch 854 batch loss 0.581135213 epoch total loss 0.523161471\n",
      "Trained batch 855 batch loss 0.491092086 epoch total loss 0.52312392\n",
      "Trained batch 856 batch loss 0.438095093 epoch total loss 0.523024559\n",
      "Trained batch 857 batch loss 0.359447807 epoch total loss 0.522833705\n",
      "Trained batch 858 batch loss 0.40787828 epoch total loss 0.522699714\n",
      "Trained batch 859 batch loss 0.418572843 epoch total loss 0.522578478\n",
      "Trained batch 860 batch loss 0.537368536 epoch total loss 0.522595704\n",
      "Trained batch 861 batch loss 0.545551121 epoch total loss 0.522622347\n",
      "Trained batch 862 batch loss 0.538376927 epoch total loss 0.522640646\n",
      "Trained batch 863 batch loss 0.520646572 epoch total loss 0.522638321\n",
      "Trained batch 864 batch loss 0.615580559 epoch total loss 0.522745907\n",
      "Trained batch 865 batch loss 0.585303 epoch total loss 0.522818208\n",
      "Trained batch 866 batch loss 0.544741929 epoch total loss 0.52284354\n",
      "Trained batch 867 batch loss 0.591453433 epoch total loss 0.522922695\n",
      "Trained batch 868 batch loss 0.502628624 epoch total loss 0.52289927\n",
      "Trained batch 869 batch loss 0.574565768 epoch total loss 0.522958755\n",
      "Trained batch 870 batch loss 0.504715085 epoch total loss 0.522937775\n",
      "Trained batch 871 batch loss 0.578741491 epoch total loss 0.52300185\n",
      "Trained batch 872 batch loss 0.566156089 epoch total loss 0.523051322\n",
      "Trained batch 873 batch loss 0.590882182 epoch total loss 0.523129046\n",
      "Trained batch 874 batch loss 0.577522933 epoch total loss 0.523191273\n",
      "Trained batch 875 batch loss 0.545610189 epoch total loss 0.523216903\n",
      "Trained batch 876 batch loss 0.472985446 epoch total loss 0.523159564\n",
      "Trained batch 877 batch loss 0.567136765 epoch total loss 0.523209691\n",
      "Trained batch 878 batch loss 0.548452079 epoch total loss 0.52323848\n",
      "Trained batch 879 batch loss 0.531291425 epoch total loss 0.5232476\n",
      "Trained batch 880 batch loss 0.483977586 epoch total loss 0.523203\n",
      "Trained batch 881 batch loss 0.522845209 epoch total loss 0.523202598\n",
      "Trained batch 882 batch loss 0.447415471 epoch total loss 0.523116708\n",
      "Trained batch 883 batch loss 0.510593116 epoch total loss 0.523102522\n",
      "Trained batch 884 batch loss 0.482423156 epoch total loss 0.523056507\n",
      "Trained batch 885 batch loss 0.593349099 epoch total loss 0.5231359\n",
      "Trained batch 886 batch loss 0.669428706 epoch total loss 0.523301\n",
      "Trained batch 887 batch loss 0.585827827 epoch total loss 0.523371518\n",
      "Trained batch 888 batch loss 0.613556 epoch total loss 0.523473084\n",
      "Trained batch 889 batch loss 0.54303956 epoch total loss 0.523495078\n",
      "Trained batch 890 batch loss 0.563793421 epoch total loss 0.523540318\n",
      "Trained batch 891 batch loss 0.555945456 epoch total loss 0.523576677\n",
      "Trained batch 892 batch loss 0.648435414 epoch total loss 0.523716688\n",
      "Trained batch 893 batch loss 0.643597722 epoch total loss 0.523850918\n",
      "Trained batch 894 batch loss 0.486516237 epoch total loss 0.523809135\n",
      "Trained batch 895 batch loss 0.532149851 epoch total loss 0.523818433\n",
      "Trained batch 896 batch loss 0.529478312 epoch total loss 0.523824751\n",
      "Trained batch 897 batch loss 0.55741334 epoch total loss 0.523862183\n",
      "Trained batch 898 batch loss 0.616591513 epoch total loss 0.523965418\n",
      "Trained batch 899 batch loss 0.555199683 epoch total loss 0.524000168\n",
      "Trained batch 900 batch loss 0.496008813 epoch total loss 0.523969054\n",
      "Trained batch 901 batch loss 0.573268414 epoch total loss 0.524023831\n",
      "Trained batch 902 batch loss 0.530518949 epoch total loss 0.524031\n",
      "Trained batch 903 batch loss 0.537271261 epoch total loss 0.524045646\n",
      "Trained batch 904 batch loss 0.474121213 epoch total loss 0.523990452\n",
      "Trained batch 905 batch loss 0.500057817 epoch total loss 0.523964\n",
      "Trained batch 906 batch loss 0.46197921 epoch total loss 0.523895562\n",
      "Trained batch 907 batch loss 0.433525741 epoch total loss 0.523795962\n",
      "Trained batch 908 batch loss 0.434212714 epoch total loss 0.523697257\n",
      "Trained batch 909 batch loss 0.459230214 epoch total loss 0.523626328\n",
      "Trained batch 910 batch loss 0.511039793 epoch total loss 0.523612499\n",
      "Trained batch 911 batch loss 0.460783809 epoch total loss 0.523543537\n",
      "Trained batch 912 batch loss 0.566280127 epoch total loss 0.523590446\n",
      "Trained batch 913 batch loss 0.497955799 epoch total loss 0.523562372\n",
      "Trained batch 914 batch loss 0.547818422 epoch total loss 0.523588896\n",
      "Trained batch 915 batch loss 0.520058811 epoch total loss 0.523585\n",
      "Trained batch 916 batch loss 0.601368904 epoch total loss 0.523669958\n",
      "Trained batch 917 batch loss 0.634631395 epoch total loss 0.523790956\n",
      "Trained batch 918 batch loss 0.494690239 epoch total loss 0.523759246\n",
      "Trained batch 919 batch loss 0.53414917 epoch total loss 0.523770571\n",
      "Trained batch 920 batch loss 0.508603573 epoch total loss 0.52375406\n",
      "Trained batch 921 batch loss 0.557280719 epoch total loss 0.523790479\n",
      "Trained batch 922 batch loss 0.653059304 epoch total loss 0.523930669\n",
      "Trained batch 923 batch loss 0.537083745 epoch total loss 0.523944914\n",
      "Trained batch 924 batch loss 0.454840243 epoch total loss 0.523870111\n",
      "Trained batch 925 batch loss 0.471339285 epoch total loss 0.523813367\n",
      "Trained batch 926 batch loss 0.445052236 epoch total loss 0.523728251\n",
      "Trained batch 927 batch loss 0.444589078 epoch total loss 0.523642898\n",
      "Trained batch 928 batch loss 0.479926348 epoch total loss 0.52359575\n",
      "Trained batch 929 batch loss 0.455901563 epoch total loss 0.523522913\n",
      "Trained batch 930 batch loss 0.639967442 epoch total loss 0.523648083\n",
      "Trained batch 931 batch loss 0.591638088 epoch total loss 0.523721159\n",
      "Trained batch 932 batch loss 0.607418656 epoch total loss 0.523810923\n",
      "Trained batch 933 batch loss 0.614515245 epoch total loss 0.523908138\n",
      "Trained batch 934 batch loss 0.464594573 epoch total loss 0.523844659\n",
      "Trained batch 935 batch loss 0.51705265 epoch total loss 0.523837388\n",
      "Trained batch 936 batch loss 0.518247128 epoch total loss 0.523831427\n",
      "Trained batch 937 batch loss 0.450447589 epoch total loss 0.523753107\n",
      "Trained batch 938 batch loss 0.535084307 epoch total loss 0.523765206\n",
      "Trained batch 939 batch loss 0.597784698 epoch total loss 0.523844\n",
      "Trained batch 940 batch loss 0.553843558 epoch total loss 0.523875892\n",
      "Trained batch 941 batch loss 0.512563646 epoch total loss 0.523863912\n",
      "Trained batch 942 batch loss 0.543373287 epoch total loss 0.523884594\n",
      "Trained batch 943 batch loss 0.612446427 epoch total loss 0.523978531\n",
      "Trained batch 944 batch loss 0.510464251 epoch total loss 0.523964226\n",
      "Trained batch 945 batch loss 0.582231164 epoch total loss 0.524025917\n",
      "Trained batch 946 batch loss 0.563403368 epoch total loss 0.524067521\n",
      "Trained batch 947 batch loss 0.626643181 epoch total loss 0.524175823\n",
      "Trained batch 948 batch loss 0.567157269 epoch total loss 0.524221182\n",
      "Trained batch 949 batch loss 0.499702752 epoch total loss 0.524195373\n",
      "Trained batch 950 batch loss 0.510219097 epoch total loss 0.524180651\n",
      "Trained batch 951 batch loss 0.457873732 epoch total loss 0.524110913\n",
      "Trained batch 952 batch loss 0.437268347 epoch total loss 0.524019718\n",
      "Trained batch 953 batch loss 0.475494087 epoch total loss 0.523968816\n",
      "Trained batch 954 batch loss 0.445553273 epoch total loss 0.523886621\n",
      "Trained batch 955 batch loss 0.454370588 epoch total loss 0.523813784\n",
      "Trained batch 956 batch loss 0.527215719 epoch total loss 0.52381736\n",
      "Trained batch 957 batch loss 0.527072787 epoch total loss 0.523820758\n",
      "Trained batch 958 batch loss 0.510972202 epoch total loss 0.523807347\n",
      "Trained batch 959 batch loss 0.479967713 epoch total loss 0.52376169\n",
      "Trained batch 960 batch loss 0.598572552 epoch total loss 0.523839593\n",
      "Trained batch 961 batch loss 0.551577866 epoch total loss 0.523868442\n",
      "Trained batch 962 batch loss 0.552989304 epoch total loss 0.523898721\n",
      "Trained batch 963 batch loss 0.541982591 epoch total loss 0.523917496\n",
      "Trained batch 964 batch loss 0.52850157 epoch total loss 0.523922265\n",
      "Trained batch 965 batch loss 0.494769603 epoch total loss 0.523892045\n",
      "Trained batch 966 batch loss 0.56014663 epoch total loss 0.523929596\n",
      "Trained batch 967 batch loss 0.524448216 epoch total loss 0.523930132\n",
      "Trained batch 968 batch loss 0.474019945 epoch total loss 0.523878574\n",
      "Trained batch 969 batch loss 0.47658658 epoch total loss 0.523829758\n",
      "Trained batch 970 batch loss 0.605218768 epoch total loss 0.523913682\n",
      "Trained batch 971 batch loss 0.578656793 epoch total loss 0.523970068\n",
      "Trained batch 972 batch loss 0.615184426 epoch total loss 0.524063885\n",
      "Trained batch 973 batch loss 0.51482594 epoch total loss 0.524054408\n",
      "Trained batch 974 batch loss 0.562129259 epoch total loss 0.524093509\n",
      "Trained batch 975 batch loss 0.554522157 epoch total loss 0.524124742\n",
      "Trained batch 976 batch loss 0.534565449 epoch total loss 0.524135411\n",
      "Trained batch 977 batch loss 0.52669251 epoch total loss 0.524138033\n",
      "Trained batch 978 batch loss 0.51202476 epoch total loss 0.524125695\n",
      "Trained batch 979 batch loss 0.479114294 epoch total loss 0.52407968\n",
      "Trained batch 980 batch loss 0.501432061 epoch total loss 0.524056554\n",
      "Trained batch 981 batch loss 0.428677738 epoch total loss 0.523959339\n",
      "Trained batch 982 batch loss 0.57637912 epoch total loss 0.524012685\n",
      "Trained batch 983 batch loss 0.450624645 epoch total loss 0.523938\n",
      "Trained batch 984 batch loss 0.600385666 epoch total loss 0.524015725\n",
      "Trained batch 985 batch loss 0.612472832 epoch total loss 0.524105549\n",
      "Trained batch 986 batch loss 0.556964219 epoch total loss 0.524138868\n",
      "Trained batch 987 batch loss 0.590317369 epoch total loss 0.524205923\n",
      "Trained batch 988 batch loss 0.609570563 epoch total loss 0.52429229\n",
      "Trained batch 989 batch loss 0.575024664 epoch total loss 0.52434361\n",
      "Trained batch 990 batch loss 0.607235074 epoch total loss 0.524427295\n",
      "Trained batch 991 batch loss 0.599601269 epoch total loss 0.524503171\n",
      "Trained batch 992 batch loss 0.641293466 epoch total loss 0.524620891\n",
      "Trained batch 993 batch loss 0.577140331 epoch total loss 0.52467382\n",
      "Trained batch 994 batch loss 0.604382396 epoch total loss 0.524754\n",
      "Trained batch 995 batch loss 0.590581477 epoch total loss 0.524820149\n",
      "Trained batch 996 batch loss 0.469765961 epoch total loss 0.524764895\n",
      "Trained batch 997 batch loss 0.471792728 epoch total loss 0.524711788\n",
      "Trained batch 998 batch loss 0.478343308 epoch total loss 0.524665296\n",
      "Trained batch 999 batch loss 0.578204691 epoch total loss 0.524718881\n",
      "Trained batch 1000 batch loss 0.491611391 epoch total loss 0.5246858\n",
      "Trained batch 1001 batch loss 0.485919118 epoch total loss 0.524647057\n",
      "Trained batch 1002 batch loss 0.510234475 epoch total loss 0.524632692\n",
      "Trained batch 1003 batch loss 0.526398957 epoch total loss 0.52463448\n",
      "Trained batch 1004 batch loss 0.599300742 epoch total loss 0.524708867\n",
      "Trained batch 1005 batch loss 0.530848503 epoch total loss 0.524714947\n",
      "Trained batch 1006 batch loss 0.507321656 epoch total loss 0.524697661\n",
      "Trained batch 1007 batch loss 0.523411572 epoch total loss 0.52469641\n",
      "Trained batch 1008 batch loss 0.56018281 epoch total loss 0.524731576\n",
      "Trained batch 1009 batch loss 0.464473933 epoch total loss 0.524671853\n",
      "Trained batch 1010 batch loss 0.52695477 epoch total loss 0.524674177\n",
      "Trained batch 1011 batch loss 0.496285677 epoch total loss 0.524646044\n",
      "Trained batch 1012 batch loss 0.483762592 epoch total loss 0.524605691\n",
      "Trained batch 1013 batch loss 0.464959025 epoch total loss 0.524546802\n",
      "Trained batch 1014 batch loss 0.45548749 epoch total loss 0.524478734\n",
      "Trained batch 1015 batch loss 0.48350051 epoch total loss 0.524438381\n",
      "Trained batch 1016 batch loss 0.434143782 epoch total loss 0.524349511\n",
      "Trained batch 1017 batch loss 0.480666339 epoch total loss 0.524306536\n",
      "Trained batch 1018 batch loss 0.44191283 epoch total loss 0.524225533\n",
      "Trained batch 1019 batch loss 0.517297804 epoch total loss 0.524218738\n",
      "Trained batch 1020 batch loss 0.570915937 epoch total loss 0.524264514\n",
      "Trained batch 1021 batch loss 0.551964223 epoch total loss 0.524291635\n",
      "Trained batch 1022 batch loss 0.501906335 epoch total loss 0.5242697\n",
      "Trained batch 1023 batch loss 0.508704484 epoch total loss 0.524254501\n",
      "Trained batch 1024 batch loss 0.541353 epoch total loss 0.52427125\n",
      "Trained batch 1025 batch loss 0.488920331 epoch total loss 0.524236739\n",
      "Trained batch 1026 batch loss 0.497733384 epoch total loss 0.52421093\n",
      "Trained batch 1027 batch loss 0.561107576 epoch total loss 0.524246812\n",
      "Trained batch 1028 batch loss 0.568362892 epoch total loss 0.524289727\n",
      "Trained batch 1029 batch loss 0.479178429 epoch total loss 0.524245918\n",
      "Trained batch 1030 batch loss 0.492370695 epoch total loss 0.524215\n",
      "Trained batch 1031 batch loss 0.498925775 epoch total loss 0.524190426\n",
      "Trained batch 1032 batch loss 0.497730017 epoch total loss 0.524164796\n",
      "Trained batch 1033 batch loss 0.489027739 epoch total loss 0.524130762\n",
      "Trained batch 1034 batch loss 0.463414282 epoch total loss 0.524072051\n",
      "Trained batch 1035 batch loss 0.466690302 epoch total loss 0.524016619\n",
      "Trained batch 1036 batch loss 0.530236 epoch total loss 0.524022579\n",
      "Trained batch 1037 batch loss 0.487909138 epoch total loss 0.52398777\n",
      "Trained batch 1038 batch loss 0.389620274 epoch total loss 0.523858309\n",
      "Trained batch 1039 batch loss 0.4291628 epoch total loss 0.523767173\n",
      "Trained batch 1040 batch loss 0.513310134 epoch total loss 0.5237571\n",
      "Trained batch 1041 batch loss 0.436490953 epoch total loss 0.523673236\n",
      "Trained batch 1042 batch loss 0.534854412 epoch total loss 0.523683965\n",
      "Trained batch 1043 batch loss 0.539545119 epoch total loss 0.523699224\n",
      "Trained batch 1044 batch loss 0.458099842 epoch total loss 0.523636401\n",
      "Trained batch 1045 batch loss 0.543916285 epoch total loss 0.523655832\n",
      "Trained batch 1046 batch loss 0.560239196 epoch total loss 0.52369082\n",
      "Trained batch 1047 batch loss 0.563395619 epoch total loss 0.523728728\n",
      "Trained batch 1048 batch loss 0.537420869 epoch total loss 0.523741782\n",
      "Trained batch 1049 batch loss 0.582260609 epoch total loss 0.523797631\n",
      "Trained batch 1050 batch loss 0.504481614 epoch total loss 0.523779154\n",
      "Trained batch 1051 batch loss 0.475819767 epoch total loss 0.523733556\n",
      "Trained batch 1052 batch loss 0.42538774 epoch total loss 0.523640096\n",
      "Trained batch 1053 batch loss 0.454568863 epoch total loss 0.523574531\n",
      "Trained batch 1054 batch loss 0.374661505 epoch total loss 0.523433208\n",
      "Trained batch 1055 batch loss 0.378033251 epoch total loss 0.523295403\n",
      "Trained batch 1056 batch loss 0.499617875 epoch total loss 0.523273\n",
      "Trained batch 1057 batch loss 0.629644036 epoch total loss 0.523373663\n",
      "Trained batch 1058 batch loss 0.586298943 epoch total loss 0.523433089\n",
      "Trained batch 1059 batch loss 0.51408273 epoch total loss 0.523424327\n",
      "Trained batch 1060 batch loss 0.629377544 epoch total loss 0.523524284\n",
      "Trained batch 1061 batch loss 0.63893497 epoch total loss 0.523633\n",
      "Trained batch 1062 batch loss 0.534190416 epoch total loss 0.523642957\n",
      "Trained batch 1063 batch loss 0.548090041 epoch total loss 0.523665965\n",
      "Trained batch 1064 batch loss 0.667836249 epoch total loss 0.523801446\n",
      "Trained batch 1065 batch loss 0.59123683 epoch total loss 0.523864806\n",
      "Trained batch 1066 batch loss 0.587759316 epoch total loss 0.523924768\n",
      "Trained batch 1067 batch loss 0.500705063 epoch total loss 0.523903\n",
      "Trained batch 1068 batch loss 0.528229296 epoch total loss 0.523907065\n",
      "Trained batch 1069 batch loss 0.576931596 epoch total loss 0.523956656\n",
      "Trained batch 1070 batch loss 0.58755976 epoch total loss 0.524016142\n",
      "Trained batch 1071 batch loss 0.50728786 epoch total loss 0.524000466\n",
      "Trained batch 1072 batch loss 0.542352676 epoch total loss 0.524017632\n",
      "Trained batch 1073 batch loss 0.532388449 epoch total loss 0.52402544\n",
      "Trained batch 1074 batch loss 0.559924185 epoch total loss 0.524058878\n",
      "Trained batch 1075 batch loss 0.609535396 epoch total loss 0.524138391\n",
      "Trained batch 1076 batch loss 0.571693182 epoch total loss 0.524182618\n",
      "Trained batch 1077 batch loss 0.491816252 epoch total loss 0.524152577\n",
      "Trained batch 1078 batch loss 0.559076905 epoch total loss 0.524185\n",
      "Trained batch 1079 batch loss 0.621023715 epoch total loss 0.524274766\n",
      "Trained batch 1080 batch loss 0.653398 epoch total loss 0.524394274\n",
      "Trained batch 1081 batch loss 0.595581174 epoch total loss 0.524460137\n",
      "Trained batch 1082 batch loss 0.475212514 epoch total loss 0.524414599\n",
      "Trained batch 1083 batch loss 0.619633555 epoch total loss 0.524502516\n",
      "Trained batch 1084 batch loss 0.485694617 epoch total loss 0.524466753\n",
      "Trained batch 1085 batch loss 0.56469804 epoch total loss 0.524503827\n",
      "Trained batch 1086 batch loss 0.556166053 epoch total loss 0.524533\n",
      "Trained batch 1087 batch loss 0.540337205 epoch total loss 0.524547517\n",
      "Trained batch 1088 batch loss 0.569551468 epoch total loss 0.524588943\n",
      "Trained batch 1089 batch loss 0.533270836 epoch total loss 0.52459687\n",
      "Trained batch 1090 batch loss 0.542024314 epoch total loss 0.524612904\n",
      "Trained batch 1091 batch loss 0.546613753 epoch total loss 0.52463311\n",
      "Trained batch 1092 batch loss 0.441325963 epoch total loss 0.524556816\n",
      "Trained batch 1093 batch loss 0.502727687 epoch total loss 0.524536848\n",
      "Trained batch 1094 batch loss 0.459970593 epoch total loss 0.524477839\n",
      "Trained batch 1095 batch loss 0.440875828 epoch total loss 0.524401486\n",
      "Trained batch 1096 batch loss 0.408332258 epoch total loss 0.524295568\n",
      "Trained batch 1097 batch loss 0.51051569 epoch total loss 0.524283\n",
      "Trained batch 1098 batch loss 0.558376491 epoch total loss 0.524314\n",
      "Trained batch 1099 batch loss 0.465546101 epoch total loss 0.524260581\n",
      "Trained batch 1100 batch loss 0.437550128 epoch total loss 0.524181724\n",
      "Trained batch 1101 batch loss 0.513617337 epoch total loss 0.524172127\n",
      "Trained batch 1102 batch loss 0.429914057 epoch total loss 0.524086595\n",
      "Trained batch 1103 batch loss 0.531365395 epoch total loss 0.524093211\n",
      "Trained batch 1104 batch loss 0.422961146 epoch total loss 0.524001658\n",
      "Trained batch 1105 batch loss 0.459173024 epoch total loss 0.523942947\n",
      "Trained batch 1106 batch loss 0.402410269 epoch total loss 0.523833096\n",
      "Trained batch 1107 batch loss 0.492184788 epoch total loss 0.523804486\n",
      "Trained batch 1108 batch loss 0.349195659 epoch total loss 0.523646891\n",
      "Trained batch 1109 batch loss 0.449868351 epoch total loss 0.523580372\n",
      "Trained batch 1110 batch loss 0.515989184 epoch total loss 0.523573518\n",
      "Trained batch 1111 batch loss 0.543433905 epoch total loss 0.523591459\n",
      "Trained batch 1112 batch loss 0.533756852 epoch total loss 0.523600578\n",
      "Trained batch 1113 batch loss 0.627036333 epoch total loss 0.523693502\n",
      "Trained batch 1114 batch loss 0.479645848 epoch total loss 0.523654\n",
      "Trained batch 1115 batch loss 0.566109776 epoch total loss 0.523692\n",
      "Trained batch 1116 batch loss 0.458028585 epoch total loss 0.523633182\n",
      "Trained batch 1117 batch loss 0.514163256 epoch total loss 0.523624718\n",
      "Trained batch 1118 batch loss 0.516898 epoch total loss 0.523618698\n",
      "Trained batch 1119 batch loss 0.51830864 epoch total loss 0.52361393\n",
      "Trained batch 1120 batch loss 0.491470337 epoch total loss 0.52358526\n",
      "Trained batch 1121 batch loss 0.481945693 epoch total loss 0.523548067\n",
      "Trained batch 1122 batch loss 0.467637867 epoch total loss 0.523498237\n",
      "Trained batch 1123 batch loss 0.446966767 epoch total loss 0.523430109\n",
      "Trained batch 1124 batch loss 0.467285037 epoch total loss 0.52338016\n",
      "Trained batch 1125 batch loss 0.540156662 epoch total loss 0.523395061\n",
      "Trained batch 1126 batch loss 0.588872254 epoch total loss 0.523453236\n",
      "Trained batch 1127 batch loss 0.486010909 epoch total loss 0.523420036\n",
      "Trained batch 1128 batch loss 0.523009062 epoch total loss 0.523419619\n",
      "Trained batch 1129 batch loss 0.426988453 epoch total loss 0.523334265\n",
      "Trained batch 1130 batch loss 0.481896937 epoch total loss 0.523297548\n",
      "Trained batch 1131 batch loss 0.524041653 epoch total loss 0.523298204\n",
      "Trained batch 1132 batch loss 0.56570363 epoch total loss 0.523335636\n",
      "Trained batch 1133 batch loss 0.556578815 epoch total loss 0.523364961\n",
      "Trained batch 1134 batch loss 0.559493482 epoch total loss 0.52339685\n",
      "Trained batch 1135 batch loss 0.569989204 epoch total loss 0.523437917\n",
      "Trained batch 1136 batch loss 0.560626864 epoch total loss 0.52347064\n",
      "Trained batch 1137 batch loss 0.706476331 epoch total loss 0.523631632\n",
      "Trained batch 1138 batch loss 0.600302577 epoch total loss 0.523699\n",
      "Trained batch 1139 batch loss 0.54493314 epoch total loss 0.523717582\n",
      "Trained batch 1140 batch loss 0.566576362 epoch total loss 0.523755193\n",
      "Trained batch 1141 batch loss 0.752377033 epoch total loss 0.523955584\n",
      "Trained batch 1142 batch loss 0.666341245 epoch total loss 0.524080217\n",
      "Trained batch 1143 batch loss 0.604585648 epoch total loss 0.52415067\n",
      "Trained batch 1144 batch loss 0.463426679 epoch total loss 0.524097621\n",
      "Trained batch 1145 batch loss 0.431385398 epoch total loss 0.524016678\n",
      "Trained batch 1146 batch loss 0.404506832 epoch total loss 0.52391237\n",
      "Trained batch 1147 batch loss 0.48725161 epoch total loss 0.523880363\n",
      "Trained batch 1148 batch loss 0.584694684 epoch total loss 0.523933411\n",
      "Trained batch 1149 batch loss 0.473110199 epoch total loss 0.523889124\n",
      "Trained batch 1150 batch loss 0.505026877 epoch total loss 0.523872733\n",
      "Trained batch 1151 batch loss 0.383952141 epoch total loss 0.52375114\n",
      "Trained batch 1152 batch loss 0.431145906 epoch total loss 0.523670793\n",
      "Trained batch 1153 batch loss 0.453542829 epoch total loss 0.523609936\n",
      "Trained batch 1154 batch loss 0.497720361 epoch total loss 0.523587525\n",
      "Trained batch 1155 batch loss 0.44148773 epoch total loss 0.523516476\n",
      "Trained batch 1156 batch loss 0.429219395 epoch total loss 0.523434877\n",
      "Trained batch 1157 batch loss 0.485533655 epoch total loss 0.523402095\n",
      "Trained batch 1158 batch loss 0.438423485 epoch total loss 0.523328722\n",
      "Trained batch 1159 batch loss 0.421308905 epoch total loss 0.523240685\n",
      "Trained batch 1160 batch loss 0.358678818 epoch total loss 0.523098886\n",
      "Trained batch 1161 batch loss 0.418951839 epoch total loss 0.523009121\n",
      "Trained batch 1162 batch loss 0.417910516 epoch total loss 0.522918701\n",
      "Trained batch 1163 batch loss 0.435389459 epoch total loss 0.522843421\n",
      "Trained batch 1164 batch loss 0.485719055 epoch total loss 0.522811532\n",
      "Trained batch 1165 batch loss 0.517594576 epoch total loss 0.522807\n",
      "Trained batch 1166 batch loss 0.57144 epoch total loss 0.522848725\n",
      "Trained batch 1167 batch loss 0.494716138 epoch total loss 0.522824585\n",
      "Trained batch 1168 batch loss 0.413110346 epoch total loss 0.522730649\n",
      "Trained batch 1169 batch loss 0.426073074 epoch total loss 0.522648\n",
      "Trained batch 1170 batch loss 0.432975471 epoch total loss 0.522571325\n",
      "Trained batch 1171 batch loss 0.41575706 epoch total loss 0.52248013\n",
      "Trained batch 1172 batch loss 0.389465481 epoch total loss 0.522366643\n",
      "Trained batch 1173 batch loss 0.44442907 epoch total loss 0.522300184\n",
      "Trained batch 1174 batch loss 0.417604476 epoch total loss 0.522211\n",
      "Trained batch 1175 batch loss 0.438691348 epoch total loss 0.522139966\n",
      "Trained batch 1176 batch loss 0.439328283 epoch total loss 0.522069573\n",
      "Trained batch 1177 batch loss 0.515041351 epoch total loss 0.522063553\n",
      "Trained batch 1178 batch loss 0.485597372 epoch total loss 0.522032619\n",
      "Trained batch 1179 batch loss 0.502440274 epoch total loss 0.522016\n",
      "Trained batch 1180 batch loss 0.464911 epoch total loss 0.52196759\n",
      "Trained batch 1181 batch loss 0.453617275 epoch total loss 0.521909714\n",
      "Trained batch 1182 batch loss 0.497174114 epoch total loss 0.521888793\n",
      "Trained batch 1183 batch loss 0.473386109 epoch total loss 0.521847785\n",
      "Trained batch 1184 batch loss 0.447595984 epoch total loss 0.52178508\n",
      "Trained batch 1185 batch loss 0.359848708 epoch total loss 0.521648407\n",
      "Trained batch 1186 batch loss 0.580527425 epoch total loss 0.521698058\n",
      "Trained batch 1187 batch loss 0.493476152 epoch total loss 0.521674275\n",
      "Trained batch 1188 batch loss 0.566840708 epoch total loss 0.521712303\n",
      "Trained batch 1189 batch loss 0.480538696 epoch total loss 0.521677673\n",
      "Trained batch 1190 batch loss 0.467534751 epoch total loss 0.521632135\n",
      "Trained batch 1191 batch loss 0.468702078 epoch total loss 0.52158767\n",
      "Trained batch 1192 batch loss 0.568038285 epoch total loss 0.521626651\n",
      "Trained batch 1193 batch loss 0.515779078 epoch total loss 0.521621823\n",
      "Trained batch 1194 batch loss 0.398758948 epoch total loss 0.521518886\n",
      "Trained batch 1195 batch loss 0.424301237 epoch total loss 0.521437526\n",
      "Trained batch 1196 batch loss 0.425063491 epoch total loss 0.52135694\n",
      "Trained batch 1197 batch loss 0.543220341 epoch total loss 0.521375179\n",
      "Trained batch 1198 batch loss 0.41446954 epoch total loss 0.521286\n",
      "Trained batch 1199 batch loss 0.471604645 epoch total loss 0.521244586\n",
      "Trained batch 1200 batch loss 0.486370653 epoch total loss 0.521215498\n",
      "Trained batch 1201 batch loss 0.491859257 epoch total loss 0.52119112\n",
      "Trained batch 1202 batch loss 0.482882589 epoch total loss 0.521159232\n",
      "Trained batch 1203 batch loss 0.552154899 epoch total loss 0.52118504\n",
      "Trained batch 1204 batch loss 0.595975816 epoch total loss 0.521247149\n",
      "Trained batch 1205 batch loss 0.694893 epoch total loss 0.521391213\n",
      "Trained batch 1206 batch loss 0.628584862 epoch total loss 0.521480143\n",
      "Trained batch 1207 batch loss 0.599092245 epoch total loss 0.521544456\n",
      "Trained batch 1208 batch loss 0.597664475 epoch total loss 0.521607459\n",
      "Trained batch 1209 batch loss 0.504071 epoch total loss 0.521593\n",
      "Trained batch 1210 batch loss 0.491260409 epoch total loss 0.521567881\n",
      "Trained batch 1211 batch loss 0.477423131 epoch total loss 0.521531463\n",
      "Trained batch 1212 batch loss 0.573647 epoch total loss 0.521574438\n",
      "Trained batch 1213 batch loss 0.431584656 epoch total loss 0.521500289\n",
      "Trained batch 1214 batch loss 0.627296865 epoch total loss 0.521587431\n",
      "Trained batch 1215 batch loss 0.515347362 epoch total loss 0.521582305\n",
      "Trained batch 1216 batch loss 0.474428833 epoch total loss 0.521543503\n",
      "Trained batch 1217 batch loss 0.490083188 epoch total loss 0.521517694\n",
      "Trained batch 1218 batch loss 0.514268458 epoch total loss 0.521511734\n",
      "Trained batch 1219 batch loss 0.667680383 epoch total loss 0.521631658\n",
      "Trained batch 1220 batch loss 0.684334397 epoch total loss 0.521765\n",
      "Trained batch 1221 batch loss 0.631413221 epoch total loss 0.521854758\n",
      "Trained batch 1222 batch loss 0.463971406 epoch total loss 0.521807432\n",
      "Trained batch 1223 batch loss 0.546022713 epoch total loss 0.521827221\n",
      "Trained batch 1224 batch loss 0.492125332 epoch total loss 0.521802962\n",
      "Trained batch 1225 batch loss 0.387467742 epoch total loss 0.521693289\n",
      "Trained batch 1226 batch loss 0.361704677 epoch total loss 0.521562755\n",
      "Trained batch 1227 batch loss 0.408403754 epoch total loss 0.521470547\n",
      "Trained batch 1228 batch loss 0.481218904 epoch total loss 0.521437764\n",
      "Trained batch 1229 batch loss 0.481549591 epoch total loss 0.521405339\n",
      "Trained batch 1230 batch loss 0.474897385 epoch total loss 0.52136749\n",
      "Trained batch 1231 batch loss 0.556558907 epoch total loss 0.521396101\n",
      "Trained batch 1232 batch loss 0.642979741 epoch total loss 0.521494806\n",
      "Trained batch 1233 batch loss 0.560108542 epoch total loss 0.521526158\n",
      "Trained batch 1234 batch loss 0.526002526 epoch total loss 0.521529794\n",
      "Trained batch 1235 batch loss 0.576674521 epoch total loss 0.521574438\n",
      "Trained batch 1236 batch loss 0.499009 epoch total loss 0.521556199\n",
      "Trained batch 1237 batch loss 0.593363166 epoch total loss 0.521614254\n",
      "Trained batch 1238 batch loss 0.531773627 epoch total loss 0.521622479\n",
      "Trained batch 1239 batch loss 0.475858331 epoch total loss 0.521585524\n",
      "Trained batch 1240 batch loss 0.461624026 epoch total loss 0.521537125\n",
      "Trained batch 1241 batch loss 0.49669385 epoch total loss 0.521517158\n",
      "Trained batch 1242 batch loss 0.488917112 epoch total loss 0.521490872\n",
      "Trained batch 1243 batch loss 0.536495507 epoch total loss 0.521502912\n",
      "Trained batch 1244 batch loss 0.525071621 epoch total loss 0.521505833\n",
      "Trained batch 1245 batch loss 0.533423364 epoch total loss 0.521515429\n",
      "Trained batch 1246 batch loss 0.579242706 epoch total loss 0.521561742\n",
      "Trained batch 1247 batch loss 0.646654129 epoch total loss 0.521662056\n",
      "Trained batch 1248 batch loss 0.498552889 epoch total loss 0.521643519\n",
      "Trained batch 1249 batch loss 0.539675951 epoch total loss 0.521657944\n",
      "Trained batch 1250 batch loss 0.482417107 epoch total loss 0.521626592\n",
      "Trained batch 1251 batch loss 0.461250931 epoch total loss 0.521578312\n",
      "Trained batch 1252 batch loss 0.412436306 epoch total loss 0.52149111\n",
      "Trained batch 1253 batch loss 0.468770564 epoch total loss 0.521449\n",
      "Trained batch 1254 batch loss 0.433370501 epoch total loss 0.521378756\n",
      "Trained batch 1255 batch loss 0.445481807 epoch total loss 0.521318316\n",
      "Trained batch 1256 batch loss 0.465153515 epoch total loss 0.521273553\n",
      "Trained batch 1257 batch loss 0.563569367 epoch total loss 0.52130723\n",
      "Trained batch 1258 batch loss 0.60071677 epoch total loss 0.521370351\n",
      "Trained batch 1259 batch loss 0.623803616 epoch total loss 0.521451712\n",
      "Trained batch 1260 batch loss 0.628103495 epoch total loss 0.52153635\n",
      "Trained batch 1261 batch loss 0.580922484 epoch total loss 0.521583438\n",
      "Trained batch 1262 batch loss 0.542119563 epoch total loss 0.52159971\n",
      "Trained batch 1263 batch loss 0.546790421 epoch total loss 0.521619678\n",
      "Trained batch 1264 batch loss 0.63156 epoch total loss 0.521706641\n",
      "Trained batch 1265 batch loss 0.493896812 epoch total loss 0.521684647\n",
      "Trained batch 1266 batch loss 0.465866029 epoch total loss 0.521640599\n",
      "Trained batch 1267 batch loss 0.500324249 epoch total loss 0.521623731\n",
      "Trained batch 1268 batch loss 0.508464575 epoch total loss 0.521613359\n",
      "Trained batch 1269 batch loss 0.494317651 epoch total loss 0.521591842\n",
      "Trained batch 1270 batch loss 0.542951822 epoch total loss 0.52160871\n",
      "Trained batch 1271 batch loss 0.518770814 epoch total loss 0.521606505\n",
      "Trained batch 1272 batch loss 0.49045 epoch total loss 0.521582\n",
      "Trained batch 1273 batch loss 0.536738575 epoch total loss 0.521593928\n",
      "Trained batch 1274 batch loss 0.576390803 epoch total loss 0.521636963\n",
      "Trained batch 1275 batch loss 0.530368507 epoch total loss 0.521643817\n",
      "Trained batch 1276 batch loss 0.51602155 epoch total loss 0.521639407\n",
      "Trained batch 1277 batch loss 0.498525769 epoch total loss 0.521621287\n",
      "Trained batch 1278 batch loss 0.524928331 epoch total loss 0.52162385\n",
      "Trained batch 1279 batch loss 0.500085592 epoch total loss 0.521607041\n",
      "Trained batch 1280 batch loss 0.536139607 epoch total loss 0.521618366\n",
      "Trained batch 1281 batch loss 0.5473634 epoch total loss 0.521638453\n",
      "Trained batch 1282 batch loss 0.453502268 epoch total loss 0.521585286\n",
      "Trained batch 1283 batch loss 0.49495 epoch total loss 0.521564543\n",
      "Trained batch 1284 batch loss 0.464390635 epoch total loss 0.52152\n",
      "Trained batch 1285 batch loss 0.510960698 epoch total loss 0.521511853\n",
      "Trained batch 1286 batch loss 0.604635596 epoch total loss 0.521576464\n",
      "Trained batch 1287 batch loss 0.543001115 epoch total loss 0.521593094\n",
      "Trained batch 1288 batch loss 0.482161403 epoch total loss 0.521562517\n",
      "Trained batch 1289 batch loss 0.438222766 epoch total loss 0.521497846\n",
      "Trained batch 1290 batch loss 0.523965359 epoch total loss 0.521499813\n",
      "Trained batch 1291 batch loss 0.576544523 epoch total loss 0.52154243\n",
      "Trained batch 1292 batch loss 0.482909322 epoch total loss 0.521512508\n",
      "Trained batch 1293 batch loss 0.575270474 epoch total loss 0.521554112\n",
      "Trained batch 1294 batch loss 0.540356219 epoch total loss 0.521568596\n",
      "Trained batch 1295 batch loss 0.509750664 epoch total loss 0.521559477\n",
      "Trained batch 1296 batch loss 0.551630855 epoch total loss 0.521582723\n",
      "Trained batch 1297 batch loss 0.484897435 epoch total loss 0.52155447\n",
      "Trained batch 1298 batch loss 0.485073537 epoch total loss 0.521526337\n",
      "Trained batch 1299 batch loss 0.458269775 epoch total loss 0.52147758\n",
      "Trained batch 1300 batch loss 0.477687925 epoch total loss 0.521443903\n",
      "Trained batch 1301 batch loss 0.434551835 epoch total loss 0.521377146\n",
      "Trained batch 1302 batch loss 0.521503091 epoch total loss 0.521377206\n",
      "Trained batch 1303 batch loss 0.494805932 epoch total loss 0.521356821\n",
      "Trained batch 1304 batch loss 0.661809742 epoch total loss 0.521464527\n",
      "Trained batch 1305 batch loss 0.752406538 epoch total loss 0.521641493\n",
      "Trained batch 1306 batch loss 0.661276162 epoch total loss 0.521748364\n",
      "Trained batch 1307 batch loss 0.682968795 epoch total loss 0.521871746\n",
      "Trained batch 1308 batch loss 0.580918789 epoch total loss 0.521916866\n",
      "Trained batch 1309 batch loss 0.602991581 epoch total loss 0.521978796\n",
      "Trained batch 1310 batch loss 0.645608246 epoch total loss 0.522073209\n",
      "Trained batch 1311 batch loss 0.634281 epoch total loss 0.522158802\n",
      "Trained batch 1312 batch loss 0.579618394 epoch total loss 0.522202551\n",
      "Trained batch 1313 batch loss 0.573667526 epoch total loss 0.522241771\n",
      "Trained batch 1314 batch loss 0.605789125 epoch total loss 0.52230531\n",
      "Trained batch 1315 batch loss 0.524537444 epoch total loss 0.522307038\n",
      "Trained batch 1316 batch loss 0.568177402 epoch total loss 0.522341907\n",
      "Trained batch 1317 batch loss 0.530781209 epoch total loss 0.522348285\n",
      "Trained batch 1318 batch loss 0.553646088 epoch total loss 0.522372\n",
      "Trained batch 1319 batch loss 0.540378273 epoch total loss 0.522385716\n",
      "Trained batch 1320 batch loss 0.572008967 epoch total loss 0.522423327\n",
      "Trained batch 1321 batch loss 0.598831534 epoch total loss 0.522481143\n",
      "Trained batch 1322 batch loss 0.555569768 epoch total loss 0.522506118\n",
      "Trained batch 1323 batch loss 0.533056 epoch total loss 0.522514105\n",
      "Trained batch 1324 batch loss 0.546234071 epoch total loss 0.522532046\n",
      "Trained batch 1325 batch loss 0.557693422 epoch total loss 0.52255857\n",
      "Trained batch 1326 batch loss 0.473464787 epoch total loss 0.522521496\n",
      "Trained batch 1327 batch loss 0.538445055 epoch total loss 0.522533536\n",
      "Trained batch 1328 batch loss 0.533475876 epoch total loss 0.522541761\n",
      "Trained batch 1329 batch loss 0.522837877 epoch total loss 0.52254194\n",
      "Trained batch 1330 batch loss 0.506334 epoch total loss 0.522529781\n",
      "Trained batch 1331 batch loss 0.535817504 epoch total loss 0.522539794\n",
      "Trained batch 1332 batch loss 0.42523697 epoch total loss 0.522466719\n",
      "Trained batch 1333 batch loss 0.464674532 epoch total loss 0.522423327\n",
      "Trained batch 1334 batch loss 0.413798392 epoch total loss 0.522341907\n",
      "Trained batch 1335 batch loss 0.440159589 epoch total loss 0.522280395\n",
      "Trained batch 1336 batch loss 0.466670126 epoch total loss 0.522238791\n",
      "Trained batch 1337 batch loss 0.501040936 epoch total loss 0.522222936\n",
      "Trained batch 1338 batch loss 0.583206832 epoch total loss 0.522268474\n",
      "Trained batch 1339 batch loss 0.547454536 epoch total loss 0.52228725\n",
      "Trained batch 1340 batch loss 0.503369451 epoch total loss 0.522273123\n",
      "Trained batch 1341 batch loss 0.59372 epoch total loss 0.522326469\n",
      "Trained batch 1342 batch loss 0.638030052 epoch total loss 0.522412658\n",
      "Trained batch 1343 batch loss 0.591510117 epoch total loss 0.522464097\n",
      "Trained batch 1344 batch loss 0.55184257 epoch total loss 0.522485912\n",
      "Trained batch 1345 batch loss 0.50088793 epoch total loss 0.522469878\n",
      "Trained batch 1346 batch loss 0.562952042 epoch total loss 0.522499919\n",
      "Trained batch 1347 batch loss 0.490650713 epoch total loss 0.522476315\n",
      "Trained batch 1348 batch loss 0.426491052 epoch total loss 0.522405088\n",
      "Trained batch 1349 batch loss 0.454105586 epoch total loss 0.522354484\n",
      "Trained batch 1350 batch loss 0.485048801 epoch total loss 0.522326827\n",
      "Trained batch 1351 batch loss 0.503967524 epoch total loss 0.522313237\n",
      "Trained batch 1352 batch loss 0.514597654 epoch total loss 0.522307515\n",
      "Trained batch 1353 batch loss 0.528412759 epoch total loss 0.522312045\n",
      "Trained batch 1354 batch loss 0.543529928 epoch total loss 0.522327721\n",
      "Trained batch 1355 batch loss 0.50015837 epoch total loss 0.522311389\n",
      "Trained batch 1356 batch loss 0.515166759 epoch total loss 0.522306085\n",
      "Trained batch 1357 batch loss 0.527513266 epoch total loss 0.522309959\n",
      "Trained batch 1358 batch loss 0.584741294 epoch total loss 0.522355914\n",
      "Trained batch 1359 batch loss 0.631027877 epoch total loss 0.522435904\n",
      "Trained batch 1360 batch loss 0.585810542 epoch total loss 0.522482455\n",
      "Trained batch 1361 batch loss 0.575833619 epoch total loss 0.522521675\n",
      "Trained batch 1362 batch loss 0.510210097 epoch total loss 0.522512615\n",
      "Trained batch 1363 batch loss 0.503701389 epoch total loss 0.522498846\n",
      "Trained batch 1364 batch loss 0.596138537 epoch total loss 0.522552788\n",
      "Trained batch 1365 batch loss 0.530763745 epoch total loss 0.522558808\n",
      "Trained batch 1366 batch loss 0.452599138 epoch total loss 0.522507608\n",
      "Trained batch 1367 batch loss 0.54807359 epoch total loss 0.522526324\n",
      "Trained batch 1368 batch loss 0.530779302 epoch total loss 0.522532344\n",
      "Trained batch 1369 batch loss 0.485762537 epoch total loss 0.522505462\n",
      "Trained batch 1370 batch loss 0.573357403 epoch total loss 0.522542596\n",
      "Trained batch 1371 batch loss 0.53465271 epoch total loss 0.522551417\n",
      "Trained batch 1372 batch loss 0.591115952 epoch total loss 0.522601426\n",
      "Trained batch 1373 batch loss 0.597788215 epoch total loss 0.522656202\n",
      "Trained batch 1374 batch loss 0.553967 epoch total loss 0.522679\n",
      "Trained batch 1375 batch loss 0.619622886 epoch total loss 0.522749484\n",
      "Trained batch 1376 batch loss 0.49010554 epoch total loss 0.522725761\n",
      "Trained batch 1377 batch loss 0.525827646 epoch total loss 0.522727966\n",
      "Trained batch 1378 batch loss 0.505396843 epoch total loss 0.52271539\n",
      "Trained batch 1379 batch loss 0.524651051 epoch total loss 0.52271682\n",
      "Trained batch 1380 batch loss 0.572060704 epoch total loss 0.522752583\n",
      "Trained batch 1381 batch loss 0.613007903 epoch total loss 0.522817969\n",
      "Trained batch 1382 batch loss 0.649620652 epoch total loss 0.522909701\n",
      "Trained batch 1383 batch loss 0.624162 epoch total loss 0.522982895\n",
      "Trained batch 1384 batch loss 0.554444313 epoch total loss 0.523005605\n",
      "Trained batch 1385 batch loss 0.532596827 epoch total loss 0.523012519\n",
      "Trained batch 1386 batch loss 0.521202385 epoch total loss 0.523011208\n",
      "Trained batch 1387 batch loss 0.487353086 epoch total loss 0.522985518\n",
      "Trained batch 1388 batch loss 0.568703771 epoch total loss 0.523018479\n",
      "Trained batch 1389 batch loss 0.544869 epoch total loss 0.523034215\n",
      "Trained batch 1390 batch loss 0.593643606 epoch total loss 0.523085\n",
      "Trained batch 1391 batch loss 0.522641122 epoch total loss 0.5230847\n",
      "Trained batch 1392 batch loss 0.558782339 epoch total loss 0.52311033\n",
      "Trained batch 1393 batch loss 0.545539141 epoch total loss 0.523126423\n",
      "Trained batch 1394 batch loss 0.481083691 epoch total loss 0.523096263\n",
      "Trained batch 1395 batch loss 0.495136678 epoch total loss 0.523076177\n",
      "Trained batch 1396 batch loss 0.547712445 epoch total loss 0.52309382\n",
      "Trained batch 1397 batch loss 0.618002117 epoch total loss 0.523161769\n",
      "Trained batch 1398 batch loss 0.560257137 epoch total loss 0.523188293\n",
      "Trained batch 1399 batch loss 0.515779436 epoch total loss 0.523183048\n",
      "Trained batch 1400 batch loss 0.564172566 epoch total loss 0.523212254\n",
      "Trained batch 1401 batch loss 0.511443436 epoch total loss 0.52320385\n",
      "Trained batch 1402 batch loss 0.48792398 epoch total loss 0.523178697\n",
      "Trained batch 1403 batch loss 0.424106568 epoch total loss 0.523108065\n",
      "Trained batch 1404 batch loss 0.502726912 epoch total loss 0.523093581\n",
      "Trained batch 1405 batch loss 0.390285969 epoch total loss 0.522999048\n",
      "Trained batch 1406 batch loss 0.436245263 epoch total loss 0.522937298\n",
      "Trained batch 1407 batch loss 0.430817366 epoch total loss 0.522871852\n",
      "Trained batch 1408 batch loss 0.412079185 epoch total loss 0.522793233\n",
      "Trained batch 1409 batch loss 0.42516914 epoch total loss 0.522723913\n",
      "Trained batch 1410 batch loss 0.485163391 epoch total loss 0.52269727\n",
      "Trained batch 1411 batch loss 0.501547217 epoch total loss 0.522682309\n",
      "Trained batch 1412 batch loss 0.400689721 epoch total loss 0.522595882\n",
      "Trained batch 1413 batch loss 0.495910704 epoch total loss 0.522577\n",
      "Trained batch 1414 batch loss 0.54457438 epoch total loss 0.522592545\n",
      "Trained batch 1415 batch loss 0.498660713 epoch total loss 0.522575617\n",
      "Trained batch 1416 batch loss 0.467525661 epoch total loss 0.522536755\n",
      "Trained batch 1417 batch loss 0.437940508 epoch total loss 0.522477031\n",
      "Trained batch 1418 batch loss 0.49869 epoch total loss 0.522460282\n",
      "Trained batch 1419 batch loss 0.590472 epoch total loss 0.522508204\n",
      "Trained batch 1420 batch loss 0.450916886 epoch total loss 0.522457778\n",
      "Trained batch 1421 batch loss 0.429632425 epoch total loss 0.522392452\n",
      "Trained batch 1422 batch loss 0.492808878 epoch total loss 0.52237165\n",
      "Trained batch 1423 batch loss 0.444888115 epoch total loss 0.522317231\n",
      "Trained batch 1424 batch loss 0.530577779 epoch total loss 0.522323\n",
      "Trained batch 1425 batch loss 0.597259641 epoch total loss 0.522375643\n",
      "Trained batch 1426 batch loss 0.537924886 epoch total loss 0.522386491\n",
      "Trained batch 1427 batch loss 0.557563603 epoch total loss 0.522411168\n",
      "Trained batch 1428 batch loss 0.562815428 epoch total loss 0.52243942\n",
      "Trained batch 1429 batch loss 0.523607075 epoch total loss 0.522440255\n",
      "Trained batch 1430 batch loss 0.56808275 epoch total loss 0.522472143\n",
      "Trained batch 1431 batch loss 0.534141898 epoch total loss 0.522480309\n",
      "Trained batch 1432 batch loss 0.460472 epoch total loss 0.522437\n",
      "Trained batch 1433 batch loss 0.553045452 epoch total loss 0.522458315\n",
      "Trained batch 1434 batch loss 0.543434203 epoch total loss 0.522473\n",
      "Trained batch 1435 batch loss 0.490453303 epoch total loss 0.522450686\n",
      "Trained batch 1436 batch loss 0.446524382 epoch total loss 0.522397816\n",
      "Trained batch 1437 batch loss 0.471250147 epoch total loss 0.522362232\n",
      "Trained batch 1438 batch loss 0.432132721 epoch total loss 0.522299469\n",
      "Trained batch 1439 batch loss 0.407086492 epoch total loss 0.522219419\n",
      "Trained batch 1440 batch loss 0.474296808 epoch total loss 0.52218616\n",
      "Trained batch 1441 batch loss 0.610362947 epoch total loss 0.522247314\n",
      "Trained batch 1442 batch loss 0.49074468 epoch total loss 0.522225499\n",
      "Trained batch 1443 batch loss 0.542817771 epoch total loss 0.522239745\n",
      "Trained batch 1444 batch loss 0.582434 epoch total loss 0.522281468\n",
      "Trained batch 1445 batch loss 0.513214111 epoch total loss 0.52227515\n",
      "Trained batch 1446 batch loss 0.517355323 epoch total loss 0.522271752\n",
      "Trained batch 1447 batch loss 0.625154138 epoch total loss 0.522342861\n",
      "Trained batch 1448 batch loss 0.525442 epoch total loss 0.522345\n",
      "Trained batch 1449 batch loss 0.687579751 epoch total loss 0.52245903\n",
      "Trained batch 1450 batch loss 0.620881677 epoch total loss 0.52252692\n",
      "Trained batch 1451 batch loss 0.524309874 epoch total loss 0.522528172\n",
      "Trained batch 1452 batch loss 0.543393314 epoch total loss 0.522542536\n",
      "Trained batch 1453 batch loss 0.531486452 epoch total loss 0.522548676\n",
      "Trained batch 1454 batch loss 0.501858294 epoch total loss 0.52253443\n",
      "Trained batch 1455 batch loss 0.528561771 epoch total loss 0.522538602\n",
      "Trained batch 1456 batch loss 0.501619935 epoch total loss 0.522524238\n",
      "Trained batch 1457 batch loss 0.486369 epoch total loss 0.522499442\n",
      "Trained batch 1458 batch loss 0.527426302 epoch total loss 0.52250278\n",
      "Trained batch 1459 batch loss 0.579363346 epoch total loss 0.522541761\n",
      "Trained batch 1460 batch loss 0.551673651 epoch total loss 0.522561729\n",
      "Trained batch 1461 batch loss 0.396458834 epoch total loss 0.522475421\n",
      "Trained batch 1462 batch loss 0.439001441 epoch total loss 0.52241838\n",
      "Trained batch 1463 batch loss 0.35231784 epoch total loss 0.522302091\n",
      "Trained batch 1464 batch loss 0.39386341 epoch total loss 0.522214353\n",
      "Trained batch 1465 batch loss 0.426082164 epoch total loss 0.522148728\n",
      "Trained batch 1466 batch loss 0.450679034 epoch total loss 0.5221\n",
      "Trained batch 1467 batch loss 0.496538192 epoch total loss 0.522082508\n",
      "Trained batch 1468 batch loss 0.554155827 epoch total loss 0.522104383\n",
      "Trained batch 1469 batch loss 0.597707868 epoch total loss 0.522155821\n",
      "Trained batch 1470 batch loss 0.596116781 epoch total loss 0.522206187\n",
      "Trained batch 1471 batch loss 0.577148855 epoch total loss 0.5222435\n",
      "Trained batch 1472 batch loss 0.636958599 epoch total loss 0.522321463\n",
      "Trained batch 1473 batch loss 0.61706847 epoch total loss 0.522385776\n",
      "Trained batch 1474 batch loss 0.596108854 epoch total loss 0.522435784\n",
      "Trained batch 1475 batch loss 0.526084602 epoch total loss 0.522438288\n",
      "Trained batch 1476 batch loss 0.517248809 epoch total loss 0.522434771\n",
      "Trained batch 1477 batch loss 0.529448271 epoch total loss 0.52243948\n",
      "Trained batch 1478 batch loss 0.558134437 epoch total loss 0.52246362\n",
      "Trained batch 1479 batch loss 0.562415838 epoch total loss 0.522490621\n",
      "Trained batch 1480 batch loss 0.510025084 epoch total loss 0.522482216\n",
      "Trained batch 1481 batch loss 0.54792577 epoch total loss 0.522499382\n",
      "Trained batch 1482 batch loss 0.412930816 epoch total loss 0.522425413\n",
      "Trained batch 1483 batch loss 0.472667933 epoch total loss 0.522391856\n",
      "Trained batch 1484 batch loss 0.444397092 epoch total loss 0.522339344\n",
      "Trained batch 1485 batch loss 0.541733 epoch total loss 0.522352397\n",
      "Trained batch 1486 batch loss 0.567518651 epoch total loss 0.522382796\n",
      "Trained batch 1487 batch loss 0.522573233 epoch total loss 0.522382915\n",
      "Trained batch 1488 batch loss 0.508871257 epoch total loss 0.522373796\n",
      "Trained batch 1489 batch loss 0.585613251 epoch total loss 0.522416294\n",
      "Trained batch 1490 batch loss 0.607652128 epoch total loss 0.522473514\n",
      "Trained batch 1491 batch loss 0.569522798 epoch total loss 0.522505045\n",
      "Trained batch 1492 batch loss 0.49571535 epoch total loss 0.522487104\n",
      "Trained batch 1493 batch loss 0.511092 epoch total loss 0.522479475\n",
      "Trained batch 1494 batch loss 0.595685959 epoch total loss 0.522528529\n",
      "Trained batch 1495 batch loss 0.445008099 epoch total loss 0.522476673\n",
      "Trained batch 1496 batch loss 0.476753026 epoch total loss 0.522446096\n",
      "Trained batch 1497 batch loss 0.501541376 epoch total loss 0.522432089\n",
      "Trained batch 1498 batch loss 0.609943271 epoch total loss 0.522490501\n",
      "Trained batch 1499 batch loss 0.576801419 epoch total loss 0.522526741\n",
      "Trained batch 1500 batch loss 0.529905319 epoch total loss 0.522531629\n",
      "Trained batch 1501 batch loss 0.547278345 epoch total loss 0.522548139\n",
      "Trained batch 1502 batch loss 0.56037426 epoch total loss 0.522573352\n",
      "Trained batch 1503 batch loss 0.556386292 epoch total loss 0.522595823\n",
      "Trained batch 1504 batch loss 0.537638843 epoch total loss 0.522605836\n",
      "Trained batch 1505 batch loss 0.57436192 epoch total loss 0.522640228\n",
      "Trained batch 1506 batch loss 0.605388463 epoch total loss 0.522695184\n",
      "Trained batch 1507 batch loss 0.468252063 epoch total loss 0.522659063\n",
      "Trained batch 1508 batch loss 0.493395597 epoch total loss 0.522639692\n",
      "Trained batch 1509 batch loss 0.546314895 epoch total loss 0.522655368\n",
      "Trained batch 1510 batch loss 0.433096021 epoch total loss 0.522596061\n",
      "Trained batch 1511 batch loss 0.493165553 epoch total loss 0.522576571\n",
      "Trained batch 1512 batch loss 0.525166094 epoch total loss 0.522578299\n",
      "Trained batch 1513 batch loss 0.507539213 epoch total loss 0.522568345\n",
      "Trained batch 1514 batch loss 0.559501231 epoch total loss 0.522592783\n",
      "Trained batch 1515 batch loss 0.577734828 epoch total loss 0.522629201\n",
      "Trained batch 1516 batch loss 0.600884 epoch total loss 0.522680819\n",
      "Trained batch 1517 batch loss 0.555186689 epoch total loss 0.522702217\n",
      "Trained batch 1518 batch loss 0.43987444 epoch total loss 0.522647679\n",
      "Trained batch 1519 batch loss 0.523835599 epoch total loss 0.522648454\n",
      "Trained batch 1520 batch loss 0.550251 epoch total loss 0.522666633\n",
      "Trained batch 1521 batch loss 0.634344816 epoch total loss 0.52274\n",
      "Trained batch 1522 batch loss 0.521788538 epoch total loss 0.52273941\n",
      "Trained batch 1523 batch loss 0.458234787 epoch total loss 0.522697091\n",
      "Trained batch 1524 batch loss 0.550029933 epoch total loss 0.522715032\n",
      "Trained batch 1525 batch loss 0.516648948 epoch total loss 0.522711039\n",
      "Trained batch 1526 batch loss 0.589189827 epoch total loss 0.52275461\n",
      "Trained batch 1527 batch loss 0.518668532 epoch total loss 0.522751927\n",
      "Trained batch 1528 batch loss 0.574459195 epoch total loss 0.522785783\n",
      "Trained batch 1529 batch loss 0.42508328 epoch total loss 0.522721887\n",
      "Trained batch 1530 batch loss 0.442105442 epoch total loss 0.522669196\n",
      "Trained batch 1531 batch loss 0.599236727 epoch total loss 0.522719204\n",
      "Trained batch 1532 batch loss 0.584784329 epoch total loss 0.522759676\n",
      "Trained batch 1533 batch loss 0.617195845 epoch total loss 0.522821307\n",
      "Trained batch 1534 batch loss 0.564242482 epoch total loss 0.522848308\n",
      "Trained batch 1535 batch loss 0.623007894 epoch total loss 0.522913575\n",
      "Trained batch 1536 batch loss 0.491842628 epoch total loss 0.52289331\n",
      "Trained batch 1537 batch loss 0.479257584 epoch total loss 0.522864938\n",
      "Trained batch 1538 batch loss 0.436041 epoch total loss 0.522808433\n",
      "Trained batch 1539 batch loss 0.408059388 epoch total loss 0.522733927\n",
      "Trained batch 1540 batch loss 0.414643794 epoch total loss 0.522663713\n",
      "Trained batch 1541 batch loss 0.37666285 epoch total loss 0.522569\n",
      "Trained batch 1542 batch loss 0.467106521 epoch total loss 0.522533\n",
      "Trained batch 1543 batch loss 0.475103617 epoch total loss 0.522502303\n",
      "Trained batch 1544 batch loss 0.397494316 epoch total loss 0.5224213\n",
      "Trained batch 1545 batch loss 0.442060828 epoch total loss 0.522369325\n",
      "Trained batch 1546 batch loss 0.493635714 epoch total loss 0.522350729\n",
      "Trained batch 1547 batch loss 0.508832216 epoch total loss 0.522342\n",
      "Trained batch 1548 batch loss 0.389338702 epoch total loss 0.522256136\n",
      "Trained batch 1549 batch loss 0.505339563 epoch total loss 0.522245169\n",
      "Trained batch 1550 batch loss 0.423701495 epoch total loss 0.522181571\n",
      "Trained batch 1551 batch loss 0.484125853 epoch total loss 0.522157073\n",
      "Trained batch 1552 batch loss 0.487722665 epoch total loss 0.5221349\n",
      "Trained batch 1553 batch loss 0.514904141 epoch total loss 0.522130191\n",
      "Trained batch 1554 batch loss 0.46603477 epoch total loss 0.522094131\n",
      "Trained batch 1555 batch loss 0.486662179 epoch total loss 0.522071362\n",
      "Trained batch 1556 batch loss 0.570792913 epoch total loss 0.522102654\n",
      "Trained batch 1557 batch loss 0.562711596 epoch total loss 0.522128701\n",
      "Trained batch 1558 batch loss 0.548066 epoch total loss 0.522145391\n",
      "Trained batch 1559 batch loss 0.567266345 epoch total loss 0.522174299\n",
      "Trained batch 1560 batch loss 0.50712651 epoch total loss 0.522164702\n",
      "Trained batch 1561 batch loss 0.503824592 epoch total loss 0.52215296\n",
      "Trained batch 1562 batch loss 0.530721605 epoch total loss 0.522158444\n",
      "Trained batch 1563 batch loss 0.524841309 epoch total loss 0.522160113\n",
      "Trained batch 1564 batch loss 0.555920839 epoch total loss 0.52218169\n",
      "Trained batch 1565 batch loss 0.582644343 epoch total loss 0.522220373\n",
      "Trained batch 1566 batch loss 0.522849739 epoch total loss 0.522220731\n",
      "Trained batch 1567 batch loss 0.541531861 epoch total loss 0.522233069\n",
      "Trained batch 1568 batch loss 0.573820829 epoch total loss 0.522265911\n",
      "Trained batch 1569 batch loss 0.531368375 epoch total loss 0.522271752\n",
      "Trained batch 1570 batch loss 0.510445654 epoch total loss 0.522264183\n",
      "Trained batch 1571 batch loss 0.595671654 epoch total loss 0.522310913\n",
      "Trained batch 1572 batch loss 0.678441465 epoch total loss 0.522410214\n",
      "Trained batch 1573 batch loss 0.67635715 epoch total loss 0.522508085\n",
      "Trained batch 1574 batch loss 0.478495 epoch total loss 0.52248013\n",
      "Trained batch 1575 batch loss 0.498345882 epoch total loss 0.522464812\n",
      "Trained batch 1576 batch loss 0.572498858 epoch total loss 0.522496581\n",
      "Trained batch 1577 batch loss 0.55494523 epoch total loss 0.522517145\n",
      "Trained batch 1578 batch loss 0.624117 epoch total loss 0.522581518\n",
      "Trained batch 1579 batch loss 0.66755569 epoch total loss 0.522673368\n",
      "Trained batch 1580 batch loss 0.608608067 epoch total loss 0.522727728\n",
      "Trained batch 1581 batch loss 0.545772254 epoch total loss 0.522742271\n",
      "Trained batch 1582 batch loss 0.521273196 epoch total loss 0.522741377\n",
      "Trained batch 1583 batch loss 0.546688735 epoch total loss 0.522756517\n",
      "Trained batch 1584 batch loss 0.537604332 epoch total loss 0.522765875\n",
      "Trained batch 1585 batch loss 0.534936845 epoch total loss 0.522773564\n",
      "Trained batch 1586 batch loss 0.492023975 epoch total loss 0.522754133\n",
      "Trained batch 1587 batch loss 0.49152863 epoch total loss 0.522734463\n",
      "Trained batch 1588 batch loss 0.484522104 epoch total loss 0.522710383\n",
      "Trained batch 1589 batch loss 0.536249876 epoch total loss 0.522718906\n",
      "Trained batch 1590 batch loss 0.532673895 epoch total loss 0.522725165\n",
      "Trained batch 1591 batch loss 0.577997923 epoch total loss 0.522759914\n",
      "Trained batch 1592 batch loss 0.61397624 epoch total loss 0.522817194\n",
      "Trained batch 1593 batch loss 0.514841259 epoch total loss 0.522812188\n",
      "Trained batch 1594 batch loss 0.592483461 epoch total loss 0.522855878\n",
      "Trained batch 1595 batch loss 0.582155764 epoch total loss 0.522893071\n",
      "Trained batch 1596 batch loss 0.530141056 epoch total loss 0.522897601\n",
      "Trained batch 1597 batch loss 0.485480517 epoch total loss 0.522874177\n",
      "Trained batch 1598 batch loss 0.481720299 epoch total loss 0.522848427\n",
      "Trained batch 1599 batch loss 0.565347373 epoch total loss 0.522875\n",
      "Trained batch 1600 batch loss 0.480214298 epoch total loss 0.522848368\n",
      "Trained batch 1601 batch loss 0.591045499 epoch total loss 0.522891\n",
      "Trained batch 1602 batch loss 0.530211 epoch total loss 0.522895515\n",
      "Trained batch 1603 batch loss 0.568742514 epoch total loss 0.522924125\n",
      "Trained batch 1604 batch loss 0.506300926 epoch total loss 0.522913754\n",
      "Trained batch 1605 batch loss 0.583434463 epoch total loss 0.522951484\n",
      "Trained batch 1606 batch loss 0.522021174 epoch total loss 0.522950888\n",
      "Trained batch 1607 batch loss 0.632322073 epoch total loss 0.523018956\n",
      "Trained batch 1608 batch loss 0.510957301 epoch total loss 0.523011446\n",
      "Trained batch 1609 batch loss 0.480250269 epoch total loss 0.522984862\n",
      "Trained batch 1610 batch loss 0.562763333 epoch total loss 0.523009598\n",
      "Trained batch 1611 batch loss 0.617979646 epoch total loss 0.523068547\n",
      "Trained batch 1612 batch loss 0.571846426 epoch total loss 0.523098767\n",
      "Trained batch 1613 batch loss 0.484232903 epoch total loss 0.523074687\n",
      "Trained batch 1614 batch loss 0.480340511 epoch total loss 0.523048222\n",
      "Trained batch 1615 batch loss 0.598825276 epoch total loss 0.523095131\n",
      "Trained batch 1616 batch loss 0.595919549 epoch total loss 0.523140192\n",
      "Trained batch 1617 batch loss 0.536408782 epoch total loss 0.523148417\n",
      "Trained batch 1618 batch loss 0.545767248 epoch total loss 0.523162425\n",
      "Trained batch 1619 batch loss 0.512829244 epoch total loss 0.523156047\n",
      "Trained batch 1620 batch loss 0.541789114 epoch total loss 0.523167551\n",
      "Trained batch 1621 batch loss 0.455371 epoch total loss 0.523125708\n",
      "Trained batch 1622 batch loss 0.430368692 epoch total loss 0.523068547\n",
      "Trained batch 1623 batch loss 0.464591175 epoch total loss 0.523032546\n",
      "Trained batch 1624 batch loss 0.428401887 epoch total loss 0.522974253\n",
      "Trained batch 1625 batch loss 0.514372766 epoch total loss 0.522968948\n",
      "Trained batch 1626 batch loss 0.580388904 epoch total loss 0.523004234\n",
      "Trained batch 1627 batch loss 0.55084312 epoch total loss 0.52302134\n",
      "Trained batch 1628 batch loss 0.522633374 epoch total loss 0.523021102\n",
      "Trained batch 1629 batch loss 0.648929119 epoch total loss 0.523098409\n",
      "Trained batch 1630 batch loss 0.589420915 epoch total loss 0.523139119\n",
      "Trained batch 1631 batch loss 0.521841407 epoch total loss 0.523138285\n",
      "Trained batch 1632 batch loss 0.579564631 epoch total loss 0.523172915\n",
      "Trained batch 1633 batch loss 0.536899745 epoch total loss 0.523181319\n",
      "Trained batch 1634 batch loss 0.498260677 epoch total loss 0.52316612\n",
      "Trained batch 1635 batch loss 0.464057207 epoch total loss 0.52312994\n",
      "Trained batch 1636 batch loss 0.492284238 epoch total loss 0.523111105\n",
      "Trained batch 1637 batch loss 0.492787361 epoch total loss 0.523092568\n",
      "Trained batch 1638 batch loss 0.615008712 epoch total loss 0.523148656\n",
      "Trained batch 1639 batch loss 0.436985254 epoch total loss 0.523096144\n",
      "Trained batch 1640 batch loss 0.436240584 epoch total loss 0.523043156\n",
      "Trained batch 1641 batch loss 0.466974258 epoch total loss 0.523009\n",
      "Trained batch 1642 batch loss 0.549480855 epoch total loss 0.523025095\n",
      "Trained batch 1643 batch loss 0.600042164 epoch total loss 0.523072\n",
      "Trained batch 1644 batch loss 0.531881273 epoch total loss 0.523077369\n",
      "Trained batch 1645 batch loss 0.56512171 epoch total loss 0.52310288\n",
      "Trained batch 1646 batch loss 0.563327491 epoch total loss 0.523127377\n",
      "Trained batch 1647 batch loss 0.557405174 epoch total loss 0.523148179\n",
      "Trained batch 1648 batch loss 0.542396069 epoch total loss 0.523159862\n",
      "Trained batch 1649 batch loss 0.507893 epoch total loss 0.523150623\n",
      "Trained batch 1650 batch loss 0.568172038 epoch total loss 0.523177922\n",
      "Trained batch 1651 batch loss 0.514838338 epoch total loss 0.523172855\n",
      "Trained batch 1652 batch loss 0.490039557 epoch total loss 0.523152769\n",
      "Trained batch 1653 batch loss 0.463776439 epoch total loss 0.523116887\n",
      "Trained batch 1654 batch loss 0.424715221 epoch total loss 0.523057401\n",
      "Trained batch 1655 batch loss 0.411140263 epoch total loss 0.52298981\n",
      "Trained batch 1656 batch loss 0.626648247 epoch total loss 0.523052394\n",
      "Trained batch 1657 batch loss 0.581028819 epoch total loss 0.523087382\n",
      "Trained batch 1658 batch loss 0.507435262 epoch total loss 0.523077965\n",
      "Trained batch 1659 batch loss 0.540746331 epoch total loss 0.523088634\n",
      "Trained batch 1660 batch loss 0.647323489 epoch total loss 0.523163438\n",
      "Trained batch 1661 batch loss 0.521879613 epoch total loss 0.523162663\n",
      "Trained batch 1662 batch loss 0.532708049 epoch total loss 0.523168445\n",
      "Trained batch 1663 batch loss 0.56039834 epoch total loss 0.523190856\n",
      "Trained batch 1664 batch loss 0.542907536 epoch total loss 0.523202658\n",
      "Trained batch 1665 batch loss 0.514252126 epoch total loss 0.523197293\n",
      "Trained batch 1666 batch loss 0.527538598 epoch total loss 0.523199916\n",
      "Trained batch 1667 batch loss 0.547111809 epoch total loss 0.523214281\n",
      "Trained batch 1668 batch loss 0.594915152 epoch total loss 0.523257256\n",
      "Trained batch 1669 batch loss 0.548870444 epoch total loss 0.523272634\n",
      "Trained batch 1670 batch loss 0.6144315 epoch total loss 0.523327172\n",
      "Trained batch 1671 batch loss 0.478042513 epoch total loss 0.523300111\n",
      "Trained batch 1672 batch loss 0.434917867 epoch total loss 0.523247242\n",
      "Trained batch 1673 batch loss 0.505962491 epoch total loss 0.52323693\n",
      "Trained batch 1674 batch loss 0.518837929 epoch total loss 0.523234308\n",
      "Trained batch 1675 batch loss 0.470275521 epoch total loss 0.523202717\n",
      "Trained batch 1676 batch loss 0.480383277 epoch total loss 0.523177147\n",
      "Trained batch 1677 batch loss 0.502333343 epoch total loss 0.52316469\n",
      "Trained batch 1678 batch loss 0.446163595 epoch total loss 0.523118854\n",
      "Trained batch 1679 batch loss 0.456032693 epoch total loss 0.523078859\n",
      "Trained batch 1680 batch loss 0.449593067 epoch total loss 0.523035169\n",
      "Trained batch 1681 batch loss 0.453292131 epoch total loss 0.522993684\n",
      "Trained batch 1682 batch loss 0.531890035 epoch total loss 0.522998929\n",
      "Trained batch 1683 batch loss 0.52830863 epoch total loss 0.523002088\n",
      "Trained batch 1684 batch loss 0.44468379 epoch total loss 0.522955596\n",
      "Trained batch 1685 batch loss 0.43713212 epoch total loss 0.522904694\n",
      "Trained batch 1686 batch loss 0.480337858 epoch total loss 0.522879422\n",
      "Trained batch 1687 batch loss 0.476810962 epoch total loss 0.522852123\n",
      "Trained batch 1688 batch loss 0.471089602 epoch total loss 0.522821426\n",
      "Trained batch 1689 batch loss 0.452676743 epoch total loss 0.522779942\n",
      "Trained batch 1690 batch loss 0.458021045 epoch total loss 0.522741616\n",
      "Trained batch 1691 batch loss 0.478374094 epoch total loss 0.52271539\n",
      "Trained batch 1692 batch loss 0.40754956 epoch total loss 0.522647262\n",
      "Trained batch 1693 batch loss 0.438794374 epoch total loss 0.52259773\n",
      "Trained batch 1694 batch loss 0.423685 epoch total loss 0.522539377\n",
      "Trained batch 1695 batch loss 0.471048713 epoch total loss 0.522509038\n",
      "Trained batch 1696 batch loss 0.543103099 epoch total loss 0.522521138\n",
      "Trained batch 1697 batch loss 0.671791255 epoch total loss 0.522609115\n",
      "Trained batch 1698 batch loss 0.556509256 epoch total loss 0.522629082\n",
      "Trained batch 1699 batch loss 0.52557075 epoch total loss 0.522630811\n",
      "Trained batch 1700 batch loss 0.575259566 epoch total loss 0.522661805\n",
      "Trained batch 1701 batch loss 0.503705 epoch total loss 0.522650659\n",
      "Trained batch 1702 batch loss 0.569431365 epoch total loss 0.522678137\n",
      "Trained batch 1703 batch loss 0.51916945 epoch total loss 0.52267611\n",
      "Trained batch 1704 batch loss 0.57784164 epoch total loss 0.522708416\n",
      "Trained batch 1705 batch loss 0.507653475 epoch total loss 0.522699594\n",
      "Trained batch 1706 batch loss 0.544005454 epoch total loss 0.522712111\n",
      "Trained batch 1707 batch loss 0.453992367 epoch total loss 0.522671819\n",
      "Trained batch 1708 batch loss 0.496286929 epoch total loss 0.522656381\n",
      "Trained batch 1709 batch loss 0.389130592 epoch total loss 0.522578239\n",
      "Trained batch 1710 batch loss 0.473029345 epoch total loss 0.522549272\n",
      "Trained batch 1711 batch loss 0.437311769 epoch total loss 0.522499442\n",
      "Trained batch 1712 batch loss 0.499389172 epoch total loss 0.522486\n",
      "Trained batch 1713 batch loss 0.556048393 epoch total loss 0.522505522\n",
      "Trained batch 1714 batch loss 0.521522224 epoch total loss 0.522505\n",
      "Trained batch 1715 batch loss 0.50637728 epoch total loss 0.522495568\n",
      "Trained batch 1716 batch loss 0.48841393 epoch total loss 0.522475719\n",
      "Trained batch 1717 batch loss 0.545631945 epoch total loss 0.52248919\n",
      "Trained batch 1718 batch loss 0.61769259 epoch total loss 0.522544622\n",
      "Trained batch 1719 batch loss 0.64836216 epoch total loss 0.522617817\n",
      "Trained batch 1720 batch loss 0.556419134 epoch total loss 0.522637427\n",
      "Trained batch 1721 batch loss 0.508440793 epoch total loss 0.522629201\n",
      "Trained batch 1722 batch loss 0.569197893 epoch total loss 0.522656262\n",
      "Trained batch 1723 batch loss 0.613760591 epoch total loss 0.522709131\n",
      "Trained batch 1724 batch loss 0.56412977 epoch total loss 0.522733152\n",
      "Trained batch 1725 batch loss 0.572189569 epoch total loss 0.522761822\n",
      "Trained batch 1726 batch loss 0.577949286 epoch total loss 0.522793829\n",
      "Trained batch 1727 batch loss 0.62797451 epoch total loss 0.522854686\n",
      "Trained batch 1728 batch loss 0.62727952 epoch total loss 0.522915125\n",
      "Trained batch 1729 batch loss 0.587245 epoch total loss 0.522952318\n",
      "Trained batch 1730 batch loss 0.583717525 epoch total loss 0.522987485\n",
      "Trained batch 1731 batch loss 0.569140375 epoch total loss 0.523014128\n",
      "Trained batch 1732 batch loss 0.666559 epoch total loss 0.523097038\n",
      "Trained batch 1733 batch loss 0.530051589 epoch total loss 0.523101032\n",
      "Trained batch 1734 batch loss 0.558435678 epoch total loss 0.523121357\n",
      "Trained batch 1735 batch loss 0.531622708 epoch total loss 0.523126245\n",
      "Trained batch 1736 batch loss 0.555663526 epoch total loss 0.523145\n",
      "Trained batch 1737 batch loss 0.50215888 epoch total loss 0.52313292\n",
      "Trained batch 1738 batch loss 0.56014514 epoch total loss 0.523154199\n",
      "Trained batch 1739 batch loss 0.54478693 epoch total loss 0.523166656\n",
      "Trained batch 1740 batch loss 0.593833327 epoch total loss 0.523207247\n",
      "Trained batch 1741 batch loss 0.635701418 epoch total loss 0.523271859\n",
      "Trained batch 1742 batch loss 0.562487662 epoch total loss 0.523294389\n",
      "Trained batch 1743 batch loss 0.493009806 epoch total loss 0.523277\n",
      "Trained batch 1744 batch loss 0.615108907 epoch total loss 0.523329616\n",
      "Trained batch 1745 batch loss 0.511205196 epoch total loss 0.523322701\n",
      "Trained batch 1746 batch loss 0.531221271 epoch total loss 0.523327231\n",
      "Trained batch 1747 batch loss 0.518987536 epoch total loss 0.523324788\n",
      "Trained batch 1748 batch loss 0.521314383 epoch total loss 0.523323596\n",
      "Trained batch 1749 batch loss 0.558230698 epoch total loss 0.523343563\n",
      "Trained batch 1750 batch loss 0.583481431 epoch total loss 0.523377955\n",
      "Trained batch 1751 batch loss 0.463305354 epoch total loss 0.523343623\n",
      "Trained batch 1752 batch loss 0.451806456 epoch total loss 0.523302794\n",
      "Trained batch 1753 batch loss 0.509827733 epoch total loss 0.523295105\n",
      "Trained batch 1754 batch loss 0.553702474 epoch total loss 0.523312449\n",
      "Trained batch 1755 batch loss 0.459479958 epoch total loss 0.523276091\n",
      "Trained batch 1756 batch loss 0.485565037 epoch total loss 0.523254573\n",
      "Trained batch 1757 batch loss 0.540404916 epoch total loss 0.523264349\n",
      "Trained batch 1758 batch loss 0.364706725 epoch total loss 0.523174107\n",
      "Trained batch 1759 batch loss 0.550069571 epoch total loss 0.523189425\n",
      "Trained batch 1760 batch loss 0.460170537 epoch total loss 0.523153603\n",
      "Trained batch 1761 batch loss 0.439211756 epoch total loss 0.523105919\n",
      "Trained batch 1762 batch loss 0.425653547 epoch total loss 0.523050606\n",
      "Trained batch 1763 batch loss 0.396647215 epoch total loss 0.522978902\n",
      "Trained batch 1764 batch loss 0.408532619 epoch total loss 0.522914052\n",
      "Trained batch 1765 batch loss 0.471612543 epoch total loss 0.522884965\n",
      "Trained batch 1766 batch loss 0.616284251 epoch total loss 0.522937834\n",
      "Trained batch 1767 batch loss 0.591861129 epoch total loss 0.522976875\n",
      "Trained batch 1768 batch loss 0.625177145 epoch total loss 0.523034632\n",
      "Trained batch 1769 batch loss 0.657847762 epoch total loss 0.523110867\n",
      "Trained batch 1770 batch loss 0.569939435 epoch total loss 0.523137331\n",
      "Trained batch 1771 batch loss 0.727415681 epoch total loss 0.523252666\n",
      "Trained batch 1772 batch loss 0.59420532 epoch total loss 0.52329272\n",
      "Trained batch 1773 batch loss 0.545727491 epoch total loss 0.523305357\n",
      "Trained batch 1774 batch loss 0.50749743 epoch total loss 0.523296416\n",
      "Trained batch 1775 batch loss 0.598069072 epoch total loss 0.523338556\n",
      "Trained batch 1776 batch loss 0.593351662 epoch total loss 0.523377955\n",
      "Trained batch 1777 batch loss 0.646046579 epoch total loss 0.523447037\n",
      "Trained batch 1778 batch loss 0.625166297 epoch total loss 0.523504257\n",
      "Trained batch 1779 batch loss 0.489741862 epoch total loss 0.523485243\n",
      "Trained batch 1780 batch loss 0.515619934 epoch total loss 0.523480833\n",
      "Trained batch 1781 batch loss 0.460829049 epoch total loss 0.523445666\n",
      "Trained batch 1782 batch loss 0.493791342 epoch total loss 0.523429\n",
      "Trained batch 1783 batch loss 0.503244579 epoch total loss 0.523417652\n",
      "Trained batch 1784 batch loss 0.521037102 epoch total loss 0.52341634\n",
      "Trained batch 1785 batch loss 0.512107193 epoch total loss 0.52341\n",
      "Trained batch 1786 batch loss 0.472485453 epoch total loss 0.523381472\n",
      "Trained batch 1787 batch loss 0.510666251 epoch total loss 0.523374379\n",
      "Trained batch 1788 batch loss 0.507937551 epoch total loss 0.523365736\n",
      "Trained batch 1789 batch loss 0.525962949 epoch total loss 0.523367167\n",
      "Trained batch 1790 batch loss 0.581591 epoch total loss 0.523399711\n",
      "Trained batch 1791 batch loss 0.551799774 epoch total loss 0.523415565\n",
      "Trained batch 1792 batch loss 0.612388313 epoch total loss 0.523465216\n",
      "Trained batch 1793 batch loss 0.572394729 epoch total loss 0.523492515\n",
      "Trained batch 1794 batch loss 0.646936178 epoch total loss 0.523561299\n",
      "Trained batch 1795 batch loss 0.594587147 epoch total loss 0.523600876\n",
      "Trained batch 1796 batch loss 0.572615862 epoch total loss 0.523628175\n",
      "Trained batch 1797 batch loss 0.59452 epoch total loss 0.523667634\n",
      "Trained batch 1798 batch loss 0.524690032 epoch total loss 0.52366823\n",
      "Trained batch 1799 batch loss 0.474562228 epoch total loss 0.523640931\n",
      "Trained batch 1800 batch loss 0.505009711 epoch total loss 0.523630559\n",
      "Trained batch 1801 batch loss 0.583460331 epoch total loss 0.523663759\n",
      "Trained batch 1802 batch loss 0.480014652 epoch total loss 0.52363956\n",
      "Trained batch 1803 batch loss 0.366415143 epoch total loss 0.523552358\n",
      "Trained batch 1804 batch loss 0.504338562 epoch total loss 0.523541689\n",
      "Trained batch 1805 batch loss 0.468459159 epoch total loss 0.523511171\n",
      "Trained batch 1806 batch loss 0.475601524 epoch total loss 0.523484647\n",
      "Trained batch 1807 batch loss 0.552891 epoch total loss 0.523500919\n",
      "Trained batch 1808 batch loss 0.56466341 epoch total loss 0.523523688\n",
      "Trained batch 1809 batch loss 0.607637763 epoch total loss 0.52357018\n",
      "Trained batch 1810 batch loss 0.59920311 epoch total loss 0.523611963\n",
      "Trained batch 1811 batch loss 0.599785268 epoch total loss 0.523654044\n",
      "Trained batch 1812 batch loss 0.521665752 epoch total loss 0.523653\n",
      "Trained batch 1813 batch loss 0.525225103 epoch total loss 0.523653805\n",
      "Trained batch 1814 batch loss 0.490813762 epoch total loss 0.523635685\n",
      "Trained batch 1815 batch loss 0.490578413 epoch total loss 0.523617506\n",
      "Trained batch 1816 batch loss 0.538934767 epoch total loss 0.52362591\n",
      "Trained batch 1817 batch loss 0.523996651 epoch total loss 0.523626089\n",
      "Trained batch 1818 batch loss 0.520768464 epoch total loss 0.523624539\n",
      "Trained batch 1819 batch loss 0.591755569 epoch total loss 0.523662\n",
      "Trained batch 1820 batch loss 0.403586984 epoch total loss 0.523596\n",
      "Trained batch 1821 batch loss 0.47931847 epoch total loss 0.52357167\n",
      "Trained batch 1822 batch loss 0.479821354 epoch total loss 0.523547649\n",
      "Trained batch 1823 batch loss 0.513420165 epoch total loss 0.523542106\n",
      "Trained batch 1824 batch loss 0.578269601 epoch total loss 0.523572087\n",
      "Trained batch 1825 batch loss 0.544734657 epoch total loss 0.52358371\n",
      "Trained batch 1826 batch loss 0.567135 epoch total loss 0.523607552\n",
      "Trained batch 1827 batch loss 0.541901529 epoch total loss 0.523617566\n",
      "Trained batch 1828 batch loss 0.509914875 epoch total loss 0.523610055\n",
      "Trained batch 1829 batch loss 0.515097916 epoch total loss 0.523605406\n",
      "Trained batch 1830 batch loss 0.440539658 epoch total loss 0.52356\n",
      "Trained batch 1831 batch loss 0.475245595 epoch total loss 0.523533583\n",
      "Trained batch 1832 batch loss 0.48105821 epoch total loss 0.523510456\n",
      "Trained batch 1833 batch loss 0.542796731 epoch total loss 0.523520947\n",
      "Trained batch 1834 batch loss 0.495555669 epoch total loss 0.523505688\n",
      "Trained batch 1835 batch loss 0.527591765 epoch total loss 0.523507893\n",
      "Trained batch 1836 batch loss 0.541861475 epoch total loss 0.523517907\n",
      "Trained batch 1837 batch loss 0.55135721 epoch total loss 0.523533046\n",
      "Trained batch 1838 batch loss 0.564860821 epoch total loss 0.523555577\n",
      "Trained batch 1839 batch loss 0.557144582 epoch total loss 0.523573816\n",
      "Trained batch 1840 batch loss 0.52848 epoch total loss 0.523576498\n",
      "Trained batch 1841 batch loss 0.567747831 epoch total loss 0.523600459\n",
      "Trained batch 1842 batch loss 0.539949775 epoch total loss 0.5236094\n",
      "Trained batch 1843 batch loss 0.594823 epoch total loss 0.523648\n",
      "Trained batch 1844 batch loss 0.59535253 epoch total loss 0.523686886\n",
      "Trained batch 1845 batch loss 0.63347429 epoch total loss 0.523746431\n",
      "Trained batch 1846 batch loss 0.540967584 epoch total loss 0.523755729\n",
      "Trained batch 1847 batch loss 0.532022834 epoch total loss 0.5237602\n",
      "Trained batch 1848 batch loss 0.623191357 epoch total loss 0.523814\n",
      "Trained batch 1849 batch loss 0.613135874 epoch total loss 0.523862362\n",
      "Trained batch 1850 batch loss 0.627425849 epoch total loss 0.523918331\n",
      "Trained batch 1851 batch loss 0.582876444 epoch total loss 0.52395016\n",
      "Trained batch 1852 batch loss 0.470898718 epoch total loss 0.523921549\n",
      "Trained batch 1853 batch loss 0.516196728 epoch total loss 0.523917377\n",
      "Trained batch 1854 batch loss 0.454731286 epoch total loss 0.52388\n",
      "Trained batch 1855 batch loss 0.477193296 epoch total loss 0.523854852\n",
      "Trained batch 1856 batch loss 0.55078578 epoch total loss 0.523869336\n",
      "Trained batch 1857 batch loss 0.541942954 epoch total loss 0.523879051\n",
      "Trained batch 1858 batch loss 0.509190559 epoch total loss 0.523871183\n",
      "Trained batch 1859 batch loss 0.447472215 epoch total loss 0.523830056\n",
      "Trained batch 1860 batch loss 0.482825875 epoch total loss 0.523808062\n",
      "Trained batch 1861 batch loss 0.507783055 epoch total loss 0.523799419\n",
      "Trained batch 1862 batch loss 0.489037454 epoch total loss 0.523780763\n",
      "Trained batch 1863 batch loss 0.498485029 epoch total loss 0.523767173\n",
      "Trained batch 1864 batch loss 0.502434492 epoch total loss 0.523755729\n",
      "Trained batch 1865 batch loss 0.513065338 epoch total loss 0.52375\n",
      "Trained batch 1866 batch loss 0.593551934 epoch total loss 0.523787439\n",
      "Trained batch 1867 batch loss 0.63498491 epoch total loss 0.523847\n",
      "Trained batch 1868 batch loss 0.594909549 epoch total loss 0.523885\n",
      "Trained batch 1869 batch loss 0.492929786 epoch total loss 0.523868442\n",
      "Trained batch 1870 batch loss 0.51580596 epoch total loss 0.52386415\n",
      "Trained batch 1871 batch loss 0.563463 epoch total loss 0.52388531\n",
      "Trained batch 1872 batch loss 0.480507165 epoch total loss 0.523862183\n",
      "Trained batch 1873 batch loss 0.567319 epoch total loss 0.523885369\n",
      "Trained batch 1874 batch loss 0.605862498 epoch total loss 0.523929119\n",
      "Trained batch 1875 batch loss 0.5452528 epoch total loss 0.523940444\n",
      "Trained batch 1876 batch loss 0.556829393 epoch total loss 0.523957968\n",
      "Trained batch 1877 batch loss 0.525469661 epoch total loss 0.523958802\n",
      "Trained batch 1878 batch loss 0.558361 epoch total loss 0.523977101\n",
      "Trained batch 1879 batch loss 0.509068191 epoch total loss 0.523969173\n",
      "Trained batch 1880 batch loss 0.565229356 epoch total loss 0.523991108\n",
      "Trained batch 1881 batch loss 0.536457539 epoch total loss 0.523997724\n",
      "Trained batch 1882 batch loss 0.603114307 epoch total loss 0.524039805\n",
      "Trained batch 1883 batch loss 0.605033338 epoch total loss 0.52408278\n",
      "Trained batch 1884 batch loss 0.597474 epoch total loss 0.524121761\n",
      "Trained batch 1885 batch loss 0.529463112 epoch total loss 0.524124563\n",
      "Trained batch 1886 batch loss 0.687285423 epoch total loss 0.524211109\n",
      "Trained batch 1887 batch loss 0.639922559 epoch total loss 0.524272382\n",
      "Trained batch 1888 batch loss 0.580213249 epoch total loss 0.524302\n",
      "Trained batch 1889 batch loss 0.576459944 epoch total loss 0.524329662\n",
      "Trained batch 1890 batch loss 0.520039618 epoch total loss 0.524327338\n",
      "Trained batch 1891 batch loss 0.538794875 epoch total loss 0.524335\n",
      "Trained batch 1892 batch loss 0.477203846 epoch total loss 0.524310112\n",
      "Trained batch 1893 batch loss 0.528457582 epoch total loss 0.524312317\n",
      "Trained batch 1894 batch loss 0.594093919 epoch total loss 0.524349153\n",
      "Trained batch 1895 batch loss 0.531519949 epoch total loss 0.524352908\n",
      "Trained batch 1896 batch loss 0.491460353 epoch total loss 0.524335563\n",
      "Trained batch 1897 batch loss 0.571192265 epoch total loss 0.52436024\n",
      "Trained batch 1898 batch loss 0.599293 epoch total loss 0.524399757\n",
      "Trained batch 1899 batch loss 0.550548673 epoch total loss 0.524413526\n",
      "Trained batch 1900 batch loss 0.506602287 epoch total loss 0.524404109\n",
      "Trained batch 1901 batch loss 0.534929454 epoch total loss 0.524409652\n",
      "Trained batch 1902 batch loss 0.538868546 epoch total loss 0.524417281\n",
      "Trained batch 1903 batch loss 0.609248042 epoch total loss 0.524461865\n",
      "Trained batch 1904 batch loss 0.587331891 epoch total loss 0.524494886\n",
      "Trained batch 1905 batch loss 0.597508669 epoch total loss 0.524533212\n",
      "Trained batch 1906 batch loss 0.525115907 epoch total loss 0.52453351\n",
      "Trained batch 1907 batch loss 0.489247918 epoch total loss 0.524515033\n",
      "Trained batch 1908 batch loss 0.52109921 epoch total loss 0.524513245\n",
      "Trained batch 1909 batch loss 0.536622882 epoch total loss 0.524519563\n",
      "Trained batch 1910 batch loss 0.504555404 epoch total loss 0.524509132\n",
      "Trained batch 1911 batch loss 0.505909443 epoch total loss 0.524499416\n",
      "Trained batch 1912 batch loss 0.497374594 epoch total loss 0.52448523\n",
      "Trained batch 1913 batch loss 0.481752843 epoch total loss 0.524462879\n",
      "Trained batch 1914 batch loss 0.476965755 epoch total loss 0.524438083\n",
      "Trained batch 1915 batch loss 0.448878914 epoch total loss 0.524398625\n",
      "Trained batch 1916 batch loss 0.501332164 epoch total loss 0.524386585\n",
      "Trained batch 1917 batch loss 0.556117 epoch total loss 0.524403095\n",
      "Trained batch 1918 batch loss 0.589149415 epoch total loss 0.524436891\n",
      "Trained batch 1919 batch loss 0.47526136 epoch total loss 0.524411261\n",
      "Trained batch 1920 batch loss 0.561904728 epoch total loss 0.524430811\n",
      "Trained batch 1921 batch loss 0.593326747 epoch total loss 0.524466634\n",
      "Trained batch 1922 batch loss 0.563894272 epoch total loss 0.524487138\n",
      "Trained batch 1923 batch loss 0.536005795 epoch total loss 0.524493158\n",
      "Trained batch 1924 batch loss 0.5089 epoch total loss 0.524485052\n",
      "Trained batch 1925 batch loss 0.465550244 epoch total loss 0.524454474\n",
      "Trained batch 1926 batch loss 0.427118242 epoch total loss 0.52440393\n",
      "Trained batch 1927 batch loss 0.527804315 epoch total loss 0.524405718\n",
      "Trained batch 1928 batch loss 0.442774504 epoch total loss 0.524363339\n",
      "Trained batch 1929 batch loss 0.486718625 epoch total loss 0.524343848\n",
      "Trained batch 1930 batch loss 0.486517519 epoch total loss 0.524324238\n",
      "Trained batch 1931 batch loss 0.507985353 epoch total loss 0.524315774\n",
      "Trained batch 1932 batch loss 0.472532958 epoch total loss 0.524288952\n",
      "Trained batch 1933 batch loss 0.499058515 epoch total loss 0.524275899\n",
      "Trained batch 1934 batch loss 0.594033062 epoch total loss 0.524312\n",
      "Trained batch 1935 batch loss 0.639648378 epoch total loss 0.524371624\n",
      "Trained batch 1936 batch loss 0.461500973 epoch total loss 0.524339139\n",
      "Trained batch 1937 batch loss 0.626402438 epoch total loss 0.52439183\n",
      "Trained batch 1938 batch loss 0.597190142 epoch total loss 0.524429381\n",
      "Trained batch 1939 batch loss 0.557201743 epoch total loss 0.524446249\n",
      "Trained batch 1940 batch loss 0.499620855 epoch total loss 0.524433494\n",
      "Trained batch 1941 batch loss 0.52970475 epoch total loss 0.524436176\n",
      "Trained batch 1942 batch loss 0.520873189 epoch total loss 0.524434388\n",
      "Trained batch 1943 batch loss 0.509967387 epoch total loss 0.524426937\n",
      "Trained batch 1944 batch loss 0.540791154 epoch total loss 0.524435341\n",
      "Trained batch 1945 batch loss 0.500521183 epoch total loss 0.524423063\n",
      "Trained batch 1946 batch loss 0.475798696 epoch total loss 0.524398\n",
      "Trained batch 1947 batch loss 0.577679634 epoch total loss 0.524425387\n",
      "Trained batch 1948 batch loss 0.553575575 epoch total loss 0.524440408\n",
      "Trained batch 1949 batch loss 0.541073263 epoch total loss 0.524448931\n",
      "Trained batch 1950 batch loss 0.555768311 epoch total loss 0.524464965\n",
      "Trained batch 1951 batch loss 0.521141231 epoch total loss 0.524463296\n",
      "Trained batch 1952 batch loss 0.539116263 epoch total loss 0.524470806\n",
      "Trained batch 1953 batch loss 0.496570379 epoch total loss 0.524456501\n",
      "Trained batch 1954 batch loss 0.505140245 epoch total loss 0.524446607\n",
      "Trained batch 1955 batch loss 0.474566311 epoch total loss 0.524421096\n",
      "Trained batch 1956 batch loss 0.45824784 epoch total loss 0.5243873\n",
      "Trained batch 1957 batch loss 0.487769306 epoch total loss 0.524368584\n",
      "Trained batch 1958 batch loss 0.593974173 epoch total loss 0.524404168\n",
      "Trained batch 1959 batch loss 0.583212912 epoch total loss 0.524434209\n",
      "Trained batch 1960 batch loss 0.530141175 epoch total loss 0.524437129\n",
      "Trained batch 1961 batch loss 0.506208241 epoch total loss 0.524427831\n",
      "Trained batch 1962 batch loss 0.529690444 epoch total loss 0.524430513\n",
      "Trained batch 1963 batch loss 0.506900668 epoch total loss 0.524421573\n",
      "Trained batch 1964 batch loss 0.589701533 epoch total loss 0.524454832\n",
      "Trained batch 1965 batch loss 0.553382099 epoch total loss 0.524469554\n",
      "Trained batch 1966 batch loss 0.580994 epoch total loss 0.524498343\n",
      "Trained batch 1967 batch loss 0.546 epoch total loss 0.524509251\n",
      "Trained batch 1968 batch loss 0.676121056 epoch total loss 0.52458632\n",
      "Trained batch 1969 batch loss 0.607578337 epoch total loss 0.52462846\n",
      "Trained batch 1970 batch loss 0.620970726 epoch total loss 0.524677336\n",
      "Trained batch 1971 batch loss 0.625170052 epoch total loss 0.524728298\n",
      "Trained batch 1972 batch loss 0.620340228 epoch total loss 0.524776816\n",
      "Trained batch 1973 batch loss 0.585094035 epoch total loss 0.524807394\n",
      "Trained batch 1974 batch loss 0.566018283 epoch total loss 0.524828255\n",
      "Trained batch 1975 batch loss 0.496354371 epoch total loss 0.524813831\n",
      "Trained batch 1976 batch loss 0.478164196 epoch total loss 0.524790227\n",
      "Trained batch 1977 batch loss 0.541046321 epoch total loss 0.524798453\n",
      "Trained batch 1978 batch loss 0.526861429 epoch total loss 0.524799466\n",
      "Trained batch 1979 batch loss 0.64906925 epoch total loss 0.52486223\n",
      "Trained batch 1980 batch loss 0.646656513 epoch total loss 0.524923742\n",
      "Trained batch 1981 batch loss 0.549313545 epoch total loss 0.52493608\n",
      "Trained batch 1982 batch loss 0.496356487 epoch total loss 0.524921596\n",
      "Trained batch 1983 batch loss 0.508554637 epoch total loss 0.524913371\n",
      "Trained batch 1984 batch loss 0.559692204 epoch total loss 0.524930894\n",
      "Trained batch 1985 batch loss 0.422275 epoch total loss 0.524879158\n",
      "Trained batch 1986 batch loss 0.487213552 epoch total loss 0.524860203\n",
      "Trained batch 1987 batch loss 0.490813464 epoch total loss 0.524843037\n",
      "Trained batch 1988 batch loss 0.507170796 epoch total loss 0.524834156\n",
      "Trained batch 1989 batch loss 0.448165894 epoch total loss 0.524795592\n",
      "Trained batch 1990 batch loss 0.449243248 epoch total loss 0.524757624\n",
      "Trained batch 1991 batch loss 0.51295954 epoch total loss 0.524751723\n",
      "Trained batch 1992 batch loss 0.509105802 epoch total loss 0.524743855\n",
      "Trained batch 1993 batch loss 0.458842158 epoch total loss 0.524710834\n",
      "Trained batch 1994 batch loss 0.530325055 epoch total loss 0.524713635\n",
      "Trained batch 1995 batch loss 0.468682528 epoch total loss 0.524685502\n",
      "Trained batch 1996 batch loss 0.593773723 epoch total loss 0.524720073\n",
      "Trained batch 1997 batch loss 0.535301805 epoch total loss 0.524725378\n",
      "Trained batch 1998 batch loss 0.530821443 epoch total loss 0.524728417\n",
      "Trained batch 1999 batch loss 0.536625743 epoch total loss 0.524734378\n",
      "Trained batch 2000 batch loss 0.486647904 epoch total loss 0.524715304\n",
      "Trained batch 2001 batch loss 0.571335614 epoch total loss 0.52473861\n",
      "Trained batch 2002 batch loss 0.525775 epoch total loss 0.524739087\n",
      "Trained batch 2003 batch loss 0.607167721 epoch total loss 0.524780273\n",
      "Trained batch 2004 batch loss 0.504137635 epoch total loss 0.524769962\n",
      "Trained batch 2005 batch loss 0.550571799 epoch total loss 0.524782836\n",
      "Trained batch 2006 batch loss 0.540240288 epoch total loss 0.524790585\n",
      "Trained batch 2007 batch loss 0.563766 epoch total loss 0.524809957\n",
      "Trained batch 2008 batch loss 0.66648972 epoch total loss 0.524880528\n",
      "Trained batch 2009 batch loss 0.574695826 epoch total loss 0.524905324\n",
      "Trained batch 2010 batch loss 0.669617653 epoch total loss 0.524977326\n",
      "Trained batch 2011 batch loss 0.49950704 epoch total loss 0.52496469\n",
      "Trained batch 2012 batch loss 0.46257183 epoch total loss 0.524933636\n",
      "Trained batch 2013 batch loss 0.59079504 epoch total loss 0.524966359\n",
      "Trained batch 2014 batch loss 0.509351552 epoch total loss 0.52495867\n",
      "Trained batch 2015 batch loss 0.540394068 epoch total loss 0.5249663\n",
      "Trained batch 2016 batch loss 0.579454422 epoch total loss 0.52499336\n",
      "Trained batch 2017 batch loss 0.569732547 epoch total loss 0.525015533\n",
      "Trained batch 2018 batch loss 0.523916364 epoch total loss 0.525015\n",
      "Trained batch 2019 batch loss 0.515068531 epoch total loss 0.525010049\n",
      "Trained batch 2020 batch loss 0.592492878 epoch total loss 0.525043428\n",
      "Trained batch 2021 batch loss 0.551596761 epoch total loss 0.525056601\n",
      "Trained batch 2022 batch loss 0.557504535 epoch total loss 0.525072634\n",
      "Trained batch 2023 batch loss 0.566666782 epoch total loss 0.525093198\n",
      "Trained batch 2024 batch loss 0.501236379 epoch total loss 0.525081396\n",
      "Trained batch 2025 batch loss 0.589940131 epoch total loss 0.525113463\n",
      "Trained batch 2026 batch loss 0.568537533 epoch total loss 0.525134861\n",
      "Trained batch 2027 batch loss 0.59367764 epoch total loss 0.525168657\n",
      "Trained batch 2028 batch loss 0.571872056 epoch total loss 0.525191665\n",
      "Trained batch 2029 batch loss 0.693599105 epoch total loss 0.525274694\n",
      "Trained batch 2030 batch loss 0.578198791 epoch total loss 0.525300801\n",
      "Trained batch 2031 batch loss 0.515561759 epoch total loss 0.525296\n",
      "Trained batch 2032 batch loss 0.550245523 epoch total loss 0.525308251\n",
      "Trained batch 2033 batch loss 0.618211508 epoch total loss 0.525353909\n",
      "Trained batch 2034 batch loss 0.661951542 epoch total loss 0.525421083\n",
      "Trained batch 2035 batch loss 0.539824247 epoch total loss 0.525428176\n",
      "Trained batch 2036 batch loss 0.593556 epoch total loss 0.525461614\n",
      "Trained batch 2037 batch loss 0.574242771 epoch total loss 0.525485575\n",
      "Trained batch 2038 batch loss 0.503321946 epoch total loss 0.525474668\n",
      "Trained batch 2039 batch loss 0.569852591 epoch total loss 0.525496423\n",
      "Trained batch 2040 batch loss 0.524743915 epoch total loss 0.525496066\n",
      "Trained batch 2041 batch loss 0.539855242 epoch total loss 0.525503039\n",
      "Trained batch 2042 batch loss 0.609535098 epoch total loss 0.525544226\n",
      "Trained batch 2043 batch loss 0.545798063 epoch total loss 0.525554121\n",
      "Trained batch 2044 batch loss 0.523162842 epoch total loss 0.525552928\n",
      "Trained batch 2045 batch loss 0.537228763 epoch total loss 0.52555865\n",
      "Trained batch 2046 batch loss 0.501202047 epoch total loss 0.525546789\n",
      "Trained batch 2047 batch loss 0.591422737 epoch total loss 0.525579\n",
      "Trained batch 2048 batch loss 0.533696413 epoch total loss 0.52558291\n",
      "Trained batch 2049 batch loss 0.586987615 epoch total loss 0.525612891\n",
      "Trained batch 2050 batch loss 0.539147079 epoch total loss 0.525619507\n",
      "Trained batch 2051 batch loss 0.522958517 epoch total loss 0.525618196\n",
      "Trained batch 2052 batch loss 0.525534332 epoch total loss 0.525618196\n",
      "Trained batch 2053 batch loss 0.462973505 epoch total loss 0.525587678\n",
      "Trained batch 2054 batch loss 0.554992676 epoch total loss 0.525602\n",
      "Trained batch 2055 batch loss 0.537312567 epoch total loss 0.525607705\n",
      "Trained batch 2056 batch loss 0.495586425 epoch total loss 0.525593102\n",
      "Trained batch 2057 batch loss 0.449009567 epoch total loss 0.525555849\n",
      "Trained batch 2058 batch loss 0.479814649 epoch total loss 0.525533617\n",
      "Trained batch 2059 batch loss 0.497047 epoch total loss 0.525519788\n",
      "Trained batch 2060 batch loss 0.47445333 epoch total loss 0.525495052\n",
      "Trained batch 2061 batch loss 0.477173716 epoch total loss 0.525471568\n",
      "Trained batch 2062 batch loss 0.523803 epoch total loss 0.525470793\n",
      "Trained batch 2063 batch loss 0.452769488 epoch total loss 0.525435507\n",
      "Trained batch 2064 batch loss 0.605533838 epoch total loss 0.52547437\n",
      "Trained batch 2065 batch loss 0.573285103 epoch total loss 0.525497496\n",
      "Trained batch 2066 batch loss 0.617219865 epoch total loss 0.525541902\n",
      "Trained batch 2067 batch loss 0.64738065 epoch total loss 0.525600791\n",
      "Trained batch 2068 batch loss 0.599767804 epoch total loss 0.525636673\n",
      "Trained batch 2069 batch loss 0.632221341 epoch total loss 0.525688171\n",
      "Trained batch 2070 batch loss 0.638571858 epoch total loss 0.52574271\n",
      "Trained batch 2071 batch loss 0.576921582 epoch total loss 0.525767386\n",
      "Trained batch 2072 batch loss 0.604146719 epoch total loss 0.525805175\n",
      "Trained batch 2073 batch loss 0.605967402 epoch total loss 0.525843859\n",
      "Trained batch 2074 batch loss 0.486492932 epoch total loss 0.525824904\n",
      "Trained batch 2075 batch loss 0.428098977 epoch total loss 0.525777757\n",
      "Trained batch 2076 batch loss 0.493638068 epoch total loss 0.52576232\n",
      "Trained batch 2077 batch loss 0.428605914 epoch total loss 0.52571553\n",
      "Trained batch 2078 batch loss 0.408609033 epoch total loss 0.525659144\n",
      "Trained batch 2079 batch loss 0.406121641 epoch total loss 0.525601625\n",
      "Trained batch 2080 batch loss 0.413610369 epoch total loss 0.525547802\n",
      "Trained batch 2081 batch loss 0.427156746 epoch total loss 0.525500476\n",
      "Trained batch 2082 batch loss 0.458751 epoch total loss 0.525468409\n",
      "Trained batch 2083 batch loss 0.484954536 epoch total loss 0.525449\n",
      "Trained batch 2084 batch loss 0.557645 epoch total loss 0.525464416\n",
      "Trained batch 2085 batch loss 0.568255126 epoch total loss 0.52548492\n",
      "Trained batch 2086 batch loss 0.545758486 epoch total loss 0.525494695\n",
      "Trained batch 2087 batch loss 0.59814775 epoch total loss 0.525529504\n",
      "Trained batch 2088 batch loss 0.600487769 epoch total loss 0.525565386\n",
      "Trained batch 2089 batch loss 0.555403769 epoch total loss 0.525579691\n",
      "Trained batch 2090 batch loss 0.534078658 epoch total loss 0.525583744\n",
      "Trained batch 2091 batch loss 0.467822731 epoch total loss 0.525556087\n",
      "Trained batch 2092 batch loss 0.474043876 epoch total loss 0.525531411\n",
      "Trained batch 2093 batch loss 0.529630065 epoch total loss 0.525533378\n",
      "Trained batch 2094 batch loss 0.485512793 epoch total loss 0.525514245\n",
      "Trained batch 2095 batch loss 0.537339032 epoch total loss 0.525519907\n",
      "Trained batch 2096 batch loss 0.481761098 epoch total loss 0.525499046\n",
      "Trained batch 2097 batch loss 0.437044621 epoch total loss 0.525456846\n",
      "Trained batch 2098 batch loss 0.514498413 epoch total loss 0.52545166\n",
      "Trained batch 2099 batch loss 0.51831615 epoch total loss 0.525448263\n",
      "Trained batch 2100 batch loss 0.506978631 epoch total loss 0.525439441\n",
      "Trained batch 2101 batch loss 0.551479936 epoch total loss 0.525451839\n",
      "Trained batch 2102 batch loss 0.487056553 epoch total loss 0.5254336\n",
      "Trained batch 2103 batch loss 0.438293308 epoch total loss 0.525392115\n",
      "Trained batch 2104 batch loss 0.517787576 epoch total loss 0.525388539\n",
      "Trained batch 2105 batch loss 0.489944339 epoch total loss 0.52537173\n",
      "Trained batch 2106 batch loss 0.496567547 epoch total loss 0.525358\n",
      "Trained batch 2107 batch loss 0.453998059 epoch total loss 0.525324166\n",
      "Trained batch 2108 batch loss 0.461056769 epoch total loss 0.525293708\n",
      "Trained batch 2109 batch loss 0.416868508 epoch total loss 0.525242269\n",
      "Trained batch 2110 batch loss 0.549493909 epoch total loss 0.525253773\n",
      "Trained batch 2111 batch loss 0.537138402 epoch total loss 0.525259376\n",
      "Trained batch 2112 batch loss 0.495289981 epoch total loss 0.52524513\n",
      "Trained batch 2113 batch loss 0.517941713 epoch total loss 0.525241673\n",
      "Trained batch 2114 batch loss 0.496283442 epoch total loss 0.525228\n",
      "Trained batch 2115 batch loss 0.518684149 epoch total loss 0.525224924\n",
      "Trained batch 2116 batch loss 0.469208211 epoch total loss 0.52519846\n",
      "Trained batch 2117 batch loss 0.495827615 epoch total loss 0.525184572\n",
      "Trained batch 2118 batch loss 0.507194757 epoch total loss 0.525176108\n",
      "Trained batch 2119 batch loss 0.469650596 epoch total loss 0.525149882\n",
      "Trained batch 2120 batch loss 0.54257679 epoch total loss 0.525158107\n",
      "Trained batch 2121 batch loss 0.444723904 epoch total loss 0.525120199\n",
      "Trained batch 2122 batch loss 0.534984112 epoch total loss 0.525124848\n",
      "Trained batch 2123 batch loss 0.557196558 epoch total loss 0.52514\n",
      "Trained batch 2124 batch loss 0.556694329 epoch total loss 0.525154829\n",
      "Trained batch 2125 batch loss 0.552073 epoch total loss 0.525167525\n",
      "Trained batch 2126 batch loss 0.643587053 epoch total loss 0.525223196\n",
      "Trained batch 2127 batch loss 0.656139433 epoch total loss 0.525284767\n",
      "Trained batch 2128 batch loss 0.594536901 epoch total loss 0.525317252\n",
      "Trained batch 2129 batch loss 0.566791356 epoch total loss 0.525336742\n",
      "Trained batch 2130 batch loss 0.618102 epoch total loss 0.525380254\n",
      "Trained batch 2131 batch loss 0.544270873 epoch total loss 0.525389135\n",
      "Trained batch 2132 batch loss 0.520838559 epoch total loss 0.525387\n",
      "Trained batch 2133 batch loss 0.671200871 epoch total loss 0.525455356\n",
      "Trained batch 2134 batch loss 0.615323186 epoch total loss 0.525497496\n",
      "Trained batch 2135 batch loss 0.529519439 epoch total loss 0.525499344\n",
      "Trained batch 2136 batch loss 0.580207169 epoch total loss 0.525525\n",
      "Trained batch 2137 batch loss 0.633520722 epoch total loss 0.525575519\n",
      "Trained batch 2138 batch loss 0.527548552 epoch total loss 0.525576472\n",
      "Trained batch 2139 batch loss 0.463433862 epoch total loss 0.525547385\n",
      "Trained batch 2140 batch loss 0.532193065 epoch total loss 0.525550485\n",
      "Trained batch 2141 batch loss 0.563312173 epoch total loss 0.525568187\n",
      "Trained batch 2142 batch loss 0.477763087 epoch total loss 0.525545835\n",
      "Trained batch 2143 batch loss 0.597270608 epoch total loss 0.525579333\n",
      "Trained batch 2144 batch loss 0.48555249 epoch total loss 0.525560677\n",
      "Trained batch 2145 batch loss 0.489916921 epoch total loss 0.525544047\n",
      "Trained batch 2146 batch loss 0.624185741 epoch total loss 0.52559\n",
      "Trained batch 2147 batch loss 0.555442333 epoch total loss 0.52560389\n",
      "Trained batch 2148 batch loss 0.52000916 epoch total loss 0.525601268\n",
      "Trained batch 2149 batch loss 0.540318251 epoch total loss 0.525608122\n",
      "Trained batch 2150 batch loss 0.577841341 epoch total loss 0.525632441\n",
      "Trained batch 2151 batch loss 0.620735228 epoch total loss 0.525676668\n",
      "Trained batch 2152 batch loss 0.561212301 epoch total loss 0.525693119\n",
      "Trained batch 2153 batch loss 0.498101264 epoch total loss 0.525680304\n",
      "Trained batch 2154 batch loss 0.552494705 epoch total loss 0.525692761\n",
      "Trained batch 2155 batch loss 0.573261738 epoch total loss 0.525714815\n",
      "Trained batch 2156 batch loss 0.585903883 epoch total loss 0.52574271\n",
      "Trained batch 2157 batch loss 0.505493045 epoch total loss 0.525733352\n",
      "Trained batch 2158 batch loss 0.482042521 epoch total loss 0.525713086\n",
      "Trained batch 2159 batch loss 0.414288342 epoch total loss 0.525661528\n",
      "Trained batch 2160 batch loss 0.435208082 epoch total loss 0.525619626\n",
      "Trained batch 2161 batch loss 0.513127923 epoch total loss 0.525613844\n",
      "Trained batch 2162 batch loss 0.430896252 epoch total loss 0.525570035\n",
      "Trained batch 2163 batch loss 0.513501763 epoch total loss 0.525564492\n",
      "Trained batch 2164 batch loss 0.532905281 epoch total loss 0.525567949\n",
      "Trained batch 2165 batch loss 0.671481311 epoch total loss 0.525635302\n",
      "Trained batch 2166 batch loss 0.599812806 epoch total loss 0.525669575\n",
      "Trained batch 2167 batch loss 0.466683537 epoch total loss 0.525642395\n",
      "Trained batch 2168 batch loss 0.567407966 epoch total loss 0.525661647\n",
      "Trained batch 2169 batch loss 0.52667731 epoch total loss 0.525662124\n",
      "Trained batch 2170 batch loss 0.657400429 epoch total loss 0.525722802\n",
      "Trained batch 2171 batch loss 0.657374799 epoch total loss 0.52578342\n",
      "Trained batch 2172 batch loss 0.579790771 epoch total loss 0.525808334\n",
      "Trained batch 2173 batch loss 0.623113275 epoch total loss 0.525853097\n",
      "Trained batch 2174 batch loss 0.530991197 epoch total loss 0.525855482\n",
      "Trained batch 2175 batch loss 0.560949147 epoch total loss 0.525871634\n",
      "Trained batch 2176 batch loss 0.579586804 epoch total loss 0.525896311\n",
      "Trained batch 2177 batch loss 0.484340787 epoch total loss 0.525877237\n",
      "Trained batch 2178 batch loss 0.521134794 epoch total loss 0.525875032\n",
      "Trained batch 2179 batch loss 0.543039 epoch total loss 0.525882959\n",
      "Trained batch 2180 batch loss 0.526417136 epoch total loss 0.525883138\n",
      "Trained batch 2181 batch loss 0.589783 epoch total loss 0.525912464\n",
      "Trained batch 2182 batch loss 0.585877895 epoch total loss 0.52594\n",
      "Trained batch 2183 batch loss 0.43922776 epoch total loss 0.525900245\n",
      "Trained batch 2184 batch loss 0.53426075 epoch total loss 0.525904119\n",
      "Trained batch 2185 batch loss 0.438301206 epoch total loss 0.525864065\n",
      "Trained batch 2186 batch loss 0.407502323 epoch total loss 0.525809884\n",
      "Trained batch 2187 batch loss 0.444482237 epoch total loss 0.525772691\n",
      "Trained batch 2188 batch loss 0.45934236 epoch total loss 0.525742352\n",
      "Trained batch 2189 batch loss 0.505871534 epoch total loss 0.525733232\n",
      "Trained batch 2190 batch loss 0.481815398 epoch total loss 0.525713205\n",
      "Trained batch 2191 batch loss 0.464439601 epoch total loss 0.525685251\n",
      "Trained batch 2192 batch loss 0.46048674 epoch total loss 0.525655508\n",
      "Trained batch 2193 batch loss 0.474548936 epoch total loss 0.525632203\n",
      "Trained batch 2194 batch loss 0.453299642 epoch total loss 0.525599241\n",
      "Trained batch 2195 batch loss 0.454562306 epoch total loss 0.525566876\n",
      "Trained batch 2196 batch loss 0.469516426 epoch total loss 0.525541306\n",
      "Trained batch 2197 batch loss 0.568763435 epoch total loss 0.525561\n",
      "Trained batch 2198 batch loss 0.520728171 epoch total loss 0.52555877\n",
      "Trained batch 2199 batch loss 0.539444 epoch total loss 0.525565088\n",
      "Trained batch 2200 batch loss 0.591547728 epoch total loss 0.525595069\n",
      "Trained batch 2201 batch loss 0.583188713 epoch total loss 0.525621235\n",
      "Trained batch 2202 batch loss 0.425960541 epoch total loss 0.525575936\n",
      "Trained batch 2203 batch loss 0.472545207 epoch total loss 0.525551856\n",
      "Trained batch 2204 batch loss 0.492503583 epoch total loss 0.525536895\n",
      "Trained batch 2205 batch loss 0.562134206 epoch total loss 0.525553524\n",
      "Trained batch 2206 batch loss 0.568190575 epoch total loss 0.525572836\n",
      "Trained batch 2207 batch loss 0.665269792 epoch total loss 0.525636137\n",
      "Trained batch 2208 batch loss 0.562521636 epoch total loss 0.525652826\n",
      "Trained batch 2209 batch loss 0.560002506 epoch total loss 0.525668442\n",
      "Trained batch 2210 batch loss 0.60785991 epoch total loss 0.525705636\n",
      "Trained batch 2211 batch loss 0.533157587 epoch total loss 0.525709033\n",
      "Trained batch 2212 batch loss 0.46736151 epoch total loss 0.525682688\n",
      "Trained batch 2213 batch loss 0.640859485 epoch total loss 0.525734723\n",
      "Trained batch 2214 batch loss 0.578119218 epoch total loss 0.525758386\n",
      "Trained batch 2215 batch loss 0.587407112 epoch total loss 0.525786221\n",
      "Trained batch 2216 batch loss 0.532087326 epoch total loss 0.525789082\n",
      "Trained batch 2217 batch loss 0.514312506 epoch total loss 0.525783896\n",
      "Trained batch 2218 batch loss 0.443031073 epoch total loss 0.525746524\n",
      "Trained batch 2219 batch loss 0.501598179 epoch total loss 0.525735676\n",
      "Trained batch 2220 batch loss 0.352612197 epoch total loss 0.525657713\n",
      "Trained batch 2221 batch loss 0.476778448 epoch total loss 0.525635719\n",
      "Trained batch 2222 batch loss 0.51161921 epoch total loss 0.525629401\n",
      "Trained batch 2223 batch loss 0.482312918 epoch total loss 0.52560991\n",
      "Trained batch 2224 batch loss 0.467830062 epoch total loss 0.525583863\n",
      "Trained batch 2225 batch loss 0.443471402 epoch total loss 0.525546968\n",
      "Trained batch 2226 batch loss 0.487510145 epoch total loss 0.525529921\n",
      "Trained batch 2227 batch loss 0.44622609 epoch total loss 0.525494277\n",
      "Trained batch 2228 batch loss 0.46887809 epoch total loss 0.525468886\n",
      "Trained batch 2229 batch loss 0.523602068 epoch total loss 0.525468\n",
      "Trained batch 2230 batch loss 0.548482239 epoch total loss 0.525478303\n",
      "Trained batch 2231 batch loss 0.497101367 epoch total loss 0.525465608\n",
      "Trained batch 2232 batch loss 0.434786469 epoch total loss 0.525424957\n",
      "Trained batch 2233 batch loss 0.506669164 epoch total loss 0.525416613\n",
      "Trained batch 2234 batch loss 0.524682164 epoch total loss 0.525416255\n",
      "Trained batch 2235 batch loss 0.509870291 epoch total loss 0.525409281\n",
      "Trained batch 2236 batch loss 0.5933038 epoch total loss 0.52543962\n",
      "Trained batch 2237 batch loss 0.492194742 epoch total loss 0.525424778\n",
      "Trained batch 2238 batch loss 0.496205211 epoch total loss 0.525411725\n",
      "Trained batch 2239 batch loss 0.633135 epoch total loss 0.525459886\n",
      "Trained batch 2240 batch loss 0.684693 epoch total loss 0.525530934\n",
      "Trained batch 2241 batch loss 0.736076772 epoch total loss 0.525624931\n",
      "Trained batch 2242 batch loss 0.615606666 epoch total loss 0.525665045\n",
      "Trained batch 2243 batch loss 0.634778 epoch total loss 0.525713682\n",
      "Trained batch 2244 batch loss 0.60288322 epoch total loss 0.525748074\n",
      "Trained batch 2245 batch loss 0.539673865 epoch total loss 0.525754273\n",
      "Trained batch 2246 batch loss 0.461650133 epoch total loss 0.525725722\n",
      "Trained batch 2247 batch loss 0.431195796 epoch total loss 0.525683641\n",
      "Trained batch 2248 batch loss 0.420110047 epoch total loss 0.525636733\n",
      "Trained batch 2249 batch loss 0.545091152 epoch total loss 0.525645375\n",
      "Trained batch 2250 batch loss 0.610620141 epoch total loss 0.525683105\n",
      "Trained batch 2251 batch loss 0.599311411 epoch total loss 0.525715828\n",
      "Trained batch 2252 batch loss 0.630491316 epoch total loss 0.525762379\n",
      "Trained batch 2253 batch loss 0.596051157 epoch total loss 0.525793552\n",
      "Trained batch 2254 batch loss 0.577201843 epoch total loss 0.525816381\n",
      "Trained batch 2255 batch loss 0.535602272 epoch total loss 0.525820732\n",
      "Trained batch 2256 batch loss 0.518051624 epoch total loss 0.525817275\n",
      "Trained batch 2257 batch loss 0.565797 epoch total loss 0.525835\n",
      "Trained batch 2258 batch loss 0.612184882 epoch total loss 0.525873244\n",
      "Trained batch 2259 batch loss 0.552896261 epoch total loss 0.525885165\n",
      "Trained batch 2260 batch loss 0.484103203 epoch total loss 0.525866687\n",
      "Trained batch 2261 batch loss 0.568082571 epoch total loss 0.525885403\n",
      "Trained batch 2262 batch loss 0.581782818 epoch total loss 0.525910079\n",
      "Trained batch 2263 batch loss 0.593135893 epoch total loss 0.525939822\n",
      "Trained batch 2264 batch loss 0.552376568 epoch total loss 0.525951505\n",
      "Trained batch 2265 batch loss 0.575980127 epoch total loss 0.525973558\n",
      "Trained batch 2266 batch loss 0.596762657 epoch total loss 0.526004791\n",
      "Trained batch 2267 batch loss 0.508982837 epoch total loss 0.525997341\n",
      "Trained batch 2268 batch loss 0.505235255 epoch total loss 0.525988162\n",
      "Trained batch 2269 batch loss 0.589382231 epoch total loss 0.526016116\n",
      "Trained batch 2270 batch loss 0.517854214 epoch total loss 0.52601248\n",
      "Trained batch 2271 batch loss 0.542299807 epoch total loss 0.526019692\n",
      "Trained batch 2272 batch loss 0.524949968 epoch total loss 0.526019216\n",
      "Trained batch 2273 batch loss 0.46964696 epoch total loss 0.52599436\n",
      "Trained batch 2274 batch loss 0.487408787 epoch total loss 0.525977433\n",
      "Trained batch 2275 batch loss 0.482036263 epoch total loss 0.525958121\n",
      "Trained batch 2276 batch loss 0.478119135 epoch total loss 0.52593708\n",
      "Trained batch 2277 batch loss 0.480089724 epoch total loss 0.525917\n",
      "Trained batch 2278 batch loss 0.527728319 epoch total loss 0.525917768\n",
      "Trained batch 2279 batch loss 0.481842726 epoch total loss 0.525898397\n",
      "Trained batch 2280 batch loss 0.511323929 epoch total loss 0.525892\n",
      "Trained batch 2281 batch loss 0.483635426 epoch total loss 0.525873482\n",
      "Trained batch 2282 batch loss 0.498890638 epoch total loss 0.525861681\n",
      "Trained batch 2283 batch loss 0.466470927 epoch total loss 0.525835633\n",
      "Trained batch 2284 batch loss 0.48204267 epoch total loss 0.5258165\n",
      "Trained batch 2285 batch loss 0.43919152 epoch total loss 0.525778592\n",
      "Trained batch 2286 batch loss 0.463097245 epoch total loss 0.525751173\n",
      "Trained batch 2287 batch loss 0.379454851 epoch total loss 0.525687158\n",
      "Trained batch 2288 batch loss 0.441029757 epoch total loss 0.525650203\n",
      "Trained batch 2289 batch loss 0.450121582 epoch total loss 0.525617182\n",
      "Trained batch 2290 batch loss 0.562492788 epoch total loss 0.525633276\n",
      "Trained batch 2291 batch loss 0.572623849 epoch total loss 0.52565378\n",
      "Trained batch 2292 batch loss 0.503198445 epoch total loss 0.525644\n",
      "Trained batch 2293 batch loss 0.57644546 epoch total loss 0.525666118\n",
      "Trained batch 2294 batch loss 0.507127285 epoch total loss 0.525658\n",
      "Trained batch 2295 batch loss 0.53487 epoch total loss 0.525662065\n",
      "Trained batch 2296 batch loss 0.572348654 epoch total loss 0.52568239\n",
      "Trained batch 2297 batch loss 0.540400267 epoch total loss 0.525688827\n",
      "Trained batch 2298 batch loss 0.514014542 epoch total loss 0.525683761\n",
      "Trained batch 2299 batch loss 0.469988227 epoch total loss 0.525659502\n",
      "Trained batch 2300 batch loss 0.514732599 epoch total loss 0.525654793\n",
      "Trained batch 2301 batch loss 0.476388961 epoch total loss 0.525633395\n",
      "Trained batch 2302 batch loss 0.58140856 epoch total loss 0.525657594\n",
      "Trained batch 2303 batch loss 0.538033187 epoch total loss 0.525663\n",
      "Trained batch 2304 batch loss 0.644200087 epoch total loss 0.525714457\n",
      "Trained batch 2305 batch loss 0.570284545 epoch total loss 0.525733829\n",
      "Trained batch 2306 batch loss 0.637463927 epoch total loss 0.525782228\n",
      "Trained batch 2307 batch loss 0.612008452 epoch total loss 0.525819659\n",
      "Trained batch 2308 batch loss 0.591796815 epoch total loss 0.52584821\n",
      "Trained batch 2309 batch loss 0.601954937 epoch total loss 0.525881171\n",
      "Trained batch 2310 batch loss 0.498670697 epoch total loss 0.52586937\n",
      "Trained batch 2311 batch loss 0.561369538 epoch total loss 0.525884748\n",
      "Trained batch 2312 batch loss 0.581454754 epoch total loss 0.525908768\n",
      "Trained batch 2313 batch loss 0.547979891 epoch total loss 0.525918305\n",
      "Trained batch 2314 batch loss 0.550824523 epoch total loss 0.525929093\n",
      "Trained batch 2315 batch loss 0.54447329 epoch total loss 0.52593708\n",
      "Trained batch 2316 batch loss 0.493426412 epoch total loss 0.525923\n",
      "Trained batch 2317 batch loss 0.512518287 epoch total loss 0.525917232\n",
      "Trained batch 2318 batch loss 0.443170667 epoch total loss 0.525881529\n",
      "Trained batch 2319 batch loss 0.519845068 epoch total loss 0.525878966\n",
      "Trained batch 2320 batch loss 0.507895052 epoch total loss 0.525871217\n",
      "Trained batch 2321 batch loss 0.581991613 epoch total loss 0.525895417\n",
      "Trained batch 2322 batch loss 0.565747142 epoch total loss 0.525912583\n",
      "Trained batch 2323 batch loss 0.596238434 epoch total loss 0.525942862\n",
      "Trained batch 2324 batch loss 0.551862478 epoch total loss 0.525954\n",
      "Trained batch 2325 batch loss 0.54147315 epoch total loss 0.525960684\n",
      "Trained batch 2326 batch loss 0.52982533 epoch total loss 0.525962353\n",
      "Trained batch 2327 batch loss 0.558527648 epoch total loss 0.5259763\n",
      "Trained batch 2328 batch loss 0.461251944 epoch total loss 0.525948524\n",
      "Trained batch 2329 batch loss 0.483857602 epoch total loss 0.525930464\n",
      "Trained batch 2330 batch loss 0.467708796 epoch total loss 0.52590549\n",
      "Trained batch 2331 batch loss 0.489273 epoch total loss 0.525889754\n",
      "Trained batch 2332 batch loss 0.38836357 epoch total loss 0.525830746\n",
      "Trained batch 2333 batch loss 0.481218576 epoch total loss 0.525811613\n",
      "Trained batch 2334 batch loss 0.429488838 epoch total loss 0.525770307\n",
      "Trained batch 2335 batch loss 0.48233974 epoch total loss 0.52575171\n",
      "Trained batch 2336 batch loss 0.47980839 epoch total loss 0.52573204\n",
      "Trained batch 2337 batch loss 0.484517783 epoch total loss 0.525714397\n",
      "Trained batch 2338 batch loss 0.469779521 epoch total loss 0.525690496\n",
      "Trained batch 2339 batch loss 0.453882396 epoch total loss 0.52565974\n",
      "Trained batch 2340 batch loss 0.454360038 epoch total loss 0.525629282\n",
      "Trained batch 2341 batch loss 0.500115 epoch total loss 0.525618374\n",
      "Trained batch 2342 batch loss 0.529338956 epoch total loss 0.52562\n",
      "Trained batch 2343 batch loss 0.611462831 epoch total loss 0.525656581\n",
      "Trained batch 2344 batch loss 0.551454365 epoch total loss 0.525667608\n",
      "Trained batch 2345 batch loss 0.501052856 epoch total loss 0.525657177\n",
      "Trained batch 2346 batch loss 0.537693202 epoch total loss 0.525662303\n",
      "Trained batch 2347 batch loss 0.478211284 epoch total loss 0.525642097\n",
      "Trained batch 2348 batch loss 0.545967579 epoch total loss 0.525650799\n",
      "Trained batch 2349 batch loss 0.584988415 epoch total loss 0.525676\n",
      "Trained batch 2350 batch loss 0.563151419 epoch total loss 0.525692\n",
      "Trained batch 2351 batch loss 0.543509603 epoch total loss 0.525699496\n",
      "Trained batch 2352 batch loss 0.461717516 epoch total loss 0.525672317\n",
      "Trained batch 2353 batch loss 0.461428016 epoch total loss 0.525644958\n",
      "Trained batch 2354 batch loss 0.459839791 epoch total loss 0.525617\n",
      "Trained batch 2355 batch loss 0.557449877 epoch total loss 0.525630593\n",
      "Trained batch 2356 batch loss 0.518746197 epoch total loss 0.525627673\n",
      "Trained batch 2357 batch loss 0.467113346 epoch total loss 0.525602877\n",
      "Trained batch 2358 batch loss 0.460416824 epoch total loss 0.525575221\n",
      "Trained batch 2359 batch loss 0.42317152 epoch total loss 0.525531828\n",
      "Trained batch 2360 batch loss 0.43571654 epoch total loss 0.525493741\n",
      "Trained batch 2361 batch loss 0.410960466 epoch total loss 0.525445282\n",
      "Trained batch 2362 batch loss 0.399464786 epoch total loss 0.525391936\n",
      "Trained batch 2363 batch loss 0.40764755 epoch total loss 0.525342047\n",
      "Trained batch 2364 batch loss 0.421812803 epoch total loss 0.525298238\n",
      "Trained batch 2365 batch loss 0.418260306 epoch total loss 0.525252938\n",
      "Trained batch 2366 batch loss 0.533094347 epoch total loss 0.525256276\n",
      "Trained batch 2367 batch loss 0.557571471 epoch total loss 0.525269926\n",
      "Trained batch 2368 batch loss 0.466733098 epoch total loss 0.52524519\n",
      "Trained batch 2369 batch loss 0.476068944 epoch total loss 0.525224447\n",
      "Trained batch 2370 batch loss 0.468328416 epoch total loss 0.525200486\n",
      "Trained batch 2371 batch loss 0.468627751 epoch total loss 0.525176585\n",
      "Trained batch 2372 batch loss 0.431368589 epoch total loss 0.525137067\n",
      "Trained batch 2373 batch loss 0.531586587 epoch total loss 0.525139809\n",
      "Trained batch 2374 batch loss 0.488980383 epoch total loss 0.52512455\n",
      "Trained batch 2375 batch loss 0.380334675 epoch total loss 0.525063634\n",
      "Trained batch 2376 batch loss 0.554504216 epoch total loss 0.525076\n",
      "Trained batch 2377 batch loss 0.496596426 epoch total loss 0.525064\n",
      "Trained batch 2378 batch loss 0.489645 epoch total loss 0.52504909\n",
      "Trained batch 2379 batch loss 0.590564489 epoch total loss 0.525076628\n",
      "Trained batch 2380 batch loss 0.572260737 epoch total loss 0.525096476\n",
      "Trained batch 2381 batch loss 0.637755871 epoch total loss 0.525143743\n",
      "Trained batch 2382 batch loss 0.547222435 epoch total loss 0.525153041\n",
      "Trained batch 2383 batch loss 0.551069498 epoch total loss 0.525163889\n",
      "Trained batch 2384 batch loss 0.588226736 epoch total loss 0.525190353\n",
      "Trained batch 2385 batch loss 0.546385765 epoch total loss 0.525199234\n",
      "Trained batch 2386 batch loss 0.690762 epoch total loss 0.525268674\n",
      "Trained batch 2387 batch loss 0.624683499 epoch total loss 0.525310278\n",
      "Trained batch 2388 batch loss 0.667479 epoch total loss 0.525369823\n",
      "Trained batch 2389 batch loss 0.654198289 epoch total loss 0.525423765\n",
      "Trained batch 2390 batch loss 0.58234489 epoch total loss 0.525447547\n",
      "Trained batch 2391 batch loss 0.427925557 epoch total loss 0.525406837\n",
      "Trained batch 2392 batch loss 0.464358449 epoch total loss 0.525381267\n",
      "Trained batch 2393 batch loss 0.397396296 epoch total loss 0.525327802\n",
      "Trained batch 2394 batch loss 0.50857 epoch total loss 0.525320768\n",
      "Trained batch 2395 batch loss 0.54140842 epoch total loss 0.525327504\n",
      "Trained batch 2396 batch loss 0.606579661 epoch total loss 0.525361359\n",
      "Trained batch 2397 batch loss 0.608998954 epoch total loss 0.525396287\n",
      "Trained batch 2398 batch loss 0.586182594 epoch total loss 0.525421619\n",
      "Trained batch 2399 batch loss 0.602246106 epoch total loss 0.525453687\n",
      "Trained batch 2400 batch loss 0.570089459 epoch total loss 0.525472283\n",
      "Trained batch 2401 batch loss 0.584010959 epoch total loss 0.525496602\n",
      "Trained batch 2402 batch loss 0.577751637 epoch total loss 0.525518358\n",
      "Trained batch 2403 batch loss 0.59699893 epoch total loss 0.52554816\n",
      "Trained batch 2404 batch loss 0.519421279 epoch total loss 0.525545597\n",
      "Trained batch 2405 batch loss 0.542825341 epoch total loss 0.525552809\n",
      "Trained batch 2406 batch loss 0.573156834 epoch total loss 0.525572538\n",
      "Trained batch 2407 batch loss 0.621465325 epoch total loss 0.525612414\n",
      "Trained batch 2408 batch loss 0.519829512 epoch total loss 0.52560997\n",
      "Trained batch 2409 batch loss 0.481442 epoch total loss 0.525591671\n",
      "Trained batch 2410 batch loss 0.581463575 epoch total loss 0.525614798\n",
      "Trained batch 2411 batch loss 0.586379409 epoch total loss 0.52564\n",
      "Trained batch 2412 batch loss 0.531318128 epoch total loss 0.525642395\n",
      "Trained batch 2413 batch loss 0.498776674 epoch total loss 0.525631249\n",
      "Trained batch 2414 batch loss 0.523511529 epoch total loss 0.525630414\n",
      "Trained batch 2415 batch loss 0.545439601 epoch total loss 0.52563858\n",
      "Trained batch 2416 batch loss 0.507744491 epoch total loss 0.525631189\n",
      "Trained batch 2417 batch loss 0.574150801 epoch total loss 0.525651217\n",
      "Trained batch 2418 batch loss 0.544575632 epoch total loss 0.525659\n",
      "Trained batch 2419 batch loss 0.577226341 epoch total loss 0.525680363\n",
      "Trained batch 2420 batch loss 0.564130902 epoch total loss 0.525696278\n",
      "Trained batch 2421 batch loss 0.580386937 epoch total loss 0.525718868\n",
      "Trained batch 2422 batch loss 0.531123877 epoch total loss 0.525721073\n",
      "Trained batch 2423 batch loss 0.583461285 epoch total loss 0.525745\n",
      "Trained batch 2424 batch loss 0.596062541 epoch total loss 0.525773942\n",
      "Trained batch 2425 batch loss 0.574345 epoch total loss 0.52579397\n",
      "Trained batch 2426 batch loss 0.601380944 epoch total loss 0.525825143\n",
      "Trained batch 2427 batch loss 0.585597098 epoch total loss 0.52584976\n",
      "Trained batch 2428 batch loss 0.529806912 epoch total loss 0.525851429\n",
      "Trained batch 2429 batch loss 0.560889482 epoch total loss 0.525865853\n",
      "Trained batch 2430 batch loss 0.523726702 epoch total loss 0.525864959\n",
      "Trained batch 2431 batch loss 0.620940089 epoch total loss 0.525904059\n",
      "Trained batch 2432 batch loss 0.556910276 epoch total loss 0.525916815\n",
      "Trained batch 2433 batch loss 0.455606461 epoch total loss 0.525887907\n",
      "Trained batch 2434 batch loss 0.4839454 epoch total loss 0.525870621\n",
      "Trained batch 2435 batch loss 0.480496764 epoch total loss 0.525851965\n",
      "Trained batch 2436 batch loss 0.576459765 epoch total loss 0.525872767\n",
      "Trained batch 2437 batch loss 0.605781615 epoch total loss 0.52590555\n",
      "Trained batch 2438 batch loss 0.545125365 epoch total loss 0.525913477\n",
      "Trained batch 2439 batch loss 0.516778052 epoch total loss 0.525909662\n",
      "Trained batch 2440 batch loss 0.545063257 epoch total loss 0.52591753\n",
      "Trained batch 2441 batch loss 0.491917819 epoch total loss 0.525903583\n",
      "Trained batch 2442 batch loss 0.483089864 epoch total loss 0.525886059\n",
      "Trained batch 2443 batch loss 0.459352493 epoch total loss 0.525858819\n",
      "Trained batch 2444 batch loss 0.503422618 epoch total loss 0.52584964\n",
      "Trained batch 2445 batch loss 0.51462096 epoch total loss 0.525845051\n",
      "Trained batch 2446 batch loss 0.523120403 epoch total loss 0.525843918\n",
      "Trained batch 2447 batch loss 0.585244179 epoch total loss 0.525868177\n",
      "Trained batch 2448 batch loss 0.525009096 epoch total loss 0.52586782\n",
      "Trained batch 2449 batch loss 0.674921632 epoch total loss 0.525928676\n",
      "Trained batch 2450 batch loss 0.576373458 epoch total loss 0.525949299\n",
      "Trained batch 2451 batch loss 0.530665457 epoch total loss 0.525951207\n",
      "Trained batch 2452 batch loss 0.459739476 epoch total loss 0.525924206\n",
      "Trained batch 2453 batch loss 0.476113349 epoch total loss 0.525903881\n",
      "Trained batch 2454 batch loss 0.462516248 epoch total loss 0.525878072\n",
      "Trained batch 2455 batch loss 0.480831593 epoch total loss 0.525859714\n",
      "Trained batch 2456 batch loss 0.492426455 epoch total loss 0.525846124\n",
      "Trained batch 2457 batch loss 0.515862882 epoch total loss 0.525842071\n",
      "Trained batch 2458 batch loss 0.535761237 epoch total loss 0.525846064\n",
      "Trained batch 2459 batch loss 0.538635671 epoch total loss 0.525851309\n",
      "Trained batch 2460 batch loss 0.4818784 epoch total loss 0.525833428\n",
      "Trained batch 2461 batch loss 0.521334 epoch total loss 0.52583164\n",
      "Trained batch 2462 batch loss 0.543851197 epoch total loss 0.525839\n",
      "Trained batch 2463 batch loss 0.513351 epoch total loss 0.525833845\n",
      "Trained batch 2464 batch loss 0.513570726 epoch total loss 0.525828898\n",
      "Trained batch 2465 batch loss 0.460892111 epoch total loss 0.525802553\n",
      "Trained batch 2466 batch loss 0.481155545 epoch total loss 0.525784433\n",
      "Trained batch 2467 batch loss 0.42076385 epoch total loss 0.525741875\n",
      "Trained batch 2468 batch loss 0.469380051 epoch total loss 0.525719047\n",
      "Trained batch 2469 batch loss 0.473337173 epoch total loss 0.525697827\n",
      "Trained batch 2470 batch loss 0.52651 epoch total loss 0.525698185\n",
      "Trained batch 2471 batch loss 0.508023322 epoch total loss 0.525691032\n",
      "Trained batch 2472 batch loss 0.462654382 epoch total loss 0.525665522\n",
      "Trained batch 2473 batch loss 0.429066539 epoch total loss 0.525626481\n",
      "Trained batch 2474 batch loss 0.361182511 epoch total loss 0.52556\n",
      "Trained batch 2475 batch loss 0.389824212 epoch total loss 0.525505126\n",
      "Trained batch 2476 batch loss 0.386123985 epoch total loss 0.525448859\n",
      "Trained batch 2477 batch loss 0.403575093 epoch total loss 0.525399625\n",
      "Trained batch 2478 batch loss 0.469295651 epoch total loss 0.525377\n",
      "Trained batch 2479 batch loss 0.588044822 epoch total loss 0.525402248\n",
      "Trained batch 2480 batch loss 0.582777262 epoch total loss 0.525425375\n",
      "Trained batch 2481 batch loss 0.620593071 epoch total loss 0.52546376\n",
      "Trained batch 2482 batch loss 0.706141591 epoch total loss 0.525536537\n",
      "Trained batch 2483 batch loss 0.69310838 epoch total loss 0.525604\n",
      "Trained batch 2484 batch loss 0.664895475 epoch total loss 0.525660098\n",
      "Trained batch 2485 batch loss 0.569334805 epoch total loss 0.525677681\n",
      "Trained batch 2486 batch loss 0.467696726 epoch total loss 0.525654376\n",
      "Trained batch 2487 batch loss 0.577829659 epoch total loss 0.525675356\n",
      "Trained batch 2488 batch loss 0.418318897 epoch total loss 0.525632203\n",
      "Trained batch 2489 batch loss 0.574432 epoch total loss 0.525651813\n",
      "Trained batch 2490 batch loss 0.515607595 epoch total loss 0.525647819\n",
      "Trained batch 2491 batch loss 0.539218187 epoch total loss 0.525653243\n",
      "Trained batch 2492 batch loss 0.558526397 epoch total loss 0.525666416\n",
      "Trained batch 2493 batch loss 0.56177789 epoch total loss 0.5256809\n",
      "Trained batch 2494 batch loss 0.552590728 epoch total loss 0.525691688\n",
      "Trained batch 2495 batch loss 0.525637805 epoch total loss 0.525691688\n",
      "Trained batch 2496 batch loss 0.560157299 epoch total loss 0.525705457\n",
      "Trained batch 2497 batch loss 0.561527729 epoch total loss 0.525719821\n",
      "Trained batch 2498 batch loss 0.523543537 epoch total loss 0.525719\n",
      "Trained batch 2499 batch loss 0.550249398 epoch total loss 0.525728822\n",
      "Trained batch 2500 batch loss 0.56681639 epoch total loss 0.525745213\n",
      "Trained batch 2501 batch loss 0.5481745 epoch total loss 0.525754213\n",
      "Trained batch 2502 batch loss 0.41126436 epoch total loss 0.525708437\n",
      "Trained batch 2503 batch loss 0.519727 epoch total loss 0.525706053\n",
      "Trained batch 2504 batch loss 0.558149934 epoch total loss 0.525719\n",
      "Trained batch 2505 batch loss 0.474618256 epoch total loss 0.525698602\n",
      "Trained batch 2506 batch loss 0.457286149 epoch total loss 0.525671303\n",
      "Trained batch 2507 batch loss 0.468013674 epoch total loss 0.525648296\n",
      "Trained batch 2508 batch loss 0.47553426 epoch total loss 0.525628328\n",
      "Trained batch 2509 batch loss 0.472262114 epoch total loss 0.525607109\n",
      "Trained batch 2510 batch loss 0.481726497 epoch total loss 0.525589585\n",
      "Trained batch 2511 batch loss 0.590423226 epoch total loss 0.525615394\n",
      "Trained batch 2512 batch loss 0.578758597 epoch total loss 0.525636554\n",
      "Trained batch 2513 batch loss 0.473702 epoch total loss 0.525615931\n",
      "Trained batch 2514 batch loss 0.441242903 epoch total loss 0.525582373\n",
      "Trained batch 2515 batch loss 0.362820864 epoch total loss 0.525517642\n",
      "Trained batch 2516 batch loss 0.500468493 epoch total loss 0.525507689\n",
      "Trained batch 2517 batch loss 0.542303503 epoch total loss 0.525514364\n",
      "Trained batch 2518 batch loss 0.552399635 epoch total loss 0.525525033\n",
      "Trained batch 2519 batch loss 0.523771 epoch total loss 0.525524378\n",
      "Trained batch 2520 batch loss 0.507753491 epoch total loss 0.525517344\n",
      "Trained batch 2521 batch loss 0.525768876 epoch total loss 0.525517464\n",
      "Trained batch 2522 batch loss 0.564819455 epoch total loss 0.525533\n",
      "Trained batch 2523 batch loss 0.507317424 epoch total loss 0.525525808\n",
      "Trained batch 2524 batch loss 0.583533406 epoch total loss 0.525548756\n",
      "Trained batch 2525 batch loss 0.549506 epoch total loss 0.525558293\n",
      "Trained batch 2526 batch loss 0.482237667 epoch total loss 0.525541127\n",
      "Trained batch 2527 batch loss 0.461247683 epoch total loss 0.525515676\n",
      "Trained batch 2528 batch loss 0.433886737 epoch total loss 0.525479436\n",
      "Trained batch 2529 batch loss 0.548746884 epoch total loss 0.525488615\n",
      "Trained batch 2530 batch loss 0.587114453 epoch total loss 0.525513\n",
      "Trained batch 2531 batch loss 0.515111625 epoch total loss 0.525508881\n",
      "Trained batch 2532 batch loss 0.596267 epoch total loss 0.525536835\n",
      "Trained batch 2533 batch loss 0.55786413 epoch total loss 0.525549591\n",
      "Trained batch 2534 batch loss 0.536396146 epoch total loss 0.525553882\n",
      "Trained batch 2535 batch loss 0.512363255 epoch total loss 0.525548637\n",
      "Trained batch 2536 batch loss 0.442701161 epoch total loss 0.525516033\n",
      "Trained batch 2537 batch loss 0.586243272 epoch total loss 0.52554\n",
      "Trained batch 2538 batch loss 0.580705345 epoch total loss 0.52556169\n",
      "Trained batch 2539 batch loss 0.506746411 epoch total loss 0.525554299\n",
      "Trained batch 2540 batch loss 0.589315712 epoch total loss 0.525579393\n",
      "Trained batch 2541 batch loss 0.555987358 epoch total loss 0.525591373\n",
      "Trained batch 2542 batch loss 0.450377047 epoch total loss 0.52556175\n",
      "Trained batch 2543 batch loss 0.435226083 epoch total loss 0.525526226\n",
      "Trained batch 2544 batch loss 0.502609 epoch total loss 0.525517225\n",
      "Trained batch 2545 batch loss 0.546344876 epoch total loss 0.525525391\n",
      "Trained batch 2546 batch loss 0.503522515 epoch total loss 0.525516748\n",
      "Trained batch 2547 batch loss 0.552755058 epoch total loss 0.525527477\n",
      "Trained batch 2548 batch loss 0.488076299 epoch total loss 0.525512755\n",
      "Trained batch 2549 batch loss 0.578157067 epoch total loss 0.525533378\n",
      "Trained batch 2550 batch loss 0.59166 epoch total loss 0.525559306\n",
      "Trained batch 2551 batch loss 0.553191066 epoch total loss 0.525570154\n",
      "Trained batch 2552 batch loss 0.49967438 epoch total loss 0.52556\n",
      "Trained batch 2553 batch loss 0.522267103 epoch total loss 0.52555871\n",
      "Trained batch 2554 batch loss 0.433789134 epoch total loss 0.525522768\n",
      "Trained batch 2555 batch loss 0.42685023 epoch total loss 0.525484145\n",
      "Trained batch 2556 batch loss 0.472223103 epoch total loss 0.525463343\n",
      "Trained batch 2557 batch loss 0.505069375 epoch total loss 0.525455356\n",
      "Trained batch 2558 batch loss 0.637877166 epoch total loss 0.525499284\n",
      "Trained batch 2559 batch loss 0.573805571 epoch total loss 0.525518179\n",
      "Trained batch 2560 batch loss 0.563876867 epoch total loss 0.52553314\n",
      "Trained batch 2561 batch loss 0.584701419 epoch total loss 0.525556266\n",
      "Trained batch 2562 batch loss 0.526723862 epoch total loss 0.525556743\n",
      "Trained batch 2563 batch loss 0.560036361 epoch total loss 0.525570154\n",
      "Trained batch 2564 batch loss 0.559154034 epoch total loss 0.525583327\n",
      "Trained batch 2565 batch loss 0.517990351 epoch total loss 0.525580347\n",
      "Trained batch 2566 batch loss 0.485616982 epoch total loss 0.52556473\n",
      "Trained batch 2567 batch loss 0.43881017 epoch total loss 0.525530934\n",
      "Trained batch 2568 batch loss 0.430958837 epoch total loss 0.525494099\n",
      "Trained batch 2569 batch loss 0.456091106 epoch total loss 0.525467098\n",
      "Trained batch 2570 batch loss 0.499097496 epoch total loss 0.525456846\n",
      "Trained batch 2571 batch loss 0.439268082 epoch total loss 0.525423288\n",
      "Trained batch 2572 batch loss 0.481427699 epoch total loss 0.525406182\n",
      "Trained batch 2573 batch loss 0.445596665 epoch total loss 0.525375187\n",
      "Trained batch 2574 batch loss 0.444592 epoch total loss 0.525343776\n",
      "Trained batch 2575 batch loss 0.450771093 epoch total loss 0.525314808\n",
      "Trained batch 2576 batch loss 0.468550414 epoch total loss 0.525292754\n",
      "Trained batch 2577 batch loss 0.435151875 epoch total loss 0.525257826\n",
      "Trained batch 2578 batch loss 0.456596076 epoch total loss 0.525231123\n",
      "Trained batch 2579 batch loss 0.539255738 epoch total loss 0.525236607\n",
      "Trained batch 2580 batch loss 0.513526499 epoch total loss 0.525232077\n",
      "Trained batch 2581 batch loss 0.458976448 epoch total loss 0.525206387\n",
      "Trained batch 2582 batch loss 0.438013256 epoch total loss 0.525172651\n",
      "Trained batch 2583 batch loss 0.383274436 epoch total loss 0.525117695\n",
      "Trained batch 2584 batch loss 0.494502813 epoch total loss 0.525105834\n",
      "Trained batch 2585 batch loss 0.552583933 epoch total loss 0.525116503\n",
      "Trained batch 2586 batch loss 0.491083473 epoch total loss 0.525103331\n",
      "Trained batch 2587 batch loss 0.565840125 epoch total loss 0.525119066\n",
      "Trained batch 2588 batch loss 0.553217471 epoch total loss 0.525129914\n",
      "Trained batch 2589 batch loss 0.53879 epoch total loss 0.525135219\n",
      "Trained batch 2590 batch loss 0.550148189 epoch total loss 0.525144875\n",
      "Trained batch 2591 batch loss 0.576312304 epoch total loss 0.525164604\n",
      "Trained batch 2592 batch loss 0.571816742 epoch total loss 0.525182605\n",
      "Trained batch 2593 batch loss 0.543334484 epoch total loss 0.525189579\n",
      "Trained batch 2594 batch loss 0.490540922 epoch total loss 0.525176287\n",
      "Trained batch 2595 batch loss 0.508437216 epoch total loss 0.52516979\n",
      "Trained batch 2596 batch loss 0.506821632 epoch total loss 0.525162756\n",
      "Trained batch 2597 batch loss 0.46081239 epoch total loss 0.525137961\n",
      "Trained batch 2598 batch loss 0.518387735 epoch total loss 0.525135398\n",
      "Trained batch 2599 batch loss 0.513675094 epoch total loss 0.525131\n",
      "Trained batch 2600 batch loss 0.46838516 epoch total loss 0.525109172\n",
      "Trained batch 2601 batch loss 0.464669585 epoch total loss 0.525085926\n",
      "Trained batch 2602 batch loss 0.441952586 epoch total loss 0.525054\n",
      "Trained batch 2603 batch loss 0.411253393 epoch total loss 0.525010228\n",
      "Trained batch 2604 batch loss 0.524622202 epoch total loss 0.525010109\n",
      "Trained batch 2605 batch loss 0.500976801 epoch total loss 0.52500087\n",
      "Trained batch 2606 batch loss 0.521174192 epoch total loss 0.52499938\n",
      "Trained batch 2607 batch loss 0.538363218 epoch total loss 0.525004506\n",
      "Trained batch 2608 batch loss 0.581731677 epoch total loss 0.525026262\n",
      "Trained batch 2609 batch loss 0.508439839 epoch total loss 0.525019944\n",
      "Trained batch 2610 batch loss 0.470264375 epoch total loss 0.524998903\n",
      "Trained batch 2611 batch loss 0.536327958 epoch total loss 0.525003254\n",
      "Trained batch 2612 batch loss 0.56506443 epoch total loss 0.525018632\n",
      "Trained batch 2613 batch loss 0.682937264 epoch total loss 0.525079072\n",
      "Trained batch 2614 batch loss 0.614153266 epoch total loss 0.525113165\n",
      "Trained batch 2615 batch loss 0.673148811 epoch total loss 0.52516973\n",
      "Trained batch 2616 batch loss 0.51570791 epoch total loss 0.525166154\n",
      "Trained batch 2617 batch loss 0.581880808 epoch total loss 0.52518779\n",
      "Trained batch 2618 batch loss 0.55566752 epoch total loss 0.525199473\n",
      "Trained batch 2619 batch loss 0.502655089 epoch total loss 0.52519083\n",
      "Trained batch 2620 batch loss 0.418248087 epoch total loss 0.52515\n",
      "Trained batch 2621 batch loss 0.352506042 epoch total loss 0.525084198\n",
      "Trained batch 2622 batch loss 0.366518646 epoch total loss 0.525023699\n",
      "Trained batch 2623 batch loss 0.403104514 epoch total loss 0.524977207\n",
      "Trained batch 2624 batch loss 0.457557648 epoch total loss 0.524951518\n",
      "Trained batch 2625 batch loss 0.556182206 epoch total loss 0.524963379\n",
      "Trained batch 2626 batch loss 0.5139184 epoch total loss 0.524959207\n",
      "Trained batch 2627 batch loss 0.454929233 epoch total loss 0.524932563\n",
      "Trained batch 2628 batch loss 0.383798838 epoch total loss 0.52487886\n",
      "Trained batch 2629 batch loss 0.318664163 epoch total loss 0.52480036\n",
      "Trained batch 2630 batch loss 0.340150148 epoch total loss 0.524730206\n",
      "Trained batch 2631 batch loss 0.390718311 epoch total loss 0.524679244\n",
      "Trained batch 2632 batch loss 0.400655419 epoch total loss 0.524632156\n",
      "Trained batch 2633 batch loss 0.40234369 epoch total loss 0.524585664\n",
      "Trained batch 2634 batch loss 0.454702139 epoch total loss 0.52455914\n",
      "Trained batch 2635 batch loss 0.466709912 epoch total loss 0.524537206\n",
      "Trained batch 2636 batch loss 0.520807207 epoch total loss 0.524535775\n",
      "Trained batch 2637 batch loss 0.459758759 epoch total loss 0.524511158\n",
      "Trained batch 2638 batch loss 0.411645412 epoch total loss 0.524468362\n",
      "Trained batch 2639 batch loss 0.473479539 epoch total loss 0.52444905\n",
      "Trained batch 2640 batch loss 0.479318172 epoch total loss 0.524432\n",
      "Trained batch 2641 batch loss 0.563188 epoch total loss 0.524446666\n",
      "Trained batch 2642 batch loss 0.447770715 epoch total loss 0.524417639\n",
      "Trained batch 2643 batch loss 0.568729937 epoch total loss 0.524434447\n",
      "Trained batch 2644 batch loss 0.551755846 epoch total loss 0.524444759\n",
      "Trained batch 2645 batch loss 0.478614867 epoch total loss 0.524427474\n",
      "Trained batch 2646 batch loss 0.525217354 epoch total loss 0.524427772\n",
      "Trained batch 2647 batch loss 0.489968836 epoch total loss 0.524414778\n",
      "Trained batch 2648 batch loss 0.452561855 epoch total loss 0.524387598\n",
      "Trained batch 2649 batch loss 0.538581491 epoch total loss 0.524392962\n",
      "Trained batch 2650 batch loss 0.456413805 epoch total loss 0.524367332\n",
      "Trained batch 2651 batch loss 0.417513311 epoch total loss 0.524327\n",
      "Trained batch 2652 batch loss 0.409275353 epoch total loss 0.524283588\n",
      "Trained batch 2653 batch loss 0.423045248 epoch total loss 0.524245501\n",
      "Trained batch 2654 batch loss 0.459295213 epoch total loss 0.524221\n",
      "Trained batch 2655 batch loss 0.383717865 epoch total loss 0.524168074\n",
      "Trained batch 2656 batch loss 0.541397214 epoch total loss 0.524174571\n",
      "Trained batch 2657 batch loss 0.428839803 epoch total loss 0.524138689\n",
      "Trained batch 2658 batch loss 0.380588681 epoch total loss 0.524084687\n",
      "Trained batch 2659 batch loss 0.429530978 epoch total loss 0.524049163\n",
      "Trained batch 2660 batch loss 0.526999 epoch total loss 0.524050236\n",
      "Trained batch 2661 batch loss 0.434191257 epoch total loss 0.5240165\n",
      "Trained batch 2662 batch loss 0.45273158 epoch total loss 0.523989677\n",
      "Trained batch 2663 batch loss 0.530680954 epoch total loss 0.523992181\n",
      "Trained batch 2664 batch loss 0.485981941 epoch total loss 0.523977935\n",
      "Trained batch 2665 batch loss 0.447536647 epoch total loss 0.523949206\n",
      "Trained batch 2666 batch loss 0.417863667 epoch total loss 0.52390945\n",
      "Trained batch 2667 batch loss 0.482777268 epoch total loss 0.523894\n",
      "Trained batch 2668 batch loss 0.460312963 epoch total loss 0.52387017\n",
      "Trained batch 2669 batch loss 0.511065423 epoch total loss 0.523865402\n",
      "Trained batch 2670 batch loss 0.443426162 epoch total loss 0.523835301\n",
      "Trained batch 2671 batch loss 0.400767803 epoch total loss 0.523789227\n",
      "Trained batch 2672 batch loss 0.53964591 epoch total loss 0.523795187\n",
      "Trained batch 2673 batch loss 0.570447087 epoch total loss 0.523812592\n",
      "Trained batch 2674 batch loss 0.696220338 epoch total loss 0.523877084\n",
      "Trained batch 2675 batch loss 0.586490512 epoch total loss 0.523900509\n",
      "Trained batch 2676 batch loss 0.619951606 epoch total loss 0.523936391\n",
      "Trained batch 2677 batch loss 0.489641517 epoch total loss 0.523923576\n",
      "Trained batch 2678 batch loss 0.42816326 epoch total loss 0.523887873\n",
      "Trained batch 2679 batch loss 0.516118348 epoch total loss 0.523884952\n",
      "Trained batch 2680 batch loss 0.575145721 epoch total loss 0.523904085\n",
      "Trained batch 2681 batch loss 0.574882448 epoch total loss 0.523923099\n",
      "Trained batch 2682 batch loss 0.556717813 epoch total loss 0.523935318\n",
      "Trained batch 2683 batch loss 0.482528031 epoch total loss 0.52391994\n",
      "Trained batch 2684 batch loss 0.500863612 epoch total loss 0.523911297\n",
      "Trained batch 2685 batch loss 0.525858 epoch total loss 0.523912072\n",
      "Trained batch 2686 batch loss 0.560133636 epoch total loss 0.523925543\n",
      "Trained batch 2687 batch loss 0.411312819 epoch total loss 0.523883641\n",
      "Trained batch 2688 batch loss 0.445565075 epoch total loss 0.523854494\n",
      "Trained batch 2689 batch loss 0.517598033 epoch total loss 0.52385217\n",
      "Trained batch 2690 batch loss 0.628450274 epoch total loss 0.523891032\n",
      "Trained batch 2691 batch loss 0.551035762 epoch total loss 0.523901105\n",
      "Trained batch 2692 batch loss 0.584721804 epoch total loss 0.523923695\n",
      "Trained batch 2693 batch loss 0.59585309 epoch total loss 0.523950398\n",
      "Trained batch 2694 batch loss 0.614169419 epoch total loss 0.523983896\n",
      "Trained batch 2695 batch loss 0.606958568 epoch total loss 0.524014652\n",
      "Trained batch 2696 batch loss 0.560125649 epoch total loss 0.524028063\n",
      "Trained batch 2697 batch loss 0.496184945 epoch total loss 0.524017751\n",
      "Trained batch 2698 batch loss 0.487588555 epoch total loss 0.524004221\n",
      "Trained batch 2699 batch loss 0.619906604 epoch total loss 0.524039745\n",
      "Trained batch 2700 batch loss 0.532949626 epoch total loss 0.524043083\n",
      "Trained batch 2701 batch loss 0.510214329 epoch total loss 0.524037957\n",
      "Trained batch 2702 batch loss 0.562315047 epoch total loss 0.524052083\n",
      "Trained batch 2703 batch loss 0.566323 epoch total loss 0.5240677\n",
      "Trained batch 2704 batch loss 0.468431771 epoch total loss 0.524047136\n",
      "Trained batch 2705 batch loss 0.542300582 epoch total loss 0.524053872\n",
      "Trained batch 2706 batch loss 0.527112305 epoch total loss 0.524055\n",
      "Trained batch 2707 batch loss 0.511662602 epoch total loss 0.524050474\n",
      "Trained batch 2708 batch loss 0.580304205 epoch total loss 0.524071276\n",
      "Trained batch 2709 batch loss 0.620886 epoch total loss 0.524107\n",
      "Trained batch 2710 batch loss 0.546851456 epoch total loss 0.524115384\n",
      "Trained batch 2711 batch loss 0.471777499 epoch total loss 0.524096072\n",
      "Trained batch 2712 batch loss 0.525720596 epoch total loss 0.524096668\n",
      "Trained batch 2713 batch loss 0.570696175 epoch total loss 0.524113834\n",
      "Trained batch 2714 batch loss 0.554660082 epoch total loss 0.524125099\n",
      "Trained batch 2715 batch loss 0.560055256 epoch total loss 0.524138331\n",
      "Trained batch 2716 batch loss 0.547267437 epoch total loss 0.524146855\n",
      "Trained batch 2717 batch loss 0.498652458 epoch total loss 0.524137497\n",
      "Trained batch 2718 batch loss 0.379475445 epoch total loss 0.52408427\n",
      "Trained batch 2719 batch loss 0.422945887 epoch total loss 0.524047077\n",
      "Trained batch 2720 batch loss 0.569499 epoch total loss 0.524063766\n",
      "Trained batch 2721 batch loss 0.511121869 epoch total loss 0.524059\n",
      "Trained batch 2722 batch loss 0.531310499 epoch total loss 0.52406168\n",
      "Trained batch 2723 batch loss 0.494446516 epoch total loss 0.524050832\n",
      "Trained batch 2724 batch loss 0.477426678 epoch total loss 0.524033666\n",
      "Trained batch 2725 batch loss 0.412178159 epoch total loss 0.523992658\n",
      "Trained batch 2726 batch loss 0.500577331 epoch total loss 0.523984075\n",
      "Trained batch 2727 batch loss 0.402488083 epoch total loss 0.52393955\n",
      "Trained batch 2728 batch loss 0.472146928 epoch total loss 0.523920536\n",
      "Trained batch 2729 batch loss 0.501498878 epoch total loss 0.523912311\n",
      "Trained batch 2730 batch loss 0.441460818 epoch total loss 0.523882091\n",
      "Trained batch 2731 batch loss 0.532381713 epoch total loss 0.52388519\n",
      "Trained batch 2732 batch loss 0.603030086 epoch total loss 0.523914158\n",
      "Trained batch 2733 batch loss 0.532655597 epoch total loss 0.523917377\n",
      "Trained batch 2734 batch loss 0.586097598 epoch total loss 0.523940086\n",
      "Trained batch 2735 batch loss 0.549065232 epoch total loss 0.523949325\n",
      "Trained batch 2736 batch loss 0.578996181 epoch total loss 0.523969412\n",
      "Trained batch 2737 batch loss 0.422432631 epoch total loss 0.523932338\n",
      "Trained batch 2738 batch loss 0.395486712 epoch total loss 0.523885429\n",
      "Trained batch 2739 batch loss 0.45458281 epoch total loss 0.523860157\n",
      "Trained batch 2740 batch loss 0.435494155 epoch total loss 0.52382791\n",
      "Trained batch 2741 batch loss 0.43527624 epoch total loss 0.523795605\n",
      "Trained batch 2742 batch loss 0.368984491 epoch total loss 0.523739159\n",
      "Trained batch 2743 batch loss 0.37345767 epoch total loss 0.523684382\n",
      "Trained batch 2744 batch loss 0.624800205 epoch total loss 0.523721218\n",
      "Trained batch 2745 batch loss 0.621152699 epoch total loss 0.523756683\n",
      "Trained batch 2746 batch loss 0.629556 epoch total loss 0.523795187\n",
      "Trained batch 2747 batch loss 0.48066631 epoch total loss 0.523779511\n",
      "Trained batch 2748 batch loss 0.685756683 epoch total loss 0.52383846\n",
      "Trained batch 2749 batch loss 0.563514113 epoch total loss 0.523852885\n",
      "Trained batch 2750 batch loss 0.585334182 epoch total loss 0.523875237\n",
      "Trained batch 2751 batch loss 0.555096626 epoch total loss 0.523886561\n",
      "Trained batch 2752 batch loss 0.431699336 epoch total loss 0.523853064\n",
      "Trained batch 2753 batch loss 0.449691355 epoch total loss 0.523826122\n",
      "Trained batch 2754 batch loss 0.43740958 epoch total loss 0.523794711\n",
      "Trained batch 2755 batch loss 0.405079603 epoch total loss 0.523751616\n",
      "Trained batch 2756 batch loss 0.550000548 epoch total loss 0.523761153\n",
      "Trained batch 2757 batch loss 0.489968181 epoch total loss 0.523748875\n",
      "Trained batch 2758 batch loss 0.564358 epoch total loss 0.523763597\n",
      "Trained batch 2759 batch loss 0.580249548 epoch total loss 0.523784041\n",
      "Trained batch 2760 batch loss 0.507719159 epoch total loss 0.52377826\n",
      "Trained batch 2761 batch loss 0.606402338 epoch total loss 0.523808181\n",
      "Trained batch 2762 batch loss 0.505291879 epoch total loss 0.523801446\n",
      "Trained batch 2763 batch loss 0.515015244 epoch total loss 0.523798287\n",
      "Trained batch 2764 batch loss 0.494772732 epoch total loss 0.523787796\n",
      "Trained batch 2765 batch loss 0.550621271 epoch total loss 0.523797512\n",
      "Trained batch 2766 batch loss 0.626467705 epoch total loss 0.523834586\n",
      "Trained batch 2767 batch loss 0.622707248 epoch total loss 0.523870349\n",
      "Trained batch 2768 batch loss 0.565172136 epoch total loss 0.52388525\n",
      "Trained batch 2769 batch loss 0.47130996 epoch total loss 0.523866296\n",
      "Trained batch 2770 batch loss 0.538948774 epoch total loss 0.52387172\n",
      "Trained batch 2771 batch loss 0.564671338 epoch total loss 0.523886442\n",
      "Trained batch 2772 batch loss 0.533486426 epoch total loss 0.523889899\n",
      "Trained batch 2773 batch loss 0.464470565 epoch total loss 0.523868442\n",
      "Trained batch 2774 batch loss 0.455988169 epoch total loss 0.523844\n",
      "Trained batch 2775 batch loss 0.441334128 epoch total loss 0.523814201\n",
      "Trained batch 2776 batch loss 0.49294126 epoch total loss 0.523803115\n",
      "Epoch 10 train loss 0.5238031148910522\n",
      "Validated batch 1 batch loss 0.572710276\n",
      "Validated batch 2 batch loss 0.587209284\n",
      "Validated batch 3 batch loss 0.50167191\n",
      "Validated batch 4 batch loss 0.55427897\n",
      "Validated batch 5 batch loss 0.536572516\n",
      "Validated batch 6 batch loss 0.556108892\n",
      "Validated batch 7 batch loss 0.524958611\n",
      "Validated batch 8 batch loss 0.594321489\n",
      "Validated batch 9 batch loss 0.638317943\n",
      "Validated batch 10 batch loss 0.671401\n",
      "Validated batch 11 batch loss 0.708393216\n",
      "Validated batch 12 batch loss 0.551725209\n",
      "Validated batch 13 batch loss 0.516779125\n",
      "Validated batch 14 batch loss 0.587407529\n",
      "Validated batch 15 batch loss 0.57461822\n",
      "Validated batch 16 batch loss 0.558143\n",
      "Validated batch 17 batch loss 0.559616446\n",
      "Validated batch 18 batch loss 0.584771514\n",
      "Validated batch 19 batch loss 0.573678493\n",
      "Validated batch 20 batch loss 0.582148373\n",
      "Validated batch 21 batch loss 0.571988523\n",
      "Validated batch 22 batch loss 0.649553835\n",
      "Validated batch 23 batch loss 0.544975758\n",
      "Validated batch 24 batch loss 0.502320528\n",
      "Validated batch 25 batch loss 0.617824614\n",
      "Validated batch 26 batch loss 0.68899405\n",
      "Validated batch 27 batch loss 0.544928372\n",
      "Validated batch 28 batch loss 0.516675174\n",
      "Validated batch 29 batch loss 0.639986455\n",
      "Validated batch 30 batch loss 0.593132496\n",
      "Validated batch 31 batch loss 0.615111053\n",
      "Validated batch 32 batch loss 0.598987103\n",
      "Validated batch 33 batch loss 0.583184659\n",
      "Validated batch 34 batch loss 0.650001943\n",
      "Validated batch 35 batch loss 0.518934548\n",
      "Validated batch 36 batch loss 0.457407892\n",
      "Validated batch 37 batch loss 0.622617066\n",
      "Validated batch 38 batch loss 0.59598285\n",
      "Validated batch 39 batch loss 0.570369244\n",
      "Validated batch 40 batch loss 0.51423651\n",
      "Validated batch 41 batch loss 0.585158348\n",
      "Validated batch 42 batch loss 0.620398819\n",
      "Validated batch 43 batch loss 0.539578795\n",
      "Validated batch 44 batch loss 0.553459525\n",
      "Validated batch 45 batch loss 0.541490734\n",
      "Validated batch 46 batch loss 0.58401227\n",
      "Validated batch 47 batch loss 0.557727396\n",
      "Validated batch 48 batch loss 0.543723226\n",
      "Validated batch 49 batch loss 0.498695016\n",
      "Validated batch 50 batch loss 0.513576388\n",
      "Validated batch 51 batch loss 0.546818912\n",
      "Validated batch 52 batch loss 0.52675873\n",
      "Validated batch 53 batch loss 0.574870586\n",
      "Validated batch 54 batch loss 0.47229749\n",
      "Validated batch 55 batch loss 0.53531003\n",
      "Validated batch 56 batch loss 0.567467511\n",
      "Validated batch 57 batch loss 0.54388994\n",
      "Validated batch 58 batch loss 0.606311142\n",
      "Validated batch 59 batch loss 0.504141688\n",
      "Validated batch 60 batch loss 0.647861063\n",
      "Validated batch 61 batch loss 0.519182205\n",
      "Validated batch 62 batch loss 0.563709617\n",
      "Validated batch 63 batch loss 0.607671916\n",
      "Validated batch 64 batch loss 0.46889317\n",
      "Validated batch 65 batch loss 0.488848299\n",
      "Validated batch 66 batch loss 0.589868844\n",
      "Validated batch 67 batch loss 0.585228\n",
      "Validated batch 68 batch loss 0.523741245\n",
      "Validated batch 69 batch loss 0.617005944\n",
      "Validated batch 70 batch loss 0.504182935\n",
      "Validated batch 71 batch loss 0.496306777\n",
      "Validated batch 72 batch loss 0.559221745\n",
      "Validated batch 73 batch loss 0.539242089\n",
      "Validated batch 74 batch loss 0.567044377\n",
      "Validated batch 75 batch loss 0.569117\n",
      "Validated batch 76 batch loss 0.539835572\n",
      "Validated batch 77 batch loss 0.517429113\n",
      "Validated batch 78 batch loss 0.54891628\n",
      "Validated batch 79 batch loss 0.628634691\n",
      "Validated batch 80 batch loss 0.618902087\n",
      "Validated batch 81 batch loss 0.521801591\n",
      "Validated batch 82 batch loss 0.672956288\n",
      "Validated batch 83 batch loss 0.572357059\n",
      "Validated batch 84 batch loss 0.424106807\n",
      "Validated batch 85 batch loss 0.546060622\n",
      "Validated batch 86 batch loss 0.669456\n",
      "Validated batch 87 batch loss 0.504726887\n",
      "Validated batch 88 batch loss 0.555363297\n",
      "Validated batch 89 batch loss 0.570540369\n",
      "Validated batch 90 batch loss 0.433072686\n",
      "Validated batch 91 batch loss 0.583067417\n",
      "Validated batch 92 batch loss 0.540455\n",
      "Validated batch 93 batch loss 0.526775718\n",
      "Validated batch 94 batch loss 0.593725324\n",
      "Validated batch 95 batch loss 0.60183841\n",
      "Validated batch 96 batch loss 0.491801947\n",
      "Validated batch 97 batch loss 0.436791\n",
      "Validated batch 98 batch loss 0.626370132\n",
      "Validated batch 99 batch loss 0.569591403\n",
      "Validated batch 100 batch loss 0.53944248\n",
      "Validated batch 101 batch loss 0.562114537\n",
      "Validated batch 102 batch loss 0.538951039\n",
      "Validated batch 103 batch loss 0.580804169\n",
      "Validated batch 104 batch loss 0.608885467\n",
      "Validated batch 105 batch loss 0.610928893\n",
      "Validated batch 106 batch loss 0.51414746\n",
      "Validated batch 107 batch loss 0.561313868\n",
      "Validated batch 108 batch loss 0.575260341\n",
      "Validated batch 109 batch loss 0.489139497\n",
      "Validated batch 110 batch loss 0.673580885\n",
      "Validated batch 111 batch loss 0.625945389\n",
      "Validated batch 112 batch loss 0.602104664\n",
      "Validated batch 113 batch loss 0.53495276\n",
      "Validated batch 114 batch loss 0.477762669\n",
      "Validated batch 115 batch loss 0.518725634\n",
      "Validated batch 116 batch loss 0.47551471\n",
      "Validated batch 117 batch loss 0.603971481\n",
      "Validated batch 118 batch loss 0.596321\n",
      "Validated batch 119 batch loss 0.537522316\n",
      "Validated batch 120 batch loss 0.600256562\n",
      "Validated batch 121 batch loss 0.629680812\n",
      "Validated batch 122 batch loss 0.689908862\n",
      "Validated batch 123 batch loss 0.659374654\n",
      "Validated batch 124 batch loss 0.606056452\n",
      "Validated batch 125 batch loss 0.514211118\n",
      "Validated batch 126 batch loss 0.611417294\n",
      "Validated batch 127 batch loss 0.680385709\n",
      "Validated batch 128 batch loss 0.600415349\n",
      "Validated batch 129 batch loss 0.540160537\n",
      "Validated batch 130 batch loss 0.58556217\n",
      "Validated batch 131 batch loss 0.700044334\n",
      "Validated batch 132 batch loss 0.472264498\n",
      "Validated batch 133 batch loss 0.589671493\n",
      "Validated batch 134 batch loss 0.540949464\n",
      "Validated batch 135 batch loss 0.445356846\n",
      "Validated batch 136 batch loss 0.39920634\n",
      "Validated batch 137 batch loss 0.513022\n",
      "Validated batch 138 batch loss 0.597325683\n",
      "Validated batch 139 batch loss 0.578985333\n",
      "Validated batch 140 batch loss 0.540271342\n",
      "Validated batch 141 batch loss 0.533426404\n",
      "Validated batch 142 batch loss 0.525399148\n",
      "Validated batch 143 batch loss 0.523629725\n",
      "Validated batch 144 batch loss 0.584558725\n",
      "Validated batch 145 batch loss 0.503032625\n",
      "Validated batch 146 batch loss 0.533400595\n",
      "Validated batch 147 batch loss 0.566924453\n",
      "Validated batch 148 batch loss 0.506320357\n",
      "Validated batch 149 batch loss 0.537430823\n",
      "Validated batch 150 batch loss 0.638625443\n",
      "Validated batch 151 batch loss 0.519255757\n",
      "Validated batch 152 batch loss 0.622662544\n",
      "Validated batch 153 batch loss 0.562999845\n",
      "Validated batch 154 batch loss 0.661403716\n",
      "Validated batch 155 batch loss 0.550485373\n",
      "Validated batch 156 batch loss 0.569059968\n",
      "Validated batch 157 batch loss 0.566913247\n",
      "Validated batch 158 batch loss 0.470904708\n",
      "Validated batch 159 batch loss 0.551911414\n",
      "Validated batch 160 batch loss 0.587706804\n",
      "Validated batch 161 batch loss 0.667974234\n",
      "Validated batch 162 batch loss 0.624410272\n",
      "Validated batch 163 batch loss 0.60012722\n",
      "Validated batch 164 batch loss 0.606237\n",
      "Validated batch 165 batch loss 0.656520128\n",
      "Validated batch 166 batch loss 0.617860794\n",
      "Validated batch 167 batch loss 0.587410092\n",
      "Validated batch 168 batch loss 0.623750627\n",
      "Validated batch 169 batch loss 0.655373812\n",
      "Validated batch 170 batch loss 0.609655738\n",
      "Validated batch 171 batch loss 0.666873574\n",
      "Validated batch 172 batch loss 0.579125583\n",
      "Validated batch 173 batch loss 0.629168451\n",
      "Validated batch 174 batch loss 0.503317177\n",
      "Validated batch 175 batch loss 0.549557745\n",
      "Validated batch 176 batch loss 0.620707035\n",
      "Validated batch 177 batch loss 0.663321078\n",
      "Validated batch 178 batch loss 0.596639097\n",
      "Validated batch 179 batch loss 0.62872237\n",
      "Validated batch 180 batch loss 0.54077369\n",
      "Validated batch 181 batch loss 0.499271661\n",
      "Validated batch 182 batch loss 0.573954105\n",
      "Validated batch 183 batch loss 0.64144963\n",
      "Validated batch 184 batch loss 0.491234869\n",
      "Validated batch 185 batch loss 0.5717237\n",
      "Validated batch 186 batch loss 0.664463162\n",
      "Validated batch 187 batch loss 0.524951279\n",
      "Validated batch 188 batch loss 0.577825546\n",
      "Validated batch 189 batch loss 0.560523748\n",
      "Validated batch 190 batch loss 0.493934214\n",
      "Validated batch 191 batch loss 0.532381237\n",
      "Validated batch 192 batch loss 0.566200793\n",
      "Validated batch 193 batch loss 0.482012898\n",
      "Validated batch 194 batch loss 0.514636755\n",
      "Validated batch 195 batch loss 0.549490571\n",
      "Validated batch 196 batch loss 0.559925497\n",
      "Validated batch 197 batch loss 0.566977441\n",
      "Validated batch 198 batch loss 0.566380322\n",
      "Validated batch 199 batch loss 0.559568644\n",
      "Validated batch 200 batch loss 0.479428947\n",
      "Validated batch 201 batch loss 0.517088592\n",
      "Validated batch 202 batch loss 0.592101932\n",
      "Validated batch 203 batch loss 0.567699492\n",
      "Validated batch 204 batch loss 0.540609598\n",
      "Validated batch 205 batch loss 0.60149163\n",
      "Validated batch 206 batch loss 0.59010762\n",
      "Validated batch 207 batch loss 0.640484154\n",
      "Validated batch 208 batch loss 0.614247\n",
      "Validated batch 209 batch loss 0.539624214\n",
      "Validated batch 210 batch loss 0.558756471\n",
      "Validated batch 211 batch loss 0.449677765\n",
      "Validated batch 212 batch loss 0.570687532\n",
      "Validated batch 213 batch loss 0.571421087\n",
      "Validated batch 214 batch loss 0.445155442\n",
      "Validated batch 215 batch loss 0.607602\n",
      "Validated batch 216 batch loss 0.518273234\n",
      "Validated batch 217 batch loss 0.480560035\n",
      "Validated batch 218 batch loss 0.588789165\n",
      "Validated batch 219 batch loss 0.557803929\n",
      "Validated batch 220 batch loss 0.587277055\n",
      "Validated batch 221 batch loss 0.536190808\n",
      "Validated batch 222 batch loss 0.550566435\n",
      "Validated batch 223 batch loss 0.583020926\n",
      "Validated batch 224 batch loss 0.692305565\n",
      "Validated batch 225 batch loss 0.688718855\n",
      "Validated batch 226 batch loss 0.54673785\n",
      "Validated batch 227 batch loss 0.507161\n",
      "Validated batch 228 batch loss 0.574374139\n",
      "Validated batch 229 batch loss 0.533217967\n",
      "Validated batch 230 batch loss 0.501476645\n",
      "Validated batch 231 batch loss 0.424752951\n",
      "Validated batch 232 batch loss 0.574591637\n",
      "Validated batch 233 batch loss 0.576299369\n",
      "Validated batch 234 batch loss 0.562741\n",
      "Validated batch 235 batch loss 0.53603667\n",
      "Validated batch 236 batch loss 0.568925202\n",
      "Validated batch 237 batch loss 0.474511266\n",
      "Validated batch 238 batch loss 0.583085954\n",
      "Validated batch 239 batch loss 0.576390684\n",
      "Validated batch 240 batch loss 0.543006778\n",
      "Validated batch 241 batch loss 0.600972593\n",
      "Validated batch 242 batch loss 0.628529906\n",
      "Validated batch 243 batch loss 0.597375512\n",
      "Validated batch 244 batch loss 0.609735131\n",
      "Validated batch 245 batch loss 0.591890275\n",
      "Validated batch 246 batch loss 0.626698792\n",
      "Validated batch 247 batch loss 0.610055685\n",
      "Validated batch 248 batch loss 0.585004747\n",
      "Validated batch 249 batch loss 0.504788339\n",
      "Validated batch 250 batch loss 0.519237518\n",
      "Validated batch 251 batch loss 0.53483808\n",
      "Validated batch 252 batch loss 0.559610069\n",
      "Validated batch 253 batch loss 0.575523317\n",
      "Validated batch 254 batch loss 0.524594784\n",
      "Validated batch 255 batch loss 0.516169727\n",
      "Validated batch 256 batch loss 0.631393671\n",
      "Validated batch 257 batch loss 0.630384564\n",
      "Validated batch 258 batch loss 0.59565258\n",
      "Validated batch 259 batch loss 0.654800057\n",
      "Validated batch 260 batch loss 0.57020694\n",
      "Validated batch 261 batch loss 0.593366861\n",
      "Validated batch 262 batch loss 0.606113434\n",
      "Validated batch 263 batch loss 0.52892381\n",
      "Validated batch 264 batch loss 0.712316096\n",
      "Validated batch 265 batch loss 0.550561666\n",
      "Validated batch 266 batch loss 0.553923666\n",
      "Validated batch 267 batch loss 0.598854721\n",
      "Validated batch 268 batch loss 0.531863\n",
      "Validated batch 269 batch loss 0.603166521\n",
      "Validated batch 270 batch loss 0.423443019\n",
      "Validated batch 271 batch loss 0.460790336\n",
      "Validated batch 272 batch loss 0.538358927\n",
      "Validated batch 273 batch loss 0.639831066\n",
      "Validated batch 274 batch loss 0.519617796\n",
      "Validated batch 275 batch loss 0.55200392\n",
      "Validated batch 276 batch loss 0.466510266\n",
      "Validated batch 277 batch loss 0.565054119\n",
      "Validated batch 278 batch loss 0.519590497\n",
      "Validated batch 279 batch loss 0.578508437\n",
      "Validated batch 280 batch loss 0.540435672\n",
      "Validated batch 281 batch loss 0.540907204\n",
      "Validated batch 282 batch loss 0.550409913\n",
      "Validated batch 283 batch loss 0.462642521\n",
      "Validated batch 284 batch loss 0.559609532\n",
      "Validated batch 285 batch loss 0.54630959\n",
      "Validated batch 286 batch loss 0.522401929\n",
      "Validated batch 287 batch loss 0.544961452\n",
      "Validated batch 288 batch loss 0.691280544\n",
      "Validated batch 289 batch loss 0.483408839\n",
      "Validated batch 290 batch loss 0.482780963\n",
      "Validated batch 291 batch loss 0.583575428\n",
      "Validated batch 292 batch loss 0.40818426\n",
      "Validated batch 293 batch loss 0.486207\n",
      "Validated batch 294 batch loss 0.544476032\n",
      "Validated batch 295 batch loss 0.541335285\n",
      "Validated batch 296 batch loss 0.559835315\n",
      "Validated batch 297 batch loss 0.44590652\n",
      "Validated batch 298 batch loss 0.466721296\n",
      "Validated batch 299 batch loss 0.586377263\n",
      "Validated batch 300 batch loss 0.534044623\n",
      "Validated batch 301 batch loss 0.479376495\n",
      "Validated batch 302 batch loss 0.488254219\n",
      "Validated batch 303 batch loss 0.586645484\n",
      "Validated batch 304 batch loss 0.472531646\n",
      "Validated batch 305 batch loss 0.620050371\n",
      "Validated batch 306 batch loss 0.643969715\n",
      "Validated batch 307 batch loss 0.613020778\n",
      "Validated batch 308 batch loss 0.602090359\n",
      "Validated batch 309 batch loss 0.504654467\n",
      "Validated batch 310 batch loss 0.543717206\n",
      "Validated batch 311 batch loss 0.608513236\n",
      "Validated batch 312 batch loss 0.611908257\n",
      "Validated batch 313 batch loss 0.490554541\n",
      "Validated batch 314 batch loss 0.442681134\n",
      "Validated batch 315 batch loss 0.518086433\n",
      "Validated batch 316 batch loss 0.484333634\n",
      "Validated batch 317 batch loss 0.610205\n",
      "Validated batch 318 batch loss 0.442928433\n",
      "Validated batch 319 batch loss 0.472987622\n",
      "Validated batch 320 batch loss 0.544790685\n",
      "Validated batch 321 batch loss 0.593341231\n",
      "Validated batch 322 batch loss 0.606325209\n",
      "Validated batch 323 batch loss 0.621277213\n",
      "Validated batch 324 batch loss 0.505571961\n",
      "Validated batch 325 batch loss 0.506145179\n",
      "Validated batch 326 batch loss 0.474935681\n",
      "Validated batch 327 batch loss 0.540224552\n",
      "Validated batch 328 batch loss 0.595055461\n",
      "Validated batch 329 batch loss 0.545185387\n",
      "Validated batch 330 batch loss 0.533569038\n",
      "Validated batch 331 batch loss 0.534983635\n",
      "Validated batch 332 batch loss 0.553317308\n",
      "Validated batch 333 batch loss 0.64036727\n",
      "Validated batch 334 batch loss 0.669334173\n",
      "Validated batch 335 batch loss 0.539885581\n",
      "Validated batch 336 batch loss 0.409941077\n",
      "Validated batch 337 batch loss 0.590549648\n",
      "Validated batch 338 batch loss 0.540219545\n",
      "Validated batch 339 batch loss 0.554174185\n",
      "Validated batch 340 batch loss 0.558374882\n",
      "Validated batch 341 batch loss 0.549986303\n",
      "Validated batch 342 batch loss 0.606512308\n",
      "Validated batch 343 batch loss 0.579843342\n",
      "Validated batch 344 batch loss 0.563892722\n",
      "Validated batch 345 batch loss 0.510379612\n",
      "Validated batch 346 batch loss 0.538755119\n",
      "Validated batch 347 batch loss 0.369039267\n",
      "Validated batch 348 batch loss 0.437496841\n",
      "Validated batch 349 batch loss 0.584334135\n",
      "Validated batch 350 batch loss 0.544281483\n",
      "Validated batch 351 batch loss 0.444237947\n",
      "Validated batch 352 batch loss 0.518859684\n",
      "Validated batch 353 batch loss 0.505520582\n",
      "Validated batch 354 batch loss 0.558349192\n",
      "Validated batch 355 batch loss 0.604934156\n",
      "Validated batch 356 batch loss 0.606592178\n",
      "Validated batch 357 batch loss 0.511666834\n",
      "Validated batch 358 batch loss 0.443350255\n",
      "Validated batch 359 batch loss 0.535030484\n",
      "Validated batch 360 batch loss 0.556716442\n",
      "Validated batch 361 batch loss 0.558695793\n",
      "Validated batch 362 batch loss 0.664456367\n",
      "Validated batch 363 batch loss 0.507900655\n",
      "Validated batch 364 batch loss 0.652316928\n",
      "Validated batch 365 batch loss 0.573031247\n",
      "Validated batch 366 batch loss 0.537726641\n",
      "Validated batch 367 batch loss 0.513992608\n",
      "Validated batch 368 batch loss 0.467335314\n",
      "Validated batch 369 batch loss 0.539137602\n",
      "Epoch 10 val loss 0.5592830181121826\n",
      "Model /aiffel/mpii/weights/shg_model-epoch-10-loss-0.5593.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "# StackedHourglassNetwork 학습하기\n",
    "stacked_model, train_loss, val_loss = stacked_hourglass_train(epochs, learning_rate, num_heatmap, \n",
    "                                                              batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02708fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Epoch 1/10\n",
      "2776/2776 [==============================] - 800s 285ms/step - loss: 0.0769 - val_loss: 0.0773\n",
      "Epoch 2/10\n",
      "2776/2776 [==============================] - 798s 287ms/step - loss: 0.0739 - val_loss: 0.0774\n",
      "Epoch 3/10\n",
      "2776/2776 [==============================] - 798s 287ms/step - loss: 0.0716 - val_loss: 0.0749\n",
      "Epoch 4/10\n",
      "2776/2776 [==============================] - 793s 285ms/step - loss: 0.0696 - val_loss: 0.0740\n",
      "Epoch 5/10\n",
      "2776/2776 [==============================] - 795s 286ms/step - loss: 0.0674 - val_loss: 0.0747\n",
      "Epoch 6/10\n",
      "2776/2776 [==============================] - 806s 290ms/step - loss: 0.0656 - val_loss: 0.0728\n",
      "Epoch 7/10\n",
      "2776/2776 [==============================] - 795s 286ms/step - loss: 0.0639 - val_loss: 0.0747\n",
      "Epoch 8/10\n",
      "2776/2776 [==============================] - 793s 285ms/step - loss: 0.0620 - val_loss: 0.0728\n",
      "Epoch 9/10\n",
      "2776/2776 [==============================] - 799s 287ms/step - loss: 0.0606 - val_loss: 0.0723\n",
      "Epoch 10/10\n",
      "2776/2776 [==============================] - 796s 286ms/step - loss: 0.0589 - val_loss: 0.0733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7d6072696850>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SimpleBaseline 학습하기\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "\n",
    "train_dataset = create_dataset(train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "val_dataset = create_dataset(val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "# model checkpoint\n",
    "model_checkpoint = os.getenv('HOME') + '/mpii/weights'+ '/simple_model-epoch-{epoch:02d}-loss-{val_loss:.4f}.h5'\n",
    "\n",
    "model_cb = tf.keras.callbacks.ModelCheckpoint(filepath = model_checkpoint,\n",
    "                                             save_weights_only = True,\n",
    "                                             save_best_only = True,\n",
    "                                             monitor = 'val_loss',\n",
    "                                             )\n",
    "\n",
    "# 모델 컴파일하기\n",
    "simple_loss = tf.keras.losses.MeanSquaredError(reduction = tf.keras.losses.Reduction.NONE)\n",
    "simple_optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "simplebase_model.compile(optimizer = simple_optimizer, loss = simple_loss)\n",
    "\n",
    "# 모델 학습하기\n",
    "simplebase_model.fit(train_dataset,\n",
    "                          epochs = epochs,\n",
    "                          batch_size = batch_size,\n",
    "                          validation_data = val_dataset,\n",
    "                          callbacks = [model_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c8683",
   "metadata": {},
   "source": [
    "## **7. 모델 평가하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3425f",
   "metadata": {},
   "source": [
    "### **예측 엔진 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2013b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_PATH = os.getenv('HOME') + '/mpii/weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02aa6928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StackedHourglass 모델 가져오기\n",
    "SHG_WEIGHTS_PATH = os.path.join(WEIGHT_PATH, 'shg_model-epoch-10-loss-0.5593.h5')\n",
    "\n",
    "shg_model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1)\n",
    "shg_model.load_weights(SHG_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5f7af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleBaseline 모델 가져오기\n",
    "SBL_WEIGHTS_PATH = os.path.join(WEIGHT_PATH, 'simple_model-epoch-09-loss-0.0723.h5' )\n",
    "\n",
    "sbl_model = SimpleBaseline(IMAGE_SHAPE)\n",
    "sbl_model.load_weights(SBL_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d10301",
   "metadata": {},
   "source": [
    "이제 학습이 끝난 모델이 얼마나 잘 예측하는지 확인해 볼 시간입니다. 미리 학습된 모델을 불러옵시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bad10",
   "metadata": {},
   "source": [
    "학습에 사용했던 keypoint 들을 사용해야 하기 때문에 필요한 변수를 지정해 줍니다. \n",
    "\n",
    "변수에 저장되는 것은 해당 부위를 나타내는 인덱스입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af877929",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ANKLE = 0\n",
    "R_KNEE = 1\n",
    "R_HIP = 2\n",
    "L_HIP = 3\n",
    "L_KNEE = 4\n",
    "L_ANKLE = 5\n",
    "PELVIS = 6\n",
    "THORAX = 7\n",
    "UPPER_NECK = 8\n",
    "HEAD_TOP = 9\n",
    "R_WRIST = 10\n",
    "R_ELBOW = 11\n",
    "R_SHOULDER = 12\n",
    "L_SHOULDER = 13\n",
    "L_ELBOW = 14\n",
    "L_WRIST = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0dcfa0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MPII_BONES = [\n",
    "    [R_ANKLE, R_KNEE],\n",
    "    [R_KNEE, R_HIP],\n",
    "    [R_HIP, PELVIS],\n",
    "    [L_HIP, PELVIS],\n",
    "    [L_HIP, L_KNEE],\n",
    "    [L_KNEE, L_ANKLE],\n",
    "    [PELVIS, THORAX],\n",
    "    [THORAX, UPPER_NECK],\n",
    "    [UPPER_NECK, HEAD_TOP],\n",
    "    [R_WRIST, R_ELBOW],\n",
    "    [R_ELBOW, R_SHOULDER],\n",
    "    [THORAX, R_SHOULDER],\n",
    "    [THORAX, L_SHOULDER],\n",
    "    [L_SHOULDER, L_ELBOW],\n",
    "    [L_ELBOW, L_WRIST]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1037dca8",
   "metadata": {},
   "source": [
    "학습을 heatmap으로 했기 때문에 모델이 추론해 내놓은 결과도 heatmap입니다.\n",
    "\n",
    "그래서 이 heatmap으로부터 좌표를 추출해야 합니다.\n",
    "\n",
    "heatmap중에 최대값을 갖는 지점을 찾아내면 됩니다. heatmap에서 최대값을 찾는 함수를 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44d9b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_coordinates(heatmaps):\n",
    "    flatten_heatmaps = tf.reshape(heatmaps, (-1, 16))\n",
    "    indices = tf.math.argmax(flatten_heatmaps, axis=0)\n",
    "    y = tf.cast(indices / 64, dtype=tf.int64)\n",
    "    x = indices - 64 * y\n",
    "    return tf.stack([x, y], axis=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc7f63",
   "metadata": {},
   "source": [
    "위 함수만으로는 256x256 이미지에 64x64 heatmap max 값을 표현할 때 quantization 오차가 발생하기 때문에 \n",
    "\n",
    "실제 계산에서는 3x3 필터를 이용해서 근사치를 구해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac4092ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_from_heatmap(heatmaps):\n",
    "    max_keypoints = find_max_coordinates(heatmaps)\n",
    "\n",
    "    padded_heatmap = np.pad(heatmaps, [[1,1],[1,1],[0,0]], mode='constant')\n",
    "    adjusted_keypoints = []\n",
    "    for i, keypoint in enumerate(max_keypoints):\n",
    "        max_y = keypoint[1]+1\n",
    "        max_x = keypoint[0]+1\n",
    "        \n",
    "        patch = padded_heatmap[max_y-1:max_y+2, max_x-1:max_x+2, i]\n",
    "        patch[1][1] = 0\n",
    "        \n",
    "        index = np.argmax(patch)\n",
    "        \n",
    "        next_y = index // 3\n",
    "        next_x = index - next_y * 3\n",
    "        delta_y = (next_y - 1) / 4\n",
    "        delta_x = (next_x - 1) / 4\n",
    "        \n",
    "        adjusted_keypoint_x = keypoint[0] + delta_x\n",
    "        adjusted_keypoint_y = keypoint[1] + delta_y\n",
    "        adjusted_keypoints.append((adjusted_keypoint_x, adjusted_keypoint_y))\n",
    "        \n",
    "    adjusted_keypoints = np.clip(adjusted_keypoints, 0, 64)\n",
    "    normalized_keypoints = adjusted_keypoints / 64\n",
    "    return normalized_keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6efe98",
   "metadata": {},
   "source": [
    "이제 모델과 이미지 경로를 입력하면 이미지와 keypoint를 출력하는 함수를 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c16f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image_path):\n",
    "    encoded = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(encoded)\n",
    "    inputs = tf.image.resize(image, (256, 256))\n",
    "    inputs = tf.cast(inputs, tf.float32) / 127.5 - 1\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    outputs = model(inputs, training=False)\n",
    "    if type(outputs) != list:\n",
    "        outputs = [outputs]\n",
    "    heatmap = tf.squeeze(outputs[-1], axis=0).numpy()\n",
    "    kp = extract_keypoints_from_heatmap(heatmap)\n",
    "    return image, kp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be16ac",
   "metadata": {},
   "source": [
    "그림을 그릴 때는 두 가지 그림을 그려볼 겁니다. keypoint들과 뼈대입니다.\n",
    "\n",
    "keypoint들은 관절 역할을 하고 keypoint들을 연결시킨 것이 뼈대가 되겠네요.\n",
    "\n",
    "두 가지 함수를 각각 작성해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6c40eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoint(관절)을 그리는 함수 구현하기\n",
    "def draw_keypoints_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        if index is not None and index != i:\n",
    "            continue\n",
    "        plt.scatter(joint_x, joint_y, s=10, c='red', marker='o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4b46c923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton(뼈대)를 그리는 함수 구현하기\n",
    "def draw_skeleton_on_image(image, keypoints, index=None):\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    joints = []\n",
    "    for i, joint in enumerate(keypoints):\n",
    "        joint_x = joint[0] * image.shape[1]\n",
    "        joint_y = joint[1] * image.shape[0]\n",
    "        joints.append((joint_x, joint_y))\n",
    "    \n",
    "    for bone in MPII_BONES:\n",
    "        joint_1 = joints[bone[0]]\n",
    "        joint_2 = joints[bone[1]]\n",
    "        plt.plot([joint_1[0], joint_2[0]], [joint_1[1], joint_2[1]], linewidth=5, alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c62f5e",
   "metadata": {},
   "source": [
    "이제 테스트 이미지를 이용해 모델의 성능을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d1ea590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = os.path.join(PROJECT_PATH, 'test_image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b45335",
   "metadata": {},
   "source": [
    "* StackedHourglass 모델 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7233ab5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAD8CAYAAAD6+lbaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOz9aaxtXXaehz1jzLn2Puc2X1/NV33DYlOi2IoiRduiHFmypNiWIVuCnfxwFzBBYv8LYP0LEASBfgRBHAQwIiRGZASOpSQwrFiKLVuyLMWiLMrsyWJTVay+vr65zTlnrzXnGPnxjrXPJVlFyqLL/n58q3j53XvuuWfvvdacY47xjvd9h2Um717vXu9e717vXt/48v++38C717vXu9e71zv5ejdIvnu9e717vXv9Nte7QfLd693r3evd67e53g2S717vXu9e716/zfVukHz3evd693r3+m2ud4Pku9e717vXu9dvc33LgqSZ/TEz+xUz+6yZ/dlv1eu8e717vXu9e30rL/tW8CTNrAG/CvwR4CvATwL/Ymb+0n/rL/bu9e717vXu9S28vlWZ5O8HPpuZn8/MFfj3gT/5LXqtd693r3evd69v2dW/RT/3g8CXn/jzV4Af/mbffO/uIZ9/7g5kYgBmJEkmTIwZSSQEkAGGAYmZ4WYYhrvRDNyS5pBAZLDNYEQyI9BPN0B/TyXRhmEGphcnM8kIkvyN3+P6tf+cyNT7SOOckGeSmXoZu32fhlH/d/6ZGPq7em2zem8RZCZzxvn7dFvq59TnP3+t/p1ePureJVHvJSNxAzPHzHH3+jmQ9f72n4UZZq5P/Btez24/H/W52V87dd/2D1a37fyZzvdw/9BPVi+G3d48bj/KE69vvr8aGQBBMomM+gRPXvrcZL2f/Wfb/omp53f+8u3DePKn5BN/l7/5NW6/h9/wN/mbfsr5dsD+/p94+Hb+aq3tvH2WSeq7M+uZcr73Wlr7vbHfsD70/b/pbfFbvnB7P2x/evqWPD/f/c96Xuc98sTnSfT9+927fZVaAFk/2Z58kd+w/Os+Zv2T22e1v6Gs9WF6pPrv/j1P/DD9jNt1xBPfsz/5/f/H+et5XrkAb73+9muZ+R5+0/WtCpK/42VmPw78OMCzT1/wb/z4D9Kbc+xd93QGpwmPcR6cgusJN9PYViAczOlt4egHOnA4Gi/cadw/bjRbab1xva289viaVx6feLwFWzrbbGQaOSEycYdmhjv05rg1cgRzbqxzY46JpdG8s1wstGVh0tlmso6hgLalNm9Am7rh4UY6NDZ86XhvmDsN6GmQybSkHTuH1rhYGm7GloM5g7kNbh5fYQ4bG60dOPZLGp0YWo7JwB0Oy0LrHe8wY5CxMbaVm9PGad0Yc9CtsfQDx4s7XB4vOdAwNwIFVWuN5XCkLUe8dawd6Xag2wGzDukQScwBTAViS7o57jowVpvkCDIgmxGeNHdawuKNzkGBwmrjx8QSmiXNEidZ3M6Hh7tzbB36PWZeMIexbYHbymm8zU08JjwZOZm1SSwmIwYZxpw6NEjovetwMANch2jE+ZCqnUcGROhgaQbu2krhek9RB1hE1u8VqHFw04bqBp6GhcLgNAhrzGxsEWSC1+Zf0ul9Yc1kzI7bgmLGRvokc2NuJ2IbjAjCAjejN/3s3jve9/DUmNOYYxITGk5YgumZKUxMksAxrZvmWAYNwxJmBGukPnc422kyTIG5GTRzhiUjnDH1/dQ9CDfCwDLxLXQf0zBrYCpaE5iuRMDRPshZ93QPWs1xAywJDLbAI5kWeN3LQcNnwphkBGMMIiet7ckMNFsgvQL/wA2SzobhzVncmOlYTjwn/++/8Je/+I1i1bcqSH4V+PATf/5Qfe18ZeafB/48wAdfvJ/bmMScmBnNHZu6MQczLj2x0EM6OWy5ka0OCofWjKM5mSszIN3YRrJOSBrujebGNo2YCsCkYTiRiXXHzbTwGdrAmdC8NrOe8UwgDNzJCCydGZORk8zE0cZ2cxyYBPh+KCYZkxnoSMxkkIQZeTDMGy2CLZMtJrkF3hYiNtydFhDbRhDEMCIGWHA4NMYcepEKAtkbkf12IwNYw9pCcwXjyMrazImYt5lAUCe5Y5VxZmoDEWBpkJ3mMPUJgcbMyYiEEUQGkUZ0GDFpdVaHTQyl+RGhe1iBZU9WA538EckMfX/m1K9wksnNtjKryhhzIzLZCAW9CKjnkTT9tCfSOzOjmZ77OTt7IhvJ0PrICAW3c8ak7zkHyXqe+gepQ7A5vRmHZhys0VP3LxJOwE0aTCdm3U+Cua+LGRiuWGJGpDFT/02MoJFm4Ia5k6b3QgY9q/oIiNSHdZIZG1hgHmAB4efKwU3B1CI5NKcbqsD0Epgn1o1uCxu1jiJJ0+eJSO2lyMpGXd9jT2Z0lRWnDgYitfRnMjPrWetnnCu3elhBVoXhZOh+h4G1/WFMciYxBjmDmYHtFSS6l1Q2DtQ90+975e6ZpkC9L+5vcn2rguRPAp8ys4+j4PgvAP+j3+4f5AxoDdJo3uhLg0wajWRiMbQBTQ+S7BidxYy7R+e+bXic2IYxbeFG5z8DaNboDVroZuwLKnG8NSKcgdNsAJsefJV7jmnhYLR0bKocbTqqmBlMn+inVXyJQEsgGYqpdAwL2ObUoqiH7qsxMVbvaC0bPRsjgzSdwj0Tn4OIG7I7kQciA0uIMMim7KMCn4VhaXScaAuejdY6rXW8MrUttQjNnXBowDYmERuZRrcqHt0Fe8R+35LuVf5mKC+JIBJsGjGTiKkMJrREZ3NOBht6ntoYUTCAMu6kMjAHKrC7Oeu2VPAHyyBz1P1DZWA4e1Udqe8hdf/3LdG4zRoj4lz2ZoQCUgU+BWSA2symDC1SWI8ySGVI1lrVyOAJMU3BpSmzXhocXetmRJAzGZnMDDLs/FpplY2h9+4Y4Y7TianXnWmkN5RrTwVtq2ezB6dQSVtvmczAMugeuCkwhsGIWisVjGYYWwbZ7HygLx28t1uIK1GGioLjmKEscs/UdyjJBHftmXbmHiRVsVgqID75dKwe3My6BxgeT8IbU3AbSTQ4puEELYw5ghjzDG8l6NDZYTAbNBpYx/yAea2bMGJOJgNvE7OpQ+SbXN+SIJmZw8z+deA/Qfvv38nMX/zm/wCsSsgxQhvFwSKxMVgSbtxoJMfwypDA+qSRLGYcCIyVscHVlqwE0ZyJ4zQ9DAuwyUyVycJBFp056UQOMieVl3PMfTEnXmBIZOCZuDcIlT0HjIxBy8RD35sEZolbo+N4BjMCD2OLxJqzuDIyA3IMEsMz6K3hsTAzMR9kOivORV/YxmPMjpgtYMHcrsmWRDqenZbKfg7tQE7DesMO7TYTSiM2aNZZWVWkRWeLhBa0RTst+tCji1aYsB6KWxC1UQfCijMnlknYJFtWCTUFi7iytsgqlarU22HJtKwg6ZDtfH8tTaWWV3ZoQ6+bG2Yb5KRlYdKZBK1yByf9wIzJzCBiI8zo4VoDHSKq9Nyzm2GVcwh8sMq2FPVSmXAoa80UFurTlVmfF7Eyk0hVL+lGNAWPmML5PBuezog9oOteOUHzynrboNtCZgcmMx3OJWGCNdy8SnkdCGMmnuARNFNJHRbQnGYL3ZVNRjM8jHUkW907z2DMoEWcD/RlcR3q5oRXip+bgtKEjEZEMGco666AHV1hXIX1DiAGkZOkn/HxzGR6wSPnA2gPUlbAwI7J1glqOtpm3buIZE7t4yCYTffQTNl7s0Z3UNRoBNq7Kj5VrQpaGYWd34bl33x9yzDJzPyrwF/9B/xuHFRSnuphWMJMGgd6P9AJNq+T3ZSneepBndZg68GhNbZtZVs3NozNkg2YIxhjMsYgR9TNnudsiAwsktiU8jdXFnmdJ5oLbEpPnIElGAP3xGgsZtoUllgETekdHiodmitAClieykLMlK3WxshI5px18nYyO+ZBX3TyrafkOz74XXz/J7+Pl958nf/y5/9DrAcjjJiXrNNUKhWmahZ4g96NHJNz06l1PKw26STGgNa4icRmkG70TMway9wYqYwxwon6LK1VYCoMUZu/quUnyiJvrs+TkJuwymh+W8pTAcZthwPP4H0SuHllEyq1SUivE98Sszxjfs2MQ71epFA30pi53ZbGFA46A+XYe5DU54pAUE19KnM9P6/NSaq03n9G2DiXcko+hdtGwDYcD1dQ6oIrzqXzflq54c1gKkHIMHrrdO+YOzNhpOMD2vlAcZX/e52KMea8bdwkZA68Jb0b7p1Go/tGa0laY53GzGROKnAou5skAwXZdWq/HA4HZjrBgW0L5kxi//djMgqvt7Y3yfT8bhtngeWeWcY5Ywdo1RM4N4piv5eVCfq+JgKmstLc8Wr26kHPyNxp3XR4VIh2q+yjGpEZOnj30nrOZIwB3QQt5BOYzG+6/ntr3Dx5ucEhQ4WRuTIyM9I7GZ0xnDRjzMHNnJwSBEsrNT9tkzfHymUDODAzOM3klMGaKhvnMMb6xIMy0wmYwbauAtPTVc6lMq3mk9YpjFFNHm+JMVCdVfccI0ynmDOJEXgavTaY7+3GBjHHuRGUORmhE80mdfIbx95INlrC8099gO//7h/j2+9+mvb4xKd/8NM8evAqv/jln+QUD/BlYc1OC4Op1zz0jhss3SEHc2oTRsKcugcnmyyRzLGxNWcJiABaUymUSc5B1iHAGS9S4MxUIHEzSFcmk86wOHfJY1NwE1x628W2+qJ51WhptZif6Kyif9daQ8VII0nmXFVKmtH6QaW5QXdlHWMqs3VrdE+VUhS2tT8P06awag7AjulaBSlIDzVrSEVPJm5iHAhjW7G0CuYGpnsVYcyAFWeeM+ZgRGNUEN2zGfMoJgEwE2eyUK+fST84h+yMGIzYO7HGLOze3EjrVHsDmLTeSS8M0IzuTm/O0mFGMsNoBT7G/jysMUL3yRJ8aI+MmEUSdNZNjbAI2LbJnMn5UWWVydV8UeUwK0sW82MPkoEgnn7eN3l+r5xLZeH/Wdm2cqNkjqxnyPngz2ZYN1q/ZZnsDIHA1d+wKulnkLnhqcQkYjJG0HvXPfkm1zsjSAKXPvHeGaYsrNnCQCffDclNM9Y0TpGsCXNOem3AiMl1d07DaSRrTG4m3MzgFEEE5JisoV9wu1B3CtH+YHsFTo8Aa8RUyeiuU761LryKJHPAXrZUhzyTwvgE3qeFMKs6qLwnrTbKIJiRMISBcejEMrG84d7FfX7w4z/MD3/HjzEeXvO1z/0kl8N4+qkf5I9+/z/DJz7yCf7KT/xlruKGLVXShYUyIYNtCCeMAXNm4TrgUZmrDaI1Yoa6hLVQcxbOY0MNrabMLk2Lf1Z2gCXrTOFn2fEKdNlc9yESlgL81fHCUIPMHGVS5rgt+jqGW7C3KTJdZdLcA6iDtTNdyr2ds9JggA+cJqyYidvArTFjEgxtOlI5ZIoy5tUomTMq4AfmapQJq93LwyCaFtucaiDu2KbttB3tdAXyytQZCio7Jpe2nelVrSnn6S1ZutZLy2BhECaMfKYOVu8uxkVUFrtjqO7MXvcHBf3AVc2YYb4wMvCAxZzWoaXT02kYEU8EL/PbLC8HLR1mI6MzZ7INGCOYA0bujZbK6lyHu9VXphVkE8ooqRyhWtbCYc/UMMerDN+vzMT3Biew4xrNdZ/dxEhJb2cWiVvQmpqV7tXRtnovMQorV2CcsyhFUeV3al98s+sdESTN4VA0GbdkhivSjOqiOVSRS1ijVVk3Z3Ai2XIjpjH8gM2gNSNc/MotgnWdBeqq5d+8sfSmm4MA7WTSIuk7BJNDTRGrzlgzrDXcXd8fOp1aVvls6AGGwd442TuRQMYkU2Vkb8L5cu+oWcd9IVpAm9xpz/KHv+9/yKdf+E6uvvg11nyND7zvWXwzbm5u6Nb46NOf5J/7Q/9j/sp/9dd5/eZrhA/GNrnajqxxoCW0KnPmVMDorak5ZoIUhkF4hzkFF5i64x51jKuXzs6py9y4mTcqf82gNwWcPGApeo+ybsOa4AyokyOVkca5U55Y28H+6qLvAayYA2mNZMFotWGM7guFugnH8jxnqjv/M6NIPtUQyQoczUx5cIrK0kxNpKX3oiUVPab+O89Y2TxjaM0V5M7cPkNlYLEaWjo5YYtJzFDT5Zx5B80D65U15ySnczgo612y0TFmglezYproZMV9OT+jM4dyC6x7dZ1dAdqUUfWM88E910lvddi4mpAz1TQM7Nyk0mrtjDAmrQ64Ovlnq+Rh7wUb1rxokOJSOuoZaJPk+T7pZ2s/pam6dq/fVICfe5+gAqtXli0WCCyViKgAMR36BU+YTbw1rNXPi2CaDpqYUyX7mHXA6P5Z+61c0290vSOCJBjDFg5+pHXRZywFTm+Z9HByo07XzhyDMZUdxKLMxQOmQW8drE5bDPMOfptNEQ280/ygRZRBIBwHs+qAw/Rkqz0uKEQ9ucgd8K3TrzprgegjNhJi0gisaQPHTjYPgfDVGqhyj+qwJxftwLN3n+WP/9Af50N3XuTtL/4Kl+MRS7/i5/72X+fgT/GR7/hjPDxd046NN1/6Kn/6e3+Ul9cr/qP/+m9wFY85zWssgguMS4fmE2s6hVtzbGlgjRnHc4Y4YxSFotFao3X9V6V30V4YzLhmjCvGUAe8LQvLUouURYv+9pbQTMFnZgH+WYHRC/MD3BreJhTAnoGaMN5J6/p9Fq4Rwk330jtN3Cxlp9pIk3M9SzatLUvHTRmIqFq1uVLZ687zO8sEbE+ArOCJWeAduKs9lNmFI1fjyfvEXc2DDNd9KybFKMit5wHHyZnn7Cct2SZ0koiTqG1NnI42JxuzWKm6onDRHebBKveujT6Zhc0mMbOyZlcCkMKuZ+oAmFG4sen+2qwgZY2xZ38GFO8yMdIdQ4fsHrBy/5UISgpxLBVJs5gHT5Dgq3x22wkC+//2aLDvNgXeM/XJrNZPdXYxLBtqM/VCVesZpyrMmdr3OQZMQU97UHRLWh0L0/+7pwD9N7oyjZs4EO0O7XAQoRs4xeA6NrY0pqsUTDfW3FizeFYZXLaON2jduOgLGZO2DhoiO2czIocelOu8o07kFkEjcJvakKHSqzVnto63hvuiUy308GZ1189AvhVFt55y2zvbVU5HODPUALFqSNhennsnbHLRF95/90X+2R/7Z3nqxnj81c/yxtd/GT+9zmuvvsSv/tTP8NGPfpS/9bW/wPW2sq3XPPjcV/l93/5dfNsf+6c4ritvxQkw+pzY0mjNORycpYtI7ktjemUQeSmuJVOQAsqUzRu+LPR+gVsXbjVXIq6Z88TcVrb1MRHJYV7SucQOU4FOYJ6eKVWKIbgjtiBy4KlDTc0rw4p/kO1W+bO6SiXMWWInImsTRXHtzFwHk6BhYWgmyleYDjiRm9XVLtDgfHgCOIHlpBeOeu64p6gkW5GUcwyI28MuK1hOS2VlzVi6CPHpaix5cEsJq65qMojsoi2dkmgpAcDcyNYZbIJvigTcC8/OCLaZRQna6UoUxOOMnYZtTyh2MlkzyNnO0NLSXCw7OMMP1N+JSaBINyvjsz3yNe21WdmrZZb4IoHJbDoUciZW0I1RByZWpbmdVwWV0YthkpVxnmFJqP2BqVnjtPo9eo/egVYNHSezstmppxwUl3XMQkHq+UdUsL7lo1oFzB2f/UbXOyZIbnbJsAu6X6qhBawEK4PVNoZNgf9tYl24mFcK38y46I2Lg3FsSWwwuzNyMjJYHHJptO7F30qCqfaL6eTEhKuoJAOacBz3XkHbGWnYhFHlvtdpCJwJupIUOtOMEYY3qRc89xKxChNXudGODm3hnt/jh77t+2ivvc4rX/scefMW7epVvvLrv8SvvfwSr70dvPxzv8ab2xWtTQ5H49n7lzx+OvhPf/4/5iofYxPcjhx6cuzJxQXcvYR7x87dywtsObIm3GzJKQ7cnCDCyb7gdJW95lhfsOVAhmMxgRtirsx15WZdOY0NI1iikRzVKGgTi70UVJcxK61MggjRq2KKo7q/VhSgHtbU6W5e+YDRsmG2FLVDGZ3nqkVtythHDEZlg+uErdbUmEXyR+B/VkBLN2wWZKJckVnYFIXXZiYz4TRCAocQlrmT9fcQ611Ng8WMC5fCaN94UQHAXA2tgunwnhy61sMIEcnHBqfoLH3BOWB2xNNZgJ4nbkJ4pKCKdm6GYU6vT1GUTZo3GoKWBsEWWpNEkhNwZbG30tDUwWb6pco4zrQYfda9j8+ZpYEZtEo4CopIEzwwKcJ5/bushOQMKJuK74h5PnSeNNpJRLHL5irna4/PSnpBShqz7fzucod00sRuCJXWuJgUrcAAk665YCJBeZ5J/+aJ5DsjSIKx2cLNlrgNZq512jljBhtDRFwm2GRZHDN1hy+WRrPJwRt3OvSUDmRYsjKFMXanOawZbCMYo7hbWSoa71UKxFnS5C4Vj7UFbCEQATXOTZCkW95iYXt3jmCmM6eA8SU5/11kqU8sMFOm11rg7ZKPve+TfPi5F7n5yud54+Vf4aUvfYZL7/jmfOiF9/GRD99hnjauT8Gvvv5V2tN3uLl7l5+7Z1zFxsmFa5GDw3Lk4tg4Hp37d448d+8udw8HfDmwsnCazqPT5JEF64Ck09vF+eS3pRcHUk2LmRuRwYzJGqPuazJZiVxJNp3oJm2NAkmH2CorRwyZqEwsUt2PyszUEZVSRyTZhkSMXRxXF1ZtDj5D3Ekk0et9ggcjkxgKbjNCpXBlmjr1VLqBKCzELNL//n2iFOVIKTlQg2KOyo48z6VlVNbWmtMXBclD60yLqsoN2qYAUfQVS+HZvRsXixKiMTsjJjmN02rM0ZhtYekL04Tf6r5swECfojJ1E56XGUoWmuhZSxPDYQJe7KAJxXUdjJmkl6rNhCHqpL/lMJrtxeye/0ndhU0FJBNFb1qne8NjVJA0hsPwkiSa1X1HvMT6k563aFGZJty3YBAyCzd19QCa5AdNKTn14UWH6qrXg0mMiYfu18xZ5PS45W1G7iAN1KFoAelar+02Rv+W6x0RJMOSR2ncxMBOky02lbNz7/BJIjYTzA8sHrRlYmyYTW3ExVRWRrLOYJnJYQe8rQlzSyNaMTpG6FQGNc+KEjT3zei5S8QLkxSWJI0olSFS+F1HIJi6y5upWSJjgIaFVAJLW7BMhhnRRQ5fWuNuv8On3vMRDrngd55jNufrb32NR1cngoU7vtDuHvC7l4zDwsXHP8ij4yVbP3DACVa6w+xJ84XWF/oilVHvncPhyMVxoTe4Y8aaC8feuNs7N2sS2Wh5YMM4GaypwOA5GbFymjfc5IktrjjFI07bDZnGdQxscdoQ3zN8UXMolZ1EgZOtGRwaMZwo3Kt7F/RwPlgmY6z44kAXuboyjIiVtBXsGtgwMbGl/LHOZGA5VD4Ht4FqL90iSBtqAlY1sKde0xURciY+EmZx6LJMUTJpaYyElqUOUnuV7rAYHBYvTE70nDCgV6DJVEZMwxbj0BsXpqyz4ww31jBiOiuNEQsbC9lViazZBBlQsIJCFqB1WtwMwQat0ZrRmjLKvhktJiut1v0QZpuOhYLH3Bs2BQ+lBflkcwqgZdFopJIyK5lpUY8Wh0gnsgETfBJVghPitloFq6wsM+M2KGfmORM2wLpjvdG800riqwxdcMu+aacZ6UP83Ap+PqPgjTrY6vsjZ6nGJlkQj02nuQ6F2EvCb3C9I4LkNOOqCOJjHZxOG+uYjKGGw9KpGwR+povspbBhNpnNuGYyl1ZdPoHbonklGU6bIqC3qI6j3ab5u4uI+y1HKwSmFbG4swHDdAK1olEINla2k6EM1nY5W0zC7PzQnFo4rV67q1nx9J3neM/d57l5/W1Oj9/mpZe+zr2L+/DC+/hqrLy1GdMXZrsg25HeGuYLMcSA9i587uJwoLUDHeF96wZXp+TROrg4Llz2A4t1jilo4cKdcai+Y0y2MG7CebgGVzE4xYk5VrZ18HjesK1XzO3Etq3MadyMZKUx7YLLiyPLsmIpDp7KwZ3UK65aeNGBMlVuuw6mrKBnljCHJJTmatictfQrkTd0og6tooGcsWJjsc4WQzjeCMacooNYFoVJnc0sEvruoJQz9TMSZaOjNuCuHTFqDYh65KXEUYda/MHNRYnZrGRypUVuTXh0ViMlgGU5KEMT0UBeA+ZMOyqbjF3hM5kTNfuyCdOsru7+ntz3zF1CienSWx8uDkyHaYPcJlhX/NrBPyooGVgOSVRNoUqUKj8TvHcVEk/g76oIghUp3vQ/aVuioI0944NSGNVzzuJ8Zr2Js1zRDap56MUkgR0CgV2Ln8W8OIxiAGQSMxjzRJuhJhWQLghnpqSUM3T4ZwpyMFyEfqWa3zQ+vSOCZGlZmLFxdVpZbybX22CdwdGNY/Mqg53F0abzHXRVwDuNyZawzIlPdftiN5pwMO86cYeaKnidwZUtnF1IKoMgk2YHPcwi2uqkdNIkzOtYlUSFe0GlnsI9MlX2LTa16a0RppMrd55fNj754W8jb1Zifcijt7+Gbdd88IMf4WuHC1gfM0YyV8gVPK06tRv0yWlxLuzA4p1lcdyVuW4B25qsrNykOqS9Hbl3bLjBZQaHnCrTujM8GSO5nka0je3qmjVuiLkxx4nTeuJ0dU1uJ9aYEqA0Ea79cIR+gCYzkd2IrXV1Ia2q6GxZJWJtjLSzoYJX40TyzDI6UW4jwnbxRobtZPwKdFVCg1dDw84cOKZI1VbNUC+uJfsBWfizAWflh7cC/KqZQ7EQWp7xLUIBt3cxFU4zoBemmlbv2TBvGE5YcQ4zS9nj9N7YlVfTnWmNMU3mF5U2RkS9jz1m7RJGL+mmCNuYAnKgBo+qXIcpdoAytDhj7tpzdRlIHz3IKZGEZRczo75p5BTMMacoSft6z2C4Ot7d2tlRKz2lV6/nliaKkd8Cm4JBdkAyospdJ9tuSNPUSEzO9Lkt2HEbkpU+dyHp3vzJc0AvktQTTBRVFrfcUOEwrUkx1L55IvnOCJKgdv1pO3Ea0pKuMVmnZE8lWqS7MIeWpdSthwLOHApKawaH7Lc4UHNlWr4wfEpbHYXrJMogivOoVJxyUxFa5a2dywuqRBmUQUSotJpmkvVVOj/GZM5BzIHPpLVJa4uCAXsWa8QYXF4+y8c//AnWr7yBLysPr17m3tOX8PRTvPlwBbtkmStksrWV2YK+XHA8HGiuw2GxxrKocwniP64hHbxlsFqwHFYuDivTnLvHhcvuTCbNguMhab0Ta3A9pcK56Ss3fiJzZYwb4rQR14O5yTorgWYl05mDnCsjFnp0milQZvTaoFYGG7f0j9xP9ApCEkVMZR9zCisqBZLZ1OKewWaCMVqa7N7CJamLZB0yQxjbxhxD5Xrx4hCur2ZE3jYRdkVMVlMNl5SQkhDq26TucPezzM7NYYqbaM0IOamoWVESxF1/PmfxftM4jcGDudER/WnLxs3sDJr4qjMwn/p5NqQ9z41kkKFCu1uVtbGbnAymlaXfRJzhFKYY2SHaWTKbFSAS5HJFMtDrUBr8PpPonbIrYLqaV3UUAeJxeuHJM9XRP7Cc12B4QSW1d+buIhTK6GY9570b3nCsiwe982bJLJekuD3EKJOP3FjnzjUWN9ZKirib4MQTkIvOSSca58M0E8ZI+vLEofENrndEkBR+sLFlsI7JOoPTkK8iYcyZLMdODGPExrEZvnQylNm5N04M1jBJjlIb2Is826JhvbN0Z1lk1jDnhJhSimDELHpAghd2OBEw3IoonKaSYRpYuvTgMekhm7OcqxbdDOY0iEPJn1JlRz2g5ke23DA78OEPfjvxMGgT1psrbh5d8Vx/ls9tkwdTzaJVBRDmCweHO71zWJZyAhq0PlnqPW6p5sGsQG3WGdnZQl6Mp9MkwrihVUa2kUe41yatibq9LJNDD1ofZFyT6w22yp5s4HhutKXMewPmNthOJ2VdDXxJrHc6F+Cd9KWaZLsyace4tCkbIv9mDqBX5oEwJZJOMhmEDQajaCJqjqUF050bGqdMZq6ie1H8xwqInq3IyQZeEtgq01QBFGGeYHZX8FekEYWM4v9ldWtL72wE7k1BodyQZiozMVycyWElCzUsjMdptCutTzDGCMYUJGFedngBycYM/YoYZR1neOnIxcUWJ3hGgJsUa4tzgYNtbEDOOiTSoXifYUHQ5AyUKzlXbFbzyoJBnIUQvqUwzmYcQ+YUMsI1Tgkn9LkbwbEZS98tBgV3xAxc3I4i6NueiRSFS8qZvSbY4QCe8JmcWbZ85bUgIUewVehsWTipOdOANHlvkrRU42rEzqzotZ/Fux4RFQe+8fWOCJJJ6rTMW8Z9s67mSJVLc4oQekDZx4ZkZM136oG6o2nCDTHZQM10Dn7Eo5Q2h8aaJyZF6p51WpdcUW9I50qTXgmJaIKJ5GwrRdKNwmnGJqL6HJVYBXNokViHmw6HGNzZGtZlr2bWuWj3+aHv+v3Eyw849uRrr3+N+/fusLjxxuNXuYrBMOQKJDKgjBy2TZZbJlVN817cQ70fSLpLVcIi3a43dTVHqVpyW6tAO3GTN6xx4OhdXWJW9tpEnolZwR4Mkc0P3YqS1Rk5udquWOdgaRvjeMPFsXPn+Ay93SlayuG2c2p7Rdtk8DBTOmu8cGerrqSMbxdziQSyK8DuXNeUGmOdKsWqz64swmDvZ3t1MaEadCEfx3MjwBW88SwD2CK5+15xaEFpgwvoTtsxTRfko2KkmkMpBDpl8Lxz0Xfrs5xJs4k1dYxn7g2iAdkJBkN4hLisuevP9UzG3NVbKAUvnFI8QQWAaUY2qXqUhU8o7HFXvMwxJHzIrRyTVLFZa7SpCqERHOgcCj8WW66Ok1AmuaVs37rDpSeLw/HQuHa4HlN4bU5Wk7nKGNWE8l2iGMXRlUWf7ftqzyKjAmPBCBaqFEcImikphySi8URZX8ceRq0ZJVXMYiEEtCY9+7q904NkJKfTyhiFFZeciqwTIoMYopmMhJzB4dg4pJ+zAPegFTdqzSfdj3XS9rwto81c3eCEYcJOwqI6nsKBRBVI0U925Ub9r9W9VyAOtrnKIXlbK0YZzAKTx8AvGtGtGP6FWU3nOz/83bz/8AIP7RGvvfkS23bFUxeXxKHxxsOXpe6xISlj0UYuWi8sVM5BBsU3U0bdmoPV/TMpbHb865RTGxFY2i05+7SeeD02Dm1hm8GjdWUdWtjNZAfnBkvRMgTwxxmamGMj4xrLA+4ntrhg+hHv97nb91abXGmEx1Ygsep4hg64bm03RhKtg1FqG4DEmyoFq8+aZuWWrbJNca6MKdLPG2zXJyuoFWZVTZjmVnxKBTw3O3s+CutSUrljzomfiexyEjKV3RTGiqhfVs2aHXYDlesZOkADRLNoIj/LieaAZd83BaRKYHKSMbAyE55jyDAX5F/Z/CyNNOvcOoFbLWeV/llZnHTNsQN0Z8xPrYvC8q06895YXMHneFg4e0XuK6BwzkxxgptNjp4cm3HoxnGD05isc8Oms6UTUd9fDTDRS+R1aihIJvNWVx5ZdmsIwkitzVnwgNeWy3Muuh8qdStBSqFaFxQUAKgTnwcY3zwUviOCZASsN7CrfVtXR3jbBjNTNJdobCkscEZiLUpWtZRyY6NFg5RL8VblSc7BzTjRptP6QrNORyMDtohb8f6Ut57dwhiyrwJaiB4dOWnWwOu0YxZ1JbGh18opegFngHhwuEnycuGRB3M9cWzGs8e7/MHv/8fIBzfY6YZxesT9u3dpMxh3L7hOjTxo1WF1C7oiimgaBGlTp7tLxdJalZazskrUp+itk+HcjA1boDEZFnR3DqYM53obXJ0GN9O4GolNYXq9Nw7LwrEbMfJcDoUZq01iDnKsJCdmrni7wNrCEo0tFmDB0gs5qnI3q0zcHd0Tds9JZSmBe8onM0psZmqc9DyK/eaOeQMGzRuHqkZayUknBllUssKf5MBeksusTbU3cvZDdKq0tlBWeSZAV1c19sNzz94ii2LUq6NLVUM7wVnPIVMlNBEqPysDJYKoLHoTV6AMF+RbmTFE3ZkrMcu4Nit4J6gOywrwDVtK+pnKxXdGRUaeHdGFZAhaCpwtGzPUtPFixSXOTDFINIZjb49pn86CFsKL50gyCg+PnBAnDu4cj8bmxuNNAW4MZ06Xp6qJ5E1QmKfgi6i1G3lrsRa5G2qUawhDMuT0wpi76Fm2B/v6/rpPWUYYTFEGjTpHCISDXXzT+PSOCJIzgqvTlD9khjCpLIFRUzbJHMSQ8YE3UX+2COmv6bTpxCw9aKxnvGsdm7pxG3i/oS0LnQ7hCrjWmARbbHJhaZxvesdrFo5KwEKJtDDYpVDS7845bk0JYjvzAZvVqbht2IacSto1n/70H+G9957n8RtfJMeJO5d3udoecHHZubp/lyON0RP3pUBs2bltocCMi1/aFufYVQJbc3oUp9QpsnC51hCMuTG2YPWpE9+WKtkag5XTSE7h3ISxpDTwrU0ujs5TcUGS3Ow4UUJmY3pZsY1GYrhP3JyL5SmW5Q6tHWl+kKTPlEXYXh3MKuNziseGTB5wY52blFJIR58ECwvTi4e4c/VKH25t0FLwSpaY3zBhppX1acMMdW/Nz2YHVHMG0726VVQ5pGscxp6nmLLEPfujhAcWszA+07/JHb6JYkzI3dsypB4qHE+tk8ZMw1jVsIokhzDEWVj52d17L3mhOufCQ70BLUgafcd064DY8cikEb7VIaua2/UmwJJlCsra77a1zjQYTVZ8Y6hpeQpnRxnnSHI2cXTHJnjmIll8qHmXjh8WNjZuNisYQ/xSc1VE6aqvqEZtq4Nr1rrVYbFzNCX5tGkMH1WGC8ZKdoZCfe5ZPu5udfgKc66OrTJruqCpdzpPMhJOm6zPBtXLTjgiSVcTm4Gup40RbKkGBFMythhbZSK1ERC4TAZzbKLfTJWo+BG3DtWBdBOlZInG0TQQLK2jBqnwK+phTZNSYDajh5HZRORFHc85jXW9hlg1WCw63rw4cjJBONp9fvT7/xFObz6GuWJsLB1ibjzz3hfZLhfZ58eomThT4vyxQWTxJNHD75WRaG+eO7e7rnefVrh387YZXG0bowWzORcmmopFI7aVNYJhTcTc3rjoF0q23Nhwch3cnG7IdZVss2tERPqR1hYOy5G7x3sc+wXHfqT1BbMFT2OUzRfs2LwJKqhMYXfosRzEDB6j+2rZiEwZZviemcGeFoWlOIH1S94KpaVx2MdI6HVNDvFFxIbbe7Xb/u9Yn3TQyjp3qtDu8j8Rj3BaEbE9yknICkusTL4ySytsQmYZu8vQ1BryRvigmaAYqSztHMQTE3+waGs5FSzM2plesyemlvr7yRQUEqDZQMJZ04aoOaoJNBzFnMxeQBD6e2saCOeGbTK8WN24tqmu8qwUrWbzTEvWOXiUg8C4ODY5oluKz7n0wnKlDrIKPedpkLn3rikoZke/brNJozi1lXnuZs9VipyVVbtAAS9Xqf05VC5sTZ3vsMRclU7+dz2+4b/xlbBOUXEsYevlMJLy9eut0vLWzxhLWFZwXLlZVZ5gobLARSvJhG5y1Y4xKDiCaVvd3X6+iYs1jm50m9jUCSYnaAWfzOQYRscYRSDOML3H6jqCqAyLLSQbLQZe+ueJwP4k+J7v+CE+9NwH2b76kjZrT7abG9rSeeE9L/LK47dYx2CLydhB66xBaQlLOsvS6eYcfZEztPyjygvCSyMdhYMJexxTTu0y+B3cnca9ptIrNmOLToS0rxlSkiz9gPcOvrBlZ3DDuq57kSL3677Q/MByuKT3Sy6O9zjYBYtr0uI8k7Ib7vMJ9UPx2KBgDpGj3TYSZ4vqTk4pZWhF/M44bwja7b/dMhm5Z17aaLOGXO08OgqP2/GrXQEieK3hzehVEQR2Djq7lVhVuoVB7tC04Yv+BWQ1Hgr/wor+U6VzjsIAtV51yO4tBUeZp56hlZEzJq5l9ZYUSIsMraA2tV5HAJNps+CGTohWfevjacr4d4GDOMNL7cMhvNgUlL38Gh9ZpwX0mfhE1VQi2CFgNyOWqe3k0UyOJ7jowbIYPkKD+aIrALuC1LmVUmM7xEjQvpo7AnyGFdShzkzR6qaJE1z9gcggZk3BNAXnLO35Hv/S0aC+6ktYkdcL0Pym1+8qSJrZF4CHCCkdmfn7zOw54C8CHwO+APyZzHzzt/s5ibGFE2PSp4BZetnbt2LFpwZFzX08SQYZLt++ETCh+aC5LLqUUdYJddZbh3hZtfB2P0FP4XqqyxXsREblTDoOTJKuQCoX9i0G21TZN9iw0mULQJ+YjXqNkA2Zd/7xH/ljsIE3dfymSVd+vHef48V9ePiAuY3C6WR/5UhZYwHZnHbsspUr6/xJ4iKLFT4lY2LJIzd1/LroJlHqk+uWXB00TjSGiMx7c6w1ZNARjvuRZTGWJel9CjKoTtJy6Fwe7nBol1i/oLe7HA93uTjcoVtXmU1iU1mAMrhqqEw9fZ37RYJm4jEIl348Y7JkDU+bCiIULy/tttx18/qZE2bQq7yftrvm2Pnvs0KbanllQq0brWnsbwdGBWILP8MBQWU5yDD51pRB92nH5uQAVME8jZxe+OsQ/SalIrIUj1HCkKwMcx/9CrBvZOGv1Xumue0kBq3K5Hzg7DN40sQRcNsAdbsp4+Cshs5uVbYZzF0G6qXOqcMjC7sNgyiea5MFlrI93zNnfdabapjNVd3vQwBr4ZXRNLG0ns0uBjkHwv2WocOOTA5nbvJu/xZq9oe+1otkL9kxdQimnIsysKjRGV3v1SvI9kJsNRDAzyT7b3T9t5FJ/uOZ+doTf/6zwF/PzD9nZn+2/vxv/k4/JKZcr6dNLrPRo3CWRDhW4UtUsyXIIjBLWWLDsTZpTZMJW0tyDkrEpdN+mkCVljWSpBQMlXWande1+HfmtVC0WkbhkpnUqSfcaZcxCtw/aQGmqzxa0AwOVyf2wy9+mI9/6Nvg5beYceJmu+a0nTjQuThccLNuXF/fcLpZWaNO9TZoXYTxxZ1LN5aD5ups1dmNynSWCYfUjJUMY5qxbhs2B9OdUXiNpbH5IIY8NMdQaZk45p2RO07WaykVhcoWui9EGxxa47Ld4fJwl368gH4HuENbFjIbI4BV5sWlSNP0PsTzCwvCKmhE6W2z/BdHwhiM3FgRhmnELeXGgDbZh62ZadhZzAljrVEDUbSQwqt2cncCIRd6J2SonNWMqywQ0OZnMD1qXnZleQYQ1UUuDC9EOzK0LFQq676RnYwQXSs0MXJMcf9mbGUmYLcpdeGC3qop1xfSXW7d7NBBygA3Ui9Y2RRTjQ2zBm2rxk2RqlKm0OKs1uTGSouF3RpkYydyU4d0G9Kab+jjWMlaZQ4TWqORdGsFcUgWPCm1Dca6BusM1lAZ703+RbMCbuSefVfQm2qizp2vVIeAQllU8hfc1AEos+vCJJvV6OQsDb1MMoYpfqj8FgSWGYRtVQV84+tbUW7/SeAP1e//AvA3+R2CZGZgsVV6LGL3NCCkmBk1MU0OL8aIMh+YpbQ4TZYtai6JkUxOlueh99aUDcp8dXewBi30pHkFaCoAlzuJpcY47HOYT1BZiMLuTmOp3alOOCI+qwwV5cXbIiG+Gd/3e/8ALRauHz/g5uptTteP2G6u6WHcO1ySY3Bz/ZDgRuMYzDm0A8fWOZhxaI2DUd32erSpchoriKLKW2WiGhkawLCN6UWwz6Zy9KSTFUJllx1YWmexJD1Z54neNLMcO3JYTtw5GEdbOPQDl4dLLo53aReXDL9g5iXWZBAxbtYzGyOKVxgedItSRdxaqOmeqokXAXPdyFMw5tDmNKN3ZaLgZxxQAXOWRZdKeC/cStQXdOppoel1inw9CtTajVd3I+VdYbVn6OWVLE4nuvfqrHeVwjl0MAoAlFtUMRua1UiJOtjnNjlP9ENjaTODWIFWUy1L0rhXPOYOxarY31c9drKanfJQlMLKC6PGgi0UqHtbpGmvPTMzav58faspM6Sc1f3c1XeGbVWiy9F8uBWGV02U2rdunVGZYOYqKN12qpl+ZY0cbungie0ZO8KLY2/QGqLQVcZct0UVgc4Rzq2YCq6+wyb77zvkop/ru1N6LbRKOosJMs+5+ze6frdBMoG/ZuKb/J8z888D78vMr9ffvwS87xv9QzP7ceDHAS7uHOhsUiRQaTjK4GLeQgZKq/1c9mhAuk7TtMlWJZwz6SZSLCXF8l2ceSYip2ReZehKykklNlElZhHUvVjCMzUY6dYoFBIRYDW4ttQjUy4o4EQGR+uiHCyNvlzyPZ/+EW5ef5WrR28y1sfEek1s8mo8vnCHO5dPcfV4rSl34L1x97DQmxd+I9I02+Ts4GFOTi9Mq50dbMYIJrClOrzDqzSrzr2HM7ZZUwdh+EZfoHtj2vNcnTp9uc9Ff46LO9fY9hXWuOFe3MX6YDl0jscjx+MFrV9y4g5rHhhsjJnYFnJqnwrS4TB6MDoC74tjF3s2BmeSLwWPREkNpXaC7DuWtuOBck/XFOW9o3xgx0rEqxu1WgWPdNuLfBn17qNOzeK22QXgZf2W1XHeZYzVGNv9F4VrC3uMaZKk5q6yUccWIJqTbdFbq+poT5TalJJLZiCV0dOworjsvVsNMVMWNREMMHaMkxpWh3D5qE64bqymY+r8qAO+sjNtSGVgTxLjzRXUpg8plqgyvCCefTYRsVVG5hoHklGNmcGcClJnClQiTPYsCVXAn0MjFmwWdFBgdSRYc+3bwlGxuhc0lvQn6FZBS72nPhvhk3OOaLCbP+0Nu8h9lpXtnZ1veP1ug+Q/mplfNbP3Av+pmf3yk3+ZmWnfZKBtBdQ/D/D083fTm+qVJye9mR/OeGRMdQOjqBAyckUYTwy2TbOEw5LeSmZUAdDrRLQiIhMaMbl3tGIWwXcOciv+lzk3FipdE7aiDmQkOWbNG9Y0P/dkcRHW3UTxyJCa4eLiiC1OdOfFD3yU9zz3frZf/xLMK2FzJfHDZHQbvfHW4wcsh4WFxnFZOCyixWwx1X2uiX3e5JST5XW4YKyx35syuK1yp9WMnjWcMRvpzjIrgEkDx9ouef7FT/PpT3w/X/jym/zcT/4sr772Bf7Eo5/lRx++wms/8iN86Uf/MNcPf45lvMrRjOXyyNKOWBw5zANX48BNVtfVhckGurfVbWHuTZpQA4LmzJhy7Tlz5gBPeis3+JzS+jY0x8QhCvAnpHLajVeHT3YKu3wwCxc2YZehWp29Uzxjal6P64CxIjib3xqp2PRz9rKbXuz2ep6IWjP1+knX+0qrRpJc2w2TTDOjBsM5NCsdtPwwdXi06ug7pBofueO1CKOLuPUK2Olu52mUtr8/6OHnIXCiPalpNjNVpWuOhOZRhsLH6mWGG5oVRfT6XKLAWRh7xNvDVfoifHgOHXhuBHLGwpxpzjDKaZwKdHY+CMfQgdrSIUJ9hhQem9y+R927SiWDyhA5N3qktnKGVerPjnlTClWvrhuV/BQk89ukkr+rIJmZX63/vmJm/wHw+4GXzezFzPy6mb0IvPIP8rPMEuuVMVBs/BbgsjbDgtnqS1hRMIAyz/UQbWVDmIM7NR+5gHmoDrBqlhymMignc4bK9qpdRaDQadRDJcxiKsnVgb9Vq1gFz+7G0kQ7kJOLGgEssBwadPi2T3wX49E1jx68jKMMYOZkHZtK3Msjw1Yeb2/TF/FDly7b/Vmn8cjCx2bQMXDbvRPE8aumyO7Rp1nYA7cj12+dmHnBR7/je3m4XnPz+HXW8RpLn/TlSOYL/Pqvbfz03/67jO0hYz7ix778y/xvPv8L3Ing+ld+mv/91/4065/4J/ngB8DWX6LNr9PS2aLTZgc/wkg8FwbX5yyxVfZxJvxGkXpN3odWncqYyt6jzbNCydIYk6qPEquGXoQLCKSw29lqMxdmvBPqLc6BLVHG463htiCzET9DFpKMVllvJpURaGhUGVzsmKW8GXdeg0j+CoYdy11jjiZKeqtOqriO3VvBR8qsdpf9Xa3lJiwxE3kY7Pmy1wodQyX9nHJO2s0vmu6Pmdx5zIrpUO8ljWJrQIbaj2rIaJTzLg2uKFJVl1eTKVRdTVOScP6uMgIJ9QGcLFVSJSdW5sFWjUWcc/A6H/pKZmJMRpT0NFo1d0qxZUk0DfBTZl+Zfr3Pfe56WNa+r1S0cZv9F8TGDslQn/m3Kbj/oYOkmd0FPDMf1u//KPC/Bv4y8C8Bf67++x/+zj9st9XSKd6s60H3RrrJ4WeqC5utHq6Be4j+Mpz0wKf4ZxYaU4ojrpdpVexlukcQTRki5Tu4IW2uo7IqUqfiCE1EPDubOUV2d512QvxpDalEkK4UD7JXJoIMYj/y4id5+6VXyXEiLGXiEatGI6R06KfrxwK/TZNHJmpDujcpj+bG7j3WarPboZ9LDlF/ktOcbDFqcTk3b2383f/kp7l5uPHh33vD9//Yn+SpF76X5f0r4/oNXn35JT7zi1/i6uGrxM0NY17RbPKjb7zEneI2Xsbk4z/7E/xbrfPt3/Yx/tEf+X7uHS9Y8gHXJ7BuhDU52uQRb1nzTxKLJqpKU6brHpph7tpmTUkWswKgjaDlJBdniYHNINKlZaacfHI/sOJ8UOwg/55sWOG3gnGKkmNg3nE6no1h1IwibXwv4Coo/AzIKuXPpXk602qkcBo5TXOMTA43EXuAaBUktGZKhqAslqZ1Uj7F+1ji8xtHUBH768covXMQc8UqQM4pM5jWnOlZnpGSKK4Ie7Vqerg1eRJksvtqbuX9GDvvs+zVZtSIFJPzOLUXBBcI21RgmjRPDRzLojFFynu1UrSIarZEkNHUsDOxTnZTFiL3eIa4oboV3YxujnVjuDGzYSOIqTWFO04yhuSVlB0hHjsgUPvHCPdaGwkMav7vt8wq7X3Af1A3oQP/Xmb+x2b2k8BfMrN/Dfgi8Gd+px9kBv3Q6XtnsS1Km6tjFSa7+T7UxRWMTAXUwrpsP21D5UcDb+py7xw5DTHSS8wWNRhdGeSZl1XZpqbqBT5SpbsXW7euTMnJqACKR02l65pQaIPeJveaa24Rd3nuqffx6Muv0tdHZ6B9rg/Zrt8kstEsePvB26zbiZlTmtkmOZWZ8MmDoal2LvWOd2l1Qy13gfzbYAzjJmWlf2DhtZce8PDtK46RfPZnf4I3Hr3OBz/5Q7S7L3B595Lx6C7TLrnevgzbDT2dmJO/dbzLnzbjTiZXZvzNQ+PBK1/gK/Oan+rv5QMfew+f/OCRZXuddWiwWJocicgD3htLggaxrUBZmDlsORljo3jxZfGvQNEWMJaadlcUm2n0YYiUvRU2KQd0k7i8NoZoQWcifbRz+dj6Qu+S8EFNNjQqg9pJzaIr5ZYMsjT+kl7tTAerh6J7X0uPJjqXDZXhqEmIWRluoM9hYh1Mk5EL7oyQp+KuqlF3Yc+QEOYYQ4QKoyYYrswYjLFVEKophwR97wRn8Qn9CbigPmvU33dHdCxFE/a0M0vvbUXH2bNo7c0o56yaZjQH9H726t1/nHZMqhKawITMTdgqlYUm58rHrUZKCHCuESfSp0t9UwdHpGh27iXZnKTNyvhVYSmRasV+8Mr6dfhQjarpZZL8rcAkM/PzwPd+g6+/Dvzh/yY/yygwvckFJlPpvOgBxj5+YR8HauS5fFCXs8i1vlMssupy3RBHShkzq7kmeoiETncsWUyUi0OZQawzmNvu9gO403uv9L9S9Cl8p1YeIsxYlY9OM5W+21x56u4z3Dve4bXTm6w3b5GLMU7X3Fy9wc2jt1ms044Lp01E9/RCe5r4kIkyR50KeZ4D3s2rQVBNDzOVH96ryzsJD9qFshObwUXCm5/9NY5j4an3fxvXx3uMvCK55nBsXF0NLNTJ/P9e3Od/9t4P8QdvHvK3j3f5m8c72OPHPPA3eelrX+TVU+fy8uO893DDuj5i5YLpTvig5C4a2RzFTywqzTY3tjidN0wrhUuGxpV267h1daxdksqwCb47vlejAtngeTg25Vt5HvVL0YVQJz8TZY9V3KZpSiO5t40TYmNWIytTEAtVpmdlU0YFXAzzqHEHNW86a5015Z/7xlepvqvGOLeYDBPlyubZtV2u7Fabt9ZWESMVH6rEPxPci6c4C+IJyjAjJGwwP7OM9sZlT6ETO/NFBPZqyBRkYXXPbhkd503OPh2A1H6UrZry7V3YsEuLpRtPcggiGyZ1HYi32GZBDVaiiIpZVWmf95v8KdWLaOcG0BD23cSjFXyitSSyudcPqbtdum3SCBwvH1q7/XS/5XpHKG72E2ekUnjGJipOUTK6TaIlWzMN1pqSse1FixvC7brUJd2d5bjQmgLk3t5S9lmQSygFNaD7ZGnGcnCOXR3HtsFWmGVGiszrrkzziSAdqYzVzcovkrKcqsU8BvTg8uIeb7/6CjcPXsXiEY2F4zJZXWYO27rx+PFj3nz4Fjgcl0XB4nCkHZbisxX6YgeiMLOgqFFPwCxxbCLfTmMxWDx574ee5kOffD+vffYlGOAjeeVLv8y9y0G/8zzBBcM7l/0uceeGB2++zLEMUP/yXfgr959h2Yw2rkg6bz0KTp/9r7h4+Dx3/D4//L1Pc/X4NR61hZOJ4G8saF73WijvzjWcbHNws51gnnCTy00UD1IV90LnQLg6uqf15mzokWe8MXAWjn2RAiONTpcBw5Skz2o8sSR8Cpx5Ts1rMdSv3N23Y5/RXkGqNqpkVrCPqCBTI2R7L5xVQXJJ8R3SduWXHoynuL57E8FthwYKQ8xZpFu0xmrULXsQzFBWO5NmoQM/EOZeksU2XUO+iikiM/hk7jO4s+Aj9m50HQjNb3HLvfKq1DPtSaoQ5cFp5/2nTH7fy1mwRuGVgTbFpIj5SbbkVI4/hIKrT/15Vtbt2BOl+hS+aRTlbk80h0j6LnzbOZYXQP7GoLcniZZnm76dNtS8nX//za53RJDEnJlNXbcRsA3WkczsZA6CqXnNdtAHKhPRiLlDgur4NZMtWC/Ux3bd6+4ULRMFUrprI/eGK8dj4+7lwuXS5TXYBu2UxFw5lZuQIf5k7qulaUSle2rwVpo6xoZO4zCShZHyBLx57WuspzeZ2zX2eMNi5ebmEXG10ts9xkwun3qa49NPsbz9mNHAx2QseuAnc+bJtIGriaEZMV6lzyw4yzALDotzaHJ8nhfww//E9/DLz9/l87/8VcaDa158/g5/4gc+zBsPbvi5r7zGS9tBzuL9LneeeZbHr3ydgwXHiwPrJkC9NWldT+uK2TXbaw/41fnTfPwj/xjeD9xc37DWwl3aBZkbmvGrQfHb3Jhx0n+HeJAeJ8a4YctR7jSNaJ1LP6h8ymqm+cpiFFG4V6ZlYAO3hdY7cwuJDwoLzBQRWpugAsGeDVb5Ji/FONNUxhzqCO+8yOLe9giIpqZhKohpOl/KC6B83npJCIPC4qqwIY0b2/XZwVLs6DmjJOJRzYwn3HBSqqkxN/Y3FDM4TzwrzPocsrx4jEhUkXUoiKivjC72uregGq8SPcywYktpLG4W11a8T3dxNtOCnMJhLRNrtzJPBb4isz9B5A8XFqmhZdDEdxK1LtWBFyWqOvNN8kUpcvfGUxlWoK8tlTUqyXYs55lWJuxYx2jrigFmGhYIxrYOuneiZzU6dy7Ub73eGUESw/1QprWi2GybZGANzoFslz61NDyaVBV14k0CWhcG1CpLkHUz7AsoNZVPTuNZRruy6z8cF+5eHLhzaDq1lsbbiJYyRskZZ1l79QK2c9RIFNFZ5A6+n2FW8r8FQtjXnDdc3F24efyQm4dvcbp+zA/+7c/zkc+/zNc+9SEe/8CP8ft/+J/h9MKH+Hs/919weHjFD/rz/P3HX+crfsOj04kcyY1aOzRT1tysS2lC6HN1w5dGO8hL0s3xOTk83fj+H/suPvY9H+brX3qJDxzv8MmPv8j1L/wan3z/gYu3Tzx841V8NRY7Yk89zetvv8nF1mtOdHBc7nP/med48NbbbDOY6w2PH7/J11/+Cu//4FPcnN5mzJXejThIUCaQysqqrBZl0ai2bYU4sY0T2xy02LBwTiRrX+jtIDqPJ9EnM1cOy4Lbgead3rzIwIuULlPj2aD4flH+kmjUh+WuvikObkiAECVzFEaW8h+tf58jZFic4vaF8mOZb8SuKqmyrUwnPMFm2Z5BGV7kuYTfoaPmtxiZNSud/d7wsHMpPYZm0OwzwYs6WI5Ht+RvVeBezRFk3lt5V444Y6+gTFSnhbD4mfIAamZy8U47445Z9nutge+zv1OjJgQHeK36yjYLKnPXFADNc6eIiiqBI1SRPWnKsnMabxO7LJ500bnqnrZQH2KinoUw7boPbjUiQq/lrvndOxZsJmcnUgfJGNu3jgL039ZlGN0vmU3ZwvSswGM0T5rv2FItsjApGHLvSO5GoX7GA63LEWjXeFvqIZl7yc+K6lA+fO4HyaXq3/dsWFvJ1siYGko1yqZrn36XIUysVCyieFTHbyYjN7JdM+PI88++n24Hsi0c2gKHO3zv3/wVPv2Tn8WAp1//DJ975i/yhafey/d+8NP8wD/7+7n5u7/E0/+Xv8EPvfhd/NIHJv/eg5/iixcPmXTWGJjBpTeOGTpMcrI0aMherZmoJwvGpRvEyjR46qnOs9/xCT7S7+HXN1zk5Ed+8Ed4/fWXmVdf5ZUvfpGbxxsv35z4Aifeuj4x7Q4rl/zhf+pP8UM/+gf4t/+P/zvm47d5+OgRrRlvvvWA97x4h3VbReeiQdwAszjvfubxAQWCRZU+wcgpjfpQyZuLcT2DQ0wRmcvpeNrK2CaHxTSlMsFHKXhWIDgHrN0kYu92yK6f4jCymwixi5QthY3GvkW9FTdPkr0wHaAyz6gqJcGyn0vAKIoOBRXtjZ5z8o82XWBk2ydGbpzBQfMaIFaZcrVAMjXOeEae2RrWasaOIWqU773cBrYUp1iVk4KvcP6GXOvnKEqOwQhlYUtD4yFINZRmnH0o1QlWwMpEjZKyL7HqiO/3ARTod99MFcx5bnDtUwQw0f3yibWxN1hj5tlFiQqoivHlxeBR43UNp5NdtCnKi0FiRP15z0Tj/LPKSyCH7v07PUjq6Og1RwNog94LxM9y3061+AXKaEGZy9MuKLJvZA0y92oU2HnRuNmZP0VKTpUpp+2gM7MxQvNgMkUp6j3Ep5OTqDKh1m475bEThMWBm64mUE7NuRllKSVzgxtef+1z9O0N4uYRc04+/LmXzs/GgA/80ud4+WLD1ofYeuDxF36OF37qP+e4vI8f/L0fY/v4h/h/vfV5fn05YQctriiqRRAaqdCdxTrQsXA87FwiWevEHJgdiNV4+OCKO0vy1vUjrn/57/P+97zI8x//Tn7vd/+jxMl5/a3X+LWvfI5f+PVf40tff503H574xZ/6L3n/++7yz/2pP8L9uxf8hf/rX+Lhw43tBJfHxv37C9vcOBwXvDKmMRQwIDXoiqJoVLk4U2VrJDgLzZPrGMwG61jl61hj+lofHI5qzmQzjnYg5yJO4dBrRMgar9qmNf/EyCazA4Czj1hQLId6X1F8vd39wa2wPEFCmoMk7NQNmOBF16pPqAbGWZNcZH0T9taHKGgTmBdlGLYXPSTWGs5SHEHRcKzKXFDWJfu5WQ0hE6/T8kyWl+1ZPwf+SMn71FRpShy4PThknTZZzDi4Svzduqy5SV5JNQzHUFZGuSJU0HF2vDiLebEAVvcBwSb13GMfvkfFvxlS29gutVWCtO8LClKLuqcaETvVuHHHfVFyQy/3e7nLU3EhKfV6s9Jr6+AgYK4rjf1nf+PrHREkDTi0hjGFR3EgW5CnlRyTGQ7RKuNQadDmhHVX1ijL8CZJYqsFF2mMTBriUuFF0jVnM1lKYXI+H+vkemnkUZ3QVR5Q5NI5rAung7wxeiRLGV90N3lGtuqZprHNZGzGNoyRTtpCbCu/9is/SS53eaobF3cW7jz1PG//0O/l/v/nvyh8DMYf/FEu7zzN9eXgwiePP/N5uHmFmScOv/CQH3n4cT7yqe/h/374LH/H34Q58eORjuzADt1ZvBG+aLxr00IeuUndghGb84kPfzfviQPzM/81d+/ewb/nB7i89zR3ljs8ff89XNx/ntFhXhx49uHrXB4uePbpI6ebx3zxV3+SL33H+/nwp34f73n6Y9x76i4PHr1MjsG9izuY6ZBxBommNgo21CCmGLvha8lAR7CNIZxqBIEw5Y5Lk536d8oYB+5Sbtil0fdBT63jW1PgmYWXFVB95sgWkVm0l5IkxobNKAmfsotWgTpMnMG9ASKteUlBQ2bKM1U9JE6UMTApXLFDkZ4Dr6Az5iizllr1OUlveFU4S2762e54N3VsN9mMZWu0fkF3GDVHfgb1b9WYEgWpeL5Ai860KZlqQlSZvHMod/wTsji+kt0uLjUXY5LN8EVrOlwqlqCmCGTSp0Fr8gRI5NSUmmUjlyYvlU2KCjblv2BpymSpgwcR48fOVaXmrhd3NA2iPUHB4haOoPTp5kvBFOVf4Oec/bahmhqvkYWZZtuIXBmxfdP49I4IkvpQMv30FixhLHSx6iO5GXWylW46TR+UGimZJL1A80A8SnIHb/OMqxhUl9LITbIpPAmfDILrEeRpk1YUp3ln6ZPsZaaRUjwM63oIpia599qgYxKrBh3FNBhyYt688crjK7a44rl8xAfe/wx3XrjPF//UP8HF8Sme+umfY/2xP8jNv/E/xUOZ84OXv8qjX/slTjziaqwctre49/mHfPTtV/if/9Af4Ll7X+A/619gdeOG4GCqH0cFSjdx9sJkycUMPGCJzsfe91Gef/XE663z8Oqa933i07z4se/i8nDB6eqabb3h+tWvcXrlS/TrN3mmDR75NR9/b+PNN4LTK1/m+j3v5St2w+/5nu/jtVf+OqftVXr7MM/dQSYbacyRnAKuDB6OiY0naCwD5lQJuY2NnHLD0fCqVjI7zu89M5hs0A4cEi6z4d6ZTYEIh/Bq/mm2pHKd3TUoEAF5b9CQRIwaMbxXdNq8+4x3KYJ2nC+rXJQaR1SVfdNJMmt+3rpqNswkZTxaLdVSRDXDm3Ow0IHfDGvBYiWb3TNVb0Rb6EHp3tXc2QtaIY17R7kwvVRmbHWwGMLpohUTowJM1lbw80/jfFgngo+8vEbboetuhlzbmaWZqWAclbF7iLSOPcFVTB00ItmrFs/UIMoAchsQJmf3mMw5JfhAWDvtVgAgmakyT6tpUWAyA3G5VxldfYpEc4+4Ldmn7fJOK5chP8Nm8c5v3BTAzixFRVRTorG6HI2tNvmEMmytUrooDDOGwOpwGDUkvZj3mUlUySzDC+lrWzlntDLm3SI1YiGd3hrpg9YX/CCMKxLxNQvncDTdrXkX478CdEItMiezsfQLYoEHGVyvycOX3+L0+Be4c7XxuX/y+3nqf/LPc+/ZF+WI3hvzZuPhy1/h+NUvc82A4TzmBNfJvdcn9/7uz/Ev/LF/jE+M9/LX2i/whaNUC9sGxzBoiR9Cphgl1Uoa62llm/B3/t5P8NTD13gfV1xdBx873Oei3SWtE75yc/WIcfUKD1/7HHZ6hefam7z4gXt8x0c/yhsvP+LOM/f55V/9WX7p8H7+6D/9p/nP/qO/xp2Lxt3LE0smthlYMD1lYOvB9RgwglgHIwfrlCHJpA6xMTSmNUNadJcZrc3EqkSKRRBDs87iC0s74G3BbcGyC3dG2PQs/C2iaDtUsIiokQhqXkA5xOxVeG3iDOneFYW0URWM5JoUbuVVqIN6p/Eom4rSbO9GsiqxjUa67kt2wzv0nrQmJ6pjU4CMMSE74PLkVCElvm7xTYPkbARs5XSO8EoFufrQqXk63kQH2nXxFkIRg93L0+rzJ+uQk5Ck6CbrsSU1Ujc0LqLHXsbfQgs9FKit+JPELc4YmWwY+7iOGdV9HiL/k7v7uALzzoncD1WrhsxOOm/muC1imLQmiWJNG8jYLQzlMGWFT8/iYRoVaK1G3/52/B/eMUEyiVyZeWLOrQBVhAOmyLUthDBuRUyO4pdl1hxgk2yxFS0hm5WgXWqAFo20YHiQ3jGfHFIGFQCZJofkkAxyS9AU04Yfkpadi5FsUdZMJOnO1po8GovGMZtA4tZUMmAaPyGP2mS97FzfXPDKzWN+7ZXPcXVceO9B3LCnlw/T7z/F4gs3Dx9w9/E1jxNRZDx5GFdcXa08kxv3/2bwP3j6eT5y54Ivf9dH+Ds3X+cz/Yrr3nh8VJf7mMkhQtDEVNm4RvCRT303n3jmo7z0mZ/mA08tzPsLSz7kauuEDW3au89w7z3v59HDl3n+uPDe597HW2+8yoc+/FGeOz7P1774K/y9z/wEP/v5zxHxkDuXLvlaTGY6N3MFYJ2DbYSCuAern7jJwSlX0YCmTGItNfgsUtlIuLEBPl3P1J1oLu5mb8TS6bbQ84hxVEmcRQnLA5a78UPgcxMe3SgEu/A9K6w51aGWI7fkqllpWp7pNY2sEtO9JK5UsC340kIB3fKWgmlVDY4p13x3VMo24GC0xbizOIcmb8u5OyYN0XfCKLJ9kF0iC3Wlg2kJJodzSCynDsUKlhqdUHxJc3oasxVGiNQ/bpR7UpG1M2VxxpQhxSJVS8vqYpuTNXeqh9Ej2NI5S0FrnoydG1xZ5bMydRH9gzb97EC0c0Bh79KrEw07V3Jv2KaMZlqDdLUnfcdUZd4hXu4sqEXY6yxq1zRX07ewWkf7TsXGN+/cvCOCZDLZtitOU4C27Mk4t/CNWXZJwi4cF7+JoFlpVWtoESTH0E0wNJtYrDDx7zTPwrBWGYbLs9Lcmc1UDo1giSDyxNIaNE1wjKyAOqM2gagFMweRIV9CE7m8u9F7l77cokisois9vteIgDvzxPHVL5N37uAX9+n33sO9O/fxyyPj6orLGWTbeFRZbLpxiEk8/gr3vnzi7tvv4xNXN3znVzd+z3uPfOXbv5OfsWt+fTzgi48f8rA9YsshpyIuiJkc3bk4wcOXX+f+Mx8i7wQPt9c4vPkqHJ7l9Pghj958hcfTsEPjFNe895n386H3fZqf//Ivk37k5a98ic996Ys8fPlNZt94/tmneebZOzy8fos2nRHwaFwxRrDNwTrg6gRzGjFOxJxsceImrsk4KbA1Z0GKjI78K8MM73I16S1YloXeNMitZYc8QBYmWAT71KK5hVYyVRrvipnqpu5CAmvCy3b+XVXUqvDcVMZXEapObY0RiLwtd7NUQKHvsYJ69r+zlFRzeJPQocHhYLTuXBwady8miylobNEY1lWKlh/ltIb5LN7vQvhkxlpZ0q5JLyMJtOGtFEXUnOzeRLjf5qb36bcTQFVyl/wy63CPqA70oB0OuPe6J0HPpCX0xSHKtHkWf1exTRQngzA1My0BKxOQ4Cx1dDOyCP+7SUfzXddebJF6Lp7JITvWOvQuy7aU+oaQ29POlMBq6qpJgEFMJlHuQLl7QJHnB76rcn7r9c4IkkmNzQwGUdwxeTrSlLpPhK1lNW4EJ1r560mJoNkZogX0IpaGZYHqVSpRp3l1J70IZyMDm7tDrEp6S72nOtO0sIrDa6UUiNywpiwWTFrT3jksjcOCFrLp9MycNBoHM6Dz1liw7Zq7b73Ow/svYfdeIG3h4pkXiDdOnDYKC9WY1HVuLOgzP7p5g7s5uGvG9deuGF/d+O5ff5MffOoZtg+9h5c//FF+rl/xheMVXzy9zlf9is1VxvX3PMWnPvB7+clf+kkOjx8zeUjnDSJeZn3zNa4fvk32jbffepurt9/m8bP3eNySb/vIJxhffonPfP6r/PTrbxE2GW9e8cFv/zbe8+IdHl0/glW41VWuPLxe2cZkpLOuxmkmNzdrmRpspCeHtg+EGixhyrbQxDyVsPIFFD6Y9HbBoV2w2EJzjazN3H8pe3IH9wlh506qJ1VYes13VlBUiVzNgESLyHadc7maIFK6Ak8WdSXPJa2FjCDYuYsz5PFYr7hnV61DX5JlgcMCx4vGoatj3yzlpbio6RF784Ms+pFVeltBo0rtUPKL6E5FizLDXZzhdMAbsTu1i8wGFGEAZaw7aBmhWUG7S7tbMmyV/tld/NfYFTGaOGlTfF1m0ausTDx0o8oWbqfrqZRXpRfE2A8bK1Wb7WebDhpTVorp+fUs3b05W6FtcluvsbskmROaaUz0Ti4P6rU5l9xkYv23L7XhHRIkdVXpkMmIsl/3xlzLe89q4JbVKARqzTCLrS8OOaabb2gWiA6JXbZVL5PQa2zlNoPIDZqUA97FvZuZ2NTJF270elhlharxsdVM8qRUPams7XCgHTqtl5o1mgT4c2XEqFnGR95sF6xj495XvsjNzWOeXR+wPvURnn7/x9i++Fl6XtOyTBwc1hlsZb57YHK9voWZ82B5wBvbxuWrK4dX73L44h0+ePd5PvLiR4kPvMDDj32Qn798yH9+/Vl+fnvMX/8bf5m/89n/A+OVR/zgP/NH4D489DfZTifmw5e4eviI6cHN4wmnE217wPXbX2a9fsyrr7zEz7z0Gg9ycLxjWNzw8Y+9gB1vWKdqy5tt48F64u3T4OY02aaxbmqwzSFP0C0HrcPlcmBpByKnDE6mHNy7NToNcmE2GSALq75gsSMtFywcbCHnQsYC5Quo8aEDt9Twp9g3PUTqcMvKdvbGgn68qEKz1pHt685Ubu/r1MpSJ9mDVtHDoNQvWhezXscWmTQsnvQOrUXBnFKD9BZYlGVchxyANXK3G8vdsmyyF/oifcunoJI3zmbSpb1OU3Bc2sJMqdls3xPFHRQ/QBhdDidmY7Nxpj35dJXerTpPhcG3VjZmadVgVck2i798/vmpho0nwiNDAXFSZhX7e68D0V1m1TEVvKIyclUAwpxNWY4wYgSrqRKQ9NWY53sQWeYe9Zn331b/5pz1nnmq3+B6xwRJs6IbpqRjbk7DK0sU38pqrjbsWUFUAZvnAUk75cAKWO9hxYkso4U6AYcVX2wEadu5k84c9POLOPLolGejZ6stoQJ+18bKvUUPsR0W/LhgS5P5AUaMzpzGYLCli27ihg0j8gW+dvGQ7fHrXP3aT/Hg/pd46uuf5fjzf5+7sar0fBIjy5DTeMre7dSMEZNHTN62h0Aw4hp7+1Uutpe5Y9/J8dGzfMdXPsOnPrDwd7/tGf6rww1Pvf9DXOeJ+89/EMZXWNcTYzxkzLe4Ho+5Whu5Jt4m19dXvP3mm3ieeP3mLV6dG3058uw94/v+ke/lhU/cZ5vXBZoP1hisM7hZb3h8PblZk3UMMmWcsfSF1p1j71zSucAZ4oaQSwUj6+J7xoHh1bEFDiwsviCbs07W0AwFyL3BUplgyqdRHf5Z6QlniRpVmWRAzX8o5Q0VJK18m62GxeUTGc6+cqsUx6spVCXw7o5j4O705nSH3jTrSJ4CUc0mleXj5KxJWeiJkG9RdoiVxYrnq5nfbqXDVi1Ls4J/Yg9+KsPncHXaU1xS9VP27FQG0XOWr2PxkPexC/IBFQc4a1pnW3aZpzDP5mIyRN0XD9Pc8D2kn41tpZyyasT4bXRHpwnazSZoTbdWEIkOmfLHdDVhWpoOo/rlv0mE3fyJEnrntTZUnZoUeKDmzjtelpjAhgatG0YL4TOjaAEtipRdxF8vhxMvHCETogXNdmuqJheVMhzYG0DTkumiNszQSeY7xWMGmYO9Hrd0WFSeTUNlWD10zKA5rfyg9geu4KwbP0oZMdPYxmAbmqE9kNNRm8nFdDY6n2dh+B3utoXD/U7vG/Pth0QRkEdRGprDEsZDS2aTS4+lJiCSSTgs4eWtGMzT2zz6wme4++FP8swnXuSNX/hJ/snPXfAjP/Rd/K0PfJjLP/UHufOh53jty3+bh4++SF495OZm8PD6xLiZkn6ycO/5j3Lv2edY3/4aN9diFty5e+T3/OC38x0/8F1stpGhhsKYg6tt8GgdXG9w2gbbOhmzOo3esYQjVhw7wRPNZPtPLIUVitYx80hPAe5VyQmvsgPJJZm9Ate8PUmQ7VimMSrzSyU950C5T+rL6tJWInQ2jPByyJFlpGyY95I9mFJztdLyR21udQuq5hZv0cxYzDmmoARaI9tW+mfYboJTyLTZpouFsS0subCG8G4NWB3VIQ/Cp7A9ojTft+WjLNAMeZxOMp05RmV1UQa8gdkEX7T5ArKc7GOK4RFmjI4OCChNdVNgGjqYzQ1bnNFacRijtNpC+wY7Yamw+lrPWGWWOatfkwUJVm6YRt+J8FBrpNFaNXVaYb5bYcE7TOZ5blKZp0w7KLogpdxTeADbVVC7UuidjkkCW0gyZtW1HpZMFx3EhuyWdMipY9mLS7XrveaicscLMFbnu+hCqZnTAbLGssQKl8QKR4nQQKxOZRE14qHKIs0rHjV6YDfQEN5YHsgKxGOTt545fTmQ4Wxj0ymN0VPNyDU1X8Vyo3PBq5vx/PUV98YbHPOGu29fEwmPkWNKhnHI5JhwcjgkHFFT4DHBAWPhiPnCkeKhhWHbYx5+/pfpp/fz7Ec+yaMvfJnjyfihf+1fZbv3HF/8hZ/ksi9cH+4wHiTbCbbrwXo60bjLM/c/xN07z3B6+DYxT+SFkwd48ZMv8oFPfRuPrtXY6EW8jnRGOGMkYzrbrGcwp9Q+3Zg9aH6g2wLWmOb0ZojGfUumzuZs88AYHRvOSG0EWXEdCBopIFq4YjUOzK3YDAvORE4z6pKaaWiStXYOuju9ZHrKIYmsgC29MM3JGt2pw1k69r0MVCPQNHCuSkjPVl1aVUdCyysWpDK7NRMbSY7EfTBz5WZzttHwKRqLTEtWIjeRfCzkeiOWL95kdksmc07BDzX3RW4Vzj5mbdZY3J07aHmLd4p/CHNDWbXBXEwqsp6yIVQrXPu0EgutfwDR5kaovzAii7kAZM2T8X1EblZXVmeKNf8NmVwiqk4SxUM2GRM7WFdC4kNJjsZRzJ3+rDqiOWbFibXduQsgyi/2tmKYA/YZRN/sescEyczANjH+p5fpbUX7keKdzYR1bJCTWYdPNSDPuJH3RnNRJzSnN8sgVDffs6afNIHHZpCuResoBc98wiG7L8IpMWE20cr0oLCNVlSNLFF9Ed/xxjI1OGLLlDoik1NMqIfuzfGWXK4LeOfBo0e89vc+x+OXNp598IhUzsRWcMK9NI7WOVpgIfnhIx+8mcn78sjd6LQug4WWt2M9va3wtS/y6MHTrPfvMK4fcf25L/LqW/85L335Zzg9PdlurplXV+RpxU8b9+8+zbMvfJL7dz6As8KW9MPCC08/xYuPOu/7tk9w7QfmCosvKpdTTZA5NphJo6beAViydN33vned0STM06buaOtOW5LDseFdQ7NiXFCuxlB8RG8lpytMuBjNCmzlDTn2pgkG0dRNPQ8F3rW8Ve2FlCgyU5GdGCTl6gAU5IJUI638O7V4NSVwH0WRexka7fy21HA3eQKTtAqww42RMDdjpEbrjuHMuQErx1yUBbuMWaaPGksgLFIsEFFc0kQ430cRRFqZBoVUQVlZb2XS9QnLICJVtWR5P9a+KjbOOTs2B3qSVs2XEcSme2NWq7UwyFnQwz59K5sqMcs8N7uK2c0+e0pzkfbEBTS6tjiR1PfW82hZj5wsDrOUV3aeUjprLSrLtNrrbfEyzNgJ91oR/DZx8h0RJElkWD3UWfM5abGbehZukqVgGJoMN7PI4q6TyKwDC+RSFZU2T0w9rB3n3IeT53nDqYMZdQp1anVk1p+EaRpZg++nnMFnnJUBSuml196Ggqx3YDpLj5IdJzGC1dVEWDJZ0ukWtIvJFvDwueex98Pxa1/mZU8+SPJCa9g2OVCjT824MuexTd5vC5OO5+S+9XIt3+jLItw1Nf7h5IGPyeWbD8l5wa9+8Qv8jX/7f8szn7zHsy9ecvVo48tf+hL21kv4AMsjT915nst7T9OOzunRhlnHL+/y3vcufPsR5nNP8ybOwNkiWFISsG5NUjtvdJ/isbUaoRrJ4vtQPmekkUNeh255llHSJ73JtMCWIojj9LNp4R4kO6SyKopCo2bEPg97Io2LAqOqgdsmDQEWdjY82TuhsJfPQDUdIuWIOUtEsNs/Y6bspoZOnfeaS6ZqqYxl9w2YMeRBWQIGFTHOzEVc1s1YhsIXAB0iB9NkNLwPbdtpbftkQSuv08ghTPacPUfpmmUCY75hFPODUUIDhOX0XvjswDywVvAdTvOkN41S2D0dp8GWSQxVcVuRxIW17rQfwVCZFLeZ24OzonVQkKQgVMwkD90HdJ1bBM0YrVRN08HlRO6FpViNA/GmoOep42TEpkO6OYspq5qo2jJXq+d3hUma2b8D/FPAK5n53fW154C/CHwM+ALwZzLzTVOY/7eAPwFcAf9yZv7U7/QaJOQohQQaTt/QIpN5aS9j1lld5aIz7PIwoPVOb12niZUTcnFUIym6Rkm1IsEkiK8PKYwC6Hhpv2vHjSmLd2rxxKQ8ubRQNknQppWt1FSW29LxnNo6XVmPRdInwkaXZOvGBUemKcN6g0u+9FHnk4cjd//+53nrK69DwjM0ccfcucnkwUwe5GRpg7vh3Md5yjtHE6HWs4lQjCRxtmk2yOPxCH9ww/ExPP1cx+6+yNcfnPjsr/86L7/5Es/YxoHGpPFwfY07z7yXw+WRLQYdeP6599I/9R6WG+fvbY9YWITZxg2nzeleXEQcXxoeztIWYVs+WSI49APLciBbyUhrtUaGOtFuMjcxbUorDf1ushSJZGi1Mc5VSGVIkUn6vjtFtI694WIyfXCk79ZQN2W7Xh6MPGEwq58iR/EWe/ZVm4oqo2seNlDZKmeHoBTnW9BMEzc3GTLPqFEVIgqVxt4pFoXKZ1n5BcMGs0mJJlw0d/o3u9M405hVB8sBycQLdshtklZzXGq8gehwQS9fyGhOLtX53yRGsKp2end6qYIMeRfMFObHnJW4AMhibQxVWrPMcqPMJMxqHEOVvCLwx55+o75KTY60Mhmx4jgj04sYQmCtNPJp+z0A74UlN31926LG+JZcydABUYdGBoxqHMVeWnyD6x8kk/y/Af8n4N994mt/FvjrmfnnzOzP1p//TeCPA5+qXz8M/Nv139/2SpSFzdJme2SVsdXKN8DED+xVOkTOcj0RwN9M3Di5gCjzGHPUCZ77C1UOsC9C3Thq0brt5beA7xmriMFTTRjfG6SZZ+mUCLi1yQsUT+tyKJmScIFO0djVFKYJkNuAawu6RXUlT/g2+fVnDlz+8Mf5wP1Ltl99ibfn4GmC983GlU1uEh5l8hYiYh+tc8cWLll4bME6C2xrRssul5WeTN+YY+UDfuRHvvgqf217g793Z/Lw5oqnDxPuL7zw3vdw7+Ipxs0JAh49eMw2JtMWLJ/j2Q98F7/nrc7bX/55fv7+St+CC5K1H0puqPGxM4OTN3qTPzlc0Ah6O9K9XHtQ02x3mo+E9UQddEZrg2iyyogyDLQUNhaTwuiasqahpsRIfW4JNaKCq52bYFHcxt06zGzWyBAqwKtju29bJZZqJizF+0t0qO+HbDaYbXcDl/ekl7FG7lzKc1Bwmu+c373Rk5p4mF3wiGskR5oO6fRZHdhxyzHMvVEij5vMmrdoyEk/1ch8cj6StZ1Hyfkw6EWnmTklevCUVwHgiw47a+qm76X3TLE9wsCm4aUomhl6VnOX8JZhV0KrsSnacwbn5xDFFvAzNri7zyeSjBpWJ9SkTb32ZtpH4Tu2KSwSlOBE1rMOiTxaQrMo2XCeq4Gw4ov+bhQ3mfm3zOxjv+nLfxL4Q/X7vwD8TRQk/yTw76ZW2981s2f28bK/w4sQUW4lBD2K0uGzFqkaKNacngIjZiknWhO9Yid/iLbhQCdtnoXrlju5V5mcsPniu5X5hTXZ7nvufDNqhEQU3UJ4l6dOwxkC0yfSdNcAEtzE1cI2YooWsztUB37W2EYGG5zxrQGkNbZI3jpMbr79fVwdF+794pd4fBp82KUvX5tzPeDxTO5a51hzXmSPNoqLKdKt0WkpJ5rWLjnFNTfbFR+PC/7pLzdetof8ynON+ZRh24F2aiyHxrP3nuGiH3jz4SMu+oH3P/88z9jgQ//R/48PfKnzB37oE3y5fY23DhuMe7hdMVLWEjMXRjiRJy3kLjurbvov6eopnMnSpdlOdXi3AUs2ZvgZH5Rxw2SpQyxSRsZWrjdZyoqc2pAYtYGqZMudV4cyTpNmN/PWzktrQXipmd2uAxFhVcIXridRnHA8T5mJpO/ZSGjzk8XZrYAxA2/qNN+ibLfXPqp4WhX7VQlpHaqJYpUgB1TjyW5LV9vBxtInFxm8L10YsZsaZJV0FJ5QDuA6rLFkyyKLq0Ff0yDlyNRcB3+6l1AjzjDEKBK95u3ofXoKLBGFPciY+nxFw9rlgbSuz76T9snCrXeOat6qg6pE33mWykrLfKNudpL4kHzSXTxUNxkRxxgab7LX+JVNfrPrHxaTfN8Tge8lNDkR4IPAl5/4vq/U135LkDSzHwd+HOB4eahPXgtjyiXS+85T0+zrWQ9Rrme3Xat9SDxumiHsaoxYasqiOoi7VM1uUWmePNkEzNesdvHeUHYoioFBC01gdHG0WjTGOiq70aJxqpyX/7SWh8toYsKZCuFVIoWVq3pOdfJpogfZwlvHAR9/jo+twWu/+mV+aW68P5BbusHJjQczeKot9H4kh97HGsn1ekPvnWWR7DLmxGzh4M4pH/I6V7x/df6Xy3v5Sy+9wS9+feWP2+QH/YqXnl547YPPk/fe5rCdOK7JB7ev86m34P52wfzUD/IDr76Xy4vn+XdPv8JreWLmqmH3YzLW4HQaTL91T+81JmDMmm09C/gfk1EHmGdJ1FxMhy2CPuPs/mTNoDeG6ftAWuYoMnPUM9u73BkiT5Owm09Y6cR39oRVvZXVTNq5fvtWvQXF6vsqaGla4qzmYjKnMkKz0KFU2Jci0S0FyfcYRTVRbveDcO8K7h5ZJGgH6+csToqSPZgX/xHOZbe8JpTm6SOVomjRe1OZLBWajZBjU3OV1oaw4cqc9//5/ilKA00TXzjOjR5ToIwyw605PZam3kJ6SRxV0UjgUz89ixNtOjAreylzjf3uKX/P/Tvq8+9TE9kP0p3bWjhbZCNcWfB+471Kf30eAGXj/s0hyd994yYz0+x3sNH4xv/uzwN/HuD+s3czqiHSnmDaR4oyID++ySa/fZ1ORXNQCp/sAKTX8KS0qbGk7kwTIV2dSGWaua9Wbln/8eSnCC2KHLJvMozoxnLotKVp/vDqXNiCjWBdp9xkqtSJDM1hzgNLKwykFoKfcaRklnG2kyzeoBt+2bHrlWjGgxx8+Tuf535fef0Xvs6z7hwzq3mVnMw4tANLPxDrNd1F1p4WjHmSjvxwJMJgOM2Mu9whecRL8y3en4N/6fIZHlwF3z1v6MDvuRl85uWv8eVSljydzscPz/LM3WdZ7t9jvvEG/MTP8j35j/Cv/L5/gr/0hb/CL7dBG0MregYWmiTpJpz3EDKO2FKYc4aaXxmyLbOqCtqh0Q+d9GRkcEo1y3Z9rfTLCl5emdw+i2WENqvbTnHZk6vqPBf+VWFL9JI0vHeSqJlJtieNep4u/9HbdKOCEip/W3fGnER0sQr2Q3bvBtttmpK145UxpqgwFD5XOdNe7ZihYOR7tiOMHgrSySDTz1mkUSU+dju1UXaykta6lGhZB1HUjByfopFpTs0tLFUyNzKTda5QYxqw4lpWKav+aKsAqX+XKaxVnIQ6GcSq18HRIUvJ5GYsreMRZcwsLPNcyZVyxxThhHGGSmS5IlXGmVVtmp9n1mQ1+txF5xoqFejFsbZqyO7//ptd/7BB8uW9jDazF4FX6utfBT78xPd9qL72O1zCTnIO3IYInmZ1UmlRzalBVBbCcqZpjkhfmnCeyOo8rzIZMJMrD5CzqXlQAm8z2a8Fk3JVUim1Tz10Y4uBb4FHcLLAmjG9cehOO+yYEdqkp9sh57LJ2rXGYEOBYZazSXgHUjZOKOvcu3i4PqtnctGNNpy1Na560j79Il9/64YXv/gmB4LWoA2je+M5v6yPpp/TOXBsznUE25zQh+zgMhlmtOXIRQzujuDr8xHPPV75dju3sejAMwQvo0bCwWCdN2yPH9Aj8ZuNzd9i/PJdPvnsH+Jf/eA/z//z5f+Mn775CtdNg+wtpKxYD51jPa9psI7Guk7mukEE3Z2jN9yD5eD0Y9s5XdosQx3hMQfdL4hyXLeYonfNapJULMqQ45BXybhngoLGVFoN272qpaferWBajYjQxjGiL5gf2IUEhZ5X20SEohmcRQR6hGqCSGIXhNW0TSt1f9bo2QbE0HuLZMtJxIUsAGeN3qCR7VAigZW0xoiVkesZuskaVUu5ZC1pZdMmmCBNdJk5pVCb2YrdMSE2RgY0U1PMTAMho+hFlqSrUsqtmidmhbOrohtenGaqaZfKDtVgdmaTPtoCzJuaPSYTk4aMYLwrGEfJFtPEOBDVKivAaR30NPFWW8OinyGNMCluog6CqBERLvxC1WhBGFFYcZabb/ht8+0bXf+wQfIvA/8S8Ofqv//hE1//183s30cNm7d/RzwS0Ok4S+lyOHMfdfOTWVMbcghbUC4/sRpy1ayp9b9rO7PoINQJQ2Le2QX81AliyM0EdCqlZXkXlm0Upfl1YYW9HzTqtClryCnAG5MD+oxZZ3fhmO4a+2mDKC3ujFF8MIH8bdYEOpy+Dg7d5HrtkN5Rm2nyYAH79Ad5/ZWHfPgGttB87hftyAv9kiX0sEcOLJ2e0gvfxMrckrv9LsvSRWofgzt+h94bYz7k1TxxoPFJCtIB8APvLwD9njnPHJ/G2yVrGIfHV1zkyqOf/SkOD25438c+zL/83Z/mvb7x1+bneRTBui2MRadUS8Oas7lxGoPTOiQxLWzL+8KhGYeLA63Lvq5ZI6amQBIUCWjR1qpSe84a/TolQW1l4jCrCaT6yssIVglZUsTmKs1umTw7KFUZXRpJmSuzQzOorN9xwizhgktu565gI9WboKHmGiOxl6ASRdzijTows0Z9DOnR08D06s06WXZ8Y898Yigxq//tw7CmUXJFO9eqE1ODqKYWTmQenSG+cRLklgUVlF484hYL9lnqJcFd5oZNDUozcw3SKrJHpNXh5GeRhaEeUetdB4LFbRZfh5iBPFAb6tKPPfPcy2yTw3tU/W3C/XdqUOQOpdiOhWCZmnFegoCog+Psp+l18LkJS/7dNG7M7P8B/CHgBTP7CvC/QsHxL5nZvwZ8Efgz9e1/FdF/PosoQP/K7/TzQfiCZq8swlZMWdjiZQzaRFjdZuEglmcJorl4ebOI51aTDVUtiLsWO6JROMZklKFAwI64zKmGX4iKMkJYxYIcVawtFD2aUV1ucWVb2cZPMjdRM3IHoA1LTbgL7wSdoFWjIc8YVLpXqRSQsvOfJnzVUXl9Ijk9d4+HH3uB7TOvcUx43jufvHiau9bVPHKRcq3wFbemU3gMhm9ceGdxr5K4cdkveI8blld8da6M3vlENFaCL3oy25GPcskxk7fGiav1MVtLDg3u2sJz/iztpV+kPX6Fy1c+yD//nd/Jc/fv8RfXz7AdkzgN5qGxNmmWZaXl9NZVhjdYWmNpjeOycHl5gXWkxIlGjGTLwYyBIZ3znEUC1uDx6uruyInKLGIvqetrIdYDIX6l0EsFRo1lKABsL4vPJatWzy1XVzCH6uZap9X1NVPjzyW0Qc0bBYgdC1WAjTJuNnbgTUwfK+xV7zmYZAs1u5qoS/o8CbYpSTAFP9FqqtkRo3Dx2waHoUOE9NsGYtRaMfV5sxzBd//HzB2OKry2SteMZHgQ1hmhe58ujbflLo+U3DOb0/dhanV3rZgou8NPtlLKWP07ij1QWHKc/22j0dTlp9ROXc/GA4k8Sk2XFYjbnhH/xoDGmdmyQxSq57/p9Q/S3f4Xv8lf/eFv8L0J/C9+p5/5W19E7iG9h0B7k5vO4k0hrHTWG8mp3FVadXSjV8nR4/bDZ9F8LKAGHqmTVbhG7sTfAsunFLkORFEMZoqfdzBljrSF6TKbiIgz/3JUsGxIujamLEwxgeiWlUmSZLbCoeDsPGLG8OJvjkFbFPQVRAcNWGPgE5Z24I1vfx7/2g3x1pt83O/ywuEOcyTWO2OtxeOTXUe72AVrnNi2G5be6N45dOe0TcKcZ7jDkYWv5SNO8ZiegwPwBwJ+Ygb/tV/zbDvyvsMdnvan6DMZ2zVbXPPm9crF47doD9/k7psvM778Jj/2qQ/wez76ffzM9ip/h9f51XjEzUVCDnaKqTHpFnQTufxyOXCxNNmm9YWWCzlblUgnbfDCwLKgMMEbGh8rCC0YLRVwCodkB/4jqsvcAWGF+zC3uXdZxf2Cc+kuvNTqcD27/JQUtaH35L47RdWc6GalitEmnBsK6og0P8dgyxpzG6nPVnya3OfIG7iZskeDQ3WIJZjYgDKKkCccbkZ3cWTHPrYVLTUxfFKNstz/SiqhrDa5W2VZe1CyOuDdSxQTQNB3w9pIGg7ZiQbmUbBFqbZDmKMwoY6Xim1PykXB0WdMh9ESn13Qyqx3WCePcRuwA8kUrRutNbJXxlnC+x57kNThFl3O8TGyDoesRFM32VIUoz0B/WbXO0ZxI8qIcdHUaTIzYuGsdQ53bFmwCpiGQPdwF0fNTKabs7LF2G7pIYj064a0rnViRykb9raTnI3VWQucduzMg0iqrcsmfoYw7CygOUHO6N052pG2aWTtwGokhUjCbsahNktk0Q5qTMFqiafT2xFfDlgvt2VbYZy0+Jpx7YPtuQve+rb38rGfesgLvUjjs0yJM89NAffG0Y2e6sBexwnmDffaZZnwah7ycKfFwjP9yN31Sh1dBJn9gB/4dTvwmI23rx7xpj/g7tJ5oT/NC/kejrkI281r7OoB7ernuPvgl7n7M/f46Isf4R//wIv8zPuu+Iu8yq/fWfG20G8Cs4XIE0vI5CIMojXMD3g7KqOYnemNG2vAwGMjNvFWmUGEsTpYbgURyHzXWpwzCTn/oHJzBslKa7L4D/S6M7JoKGqueQpDE8vBpcgpErVccHS4OcXVQwPhJImz6gwrUMTcZ+oYhEZUbHNjKzf2vksXK4M6bMZonenG0ZwlnnALZwdd1fE2D/lIEtWcEvPCwuTebUnPanLVftmVT/osUdnTZKaMMCIq2/cDoyYkWlsITsJvUdbW06XRByJHlcpd63YIa4/U8WU5S+nz/6fuz4Nly5P7PuyT+fudU1V3eUu/193T20zPYGawDXZiIUCBIEAABAmCtGiRpi3aXIJgyKIcCjvCluVFlGWGGKKWkEO2ZNAURdIiQYbNRRIhQSAIgiSGWAbgLJh97em9X/fb7lJV5/x+mf4j89R7AGcGkEBFtGui572+fW/de6vOyV/mN79LTozLFlmicLoJTYjFTZ/DeV0XO0RlGXyCdJ6YiYJroTxE37ESxXCB11zyvYBQKgkPbNdYOnxnUoMluvaLPN4URTJyNHLbRZx8C7+sDIWiNfziutFby9wTzRNmOb0EuqQYJi4ck1AaSMmuUcLUM5ypYysa6XpxMxU3rM8gesAwkTi1hhpjtVmnzcmvIzTeOlaGUii9Rz50z3anR5egpVAEBok/zSSzBMFlYhyVcVVYDRaZJ7WgdcR6dJkMIdNy4oL7xNtWfM3HTxhbxdqMdGGg5CiddlqSN58JVUY2pWK9s5+NYShULahFEFctlSNZQTmm9zMK0X+fjyvePlxHZ4DO1mfuTRMXfp9L7rHWFY/4zJFNeH0EPX0W1Y7v9kwvfo5HXjvntz51k8e/8ev5Kxef5hfL63Q3xqZI68xFYKxoD6DdqYgPYUnnJQeshT8ZRSqKmuIEfhb4lFA9Fxha6FqCyuLLAiIPrN5Ttpfd4oKZEQuvgDcfBGg9LNnyZfTODrXJYQGMSaQnZh8aRTKIkSkwIA9lw1sUKDdn7i1G++yaLl0Ym0eo25gdj4UOusmSy+PLTRPb6ZyQGtkt5SLQ3R+yP4sCx7Iph9w466GLTsl3dH0p93Xi55WafNQczoJjmE+T2KwvC9Clw44XEHKxkn87yCgjzdIPG2i3h173h5+KhDJUsFChxsIWSbYCh4NAWaDYRd3uSSpPW7gctReqkTv0uT3ARb/I401RJEWFYRVZMOQFKyVGGakKNUjcYdUUhrgLPSDAWAOvwbPzfMMTBJHyQC8akre48ayDNzvkmSwny3K1qEoqepZElMWfLjpWp6QvZAQtFTVqCQrPIjXQZmgHHYLBl9cF4c2SJhtDoQ6FdWJyuipoqZjDLAncGSEhKwUqXNyE4ytX0TtTYLEYAxYxrOnCHge2JogfmGDDmHrHmjNKR0UZhzAlqFbR4Tr3pGDtnM9rZ7s/40kqj9RrrGXFCZWrVDqXTH4G012O+ja6lOllzs4uma49zenRMSMrumwpL7zCV6yv8Ee/8xt549Y/4FNyzs4naimR8tgb3js6zdTSkvs6gNc4JF2jgHoUUSMCnNBY2i13RkUjglVL0EDEwXooo1je/6CLHTLZJTCzBXuDvOGI6T6KZI91xwEfi/9u+X2FuB7VOmmNH91Mf0A8X5AVTHHT+G/Bjof0TXWEuVTEapDYLRUuD3MplcTr47mtR0E29YyNjevZWqO3nu5AifOVVPJ4llmJbPnFIiwc17NKeTs4h0VXHCMsLmlrmyYR2YGHRtqWm+RQ8MhXdHlBQ2jzIJkwzHlDbqg53guLj0IU0/yB8mdMjNXje/eEQPygFy8PvmVypz0pgUWDRiUJoyxS1kPi45eoT2+OIinCas0Brws+IxR1MvUGJ3mRPSRo4QoUF0lInhYKRlzKrbfUAg+IxQhbUoeqHm8CvmhniVde7WANTwlfupJLlWA99ANVB6J7sx64p2iMxGpRiHWAPhuDw1Ad1YzL7KBNIrO5KFIrMg6shsJmNUQioAi9RxAWhLmAiqAD6ABc23Dn8WPecnvPqRum0fFID4xTNLwZRYRSPcfTeH2qxgjWPF3Ua4mLuhUm7Yxylcthw9jOmdoFn9m/wR2beXZ1kyPrqLXYRssN6u72g/EFWE33eOGNzqWecnVzlVVdIVWQ54yb/7jzL/6mr+PP3fp5XljNqVbSCLdKzrRJbF41wfueqphoHYJUrSUWDEg4c7sq6hl5KnE8aHZ7MZrH+xzb6xIFsi8g1NKJxG8hLhQ3WCaQLHLRsATYedBue9Jckk5jRo6UcSh1g5bxEaGljgmiu2JUPEUR0cUGK2OQFdRCJwxmXTP6wJYNkC3VOwLDLF2NciQnf11ap00zUgviBdPopqpJ4uXZOS8GvlmIQ60WrXHAFaAWC0XPzlBIDBWIYL0SB3G2ljFq+68o7m4SBHmCoB8a9xBf4InvJia6NKDBTY57n2yI8odIxVx6X8alEMmVBMwStYLsRKNjLlls42lyQhAHyoHJ8sUeb4oiqUoUCO+0bkgLzVrTng7Ehlllno22N2zOUVkCN3IXRFLyn8UTHrDrBaeMCUZbdB+mlkahZJdKnPQyAAVTRQcJWzMJ37yI88wxJNGdocSCxkyZkRztoXRQjeUDxakJrs+p6xbAh+Dpae3oSjHNDah7ZpIIobaQcLUegFHZuvHyzcJX9znGfkuEXjykiAZVK63PQdIeSlAofMGTYJaOqdGshZ53yEAlF07YcFwrZ7LmVrvLnX5Gn2YeP77G1WnNpg9BUB+uotNrh7Ney1WeGq4y28Tl7ha9HDGMGzbeuPYR51u+/Pt45Z3fwF979ee414xxWEdMRx0odUUpAzOV0nuMpsuybaiHjqdIWppJLC08LbZaiZtcuyI9CqsQ1CMzTZpQGlqoHPhySyd2GMGww2JPLfX6lqyKnP+i2YobLm7qoHT1RTOf6p/wGNAkRCcVJbmaMRBC3MaLS5JCLQcKlCa3VcyAhmuPjql3vO2Y+5z0nizzywKqB5bZW4siWDQhLM/cm0wjzBMuEIkonEWUlmFdy6si1nGRMGJJNoEt98BhoovN8+IFHC9n3lglTXRVDoozO1TExRBhObDIexhihsslmITKbnn67g3t0cA0i+Wset6fElo38aAyxc+0KHZyl+CyDJuH3uyLPd4cRVKEdR1ojTRnNaaeuTOuzMyBrzSnT460eDWXICPxBXdMXmNPrtsiZxs0RjUpuA649HTr73E4aYL3Enb4uIMSskiJDJzJDPUhcCBC4iWlJnUhVQJIRMlqjNIqGpLDErhMmxqT6gNRfpHYtqdHYF9ynPMiaZ6UFY0sFCkxFu0x7tcE3G3piqIjUdUg4cpywcXFGKYJ/uBAkIhg7WT37J0iaUoaTqcclxPGCkfTBbenxp1+F9lcQzcrys7pR4/F+zffw4ZTyupRTr3TZGDbJ7Y202YBv2Aljv+T9/P9X/N7+dTZK7yXTzMwoMOGMqwY64ijWCv47OGuRI7TuU4qZTxcMw9yZyQoYiXI1CW31gsPaiEbo8nPy9cj2AP5pxwQs+TI9sONZP5g3IMHHVIMyDHCKUOO4+klmp3Yoch4ifdJNfvQRV0iye2LA1VSUlspFELj7rZEKXTcGliL5Yg1rM+RKZM2b6olrufc8uZLF1DkAilJdN2upKPQgsrmb6gZA5GYvUPQsbCEHdIDIV//YvE7OKR5b1u4V0g3lkhYKYcjiAWrXDpDc8uDNv+dJP9bTAW1Jk1PlrJG3Nd5iPQe0oA5pySXHoq7llipkyU9D8VloQZh75QilS/2eFMUSRFhqLFYwJ196yHzG5SBkgaeYczqPXWWDktWiRC8RvVOyaWOEdEPy9bXLXaSM/HiR2BRbL4l8WwngP7qPTAR88B30hVIzKnmDBIZyl1S3G/Z+hNjhCwzuQbx2WjsgLlWemuUAkMWv1ksxkQzaIktqRPM2jjZawkTWqmGSaNrYcoBUFUP9vQLUI1wcDhawHKyX1HNFEIPs1YkJV4Ch2D5xJ0UZ1NWnOoRq1HZlgs+v3+V3XzJo6vH2dQj/PRpen+CIh1rE1iniDB0ZWakTRO9GWbnrF/6APKP383v/sZv4tNnt7lkExZ3dQUMtGb0+RKfnDaHzFBqYRxWy0Ackrp41yLfBQ7ysoXW4xCLE19G5DBdiFCqnuOpHd7zxSV7+ZxFHmc5Zkh2P7/ymrXUeEdnaBC4aeYgxVOmmipO4sTaC8G9NBZldLxXQ5hAa1BrxGtCnPGzdg8fSE+eZe8EDbjPNAzRwhBvIp4MkbB3y8JjARXZAed8aGmjiZH6Q6/hctgQnWJ3wT2iEkSFXkrovV2yGJLqoThYFB6Mz8aDZeJC0cvXB6K45u18wHh7Gsv0hBJq0YeKZ3SwjXACK8S4PzZJGC1enM4Q+VjeDhlJxX/ldWIWAhJ9sxfJOMzkwT8FpAbu0xNnI508EE3qQ7REpeQB05c3Oc/wAzE3/imJJnVr4CG1esC+D+5cWKoFvSG6DElnc/J0nlOZI/SqKfUq9Fbw5nHVCjEClbBQsR5YZBNnKpJ6U9JiPoB160ZXoxPmtgupvgoMWhiKIIMseVe4wWqCox6YXtw0kTGDLrGn0Ul166lESvJHLhIiRTDxT4AF83UNp22fWVPY65rdeIL98d9H+fAnePYf/AwX820+317mETnhyvqUo2GVo0sWCAtf8NEEY2KLctbPqWevcvqBn+Edb/0dfNfJu/l7w20KgrfC3mA3Gew7NkdXPRs55pbMpY7DyTS2/kMWSSQOz8gtIrqMJGWmDUZs/a3TWsNJc5S8Xpb7w+UBkTmgmaD8SH78wWNpx+M9j92YYF5j/E8XjMXkwnP0hhjdAzdONZE/OMzUanAhJcKu0Mjy7kQ+E3SsOHNfbu44ILyEga+6Iy027wsMENejH7roatGtimfa4SJbibk5R99fWSgXMQauqfMNI2XDKZ4hekI2ES2uQwtxRxpn5bIlJ3ALcWfJxVfTKFgLZSckiA8ghOXvEX4X/knVJK/3xCndmEp2T4mzSvf8edOZS7Lwu3PgIvUHBfOLPd4cRdIXLpRklkgGei0tcrxauBpe4+JQCN9ACMeUHKG6BOHU3UP/mmt/ekixAg0Mr73FeUSSdSsqIGl1nwOA5fgvPb5pd2EOoWtKmzIlL6QbQcFIkBoxfG55koasrFKY1ZnzRhryNsYjbKrKEBxRIst7CJgK8g0uLnhXVnuyCwlyvPtBBMbiug4LCyAMVlVqbn4BdyqBl0UXtHxt/E5VgiKEKfOmsH7rE9jv+X7OfvC3c+V9H2D1i+/DX/w08+4WF3YabuO94t5ofaJ7o7eJbdlzT5x7vbHZOu9+5bPYhz/Gt37zO3jv/AoXdcT3M3sDdso8LwdOOXR8+2mCEoeFdsFreHi6tdjn9MTJ3DMKmAN9BsIk1nrKW1tMElJiPSTq6Tka74H0xLklipsSztftsK6N1zTo3dFzRxkb42ZbaGMuydOFrvrA4JeHtOYs/pb5nlnMxqbQ1ahOHCIW00U43IfbfMufoz3U+XagdKVXScdtzedObXQuNCTNeZfiv8QgVGKaWNx0zJYCEkegJ+fJkIcklbFIO8h6zUJwYL5c1nEveSxXymI/x1K8szZnHdDsnM2Dv6iiSKnMPTBqzWtZiCkrZJ4Z39Dn1LHnIsdaetRaRGeUlAbneL/cA4cm7Ys83hxFkrhJF/yt1hAyOXFUumtoYAEvgf2IhyjdExyOvF590G4f0JbADy3HscUKK+gAcSPFhR1g9cIlUyn0HmQdt5YHlKR+J9rzYPK3DBuDJR0vbsQc6zIqtUmc/o3A/dwtYsbTbr56DbIuQeFw6bEMSKfx4LSFXlU9dMqtOIMlOK+LJVWclFrigh+GMU9pT4w8NpVF4mAIaXq2p8vBIP1g3LLCkXbG/Gf+78jJo6yv36A8ecrwtrdzfL5j5DV2voZ2jkl4Wc420/pMccFM2bFjp7AT4fb2Fk9+6KM8+fSTfN31q/z94YzdXDmbYeyFtVdc7CBVC2llRHAU6sFFqGek6kLQ8mXcTmeaWF7Hx5q3sJdrQTfqqvSWXD4VpEvSB3uyJUhqVRSECJwjKTJBPVlwPpe0QcNjQkHjIj1spxc7MaNIjMWBdRJyw5TTOmkN5wFzhKekJEUtrqNOj5C5HgquXiI0oiROnsMroy2jtCd7w/PQyMFKJJfkOTGV7PCWcyCLxgIbxRItl5wBFlAsCOBG/GzVHFuchgCrqWRJDqhrXmfZ7Ej6bTYPbPKgu3aD9G+VxGy9B7QU7IO4hruGAbPn14j1tBwkMAUDtzmXpHH4e4pLMA5Y8GK2uxw0X+jxpiiS7kGZQBytlhK/B/QJz0ya5UwL4H3BlqL7W/IslgnCWNQK0TV4HjHhrxfPvxCKsRxPCwQTrKIyhspniQ/tkeGMQ29hA+bpS+nLuEZgN8yxuXSHJj3GCwkTg8VGK57LscVfzyNkSnsjiO4hQOvi9FqQVUmdcHRH49xRAqN0B+vpeJKdsvVl3ElA33r4XZaKmzArzCV6pVUTvJRfoWcNPmIU5HWD9XwJF59mf+tjrD69pvQhyf+njAJVC62Hs0/zIISaLYFcGU9g8IZveeKNz+Gf+AQ/9PQTnJ1e8FOjMPZKNaFKRYoykdhXm7NIGLO3/N3yZqwPuIvLex0jrOfIDbgwuYUBa9dYqngcflYq3YOmtdzwPekuB2zWIxYY4j1yD9MIs2UxlOPkYanguHfmhG7ElQXsWBZQTryuyzh+KLip2TY0aPRJESqeW/beaH2mzxO0nrHJUSQj+TDswVZJuJ7GuD4fbpQsC0aQ98shq9o9Osdl4Ufip3jQgA6HkCeRgugaF3d0yG9SclLRfDkSnNSk8eGe3gbx+dJzaZIHRU8cOKt5vqLLz54UnsNT2wFOEAGxwrKC6u64xSJzkThG+e4Hn92IlsjG4UvUpzdFkcSjezIPhQLdIO3rPAHc5V0W0RDFewzOfdFdi6aXrkXHVRbVTXLaLLrGDrktDAcgtRipfQkeS1fx0PQreM+tYQK8RId2oFDkuGQLT8wCJM8GN4qTWy4bOBgFe15MKgmbYNQeG+Zi4JOyt85Eow8VJfluGCuHcZvcO8kt37J0SBoQSYJfOgSR5B1ihJOOUi0giYWvZuYHuzDL11pIapCEa3u1sNlXlk5YKcyYl3jtNRZoYsoUzH+ue+UGqzRi6Eycc/KJj7O+1Xnnd9zkJ/rrzEU5lWMahaFUBgYaRtGMA17cfSR6+YV03PKXU/RAAA8u5LI4EGY0CqFHRyOZYhlFoB3oQkveu6cppHhQjPoS/WDZ3QGLnFVyelg6L89xLw464ZDyRwqxiGVOkeU64tADSslmIDukBxLMyDOfzWgWv5/ZHBLcBVsXYzFOmdQDEpcQRYhoSivzmshC03KcR4IvrPrgWjQh7IyMcCiP+Ti2wwlVRKXxhHMOLcxBk+3qsNCrksJlpMtU0dy459JEAuO05d21ZbiJ+zKaWMlI5/hdluwbSd7kgmkGsGC5Y8j7TWEJ/lsWUgohAuBXLuV+9ePNUSSXU6mHEqYF0yHx1RxZU8IkeewuOb6eRHJdXFC60b3lQkeASmSXx8hsNYxMJTds8fzRuXXAB6cWoegEkFZoYRu/bNLjR84LWxJvARan5J66WBHiIk+MsIhE6JUCVZChpEImrojuUQSlO9ZKmCFI5MUUGTJyIBYudQ+LmmI5a12C3pDtaozuCVJriQss6CJBAK7h8sYeDr+f4wEzyNLixNOLp/+jKaW0eA/KCvEd4o2JyNLpCGi4kNc2sS/CeREufM/e4drUaQM8cXGb+o4b/Ka3fSu/9fn38k+6cF42bGoQ7CsKvdGY433rhAaXhkoU0vDhjGuiSIzGklQsrAfPVCV0xUmf6gKzhIVZHUDUkn/olOSkLplIIoWWkETzOHRTIX4gXktCRCZLBySH7ftyo0eHFySULiHK0sTpDlrjXH6EzC+9GVVo6kxu7Ioz5YHrEjhiT+OIKAXxXinOXKBVGHNSevi61cNIbNlBacr2QouNOJpYh6fBxcMG175cZ/m5liNs3IOxHFFfutGle/PD/0e3DoeeL6+5w2pMYsSOCNiFtJ40P5aoiuDK9vLgPl7YA4HNR0NimodWZrVrOowtt+8B1/w1qtObpEiC1IKnPnWuUTSbwY7G2HLUKSXI1xJEX3FjTCijSwS3RyDeUkQLVWIUasnlGgjqSBB0NcBsUaIzaNAVZ84LS3Hi9K49Tr0wpEk8S3hgOZXjnflykURH6yoZ3eBMRLHSWpBBkBokXwyYnF5mdiVutJ7O43G/G7ptcbLWigmMPcLQFnJup6New93FHoDQB7cah+phqRZbPoIwjwSfLIwA6URqjxzA+eiSS3YlLV/vCELbA+TSp2HpQDPqyFicc4klztDhLQmHHEnhiBVvDCs2l6/z6M+9nx9G+ZtXB/6bR5RBNDrJycCVPUGu72bMuTFvGg5CUirCQEn8DZlBa7wXauGxWCqlrMKklSAcF3IxqC05r9MhrwUDM6UsJuuy3PF2WHYsUlUrIR3VLJCSc+qylME6EdFR6Cp4zRvfoyh0/LBRNjf2BOSyFmFEGBaaDo201WRvEUuQ8F6eYcEqUHEGcUbPIlAWoxSWsSYXPnE9lBLWf7E8CvetwQvFZ1wCPzctueBMQ5C0LYw5V+k9THxhRmQXfqpVD2N2LgBYWux4icIkO9YAkmbRGbTmkuR+Cax0KZ8Oi5OXEAkEYX4RJ4bGzRiNg2iaCMcUtBwqgSFHQY9kycjB+lJLG3iTFMll5FBRSimM+eZMOHMLr0a60ecZGzqDRvrZgecFySsMpCGYD+mVl29SlUUe1ihaEbHDiLFs36pltycwtxb4ZV7IQfIOSWOxaO0jFyM5PQk8L9eGEx1cz24BDallHSq1DlEok2zekjvmxEaP7kgPh5jeDaqyrSAGY++sxKnu7MVYST1cZDFTc1A2LIRZOZCiNbEcpZKO2RiLkUf8JjFqaZuDr0Ze2gkxiGl2UxbSTSdvEmiDsiaAwktvHJWBDTDhbKWzs5nXLAxFbpw5q8/+IvriJ7l+/R18z/d9Gx/yV3l1uIHpgNsepKFW2WplKjOlGbMok0LRwlhGqGMaTdjhZzUXSr6+UiqiA1BTG/wAQ3ORKKZmiQ16KJ4SO6YllSe3rIsZtLpTNTT4S750OAA90DTPC16+jBmaWTIeXW5jUX5kH+MLjCeJ2cVIDoErUxPfDEH3oYN0KRm/HDezLz6TS2ESzxC0YEW0kubQfQrIJwulJxuieRDTPV5SBKelwUE46y97/Tws896JqJVx4YcTPlgPdcn2gOZDFlxRPwSF4WOCKD2oT8tCZ9nOs3SlcW13gWFZRC1KHiKGo7tnVKyhLe69WGzGIvRAFSOI+hGh98VH7jdFkYyhOIjRIZh3rAqzC0N67FnzlFt1WnXaMurk1+vUcdPEp5ZNbRYwaYFl5ukHgbcVrUHuNQELD89F4znlCynEm2EWkQsPRiQ4EOyQpA0tb+Lyd2Nxti4EZaiWwjiE9Cy06Aa9BWifF2XJn6IkjuKDUlahpKkNNq0ztB6dFdG9GjF+STTIHNqMBR9ahqWomkEzIbtpCRfs4MeFAcA2dE5M1plyNMOEIoURoRLd/CARclZdKDvnfhHu+J47tqeoHAwm9gJrW/MknetuqGyw1ZOwfgI/eppnXz3mD7zlXfzo7nXunRYYhmAIlIpbwaXQpMfoJyGRm1VxKdSilOpJWs8bgkIpFS1DbhqC3O+iFMtC4uAe7k4PWdxEXSsO1pC+GP764VpbKFQsjRLgaX+3LDXcg1aksjBUSYWqIBZFqLUwZA7jiVj2lXSpNymwuFbh4Ml9cMdaHAfL18UCzWNDrxGk5tk+mTeQypJ/4z1iDnCj+QxKLDySg9t7Z05KmVu8JMXTIi7ty2xhmnhMJbq4bFGSn+uHzfhhrmX5nMQ1lxtflssyIQ7lUNAWOpUv+4ikdIVnZzyBZCk+HHzkQblAGrrAqbmc4gEFK2wVe7xeC1/0CzzeHEVSYrSAAwMHq8JaArzurswt/CBJSg3LiLv8bp3YgovhzBk2/zAxouJSkAOeUnJ0CkMLy22s5pZYSLMFDy1w6aCeGu584w8ko+yEDxGX5GmPh6lq/JJIcsTwHN17R71Regs5Wol3tGbx7wo+VHxQjusQC5ruHF8K4xy4ldpC9s3RZCEyL5t3AqeJDaOxOFEvGeKkdX3utYKb1uPu6N7Y28zWY4Fk7hSvrARGh2qVQWsobERx6dyyPXf7hOJsRFihXPGBR2yFVMVkw+sIOzGGs09z484n8XtHjHe+gm/6rb+dzz12yk+2c6axYhPYKAy9YFbZibOygC+cwmTRLZs42nuYZninEq+llEpJDNpzSbIYLZBejYKAD5ntArAsVTxccnKykEWpo4KWwLo8KTYxLjuWRig9Le1Id+2loVSXUMH1gDSWLnL5M7i7uYkVRWWkSmRD05Te4gBbRntJqkx8PkiFQxZ4vPGRt5SYpOeGfLnXmjSkB/wUk1hkps+WeQx9eY45LpNlbX1gQUSzofKQrFBiGllsaRbytmrSecwfLDXzf02cUtqB4xjX4sL7XRBeGNwPQ1s78EZjYqMv/JcAgrUv05GkYijdf6JiB/OkNaoYXmKJ9cUev574hv8U+EHgNXd/T37sTwF/HLiVn/avu/uP5X/73wN/LMoW/yt3//Ff63u4OfR4n4YSKpPZR8SPUBvZ2yXYnCf9RPh9ZBtNQVq4BZlOuHcWmDpO/8iU6RrYVSS4EVwqJzhbOZJ7iY6u52lbJaIwG44VR3t2qpkb1LB0Uo8ApQCROzXJquZhbGsAWgI7s8iLnpMPVkTpqpSxMg7KsBR3rdQaoEoY/gZ8MBmMNkeR1BILAodgtYQcMxjRywVTYgyxfjhZVQvSC6PEhdVkkYEFvri4rERW+Ux3YxYPOpM7O4/vVb2z6p0THbiCck+cV2xmwhkI7flkjQvt3LbG3JUmziiwdrjmAvWEY7lB4wp88jW+/5H3cKvAz5W7VFVOJmHWI7Q0Vq7svD8kEjHwTmtLd+TkWpauhkpwK8NlvgfU0hu+pPFpFj03VAc6qyDFx2mBEvBG87bw0h90Pup4T423BMEFsdiMJ+m5F2M+UAwEcmlC98i8WYqARxTwouMPJNxjyejEHaxO9ykldj0WVOYI42GrbeoUDYqUpDxLgO77bApCadQPhTZHcQeZYimkCzPDAkefpSWn0Bg8VS5ED+ceHayq0dNpyKUjzLn1lsMBIBLv1+LqHm5E0WxILk8WnwE872cBJzJ84vBKbnMHt2iS9GBMrHT2LANTnmLMErhsvA4WURMGZsE1je5cmR/C8X/149fTSf5nwH8E/KVf9fH/wN3/3Yc/ICJfBfxPgK8GngT+roi82/2BJecXesTtnDhfkpiGkomJDODrAMDD+z8ssoRoObvQpaX+c0Hc8gbKd2fJ1agqYWevC3ZEalpzbE+KnQFdBe2AhzzQRSK/WhbAHoZMdewBzrA4L/eHOHMLVSLGjOh6FgqS5oa01BVlVdGioSLSEg5CCqLGOCqIxxjWnWNXRnN23pgZGA8UlZ58UHmgOwaQ6DwiKzyx2uwcSFJ27+3Q5aJQ5gH6jKvSpNMtsr6bGFUiUmBkxdVh5AoVBO5MW+7TmeJaoNJ53IXHvHBTR069Rg56YlBDXVO8oheN0l+L9/99I3/4W7+Oq7vO3xvvY6rMpTAP4bs46IAV8BqphGHB1IISJh31OHgkvR27TTGemaaz+EyxDhpfb1VwKsgYtnVOOL37FAeMP4iIIA10O9DblHI5yd1Ej+vRS977HubBcQnFnz2oPJoFzj313wussmDJhPrKrBH2svneeaVRwivS4voMTfTiIJWtGOE4hDfMQtoYhyPx5vYe74NLmM6W+B26tfxdAsqqeI7XfvidAhMK8o15LETL0hMKQf/K292zhXbveDhcJD4eWKknVrosd5YpyIyIqF0Kq0OsZpI9kiwFxAm6XtDmQgkk+YIHfiom1NloxbACms+NpwFNz258qRdf4PHrybj5ByLy7K/1efn4PcCPeqw9PysinwK+BfjHX/p7wH6OKFlNJU3kWCjjekPRAfXKzBxW8BI0l0ps0WbReONsTo5inBAxiWguTaKb8FrS1DNOxyiUliPwA25lYBqxfX5g6BxYaFzMikjJ4hPa0OQ2JOk4LqxB4/T2kn9KP+AiCdagpRCbtsKUXFFNtKVIdB0mgbe15oz7zgrnjEj7cbPsQpy5zVQZIllO4gKFkDjG8iFd/XIMcrdQ/Rwu9OhiJoWtw06MCWfK9u3ElUepPFqOOdYN6rBtE5/nkldpXGjkAE0em9qGc6bGG7JlB+DCNR+5poWNNIbirMpIkYlhe5/xpc+x+gXlX3jn08gjA3+fN9gNBR83WHeGlh18XXAqW67TGLNk+bvm6Nehexg09JligQMejCQkMLwYD2LpFPBdOeCYguZrnF6kIvQeHNoF+o1isJhWLGqZeI3D5zBSFpkb9J43v+foHs9gthxaCXdYmKscuJXZ9ZAYejx9qKsCOiLHe8leNKpzXOsx6prEe61BcsV7wDVBq3tg1Bsy3cWHMZgAeKPT0vEqryHRcNRKpyUlTGKCw/mA3hN7xfgBY9+ZY3RS6CCnSSflkLlIsVyWsfAts9tUmOkU7xnxYgesblm4OpoxV8vXBC0r9oySZseebdVvoEh+icefFJH/OfA+4H/j7neAp4CffehzXsiPfcmHA60nZuMxWtJD39ytUHRkVZ2yrrSmeKoxRo9OQpL4ixFjb8oMF5mVAaKOV/AhLppiAey6JyHdHYhkN/FEFWNGiO26xueXEht4eNBFBikd8KCdtCwo6mmsoctpCXhPkrKw+AiKlFQ7CC6FLp02z6gG79HmxL8M5uZMFzu0d2b1g09i5PM4zTrSkzCboVXCgr0GdwxVpMUyxg5LmYQdPAwq9jLRmJDeOBblugxcKyPX6wroXPSZz/bbqBVmlA/pxEsm7PNCbhrd5utFqA5fRuU9vuZEla1M3O23eaErs0eE7uBw7WLgy29fZ3XnJezlp/i9f/qPMl28zk998rO0YaDtjDJMYFPcdC26JE9OX2xDg9tquXHvPkVRyXiHQtClFucfLRXPEK1l8bdMa3GIAISZsqVpg7hjHpYpy8iaKzTAY3xP6o3kAtDJwtYNax3pnXo4fZ3eiA02ll1po2lLBoQf9NRR4PzBZMLiPxoQVE1bs0QdDptoiOJjdFSzwnYFCq1NDy5PC+6gFZCaooRILSMga2fO1wDzA9MDNay0eAU8nKliox1NxiCepsQcKHp+6BNq/D6LKYZHMdMu2YWmTNQepFwWD4iB7DTjF7AHG3Bis+0WbAh1ofYoiSyHRxSGeH/54qDkf98i+R8D/xZR3/4t4N8D/uh/lycQkR8GfhhgdTSy3xsmM6Jz0De60rXQJGkmrqh2hiG2myqVAQKMLaA2UFpl0kLz/cHBBQ8CrSwZOpK8/MVmbQHdU7FSPaRMpklOFg+n18RHVCL7eQHLneheSkZM0DuDVWzxjZROST/J0LHG2AfQk7LjsyM1fS41fPxIbLSrx/gyd5pFWqRetMj9s0IrsErNttMwn+LnZojNpJYskCX4eh7+mJrjCh78s4CpOtYbzTv4niPgWjnhWI5YS+GcmVfbjjf6JXM1bhalMvBhOp8xeF2ULpaYrbB3YyWRv/JSn7hF55/zNe8ejnkn14ENEw42UWTG1Fi7sD+94PK7ryHvHvkDj38nw40r/OQ/+TgXRyv6fIZOA2KXYXum0QWEkUkUu4ieDQPjZoXKEI7j5kwiDCk3VQnxgiRWa7Q4pD3dorLoBXcSelsYgoJnnKpJmjxr0vptmTweHIpYLvD6Ag/E87akF0VDphGNKkF2kd4RtbD0SaVRlyyIEtgapoweEklNGW8XQ2cP7M0rgxe6BCl9iSvoeQBUsaAZLXgAEqN/jWWcEB4GLh5UC1fUSkj9utAkt/62XLOxIIuGI30EiDE4CuvShZOFzfKDHiIED616Vtrkasb7Kzn14RKmJN6zU6/hx6DC0OM1bxKrnlistYSqspGC4DXnLGfZ3X6px3+vIunury5/F5E/B/xX+a8vAs889KlP58e+0HP8CPAjACfXjn0/dWaNE2llneoVLwH+NuKiLkUy6Y9DbLxr9H9WA9gd55AkeRZD8+yQNBj5mi1+fFZe6JqjlAX/Y8FXSuKJ6YWdNIGFt7X8zQ5mFtF1RosvPZUAhdw0axSqdDcHAowXp88zWLg318SHXITZnD6H9VRpOXabcbx3hqQ8SNGIUSXHLSxH7Bwdl/EqWuZAf00watyMyS7cqyEmaBk4lpEbnFIQLr1xq0+83u9y4TuuUnlST9HWeXWc+dBbBv7R9SOev9xzfuuC6+edR10oRXhpKMgMIwPHYvyCCM/bzFe3S75ppbxTVqyGJ5jHNSvZM823uP/WDfPv/818/Iqz+cD7ePLdhd/3zd/Mpl7hb7z//Uxlg1pF9/uA1/KGLEgqQwIvC80kB1ftcDnXoEOJZJ6SJ+QS759LLDUiAyfW3ZpFrffg1+lhUyyQrjKNSCbUErkxjbzpF/oJsHD+upbQ61NyZ9uTf5ndjcNCeV2Wh4v6ycTjEJW+2LMnDk3iiLmy9FgBCxK8SM1h0uNQj84sDHqzRsXlmI2DeVwLwRQJVmw4oAdEcTBiiR43v7+DlGjMlkVsFt74vvEDxA7IH/ipCNkRxv2UHrlxXUoUQc8pIBgGkvLbwCHjWQPPFF2wezvIhOPa57CwbZ5LMULOOBwyjv4Z8yRF5Al3fzn/9X8E/HL+/b8A/oqI/PvE4uZdwM//ep6zm4Z5qHesdYZmcRHS6VLDAj5QCepCbl3+p0HtkOYJ2les1njDFwfi3IzFpaOoVjoaSoY0JD1s2ZaLh6VrWIqcJc6VGtOlK8jRQ4woQJaAM4GtxoWb/LCF60XcDQ1ncsd0YuwVaqUMSq9RxLsLzSRNZAXxymbf8DTZOOBNEs8aypsZbODAU6Mn/SIUTZbEcZFOLaGPPp0IV5la2Tncms54wy657XtMnSsF3l1GrvSRV8R4n+35zFM3+dDv/nreqGvUCptbl9x+/lXuvfg64603WF9cckc7FzLxFhfeMo58vnfutQvu7S65p+d8fZ85sse5eHTNS09f57XveYbpnStefN8HWV+7z51d58Zrr/H9X/2b6L7lr7z3fcy90lxCCSUDciAkx6HRBHppaJ8pNtNpgQknLmbJSohRzXJBElfGnAYZYunP6Xa4xiLrJg4qyWNalj2BO06hqzKbJW8x+Y6BbuOE+7hbbK29ZSemcX12cnRcsEPJcmQEJukDo6/TECI7sJqjcmLgkRgaeJ8nvjnkiGm5rRSN3zUu3eVwT4xXlqFVWbZKrgK9xHTimuNzHDZNW2rHs9LmQb0whReXLvLvvTUWJ/XDjK+B64McIFfFErYomD2o8kuASjAQ5AEFMAYJ8GwIJNyeIt4kII/IL8q7TwJD7odO8jcwbovIXwW+C7gpIi8A/wbwXSLy9fnMnwP+RL4IHxaRvw58hKDl/su/1mY7vg7mNhP5yntaGeJCaw3VISRzWWTksEkOnNYSr1hs2oMOkaevaICFyc06cMw0OqhFS0pcvlgReg/38XroPiT9JiO/I+z+8/pBEMLuCotclt6MKa+L6uH/qEa2+pa40IMcHic2iYqGL2KRdFMW1Atrd6Y+hwFtlESGBJxjBQSjFGaMyY09/cDzjMUTSGp1u/cwSlDoEsbDPSGIO3XPvXnHfs7iOwhXvPC1dsSRDZR14a5v+bhd8ovW+cjxiH7Ll/P6ySnDtAIz2uPXWL/lWYZvcMr9e8jzr/DonTuc0vBrp1zefIL1+Tnnv/xhfvbFl/nQsOdbps/yrt0rfG5Yceutb+HJ1WMcvfE6n3j9Mzz5xm3u377D+f03mHc7fse3fjvPvfQiP/GBj3FZnNqV47rGS2UuA5SBWoLuZLaD3WU4/+B0DflZIXXK5ult2LLjXg6zKGmFGH29ZOcpGpp7y6TNGnNhyh+iSCRxvGg+t+QyaInArQWxIS5Jc/DImTYNcv9yMIcXaRygVYRaKisfmMtEqxLxJRZF2DW1Ii28Dzhs3AMiwkgnnJRG5vWrxDW79G0P34ueE5RnIQzyQnTqBXBrFDLIU1IifChwlvenHTBcfEklTTMZd5Y43qjKKQzJiheLGk0qFDGtZd/qhNSwmMe2WuN9k0yCPIT8SZDz007hcK8LS05SjuYZRyEPvQa/+vHr2W7/wS/w4T//JT7/TwN/+td63l/1VRwiNk3ofYJhjYtiOuM1E/RcovMbJGJZ0z0kmDp2kD5JYochzZOkQUSnFNwqcJ9Z5qEIiIpxVSVIvdFN5CiQkE1cQOnjbYKnO1DxgAW6leS1hg+miCOt0EomLhpJYk5ulwQBmZL637CNREUYk5bkKCtVVsWYcktb2oyKM0kBHRjEA1tTPeRBR4MQBWLZCjaMSUJRMXln7g2ZlUEqjcYoxokKXoT1vOK4bqhF2drE6/sLPuMTnxtg75VnvvJZPvjsKSt32mDsmYCBwQfmobJ7/BHKk29hZY6UGO9e8w0iM1fe/Vbs089x3owfu/0C/eVPsjm+4K37wvGrz9HvX2ec4fPbl3nSHJud7f2J9SOP8Ad/+3fx8c99lo9eXDKXkQtXZNwg4wYdV+Hss9/jPWGILri3g/JnJi76cuAexqLlsA8VqNICgitgoiFJbdltFUVrHk8m9OwQNbXvIY8b8/stkEjFdYznSrxN4y/4PLBon4bldtAkvHjIZosOTBRsdObe6cVQCdksnhY9MdODQK/Qix8MhYMDXg6sjaDSRNJilWBHhDx3MbbNzHkXXCvWQ3nkfU5D7xjh8SUOJD4m6knYftBIqAaJu5E8XBYBSJp4LDP38hxZrjRLpYgEvckt7lvNHUBSfCLPRg7qI+BgkacWyh0LC6Bc6mUn6sSCaEmM/Gc9bv+zfwRlJn9+rDeKGF5KYkzhStNLpYccIsaZ5lF0JAwueo6wMXLHRe2p2AnjW6FoEGIXwbvmeOMsnWqO0ympewg4SaeXpHlkSBOe0kInOlhNPDJuHQSleYzfJcdt85geFqVMZXGGiWvdlj81nE4UZTRhQOLG3+8YrYf1Vil0oggMvgj6F1MAW2hzLF6JoRVvdGa2dOb4CekaHfTswmM2cqUecV87r89n7H3mgsBvblrjkSeu8xO/5QkuT5QhWDB4Kcz7Ke4tq0gvyecYUCtQO+o7QDlfnyBf/k4uZ4P2CCcvCPLCJ7i4/zqvvzxSrjlPXn2cz1++yGuvv861NuP7xid+4YTvePpZ/mc/8H382b/5X3FXN9h4jIxHlPURXpVZBKuVvhe8ddSMYb5EGrG4IPKQapPICVKj5lLHc2qQEh36olpCYvFnh5s7tteRhFjphLNHaNwLxZMAneYJ5opJcBwtHajcdmgRxDNXXWIhMyW1QnDWCCOR+KfLVNIDG1xiIpa4ElmuGhdmDCTAqZKXsJkntLkoZcjCoDSbKXmQu4ajavG0MGu5TU/fR8/OsCUGWqQGpJW5UPhCbtPDBOPaw5PTglYkid0/IJovGHHWgoU3CQkXxZhcchkarKEooE7we3ur2ZUuzZbSSGu1vK+VvN+XBZPkNWELOPCFH2+KIiki1CEiOa2BS02HlBnoFAMpQpMeVIjF1LZHvOziiWhKdGWa+lIPEHq2KACWIWLRnj+4aR7OMGkH3l10nkBcoIGF56EdJ3xCIEGDF1CJwKGi6d6i8YYnxo5LGl7kiFs0DGG9KjpWhrGiNU781n0hlNCMoGGYIPOMzOlK5MEN8yTJV/FlwooL28NXkSJ0GhEoFcqKLs4ljTOiaB935YqseIwjztV4qb3BgHPVClekclY7k8+889ox7/3+Z/jgMyMbNwotQPo2oF1o+znUO2lesPagAO09iL0pmKBbHGDDcMr81Lu5O+1pr3+W7fYWZau86x1fwVNPvo0XnnuO+2cvsaJx7/kNz/3SP+K3/I7fyy9+6nP82CdexE9vMtQRqQNL1+zSkWmfRSRcya0FiVzzupm7MnkDmXMqgapKHZeMdYfFMFeEojOL5Z15OQzaRsG8oJS82eIdOEj0xJPW6NQSGvF5v2MxlLBom4JeJpp66Ti9XcCrJh4aJa046SMQDI+FwREUp+hAB60BtrhGrLLA4vCPBHUNXcgyscDrpOuUezpChUUc8z6mJXvIXHcpbjwoLp6/eXSJDzlr5etQrWAp54xTIb6XwgPGyfIEEj2le7wvg+URoAtxP8nknl2pk4yBpPfwUGGU7BPydQqYLr5Hz9dtCcr7Yo83RZEEScqEgA+hTPAQJJUEmbs5rTUwpdb4BVtPZqiFa0grgZ8EcbsHSXvZ2KkesMDiSXFdPCnJF1Ulc2vkwbIrr8HOcuLE+xjGAvk8KTksCtrS8JW0SnMOdm0dQ4fFlCCefKyKjIqsBRlC3WOJb5bMIpnF2BPjkKhx3HPTKYRDTfagRmefbOhcI1CkhKzSQ4vdMwZgxlIaBqKF3oU72nieO1y3yuNas5MR7pWOMfMOWTM9+xZ+8W1r1lPB10aRfYyGNiLV8Z3hPZYC6kE9CaPb4dDBL9AFNNZ7pw034R1fx3TvnO35q9xd3eW1O69yfOVxbt64yd37Exfn99DyMp/70D/mqXe/hz/4nd/BR1//CV4cToIQDqEQajPsLynn9xm359jujN5mrAcZ2rsfxvBZYlE4qCbXsbNjohqMpaSdmiRs5uE7mbjjwcuSEh1S8HhyEuho7LepGvnZzQQh5JOtt1hQpmGtEAexQHgHmCEDTGLsqjEMhW6KTIqUEo79GVG7TGGLj4GLoLXmACE01TDNJbmbvky2i0dkOBwtTk/RLBsuibWTJsfLRGzLZJ8oYT7Hg6jYuNZdWn6e4F5TXaYHV6zkzz/4ebJAHthIeDYhKb0QB4IOGK9xTn8Wnxf818MtixMFUk2y61w+Hh15t7gnyGXmr07DfPjxJimSCz5QMqzHcJ8PpFMjLY5cwvzB9UD2RYQ5uWiLCfvsnXF4QP71ZOO7xIa3w8EyPvikktoVRcsDjETTpSUhjvg5kcMpFTi5gJawPnOlak8QvQVnTgHr6WgTSxRRYUhSuqpgC2mXMGe1xAG8xyjv0pg9zApOVLhqlTkL9JAkdnIxs1VjTW5nRWOb6kZkbHeahKvPlCNME1AzdlI4MeE9esygwrk17qhRJDrCmyj9ypqf/fpH2cpIxDN0qgamOYvSZY7DobVUlAh9iOIjS3Zz3mlx4VcuaqGLc3U4Zv3WZ1l/4g5tv+P84j4b3XC0GmibG+zOLrjY3qXfep5PfegX+Y4f+h/ze77jW/iPfu5jtHHEvdPnCd1vkfN7cHEHnc/p0wVtb5hN8Rr0FiNpLsnUHbfA94p6xgSDm1Ml5QUluHdxKMZjaWCEnBgSsomupCHMgUOXOHRFhNYmfHaqdfY9rcQ0n81gxlNOG9txtw4WTvKLn6fklBKHzAOAZtnyehbqKKaBHS5Tjz44ndLAKjD+xQDXe1x7VYwZeWDBZ7FtXrTq4IdF1+KGb1msVQLHd0mqm0SAjuf1tvRrSyGLIs/hdwrWBdEV59TVUymj8oBWJJJLl26Hg8k1v5igCLVDJpU9xAAInfcCG1hv+VPYF61Nb4oiuazkBQleoeTCxSUcSQDVwqgVoZC/V/jDpUPNgsfQI/tlX0i6gqQ/XoDlgUGQLivKwRuPwIREgvCbBvWHArl0o0kgQktoeosXKAM6rOKkbo2iEz7vaZMz2wyyOM/kBq+EbrhqodT4EwLbsRaUikpFtYZpAYkzIVzpwhOT0KwxuLNWwGcqSqWyV2HdI3SqE3SXWRp7nzAPbe6Mc+GdrYTj0DWv3JQNJ3XkZd/xcr/gCOGaDJzIiGphlsrF176dj7z1iEs649zoXii6YWRg7wYeHUnLsQ3rlFaRHlk/paThsQezMcxVO0LjkoHh+tvxGy+zuvN5Xnn9FfrugkeuP8bKV+jqFLM9Om15+dMf4+6LL/Lbvvwd/MNPvcj77k5B/j47wy4uKLs79Iu7zHMQzq0ZZW6oZnBVhoqpBQevm0PvmckdvqJWLeAdLNxziqeLdYCwi3u4lCgQpRewWIIsKqu4D51wH0nz4N7pzWP01Rbfq+RN2iKbyNXDy5J+KGpLMmdEEOR6QwmakHG4WkveQDGeKurp5pMuTZJ8RqeGUbUb5H0RVCniWkyUU/DQvGPIPOeP4wd2h3vG3RaJrizDnQ7bdaJrDL5zNDPxm7CwLNN9KorUgyIqNF8AlHhYn7PoRRH1hJNENMj2mV+b3za+MqlcEQaYDZEFG0Z7VmUkQ/++8OPNUSQTa124VnEaAS1/0VjxxUmUvC6zHlZRHhel5RPF/dmjlNUg+MYkYaGZNc+kNsFKoVhgmFWX0zGHBw8MJLAlyWMszSE8vCjVC60W2jCwlpEVA7N0umm6hk8HQF0RBi80BEoUiFbCRTpSDjm8YSxdBFAkbr6qhaLCSalcmzrFDSlQreWCKC/phTWMIFLBa6gO1BkQtmZc0rlf4ZE+8M5+hJaBV23L5/olsxmnMnBTjrhW11wrI91n7NqaV7/qKW4fzYxThdkZvaA13Dt7s4SalmwdofbosppYQCHogStqHoupGM1i876tA37zWdZ3X6deXHJPoMiazfokeK2zUfZn7O7f56Mf+jC/+Zln+H1f+VY+8pM/yz1bcbzdcT7dxtp9aJf4dstijqwOferh92iCJG4XUa9xcAbhHrRJ5tEEdUWHGNeacuiWZouuKgyHnYGe0sWYZIY8pL3HCBkbZWPvgasXF6QrA8H9kyR0zR439sAyTsfUWyQSQ1XDPFg9ZKtVhji8D5ivLQMWKXqNiUlgOiRMkmOtHHiZsXGPoaSt8vta6sWJQ8TNcjufnZnIgaURSpj4ni6JiR/aW6d6dJ2dB6T9JUyt5X2tqRW3dPZxI+Kc4ydNzrMnpp/GMjniLRLj/K2iptjEwW8zG534xZL6JI77w135F368KYokopgMsVkUTyJ2UEeKVkziRuvuVBWqFsT0YEhRjOjAwvEUEJjDNs2tpFLGcnzxg962EGA9Emz90JNKEjIAqbmAiRN8yI2eSFysJsBQGYYRZUAZGKQis4Puo8B7pvBpkI9rqZEoV+J06xKjfutGJ7CqkosdyXGnSqH5RC3hjtR9pkihorGFdLJwlwd4nyQC4x3tMAwrzmXPnb6jiPDVdsK14YjzNvMJuY9r5zFWbKicDEec6g3GoSDtDB+cuyeFT1+vOOFYGNvwkqaXgjZnmg1MU/kCWHRO7lBzRDtgkixdluM0pM/Y3Gibx9jd+DLspY9QLjr3ywW9dGod6V5YmdMvXuWzH3sfb/3KZ/imd3wVX/Xzhfc+/wrzvEN7w6cJeoz82kn2AuBRnNCBXgZMS3D+EhrwFl3VvsDkMPTOUQ/HpdngwIyxTnPDWtz8irPLjihMc42W5sBLtKup0TqUFpK+hfMaXL524AO2PuEoRyUXUVk01cmRG4omj5Yo7D0XjyVxzXhxE+eXsHFYXNu9Bw4b3zthGZYFUvI8PaCfaEQCF7R5Dtgoi+4y2gaTIm34HHSJREjwcrl/SnNGrYkDOvucKPBYuJqSnX6SoD1wXxy8LeKL/J5kF+pCRC2XhVuQt39KRkSQEoekJ3thEZAvzAAYEvcsX7Q8vWmKpNRNCO/pcaEj+WKFUYIkPiEO3nuA7y1++UXFogeUCFQd8fCHFJfDm0ueKNUlTagWkmqesAvGRLqJF03bMmXl8VGXEsoYUTbDmoE1cy+xkW8trPQDjce80IpjWuiZYijNqElXmGxCkiFXcaQS5GJvoEYpy0loFBF669xTR3RgZcnvsh67VinLL0jRIP52mami7E25tAuuMnBDjyhaecm23PIdj9mKK6JsvHJcNxQKc3udvQvjamReVe5+7dP83PEO+oB0RZqHMUjr4DPSDJs85Hr5LrjGWEoz6n6O7kNJmWC4N3UMtRltE9o6WxX6o2/lynwXffklvN/F5spmfcSgJ3QzhrPXkVsrPv7z7+WRR97C73r3l/HRj32CrSj1stMnx1pY0UGoqfpyk5DjmSpzLq0k9duFEoWlAcSbOJlhszOmoYm74E1oXfDJ0iMAWo3FoJeOWMOHIS3xMtWzKG6VcR+T5VbjvW7mzOkROfQWShPA+4TbJjq1xM9ZvEIlW8O40nFNoax4qsMeGlslviQCZJOW5ukIZB28YymPtbwv1haTTLOI6LU2Y/MUXF59+C57mDjzoHPUFHIsyhpRZa8WIoUs9pUFq4wFZMFjqsuXvpnn5z+E9YqgNZeUC2WLh643TyrhYUuzHCQP5rlYJIX22zKqo3c/pJh+ocebo0iqUtZXKL6HvsXbgGIUaUiP0binZbz06I6M9M+z+OXDby+pOYuGdEHX07U4XKVLUg7AwmE1XMFrcsQWcNiCQqOEYoFBaF3jgpNl9K6UskbKCu0ep63PtBb4X5gvDPlDLORwQ1rBbWAWZdbIhDagFKgtuJE+1IW2Dh7bRhPjYt25txL2856TEoYDXoK3GVxMSe5fdNilwq5M7M25ZiNXhyvc8h2f83Mm6bxdBo5UWdU1hnFvvs+xrFgNNxg2J7g55alH+Ozbr3Nez9AmTOZ467Sp0LwxAz5DsUJL8w5BqS0XREuqpSsunql64eJkZjSb6Q5blKE5l1TKo++gTs6dN15m17Yc1cbRZsvFMHJz7wzzxK2PrnjlK76K3/QV38RX/fQpH3jpBabJqH2mdw8yvXrkFaWpquUhpGKMeZEMZcgFXfhRgsXkMgtilb4qJPuYZso0wwQwd3yeEQvoZlopW5z1XCiTZsyA5fZVaNrZJRJXu6RxedB29i6UThisoOBDdFne6FLoYuGebZKSyRlHmHOWVi3BCV469h7Z4iJDQlmdkSBph5VdJyg6iwNW+rAi2Jz55VoiY6nvI8aixNTFYldIOzinRzOhxNIqIihUPLr0nPK0hzWc+RRQlsWyc0yuMwUaPYaTvFcCSDOsRKDaoAIeQXC+IKd5v1YtByUbvTN4ynmJQ2CiMWTxnFUpPZXpJZqhL/Z4UxRJEaWu10gTjMZcJjqRY6J5UsTIrNAIW6jeD/zFhfe0aFCDA/lAhx0bsuQj1hoFjvBqRAui4f4dPK5w6VYTVjnqSPcAvp2DrLC6oaUkEyxGcLM5xsmyWD7ljwWJcwYGEvjKHG+yRX51l3AI94wdMI3OpSVaFaa/lYZwd61MbpQQuWJDKDnEhVUDt8JelE1ZBc4777miI30tPDed8arscBWe7CPH6w02z7w+3+MKA1fHU4ayQXujTTvm4ZjVU+9iN87Uy3PEnWLKPo3isfCOFJQ+xyhVDJjDtcbFQSOdzokUPungXiNozEMemctHdqkzvxhPsaffyWqu7G4/Tx2ciQnZTrTNnhMduPoafP59P8tmdYPf9e3fwif+6qeZDPq8x1oLB+zMcxevLJitANKNWqPz7n0JRUsbf+807dSkfHmPftAtFBu9G63PFAtvy9nDTHfYlxy90ziqxzXpGVzdBZq2QydWJqOKMoogLV6bvsTG1vgsMz9EHvhyL7iniiUeSvhcJtsQ8x5OOKooi79qUl4w7JDuaeBTvIk6oD4AwlRSOthbKtYcakmlb9CQivbEn2Pr7rrsFJaiS1CJLFzU8/ZMyl6SydNdw3JiVAuxB74Y+sYN5El0Nw8lURkqrXdqz46RWLomC5+llT7ATvk5bunVKiX8VtMpSkuhljf5uC0qlGGMk6MPsdmVIeViix2TJFAMZpoFJe3gBVLqwnIGLcL6JdukltC4VhNqrUn1CasSUUfTFJe8oARnb7GRVZTuxkAFNWbv2LRHCSXFSgtVh3xjDKdFt+tOSeoDLO+VPzCYIEbCokE/sZo/Ry1Q07KtxYkoMTOyKYVKoaThL0uxnBoV4xqVFR2XPVubQ2+rK1wHXp3u80aZAbje4ZoMvNJ3nHbhRrnBiVTWrcJcaWMJWGDdeUVu8X7fc1aduTfKJGnBL2BCnwNbbQJz0nu0yCFHJ+hosQkOfl2OezEvRMd+kJJF7O/EivOyob1jzX6jbF96icd1T6Fwa96zsx37y89z5Zffz8W+8M5v/218w1uf4ic/+omIVhCjW5hg9BxhSW9GSWzb2hRjZhpdxFohriE7GDKDd2he8vBbik3g4PTgfnYVxszf2SvMHlinSqFrHMxhnLLQUSSiEXJ55N5TzprBYAdKSoZ65WZ7iRqJxVxSbsxplh2cL8YYDr1xIAhqMB2sx72kxqFL0+zKYpse3e+i4ZEs1hwiRNKJSyRfHJLjHDjfYqLrxOLLiK6vWwMvqeVeFihh+baElsXrogmXCXVR8xCy4Y1EzpXN/dBjxn31YOhPKQhFQoUnJbbYjsd9JhFnYlkuFl6p1C9eCt8URdI9TmjRitQRHVbpkBJjUiZi5AWQWz4X5tJYpDBK2GSBHQioThbJ5EgeRt44zOKi6qEtLbm1VA1bpt46zhBvoAo1N9uBtfTAc/qEl0rpGiYY3oNOkadn3EAW47DIQTc+l3BVVpXoIkQYSoHiDMEGQkrIyNrhpgqVsHrjiXnFqa44t21wA6WGv16NpdaqRcLj7AbDyLkaL7bXuaUzO2s8rRtOxLiUxlvbhuO6Do25Tpyxo7gxNeOYFW0UPvuU8fFNYzVvmB28RdZM4L01bLXSWKEQhq09HVZUctTWIL1LXsaxoCPjEYibOUnw4UFTGNqKXkf6274ZO36Z2y99kCt2n7VUpsuJy805r73yWe6XIy5F+e5v+kZ+5lOf4qxNQcExw+vCKQv3dy0x9naPzO0mHW89xkfvuITPZFxvyXJoQcr2Ra3k0fFVy62tJJFMQ4IYwRg937MYJY3gLQo94pBNqNURNabeKLkpJhcoC3YuC3k9C+MiRIhgrVCkdMKgmnxNSQlm1QIaiHuRmpUr/DYxMqve0yA4vxBPQ9shRughJiqvyrAeQprYAhNsSFyfbnES5tYZj3t1NmPuE7pEu0Ym6gPOomRRTRWQKblsDaJ+KYFnei5spEtKJMPEZT5s/3UhUC69dqBUPfwyRUKf76pJ00pVWtH42FDQ8U1eJOmdfn6ODVA96A+uhVZKdJMd6GE40emZIRNdoXYNQ1EhQuiTOhNdXUgRg+rg6f5SKRjawiGHQhgBYFAG6CRYHRsvyU5USoGSZvaJhYoSmJHPedE1ZmlpZx98t4qkJ2COxhZdarEcn0r8o4NGh6swVz2Yto7DGCa5PU7yuXfOTpW5rjHr7EeBtkeHEuYLply0GSEw0dtlz63WeYU4sb+iXuWRbogrl1J5scAt7nGUJrvXgRsysJYj7PpT8K5v4O2Pvp3Ts1/ibgmzkAmwltZbS6OcDuTFgwPYvARFq/hhoy2ysNwUc0kWgyTkEZzWktvxQQNBA7hfjuAt76KdHNNe+gDXXn+FK+uCl8K97V3s7ud5/iOXPP7M03zb25/m73/6U+yGkWGv2KowtGA4RKJgYWwLRl2gJ5zRYvxcYwwOXgbaCL0bVZVZW4zcGnKvQQoMaRJSFa+FyT2kj5lB4wJWDLRH/IZ4ODglN3Khm2mNsTPc+JRCefB+lhI3c26rg+BdUpIYCh5TR2skAkoqhxBhpqF0tNQg/i8rSl+wUMuMHBjIpYwqVGXQghaoG0XrimFVGDYDvRvbaYttBd0NTG2mLQqx9GZUN5ha+GGiePKPo35mZ5oqFyE7UiFI3tRYdiWtqGpgu57TlvQeeeFENMqyLFpgiWVqM3eKjuGsJMYgkXG1RK7IbOH30ASvCquDvcg/9XhTFEm3jl2cYavg3FmbcA+zWe8WQJGFBKm70yxMdd1jNHOCVH7IqsmuEHLasHTbEWK8xklPpdiKWUi3YsjIG98lLygHMaz32KwURXRELU5fT5NWrYVShT55pPM5LE7onuNnGQccaHNPTW2hFBjqQK+B9ahKdBUi0eW6IVrwYvQi7IbK5R/7F3nlL/w08oGfZjZl9/ZvZPXMVzLdvuD1+x/GPv2LsRnvna137stEEzhmw2es849WwqsIl/N9vlaUd+xnTjAG2eCsuCtrdtV4TC9Y33mBW69WpmtBz/BUGYnHAYI6dTAY5BCkFnEYRCed9JtSM1IiRyMt5MWe1OI8SIoeRawqJFUKjuuK2Qf65ml2V69w71MfZPP5j7PqjbPNJbvXX2S9nvjAP/wHfM/3/gCfuP0Kb6hQrcIYNymqGfNaETmN7Wabsf2Ovu9M+4hQEK+Bf48jQy2IO4Mr6qmEEoKSIyWoRRaUrV4UutF3e5rtI3O7CAyODCmxsxBFlJK0tN6S8gNScproQrFKHYY4ODS5vhZqNClhkmGaRbUq6lFapD9EYl+24hIHkkmnlM5gRulhpisSvqWeLlRCbP3rWFgNBR2U8Xhgc3qV9fHIsBlo7kwXl9y7f87l+Y5xq9TJ6AuU0loU4RKvj7RwqrdFqy3BFnH80FGiiVcu928Szkne8kEjvrz+ebCWh4qkSEwrpZbgY1qP4tqiwy1VotHJSVG0ZdNekLFQV2/yTtLNsO0Z9Ajpam6ZOdKwNid/bQHWLXSXoiFXoqSYPsYOfNFm24MXkOiiFq87c4KcXsLmKRQ1wXdbAOZYSvTD2KNOupyHYW/oYO1QdHubAgjvDVpHuh3UPjIUykrRUWlArYJSAwJCqMOIrCoHaVcJ5UrizaEKKaBjYUR517d9F//13YGzq4/wjaJ8+//6hzn/qq/g4tY5n/m5n2P4W3+Lj3/0A3z1e97D3/3wB/jA/TPeOL5KeeJp+qNP8YobNhrl9TM+rAOr0TndX1JXA16M9/+Tn6fUyg+dv8z/9Om38Pzmgl4aQo2fD6fUIDgP4lSFaRXdtZnTpsRmF0yPhw4c8nBK94HoJuLjglBklZ038fOMwrha0WSDra9S7WnqzUe4Pxa2n/gYpzoztfu0PchnP8HF81/Od3/T1/IPX36ONq4oCGMtuET2n2gNmo11mHb0ec887bmYZi6neM/Cw1EZVmMsQ1zSmCTzYaxjnkoXA7FwxbG5s7u44P55ckaLQGlIcYZhoKIMoozDAEXwrhFb4kYtJaAIVUqvaUG28CzDPUdLycWJhOstiakhVANTodtCxo6Nc5UBHwl7tVLQOS9wNZCBUita49CSVKxUEWoVyqqwOT3mkStXuXrthPXxCgfOL7fU1X3q5pL5fIttO2022rRn2m4x6wd+rBuZgR5atQWPtHzfF2hsqDWI9xYdZ5UoahLa0EQ3cxmTE54WOUxc4SJioGmUU5QyVFw73hqer5lWTdK/ob2jIlgF6gNc81c/3hxFEqNN51iLm8YOTP2e+uWFljBHoXNQGUPOJanQccvgefC5BXfscJEJi6WUkzpqTZ/zZfllmssEwIN0axZ4oAGUjmnwD7W0LMbLAiU2gd47Nu9DVigBEpdaKKtK2SgMWZBN0QbSiBGfHKPy9A1nt8BQisa4JdVQda5tjnn+uU/zo3/n7/HkN38Lb/ne38qHn3iSH/1//kU++eFPcvroKX/8//Cv8p//v/4f/KE/9kf4T/+df5fnn38Vv2z8H/+VP8F7vvobeOFTz/Nf/v2f4LNHd/mIKZtrVzgpV3j05IRnrq146XPPs7mm+G//Ov725z/Hp07f4GIQVnNCe6UfAPmNFopWtutF9eDMe2FQZyfGPPd0wA6axdIIJG6Bagv1EmkHZlF0h2HN+viY9UlhdTpSVieUsWLS0XqT4dmnefm//pvc+uAvcePaMbDn4v5LvP+nfozf+Uf+BNNx4eXirGVgHAILA5LqYVRz+n5imif208zFLgqlqjDEW8UwDLE885AyohFnLN1o3cIf0WGeGjsm5u2e/WZDH2d0Ox+SBodhYLXZEKicsF6PaIk418utME/BIe3uaB0oNqBUSqmp74/iEl5DEmPjUEKQIxqdt0UXVyYPnFCVYRgZBqGsooDiyU0tca0ihZr57lolN76kAk2p48jRyTVObjzGjRvXuXq8YZDCG9s9R0cXbC8mzi4vODs/5/z+fbZn9zEz+jTRZweUUmoqg6L76xIcyPBRAM9DlhILrFhQ55JWFa+CaORVycJwcUmckoc6yeVjjgzRwDTvqDo6FkpRZIyFaDEJHXoXSlXa/z8USfDA9Zpk2NFDAKxbvKi60C96tMiZOmj04E7mibJ0MLG5im4llIkBoLumwzSeY1N0Lbacagv+iQS2RoLtaVPvDqY9x4BCNxIHXdhhjcV4TUpFh0IdhTJKRDlonJaG5Og1YOn07CW5db3lzRnY01DAh0vMHPUb/Pm//d/w0iufwH72Fn/v3mu8+M738N/+7b/AdvcyX/bWr+Z09c/zxuvPIVxw/upn6K++wtjg2956k297z1uZvvIpzs8/wc/923+deTb0aMN6OOE5Vd7nM7fv32JjAz/z/Bk3rz1Gr8Zx0US7OeSrlCLU9UCpzqb06ICbU0u+vmVAdtCm6CoORN4E3GvRsHgrsXCwHosMkZE6DpycrLly/QpXr61Zn6wpm4EyCEPZYPYWnnrkD/GPz8947TOf4i3HnVob27uv8qGf/Cm+5Q/8EJ/Tma4lbpriFIkFDBK0Lm+duXembWO327Of9miJIikGpVQOmSoWm+AK0Bv73cTcGs2cy92EtMpKK6MWujdWso+RXJzVOHB05RRXYaPKer2KALF952y75uxyy9wa9G10fnPBuqRJbrxWEVTXYjrRAiYMY25mEaTF9bsvDW0VrUrdwHqEsoo3zZqg3ZithMdproMiajglpmqx6ExYSVjh9YT1eMrR6WnY0q06J8PE/mTmzvkZq/EujjLNe8q0j2A7bymRjfdcrcdGOU16vWduoQxxQKgiA2mSbMzJ3IhmJIrk7MEZtmQUqGkE4hGdZrGMnSgxUrvlNGlB25IGseOv4RBGYPxSShDlv8jj1xPf8Azwl4DH48rnR9z9PxSRR4C/BjxLRDj8fne/I1Ha/0PgdwKXwB9291/6kt+DwBG9tVBG5FKEpEJIsvfdnVJq8qziCxcL9zmXA5qzczGSwNzpkpprHuhZY/MdL1TIUXMotCiei/oiDcXofQ4bsykKdtWM7uyLBlViY2dBNZCilLGgK6UMkqe90hVmFWysqBVGAnMqSQsKqDLaY68xgrfVhJZK6Se88pLx/IdfYnfnnFcuLnnj9TO+/3f+c/zA7/tenn/uw9x+7h7/5Gd+luHS+cW/+17eevooL519mnmAP/f/+Sv8ww9/kDv37vJ3/vrf5PKNe+hqw9QumUdntRrYjCM3Hn8GaxO7y1NuvvPdnG8+z251NwwYsivEAzfzQbAxOkdU0GIH1gnFqaUwFaW3ON2n1qglowA0XJfKMjkIuIbbelmNrI43nJysuXHtCqePHKHHA6XEP6LK+PSj8C/9L/npP/Nnuff6SxxfrZyOlc8/9zGu/dSTfPUPfi+vXK/sMPoIw97YlJG97g5iBHej7TttavR5AjwsdFtwI6fWmVuPrTCxiPPeGHRkO01MrWEIfRqo9ZhduQh+HwXDGcbKehw5Otmw3my4utqw3qxo3mj7xnh2RrlfuTy/ZJoN8YE2B+tCSmywLd3sWcxthVzsRCHXOlBGwUqj2xZvUEthGD1t2AtaK+49OmGWDbxCpmmqhCYc78klHcEqbdvZX5wzHR3Tjq9Qy4paYTgasDKz7sZJMy4uLjjTge6V3nZx36bDOiq05P2Kd7R33Iek/Sx3pR4chBY7NTxkkSQco8mP9t4X8tKhfrikMmfueE9eJXHASU8YIzmYs8Y0OfsMLrEtb7+xTrIRudq/JCKnwC+KyE8Afxj4SXf/MyLyrwH/GvC/A36ACAB7F/CtRPzst36pb6AiDLUw97gYbfa4QCp0jVQ0tQB9l2UBZBZv2uaPpoFJdBKwTjpPiU1iUaUiBxpPLxHStEAaB1mikBpPUElGf9qS2eKQ6kLrHfd4cxeibthtJZ2iOj44Muao04Pb4yntElG8KM0DFiiSUBHgVeka6qHVEDja/n7l5efOOL+1Z3e2ZxxOmNolF/fu8Jf/8n/MjWee4GxqvPb66/wb/+a/iRTl/R/4IN6S39mMv/H//lF0GOJiTZfnPu8RXYcqaYqN39Fmw7hZcef2HT72sRd5+us2jKs7eNnjpqgX5hbLKq+aF3YPxxnxAPwRSi20UdlXZdorc28w1tiEesARgfEG5UclBASiBakDdVyzXlc2JyPHp2vK0YDUNaqJpZXKl3/7d1H/lS3v/Q/+DH5+Qdvc53g18pV/67/kbf/JX+DpH/w+Pv1//pd4VWdko0y9MVBp4ph2cKVKgXHA20ibZ2zuzDbTm7GfZvZT4M0qSsuC3k2YZmM3NaaUQNZaOD4+ZvJG6xHbO65WnGyOuH79GkdXjtiMK8ZxxNy4uDxjCr0S1Y2zrdGnwMxb61AsqGhLcXSnilMHRcZjympgszqmaKXNnUu7xCQwUC2x2HCV2JJroTEluTuGgZJ4J1o4EI2kxOJtktCqzxN93rHf7rh375LdFIq13jtzm+Pn653WOm3q9LlhfQI6pUrQurJTBKNYR+ZGxP8uZhVDRPmS+wLJP1vwZ60CxJSnzRIWW8Czw86PxmLukRQg1Vwm9eBCS0xyVoxuEyYNTVWOz7+BIpmpiC/n389E5KPAU8DvAb4rP+0vAn8/i+TvAf6SB0r/syJy7VelK/7Tj+yetCjiS+HyQ2eo5ulGkh2fRL5FIYii0lvosN1odNoie4tZjl4dajl0kb0KvcBBLyMc+F1IiO2R1G4nh8zJF9MNpaSMzXK0jCLnhB63aIVa8FppnoarUkEiFCqCnqLWB9U5tvYFUK14ifzqQQulKa8+d5uXP3XB2Z0tpyePcjHdjyLvA7UIH/yFjyDv+xxUxVvHjk5wm7HW+EE63+vwmMGX73b8nSP4Px2tYNqjZWAcx1geCVibmTF0s2bQgsslasaRX6fVMy6YMg/Isv+eWUxLQwMcoLsIMFSKDOgwouMQBPRdhIxtpyleS5JXp8md7MLgCy1E6c0xKUwoO3eGHou3VVEGBqwNjDLw7G/7Pu69+hKf+Ct/idfP3+B3PXfG1zx3jgBXfuQv4tuJm3/2T/F6u+S17V0uW2MuTpeIvUjvWpiNedeY9hMXl1t22z2Xl3vmacY8sdPFRHee2e727OY5FCPV6KIcDwOnRyv6vMFdGMc1j5w+wpOPPcr6ynAI1wQYFFpPyxCHuQrTRcf2jb43rHW8NKwErNSSgC9loNY16/Upx6dXWOlA28/g95l3jd53cf16ZVUGBNhvJ+btTJvbIdZCyNxvZkqt1DJSR6Up4WTPlFnte/Ztol2cI9Oe4nH9T23mYnvO+b27nN+/w8X2gmm6xNkj0nHSNMZDEAI5JTZBe0b+qiIaAgwj+5z0iMQMb4ZZavznmPIsvTtFY+zGw/QDOODASzRKWQqMp+ImXkGYHW2SOn1DD/qlf/rx3wmTFJFngW8Afg54/KHC9woxjkMU0Ocf+rIX8mNftEg60EbFPUwjtOgh+1hc06EnOWVLN3nYRMcN1jRZZyV8H6svHUlBVhKmqR6NfWzFQTykc1VSbUNgE2KW3KFQs3puC91KSgtjsxtaU8/skOD51aFQV2PKCTW2qqI5RpYo1kUyfiI7Z4vNHRIb1NYUqSvO70w8/6HP8PoLl1w5eTp0zr6jt0uqFoZx5GK3Z26dVXVWUtn2wMnElR8y+Mvnlxz7gZ3G15xd4M34U1evMQwrhBpFXePEnaa4iZCR8/Mtt7jF469d4+T6Kc3v0n2iT1NYefV+cDdfzGNFw3GoiFGHNaIbqq+QUdDa2e9nBsliPoeRsriEJ6cqpcUyrLgwz8b5dqJe7LFVZZwNhsp6VKYaKqe9GZc+8vR3//O89vJLnP/Mj/Puj50nph2/8/FP/DQv/txzvOOr3s0z157guXsv88r5q9xu94PUTtLEZmd3OTHNjfPzSy4vzrm82AZnVmAoA0ULw1CxPjHPjdaDH+ji6ABejOIwbNbspwZlZH3lGusrVzi9tqF32M9zeFqWkVMruI+xiKwDZ+xo+z11P+NzMCqqlNiwI6CVQYRSVoyrY4bNKceywnSiTZ3L4ZwtO9rUGFYrlAFvxuWu4dsW2KdHHpIklW4YSmTMSGRJSczi0UlfTFhxfG/M44B7w5qGjLZPbLeXTPfPuDw/5/Lykta3DBIlqvUwmMF7ZthodJBdaC2kj5o7A7UMKstGppDdJB4CjRbc5WYpYxSPXUM2WJKNCgSVDlFaZpJXXfJ7QDVZIyahpGo9o2u/eN37dRdJETkB/r/Av+ru9w/ebIC7u8iXQD6/8PP9MPDDAMNmwMcYvWoBb4r0mY5GJGRfZFpBM3GPc8ctVCu9ZJ6HBtdsINb6pVSkFlohiMwOQqFoWSLXg5+YBrxxOgXYGSKtXH3nJi7MWFJ9v8De6RatSZallDAu1QwukiBXo8Fza5K4qTu9dXqbkwSr4SDjRt2NvP7SLT75S5/ALzs3H32GUkbGYcPu8iLG/Tajo6InR4ytBpSgga1ULdRhzfddRIGMn5bDn797nvk3hzFJyeEi01NSJg6Xl1tuPvEk637K/bP7fOqTn+DZ9XX8mnHZznAX9t0QqVFcNCAFwRnGFeIDdVVQjY7HdBO8uQJedoDQbIuV6KS6hVJFRFgTUQraiZTEXaNcTshqZNgbWnbsRqWOlUF2qAlnfk7fjjzzrb+Lz33ol/nlq5/kqcsI83LghXd/GS9+7AO88bnP8MRb38rXfevX8K4rN/i7n/ogL92/g5VwvJl3jdaM/XZme7lju9ux3+8BgvPoUIY4bFerSinhI6CqSK2sNwPjUNht93A5p5JqzWZzyrDZIOMpVSrTbgc2I4OxsoHVrtD2lSM1tirUsWLjROv7NFeOahDcU0/GRWDohwjV7Ky0CzY7qoVVXYffpAg6OvPO8TYzFKWKZCicIT2dkjys0JoQgWQd6g5Knzi/uGBfBe0z0oX0uafvG94a+/0O8eBhSidSHdPwVq0za2z9VIag8UkGqnVnaM5Qxoy8jQanuDDnrmHxjVWNDXm047G4UGIhVyT4sOahGCoWMFiXaIxCQiJoNiTmISu1fTxXb4vI8Z9+/LqKpIgMRIH8z939b+SHX13GaBF5AngtP/4i8MxDX/50fuxXPNz9R4AfATi6vvFRJI11JUmKgvZCi2YuCksCvdHNeV4auf5PXakQNzpDrPsp0d0Ur0kTSLDYc8yOS424pSTHAs0tNCwbdoQMGUtuVpEsfMG3XKImuieGqpHrYW0KjLTkJtFCc2vWadPEPCUQrx2lMO/hpY+/yEu//Bw0YX16ShmUNp3h8x6RmVErk8/47AxjFEQ1p89hDlEA5pn/VpU/BBwvr1P++WObI0otqHmM5Q/aWAB2ux0vPPcCzzzzLE8+8ST4jG9HymrF7nJi9jSj9ZmgL4EVYRwKKyJFsJcRLUNcnmXFqq4idrULPnasdZrtEDHUJY4lC2K5mIapSDNac6a9c7ltlNpR6ejOWK2EdY1N6dQ7+3sXDKtjHvuyb+DH7l8wjPf5mjsTzz37JB/9mi/D797i/tElz/3Cp7i3vc07v/EbeerKDT5250VaE/oU8labIlyuSmEzjow1qGGKMw4j69VAHSpD8lrb8p57dGSrccB6QTS3u7pCypqpKczKOK6AyKoOma2gpYd80FYMBY6Pheozk9zHdrvY0npIZ+duTBbjsU7OalJQYdp35r3Rp5AN1s3AuBoRrayHDW1YcX+/Z5wLI4S+HcIMoxkqwf5oAlOxWC4SJr3VnLpvzJNTWhTmiE+LgrhcP2JR3DzzvIsn9xJBrQf9R5y5CnWWgxIIDfgrdPUWUQ2u1BITWBzC0SYVK7GI6Y2SOMlirPGwr2Yh3sPwhFfECtqVYj2wzr0jl470ivSeSahf+PHr2W4LkbP9UXf/9x/6T/8F8L8A/kz++bcf+vifFJEfJRY2974kHrl8n/TMC0F7T8lVUESsRBunHkWuS8iORDi4kiwCdUOiU6yVUitSlRXRfYYFXwYHeRYtYlFj8Z+SmB6nlKThBaSj9nJqC1jRVHGUg+wOF3QO3bJ2RyU4gq13ytBiK4/HyNBnepvjAtUo5r0Xbr18xqsv3Ac5YjwZkc2Ki8stYg1s8eQLrqj3TpkIgqwLrYV5QWemVOXHT074w6Xw26eJx7rxrt74O6sN/5drNwFPTtueUgqiIxDLMdw4v3+Hj/zyHZ59+zu4eeMmpR2zEWU+fy47kKBi1VrQoaIoq9WGNRvoG2xeYbXSDjDEQBmhzxOtFaQq9FTueA5YouHvR0e94TajfY03o8+xdS0lOIqtObNOgbe1LXU6Yyfn7G8+wU89v+cnzrc8Ngpftn2Vxz/44xxfe4STk2u88MLLvPDBn+DFX/4G3vbV38G33vwy3hh3vOa3uTfP+GqND4ZIp/uK1vZYM4ZeKSkV9CL4oo46xANEVs/F5OwmYe6FfS/I7NzfdcrkrPewbYbvnJKyznmuzK0wNWVvA+v1Ecergak0LsrI7uw+Pu1wm0Ny2DtTm/ALY2wFbZVhtWE/bzm7f5/d5V3wmaFuqHUMowitjCkCkCyOLRuNiDYJgQau6ZIDRYPdITi9z4gbMx2zgqDxHhE0HOnCIAON0MODMKZZSdD04s50CQMUfNFUZzNRQqBheIzBeS8VgvXRVfEhlVOzhJxQeZDemNcyHjnb4bvpFHUcTTUPucQxbO5MU+cy76dBNAvuF378ejrJ7wD+EPAhEXl/fuxfJ4rjXxeRPwY8B/z+/G8/RtB/PkVQgP7Ir/UNQkuawnoWc4DoEItK8oDDbqtZDwt5CfrIMFbqUNBxjC7TnAaM1KA1EN2GZQZwJLw9KGq442k4WpahNDfgYWIca+c43WIkVY3FkSKxFU6XkXAF0jAD7tFluAuzG8Vnap9Df947vTWcsGoSdboP9C70M9joCXrtBFenqdKnxiChElgMhpetXSGs8LVUepthKbjEguvHNht+/OgUQdnPc4zDwH6/R+ZGlSDW9j6lG3qht+A8ujVefeVFrl+7xgfe/zG+5pufYvSR/XweXVIxtAxUqegwMJYVRQacEesDPRg10Qks45lNiDp1VUBXh/RC8TAecDMiyl4xZrCGtIbNE+Ia0RjSgJkmF2FY685onb1UPvriHT762h02tfI9P/CDcOt5LqfbvPDJF7DjO3zwuTc43ze+7ZU7/NbnXuLd3/YDfMv3/m4+8vot/rf/zp+lnFxlXK24evOU49ORzdGK4+MjWFdcBoYhxkXmBXCpuBl7wpLNunN52TmbjU5hf77lpVu36SOsZhAqdepsEq/eTzP3znbcu9gyaeXa5gpjHxhsjkWfFXb9ThSBNH82d+btlnsXe84v7yJlwLyxPbuL7ya6C5axHKUG5lpaY/BGkzCmCKVOQESdgA0kdfbNshHJzyuWG2OPDXJZ+HeJMWrCAE5yLa2zZkyIIizORAPKat6YrdNK3PNaUgUjYZDbNO6lpgTBuzzAHQcvea/Gtw/EL/7FHXSOrZgDPdVJlvJLcY8cm9ZiMy9GK87g0VTV34gLkLv/Ix5AWr/68T1f4PMd+Jd/red9+CEQMZmQRVKRGm39WAe8ZODXMOH7PbTYMJdaqetKHePmHjywsb0LopVSBM/kN0TDOLT39JcseerAEulAbs3J2ATX5D+mEP/AtBSNQPugsh88AJUCEosH1YAOQk7leJ+RfQDO1hfPPUEkOpEugnWh73YhhzSjzTO9rIBO646XQte4EeswsJ+m3CxD9z1WjGaN3oguWkiMNcaTcVC0z/RdA2uUUhjrQGvtMFqZNboZvTUeffQ6FxcXvPry87zjbV/Jpz/6Ipu3NOq6I7ZCRKgltqKr1YZxfYSXFXMfKH2g74RuDSv7kAHS6X2P20zVkB2SSwnv0PbhyKMeCg0Vp9k++HpdYsHkgU1H+mMecC7sEFbisGncfOpJHtlUvuLtV7nbb6F+E6aZx67f5I3bDjev8/Mf+wyfv3vO1758jyff/yE+9PJdPvLen2F2YVOPkTrSNTahx5s1vhk4vnaFm1ev823f8R380L/wg5gZl/s9Z5eX3NvdZ7/fM3W4PV6wsjvsLi/Y6szd83N2LxqrzRsMMnBURq6MR6xkxXbbuHPnklvn91kNIyf1GGlCKZWTk1Nse5m+nDBLocuAdmM1XaB0thfKJLGQqMzUUmITvr9gt92yPj2i+Z7L3Rmtz0EJ8rDbEyxpUNFNqkdka9yPhVnCnFakpB59AOZQv3ShoqHQ6ZLTQMR7aBljQaLgXqIYu1Okp82fsPd4D1U9RAUe47Kmck6WZSkFkYSwRIMgYgZdsUxUXXTxrceYPRBFdxKl95JLnRQs5A5DFKp1agFZFfzNbrrr7ljbs4RsRY5HpUpFtGIlwoZKr0jxdIMO04RaB8Y6hLtUj05vRclcHE8LqHBekexOg+0Rp456YCeK4CU210LIIwuhEXcFTALfyBNP+0MeihltGRQewYggdjTGIzdDWsfncAyP0y66V9EgxHvvsQWvoYHtl1uszFipIEKtAyqV4kKtY/gflhhlVBfr+uVcje+h+sBlZQHm971TtHCka+pQadaDUlJqkHz7hPU9xYQ3Xn4Zx/js3dvMu8bXfcs3srpxzL3p8wiFFYVSVwzDilVZo/UY14FuA+6ZANmA/UQZwmTYbEZ6owRvAxmUQRWfZyab8ZXQekufx3BLEqv0HkFWkmbFnnOB5u8qEtHC7/n2r+c9v+Wbke05f/ev/yjb93+KRzcb7u4vafv7vGUjPPLIxNuPK7c2V9h/7dv58HHjYr3mtzz9ndh2hh3M047Ls7vcv3Oby/PXuLjX2N0Vzqywmu7w1DU4Pj7i+PiIo3Hk5lg52hxz5fQ6680prso0zzQLd3bPre6uz3HtqXJ32vHGrducjHApCn7COFzn+OQkOvr9jnO9j7NCCwwujMNEO16zG7b0aQIaUpJ/68pcgzGwn7bcuv0CYicBp8w7BA9sjkJ3kjNcQMKGL0yrYfSSxS2mp700xBpaItqjtgIGIxL4YOLZqaFJnXC0enJgo8RB7W6MVUMd42F4rRqf1w3EWqrf0nw4egAyoBY3YZ6E1kIN52mKgYdJ70AMU6olzYBzvyGCFOhaaKqMLpT7FjzssVDXqy9an94URVIIGZNo0GOcyGuJIlnoNT7mQiTaQYzbukLqEOmDtMAWLWgERowGljkwYv7ga/PhyEEn7hz2RYFPiIZJrOQ2WzQNHPTwtT29Ba1nwFiuRtwsZWzCIIHhLN/xEESGpYN6hDrJfkbdGI6FVhrz4vLc4+PNO4OMDDqEpZQoZVxF16nhGBSYYmzbw0Eli6aEaXBHqOOKmv/zpEwMtTDRU4ESP19vO4YysNkc0dvMvduv8f6f/QV+03d+OScnjzBLZxxWlKEyrtYM4wqta0ziBms9sF2ZW6gbpuA4+uJWLWENV0Sp4liBcRBaa/SapsVmzL0xzU7bT7Q2oF3iniSjDZbDyTtjCb5hF8fWRzz5O76fzz3+BGcvvMbdl15j/9Qxu8+9weXVp3jkW76Zdzz1LqZHr/CIVua5UsaBMm7YrG5wsl5j+wsu77/B2eUd7tkl4pWrOlLVeRnh9r2X2d46Z562gQU3qJQYC3dbaBH1q9VZHx+x2mzYbNZcWR9xsj7i+OQqz6w2fPk7HmMc346uTzjdXOFkcwVzYerG/a94ivO7d9henrHdXnDn/IxXb7/B7bv3uLw84950n60LrRu232N2ju8CyplnpVxeUtYF3OI90pD9dckjRoUqA0rHZA43LI1DzpOUvbJjVuIUP2Ijyt4vaNaS+tQPVLkHIH+KJdwe+ngsapwQLFRbiD5xX7gWtBlDD+ZKL56ZOkJTZxanEXj1pDNNZ3QKJx9xp6iy8eBc2krZbzJwD4lphVB21VIQcWonTDnmOYxA6pvcmRyy+pe0l5JCyRdeSnIQHaqEEsd1ZAK6VKQU9rmBdk0zkNbBSrgFpY1UzRvr8P1EkwTkmTqypM1J4h3BtyQLJq4Ms9I8ZGqebjBB2QozXZEoAtINbWEyUHpwpa0Jc5dDrEPuf+JtF2GcQsi2Ko70iSvrY7rNIXuTialNIIbWDnpMGUd8ipgCSXzHbQm/4hC6VLXEYaAVlcJYBgatNAn+Z7Vc4LAPSKAb3kDU6NYYxyNuPHGDi905ZSzcP5u5duMqdexIHcLGbRixMoTJAj3oFWZY3yG9oUpQPiRe3y4ZpdGN4h0fVphHznJvc4arOr05U2/oLDRmig9hiKuh1e098C3azMo7XkZkqLTNmnmn1CunPP3d34nvG4/uQ/00Xc6UzcDMmltcIJdnoAODD/StMxwpJ+sjHtk8ynjUuFifwBuVs/NbDDKyFqEwsTLlvAh3ZWYvO0QG6npkGDYcM1LWp8z7zuSN1/d3Obv3Cv3ejllCg23dEVlR5xnZG4UVWo544vqTPHbtKU6OH6GoMk87zGZOjlZshoGqytNve5qvfs+7ubIa2Qwjc10zd2JUbuec72a297fcPbvPdmrMNLZ95u7dc3aXW3bTJdt5y+zhIzAC7jP7+YImM6qhJlOPgK6jcsp3f/u38XVf+XWclMJ/9tf+Ip+/9SquNTi5JHSVy5FFaLHsA0SzGenRUYb8MeWrTlDglOhoPSejIB8jxekaB7fMRpUGw0zVhsaFEDJiVfoKEKhjSIKpAyYVqEET0oBspBszjf044z6jCuObfdxeisySZzNIjXxh9IBZFRGkSNiIdl88FvAMXy8uydR33MNVpjSohDSxYDF2FGFOkfSSGpclJi2rgIWTKeOBgC6q4Q2YOd0RZ2npLgSHrEYPXlg3MoNDIyOltVjWeJrullB6OI5OPcZ6bxwNa+ronN+/TSkbqIViI4MpPnd288S4OaZ6ZZSBucws7tfhqBIjeK019zuOlMJYoisHCdncPIW/oQjuM8xb+sUlTOk4XsMZere9oPM473j3NzJNM6dXb1A3DStnFBkRLcwGrbcwBjZofWJqHWtxwreeNKpk9FEkiptHTkvZh4Z2nhptPzNPEZ1rKjALOxHabhe0mRI2WgDNOxfTHktPx2GYWNnI4A0pa7YWZiPNa5i/7p1JwKaO2A5Had4wgdq3VLnG46unePLGM7zr8cc5WRXOd+esZcO03TPMF6w8pGzN9+h+j19c4n0fBi2bSumVq8MVTk+O0I1wMW0ZhgIXnYv9YlKhbFQxHcIEZJyxecW0OuX6jWd59ubbuXLlKXxYcfv+LV557VU+99qLXFy+FJ31bs/cLqjeOdU4GGpZMQR9AEphNay4dnTK6SNXeeTqKavhCuPbnuHk+ITT9YaVDkyt0Uw5OT7BZea9H/pFfvoXfpYuM1KN9Vz58se/gu/5zm/i05/+JH/vJ36cP/kn/jg3rzzCp++/weBwZDCXpOMtU5I7B3PRnGwUo/T4vCla/2hQEiITMWzqzNrwFtQfKdEUDBoJj73OaHfGXrhE0GaUJkiPWIhSQMchTFdWMZWKV9zX+TMQHf8Asi+MGumTMzNTnb9ofXpTFElf/j9HXZWlgJBLEVIfHPQPM2JJoh4peL6kdSTl3I3RQ9Xh3WEwfIybThK0lXTAFuwQBLU4rEhSjNwiIyQ2bwEs97ST9z7njxz8n8BNwmJLkAgZ6iGC8rnR5haEbdXshGJxU/K0jrlhYJoGXI/YtUva5S1KUdZHJ5RBmedG742+u0UdrqEyg000N1qe3l00Q9sJ44KSm8PeaG2mzZ3eZ9hf0EVgELwYaivKuMa10/YT1mb6PNOb8/xzn+X46ilXrl8HdbSsQKYY13osebxbkJYtOvnZjFkkO1dJh+rwA1VVpPfYNgr0tg1ivTl7Cz4sOiBWKXul2cTOw95LXBhq5AntW+Nyuwt6mArrzSq6ZldMdwGDeDhMxXI2YAqkxgKhRBiZt05vnfF45OYjj/Lsk4/zjptHHNfGdrrGuH0b0xuvs73zAqvs2HZqXDqcedx4cx0Z64bjcoXrxze5sb6O7Tu13mVfGjvb4dIoNiHWqRK5OOEwPuDDMafrx3nk6tt46pmv4tqNxzGUo3vXoVwJtKdvkekcXwPWWSlcr8eMo6C1MDXn7oVy5+5d9rspCNbHhdOTDZtxoHWntYZYLERrHQI+MuHdb3sr3/1dv41pa7z00ovc3l/wm7/tG3jH9Zv8uX/vP+GTL7yfr/mmb+Yn/uF7ud8uqJtYrIanQmCsTlgMBoTveV9oXot2YK8UMSbtyygVJiNitAFaKfgctm06VmxdqDZRm7EdFLGBapVRLQj5zcOoONQbQS2rig9J0/KB0jeolVTShfGGqNJkpkul10Yrv0Ey+f/QD5HwyCuJG8gBqs1R2AkqgSm9GfNS1DQ4gpKLi8hiAHBm9QyECmv7A7vHbYlSBzjk2oRDSXIWJVcDFje+EKamXQ62vCyh8bIUBsITspObPs8x20KRgHWKkFZUGnpcjYKgo9JNGOQqt2+fs7ts1HGkFGPeXXJ+/w2G1ZrVaoVIYZouuXPnEhB6yzjOGrZrPREFJ2gcC2azSKRrdtNtrJR9p19eQpvpo7A5PmE8XmPdmXcXzPttegPumacL9ns4P28M0zWsOPtmYHt8bvTZmDWTEpNY30ooIqoPEfZGcNtqzzhgcTrGvNuz3+6xTtBsSqV2qF1pxZnaxG7eYX2mutJKxVUjTmEfdDGKwhiY9NwIjLqHVnjWHiMhynpYc7WecLI6ptSBXd9xdnnJ1Pesh1OunV7hxtU11zfOEY2TsqZdu8Jrpzd47e5rrNrMaKDibKbOsQtW1wzDhlU54WS4ymZ8hOOTx2Bt2HZkp53dfMHkl9SuDL0zaVBpBskJpJxw89pNrj3yKKc3r/PI46uYoI5usLXKfr9lv7tNsznMf72zKQPH5YiRmVor2wEuLjobqQxjTBRlHDhZrVmtamD1Gs7+vVvASaUyt8bHX/kMr/zN13jqsaf5pm/4BiaDFz73Wf5v/9d/m929e7zra7+SC1P+6t/585zcPEVXUQQzeYEm0ZFjQdE5lByPhqBLY9Kg6jSPNYyKhBcAC9tHGGrcC6KCjIKvcnmnjVkA08gGV2OA0Pf3wPZZDD1UgmzPQLUVg6+RHtSkRocGfTZsFsoUVKjBvhiB501SJFVgFI1TqfXQXwev9KAJDjmihAC+GW02pBCB5lUIW/5CUYlcZ5YnSJfyhVTqMfS5G60Z9Ae5yAt5PIRLindlVgib/gyNb5H81q0nwd1xFzzjCDoxhmMSXnfeD0FJkaa5VK7YWlupoJ3VsOLOqxP3b50hux2IozKwXp1gfWK/m9juO+O4osqGebpkmrcMwxiYX/NYAKlQpeCaqX4GEJvAhV8qpQbWq844rpm351SUeTJ6nzk5OmV9/YTetsz7XZC393A6XudtTzzN+Xwebi9zY54v6HNjnkOpUc0zS4QwUnWPcUwjoc7FGHpQPlSEaZqZ9p1pMtrU0OroGK/17Mq0n5jo7HZbmLP7GAdkvaaWAZHwBvz/Mffn0bblWV0n+pm/Zq29zzm3iRtN9gLS2CAICoigohZoSWuDiigipaCI5bCw1FFalr1VloWUNWxe4bAstHT4rLJF0afYFTwQFEzpUiBpM5PM6G9zztl7rd/vN+f7Y861T0QSEZnW+yc2I8gbce895+y115q/Ob/z2+SSXVtNZagTj5N55k8VmOuei3zOvd1tnrh4nLu372EG1+s1z8wPePb+fVRmN7HIikqG4V6HU+nsh1KuG/rggKzCVBsXrfN4mUkIV1qYbeJ8uk2utxnTbfIuMU3Cblyyv56Z1OlhZ5J4WBVjYhJx3uc0cXF+m9uPPcX+9hm373gTsBbh7NGO/f6CeXdGORZ3726Zoq67nzUhvTiNslyTkwsYyNmNX4Aug56TCxJkMKpgkTOe8mC1hfcc7vOuH3o36Z2ZezbzXd/+/Xz8z/8c1rFw6wnl0fosu5I5pBXT5JPV8AZnYyTkKZ+kw66EiejkeKrMexAy1bt7U7K4IEQDj8xSMBkIju1r7tjU2GlhSGKyRM2G6YRWQ0rIDsWlkA59JRgFn61L0JEU6wm6IsMlkkUqwwbSX+eYpInQJijD+YAjcDTSIJOBSrPhmOMYzqHSRtZQbJCpxbGqFEYSFhkXnv1rfgPFqW0WcrKIW8iqmGRKnjzEq2byiDjSjbBqbljqMq6GjQ6mcZJ655hFTglvmAVtYbtZogPVRpJzHztxWVbVCdZzHjz/PNqVXakslunqaZGlJFKe6WESq2OQS6UWRXs7mYKoNqQQ8Q8evp7MMD+KsSlRVKgxoogWjqJMZ7fJGourJCRxXDjXc8q0dxOPOnP3/K2cy1M8PF5y6FfQVta+OMEfh0ZMQVNy01eg64olZZGMDvWbv8CUMhnHK9d19Xzs0dFk2DCO1hG9RrrR1kZfO3SXy00GZ9POlT51Zp521JrdgXqqDIsUv+yxvCVX7swXPDXf5m333sBb3/iTuHvnDm1duTounD24z9I799uBq0f3eeHRm3hqX6i4Uevx6gjXB9J1Jx0Hc8pYN/a5spuFlKAj7PKM1UyeZ5jOoFZEFvJcYKpgFXJj6Z7NNLwDiElFKDJzNu3ZzZWUB3OFMpvTs1J2SGmE5n/tHE25b4XWB7k12pyDKG0cdKD0MIeAPArW1aNRxEUZutnEjRWk03TFEOa8Z673+Hm/+LNotfDM1bt59voHyHVBbMdoQTqPBmAd3sC4PtHCyxFG2pz+5SUmNdtrxMLUZZTOCkmIxkRn3pD05FShZDNlFLK47ZloQYcxDVfaWIJEdWqcQV6NLRikleHP5DDoPbK/BUuZVVwlZ8ur16fXRZF04nZGgw2v6sRtMfe3c3q5kYOSILH9SgZp4NnYatQaDkLc6DgdOHa6wZbhIWNAE1gT9Iw2/zB0CuIyiWru4jM2npeBaXMwWo2+Gr03RH14L7VQbCLV7B6XauGX5zEHOQvZMmp+wpkdnRmRKhfTEzz48YW8qjuopOR8LynOY4slUclGKdD6EWw4for5CAaUOiNS8TPYcOW0U6JsuAbXRFhzmAjg8RA1OzarOhz6KC4bLKlGRABUGTy8/z7Odo11foGl3YchUSAttK+2AczYcIFAH4qNYxj2diiQa0ZlolhCUda+OCyhg2QVGYk+8CVH74y1Y92J3T3BVApESH2ZZ+bZQ7s0lFkejCXhU2lQXVd+59YZb3jiMT7kzU9w985dltZ5cLVwLcrZC4X7ywNevP+j/Mi7Zwpv5omzHel45LkXnue+HjgWOD/fIyVjI7nZRVlY2yF8DYVcM/Nux63zCyRDXwXJGauFcfTDWc35h8YSB+lg9Gt6u6aPhTGMpQWBXBNindGPLH3hejRW7bS+osfOOA5W9fc6dplLPXBYOr25NO+4S26Gcuw+0YzVlyQoSqZbRYeC+FIRgZVr2nTg3e/6Dt/cz9UVXUePcEriZhE6vDHx5aVDRyMmtWSumhnRoIlvFh36gfCY9OKaRuQfGe5q3rzADYMRfEvTHCo3Z62IBs9zpCCdS/ArfaRX9cNekvl+AMOaYqu4L6UquSulK9J85H611+uiSBLt8ejqY6O43XuefJwQcTzPsifCkcGqF65JwrB3FlpEmZj4hdbYpiE+TlsYbGo3H93WBsODgjyBLnhcyfErk+Iu4YT5AR6TKWPQe6c1V5J4cNWWTePUhRQLcsvuEpNzZlMPdFuZ8kBG53z3ZmjnPPue93H9/LPktNLUTrJCCdAnReQqArtp8uxvc3u5JImm5u7l4rikxoleJDFSZiq+4V0FrsWQ4Tkm/vbUC61ZHFLqQVjZb6hsQjHfmF49egjFaMce58WGQUXXHCJ4S24o3HpntIasHbPufpt1IotTOEZSeh5IBRGjqseJ2vDNZ2uHOPnNyfTi6jwpQq2FaSrUIuTs49QwdegDCZuuTEGZinF+NrHfF+Zs3N5X+n5HN+FsyuwyYEeuD8/wzAsZyweevnWHdH3F8cEzXJYFu3vOzMS+zAw9MnOk9kfkyxeoOphKZT9P7PY7bp/vyFkZy8TDaSbnHUkrMqqbfGhDteGSFXUKzvGK9XDgeOy0VhjWWa4z2laW430OV5e0w+JGJr2Q1IO/ZLoIP1Y7EcITHmhGT5S1kIcLF2x405HNnFNsCaNHnlPBxEUT7374DFOJ6IQ1o2MCnSmyhfGlyMnxlakGBc2NXozZLz1hYOR2CBquRQZdnP9I+BxYCmJ7eDwOwJrfH7788W14pThsNnRD4UKEZWCT7w0sJJfJ7flEPYSl9U4zKBgaW20zX6iu43Weluju1BNIczeSpC5FkkTKGhtLt+SSKtQsMDnnriLsEpSS0eKFCPOlheWNuuV67mQRqj4ce+vDJYBJgjLDDYExhSzONoZ5EHBhINaQvMJw+aCYuVnw6IFke7piNocD5tmT24ZkBoneOjlVChdcyBt4x/f/Rx49fIasC90GR0bwNr1DFlWa2U1iHhK8MudxbuR0Bx07WRLD8enoNDsJhyIsZ0rKiArLesmijSKFkovzJYf7/FG9sNrw5L/7Dx7x4R+yY9gjjsvqSqjWg4HgD0kOOy/3xxxoWzmOla6N1AfFwIYhbYnwL4+z0CxMGjzLJOQUprzWffySBBglT+ScKVIp4atYJRIKLJGkuN2XRebQcN1xwjNtSJ6h1JrS1u6bzrbCupLVoBttOXJ19ZCUB8vhWezYsGMjp8TZ3XucpXPO5wu0X9Pagfl6pl53Zjq7umeaM3WCMsFUhLMpsa87dvkWe+6Ew1HC2gpjdb23rqR8ThmJcWz0w4IMvz52LbAs9H7J6IM8MtiOqRZu3b7gbH/B4+ePI1m47o8o18+j7UX6cgmASSFpcYMWBqbTaVkiw3X7kgrJvMNQjeXjgL4qidmXm4Hrq5bQkbsZRrdGMlfsKKDm4W+iMLLETsFNXzYJoa8LimOYrtDwJWMurs/34CjCjDBexSl74tMT0gPj9KWr2w/meM8aGu+Iecl+AEsKDDZcpvLAZb5Fob7et9tJmPY7jwAYzR3Do/AJ5rI/g6SZVNw3L4t3d8kS2GBHIYmHrbfh2+MWYn4PoNJTeLv0BAvYEFZzHDGVQqkFKR47UKgeR5CcChSaHleC0KniW2opSsW3lJtHpQ1fmBiOJ1Zc32ylMMhI7Szd2E1v4H3veobLF54nsVDmQhuZKQunLJOU3B4K2JDNZJ1VR+ylXPKW8IJQkqEKRdzLcpUB+I20WnNYAhiHI01XN801cxNUnB85ZDCOg6lk9tMZhtHWhbf/h2/lqTff5uLDzj2AKlXvEjViGyyButPgMMBW6Et0qu5VaF2R7a4zd5CRLmgDJKHVYws2loJTrNJJfpjLRC07ai6e8GfxsMfirarjyS6J8waDNbFedl544ZL99JDzchvShPXB5aOHXD98wHpY0cVoNI7TNcU65XCJdchaOSu3ONtfsJvvsDu/ha5HjscHTNbZ12t0XZmtspOJIsJUjFrchHlXJvZ5ZuIiZPmGWcV0R+6DPlZk2lNtItuKtSNp7EgmTNZJujr8pAmVSqqF2xf3ePMTb+benad4/M4bGdp54eHT1Ie3uF5guVbSUI7daClTS2K1ldTdczInIat4Q5YqaASKWQg4zGAMiky4dUXDbGA5RdGUmI4C9ze3OTMMsktis+gJrpm2IsqWSy9Bl4uU00Qsb3xCzB2K4OmWodQpGuKOZLhEJCS9wZsV/N52lU34QWRBJucmFzWKOhzTV0WWFQue8etecSM5Md8+o00K3YPL/SJnehtYW/0ERkjSkTw8bMvEHzKNUUs7XRorMzVwPALXNHOFhqmgwYtTvItJc0HmQiqZMmU3o1XfthvqI0xS3O3MNaLFJMboQZXMLJNLoswNXIcZSKJYJqWJqUyk2TeC7nyTuP/0fZ5+z/vo7RJCV62qWAsXZobLeAifSwvCvXXH33LyZYh6brNTi7yo1JzcjT0ReI9vGG001tahrzQJXptlrHhh0XCxT7VyPB6Y8sy9u/foY6B94b3vfS9vePyN1CfP3AG+Ok/QzInsCQ/9yn2wjkR1NJgunjkt2cfkQmasrqgxEVoCS0ZBkZT9a4Xjtw6nV2l238GUE7tcmUthypW8GcuqBB/OQ7gsuTEKzTho573PPOR6KTy4VN70xMI+FZbLB7z3hUfcf9QYS6KqksriHfzZTEWwtgZUkGGaYbcnl4zpkVR3zPWC1q5JXZAm0LyLti1cC3fZUQO1kPLl4ouFZiSd3BTFhDGOjL5iw+lmWRZIjVISUynUnNntd7zlzR/Cmx9/K0/ee4Kn3vAGWu/sn91T93uePdznwf1L6IO5hgdqhmoVLSMgJP/McnUzaJfvBjsippJgH9JTQqwgYowkZAzR4emh4p+t4jjfrBs/UkhRAAVc2SWbkbXbkyXVoO5sRQ1ad6s2KQLJPAa2gjCoQcez7Z/kfExLxGTlAmAxnLpn8edT+FSat5VmeMfeVnqDTCbtp1etT6+LIplyYr57Tjoa1mtwrLz4sTbk6DeeqWKirqlOPrZISA9H9wLVMjQZmPtzOTcPjU3CcDK6uXnrpOIxCIElSYGoKph1wHXZho/p0zAHfDX893ymds14Djv5nujdf9+dbHZInkglU0NpIE0Zh8YzP/IMh0ePWNs1OhrrevTwsfD2Q3Jgk35FEkQqj99QNtyteaLQ18YKMEPN/vD5KZzQ7l2Ae1r5MgXrjgyEFZxFV57Ns1HaaFjecXV1jY7Ebr/n9vk5H/aWt3JVHmEqYSSQiz68igAAu1tJREFUyFIBpWaXuNEH14eVR1qwXtwNWzJnpTDXRC3O72xqZEscbfiyyYwiiX3N7KdCzYUxlGMzVjWYEvNu5tb+jMfPLzjfnVHrdFpuFXODlNb9YfSvWbBJaCNxfVx4dP0M73nxAe9+/hH3Lu4gbeH5qyserIZJpQwjXQ2yOhdRgdEGh8WNckUKqcys1jgMgzJT5xk9LhxG5/J4xfnhEYfrC0arrIcjbXSXgdZdiAw8rY9kJJuQNsip0qxzdXnfM2Me3GY3JQ6XD1kOD+j9CtMjOSm7Wrl9dovH7jzBk089zr0nK8dVWPptLtcrUs307NI7Sa5LTkWYyPSSwv80FptoCDckVFvexWE3/Fqp+cQKcZd9DVpch+zekNmIhYidoLHoK93ykBTmxDEyJ48YcTgETBJtGLIq1sVVdFmou+JxzKlE2iFRIsOo2UsFItC2iBbzUTsFr3nYCPaJ86NHRFTPi5BaJiOc717vmGTJzI/dIR0z2nwXb+pGq6NkShLSunpXqQIRK+ojpNHGoGlnrGDZx2LF+VS+VAF3/BHyMLK5ljsn3ySXDHMR2BWGuryxj8XVOl3j5tDYMrtsz5PocC2U1ACMG8Ng0XAFko0zFsYbw3wb14X10Li6vM/x8qG/3+SuRjllPxyC5ikpk+t0OiGjHYEMrR0ZrdHCes3EDSKy+HajjxH7bQkKyQhzVZx027pv0LH4OeO9YuSSWI+X5PPbdGtcXa8cH73Ih33Ym8l3drxoj0ji7tQ5u0tRwUcoJFHa8MIbxOB5KpzXiYupUgss60rbKdM6yIu5VDQndueFu7cmbs8Tcy60Mbg6dK67kefK+a7y5K0z3nDrnPP9GbX6LexsB3+ImnkWyjChKxiVNhLz0ThcDw7tyKMHL6DrSk6w2uoPe83++eKf1Xp9hNFIklnN0OU+JsKUM60mVu1YSegER2k0Mx71S263mevDRO+FZbmk6yWrXdH1iCW35zITxjCGhJlDgdUW2nqfw/0f5+FkrHXi6uoR188/S3/4CF1Xqgc7AkdyauRa6Irb3VljWa/Q9UDSFWzxz7NOsCuEspZQ5QKuqVcJQ5YNi1fYpLmGYXLEMiQSsxWG6In/KwEyitnJODsW3dgW6hZgkW0ApQianDJXDHLrjCS0oHj17nuA3VTZ7Sup+s2vMUn5DsALo4skHNfsubhh8MnG0OmEG7op4VNgGDknZM5++AEXZ6/37bYk8v7cGftFqOZvqOXmG+q8upa4zJ67LL4260PodFQbYx00NeiDoc03zJZIYTXWQ7pIN09dQ9EMpeA4Z1GXQ4aTTFM3sihNseGB7RQ/UZ1LmGJh4K7MqHdCfQza8EWBiUv2xhiMkZ1PZ4pQub6+ZGCUyU1uYVtQSIDRG+fMYyQaPrLJcPt5Z+IqKYjihvmNsnpHm7I4LcUGGdeaq/nXsuwnaq0FG0DKkSdtqDUGR3IXUplZjgdyqdy9fYvLF+/zzd/8b/jJn/RTOe5XCispZ0yz40h1dpwI316ScEPkrOx3ld00M82VXRVmrbQxOB46+TrTB6R5x3xeefz2nsfOJuYsLEtjVxsXCtNuz/ntuzz1xD0e25+xnzxTBjFMnIenagybUEu04Usly9Xvly64HWkl5QnFR7pKAhHW1cK30OiyIkOptknZMqQV0gG1R8CelAemR1a9pOtDunnXe2yVq2NGe6G3A609YOh9cn4AFoYkYzBk+Fg5jNWEYztjORauXug80AN92nN9PHD94vMsly+StDmtyVYOhxd48cGPU6bC9eGCpR147oVnuP/gveh6yVntzGK02im7TJ6hZA+987PEn4chRhcCygo7QfUKtMW2FnFOsHdp2Xmzp2HcWSeW3KpvM9rKwfOJIGdIXqDSyZ3Ki11FmKZMT+K5OFNhXbu7D+0m6iynbX3WYHgonne/c48G6c4HnVKJLtOLpCcyGN06jBHSVNiE3uc58XDnz8LZ/nVulYYRjHnnS7K4zbpntpjz4sSJwbU6ZyvJTFuFkjqiV0juSDqgQ0JIP9zYEy8kw4Lxr34CqgMnTCZUlDJ8U9u0M7pHbRbFt2BJcIaAL5QSKU7eRC4Tpbj+tfcKQXJ3eVanjxExDQTGAmlUjpe++dYYcdxTr6Bq7hSTfX9RuueFWPaOr2ZxqEGEEVQkr3x+d5oNelvJ2ak229hkgmN2JBYJqoZ6p0opSO9IKnGjd/pYKF5e0eNKeSzx5FvfxAtPP8vls8+Rnipc07C6oxbhgsRgZZcy1jtdu0eH9oSUiZQnZilMZSKfFaYCexF2i7I/+CIrzxMXu8rdi5k7Z5Uig2VZqGczqwnz2Z7be/dbPN9NnLs/hHPcRFyHbYTG3yK4zQ9IWOma2J3NSNohUh3/SomOULLSjgfvQiJPPYVhgyR1Y+cpYfPCUh6hsjLswLq8CP1Z7taHJEucS3JF1MgeejUOkK7Zzyu3p5U1rY6rhtFyM0M1kchcMCjpmmGd62VF+p7DsrLqQ/L+yEVZQWA/H6n1Pl0zL7544OHVjFTj2B8w75/mDU8pd+/sgIakipRM3u2gFI9rsq1IWjA/4h5LxlBXrbgXoyce5nwHjftMABurTyDqUNZQcfGCZEzd6dtdqXB/BEmkEoe8CKN3Fyyk7GKR4amNDfXFzYCaEzm67mSZREVF2UL78I/J1XLqYWAMl102cxpZEpwnkpzKxBiYONtDVKGf0fWuL6JEXqEw+et1USQziQtVDrrSmnC9rvTWsO7WXWpQS2VXCvsysdvtIE8cjka2BeuR6K6JFuDwUHc/br2x687kik/Y7cyy3LTu6vkwbvIp5G4U7ahkUvUNu2Unp2LJO7/kFlElZ0pxDmaSidEUSwtavatpttJ1ZmeVmiqlKMtlZrle0SFg1cdfvVGsI4msRlEHojULm6ay6/DQry0Cwm4I3NtrjIEdViYLJQ7eMbiBR/IwLUvYiNHcPFe6t9WXXGRMa9izuT/kshy598ST3N1fQOl0g6evr2EC3Tn5t2uYmA5FV6XFz5oQGsljBQKTKnPlbN6RLxLrGFyvHSOzn2d2u5k0ZVcaTUfqEDqFupu5fXaHvJ/J80yqINm9KS0WAiKOSRXNqPpnKnghncgMSeQcpPthKI3jOKC2orMT1i020MnU6VJAmpRpb9R89Gs1wHRlPy089ZhgF75Aqimx261Mu4V5BpaFi72gu4m7d257WFyCWVxLPMC35nWGxbhzfk5JBjY427kd4FPcpckdVJWaq6dh5krOCcsvkEqizoVlPTLxGMptju0ASRHt5DzRhrDbX1CZyGRGLBgVl9lKYP2qHbrfd1MplFRoml0RpQOAjideqjrk0/sKlsipMLpTtdrw6NplOXqXyo7WGoozKapkdtOMsOXqJNbuHGHU3JzDPLZjymeYVVpSSkp4l2gk6U7pa+qxITKcuE9ka6vTvFIOR9cxKNXJ+ZVBNqOT2XLk/9Sr1KcPJgjsbcBfxXO1DfgaM/uzIvKHgS8Fno0/+vvN7Ovj7/w3wG/GOaG/08z+P69dJOG2ZvKoHNaV9bqh64IG583MKLuJfZm4Nc2czRM9Zc/irYk+ZcYo9CXTm3vXJROm7uP3piRw7GQwzDequUyYKcu6kqy4O5AU6MM7KzG2jO8inv9t5ps3kUyWylQrpTrLn5rJY5BH47AOFkB1sPYGNrOfKvNcefrhwnrwg0DAQ5CCRuGLEE+cE3WZoAbQr6YMbR63qq/O6zLzU7uvjTxlRs307I5DaHfFgiPxmDZsCKlOqC4uoRSCGDxo/ZrWC+th4XhYuHf7Md7y5rfwvuNzPPvweZI0V9CMQpPZ6RjdmcRKQsVzecbaOEzCnbznqemMx+Yz7px5auMqnXkdmFVq8s7ckpuUpJwpaWI/n1HnmVonbKosKdHToKTsOvDisMpghEIqdM05odvnJ27BZeYYaq6zfz6LkPPOr0V3dRQyODlrm8so592OkgtrW5mmnVOmBDenldVjQTRchtJELYVUz5BiXPV7Hj8beNw+J6Q3hiq3L25RJPHgOJCaaX3BRudsf+EQwoatjQUY1Fy90JTJC50tiAxkBpFrbAz2GG10juLKpYnKvioyDpRaab2REeazM1r3zXVJXjyl+AwxlkYiUfC8HRPPnZcm7OoeSZm1L6SUyTKTrFDr3icXa9Q6+aGehJr95xmmHJeFah76VqeJedozS+bQjhE2Njx7XRJXl5dc7O5R5AylU8RY+xWGE9nX3ri8PrJBn603GMacyikGJOONgxpMZfKyNFbSGOTqTJDkZLhXfH0wnWQHfreZfYeI3AK+XUT+WfzeV5vZ//TSPywiPx34AuCjgTcD3yAiH2XO8H3Fl5gnv/W2Y12PzMugXR1pY/Uo1pKxPLDioIc2N711PqLLreyUiZEwiaIwDEQ5FlAJU4XQVCcxrDfMEiUV13B7zxi8yE6uEiZ17mWZxYPGpBRycoz0bL+jOnULMdzLkEy3wUgZtRz4n3DrbKLUCx6+8IC2rggjRhCLaxdMhuBYDvNtPMMlCEL4V9qrjwb+hfw69DHQ4Vk21cxNPMJApFvzbflomMGUduz3ZxwPV6h2chbXCA/lcDiynxaO10eeH4+4fVe5eOINlOMPkSvUs4lSJ27td+Qpo32lsvfPYh1oMc7Pz5A6cefeY9y52PPkY3d48+NPgAyWtNAGVGb2OqhTpk6u3UfMM8d3Z+Q6cbfsybs9rbkccJp8kZJyDnKx42jJKjVPlOT5LZkokuLdo1Ep+Qwj0cYtSsHx2BFUKecVsHQvlnMplAgBW1rzbjQkdjoUbAHxjv3QFvp6xaKDOhWO6xVLWxB1dVitM9drj6+duL7yNMLrPjheNo/7NSjtitF98hlD3VQlCTrUw6tyAenkouhoHA8Lx9EpuUSsgXBrrlwuR6QU3tsbuipznd1Jy4yzM9+4iyVuX9xmqhPocG/KzTu1H1FRlr6yLAculyvOb9/h+nrx5xAl55m+KndvPY41ZcrF40gkU4sT2A3jajnSVZkR7r/4gHnecXHrNsv1tS+QIrN+vz8DBGvw/AHOzwZLv6b3g1vrBRzURuN6PWDZqDkzT3umMjH6SioVNXWtfXHGRq4eqdG1U6fC+W7PaEdvIF7l9cEEgb0XeG/8+pGIvAN4y2v8lc8D/qaZLcAPi8g7gU8CvuXVv4efAGOsJ49HlUEbHet+clnoNfPqXLlDMpY+OB4bx6vGcjzS+2AMYDi1oItgJZODeqo41lJ7dqPN7HSeqReUwoHItslAHqCJOhIjnJWrCbtSyVlQCnlX2c2VnWSSuNpATTn0ztRBjoPWnDuY6/CR6Lpy+fxDSl/8piczrDsdMuSPA994ot0dztVOm+dte/haL0+Z3JFrZz5LrKuiNjFKRrOD3Mma009kx9DGGAdyuc28v8vh6jnf7OOKH10WLq8esV8umXcXPP3sj/AhH/4Gft7HfhwXtTDvM+f1jDc9eQ9SpyThvO44292myJ5SOrtdodjMfr9HslBKoe72rLLQ9RFgZKlO40k7aprJCAcZGH6tmmXu2i1qmbmmcb89QBjuCo/7S9oEOo6clT2adiyW6ONA107T7tZ5uMlrX6/ZjF9zuGa3fqS1Be2Nutuxtu4WdUMZ194Vp5TIybmEa28c1gXrg1JnuhnX6zU2Vi9klxO9N47rNWSh1Jn99Yy2sO2igO4xGm2sToebKzo61q5jSblRtQo9CjGjw3EJKao7Yy1H70wX6dTq5iklFVLa0Zuiq4SixXFyXVeutSNTIWkiHy9Ze6I3dc9OE2qu2LhiZOPh1QO0r3QZpINxuFoYKPM8s4wDa2+IHbh9dhuVmcvLK5Z1oZTCuvr9q/hSSEwZw3PO+4OFq6sXqXOF5I3K/UdgUtHVSeVTTSxro5QJ7QMdK1UyD6+vMVGyDEjQx0xO4sVvKN2OLO0KQz0apA3Oz+7QlsFu3rPb7+jLkbvnF6/6PP0nYZIi8qHAxwPfikfN/g4R+Y3Av8O7zRfxAvpvXvLX3s0rFFUR+TLgywAubp/z6OGRtbvlVrOZrourQNQ3vyzK4eGRpSoyFbp2jmN4TsXaWHqnN6N3H02IVb+GwWbu3imiTvOqUh2uTsKaCikLk7n0CjKmEytK6tHlDaFVP73necJEmHPmPBfmMjHMfGxPmTrNTmtJSygHBdVElnPe9+NHLh9dgwqZ7BIqcz7tGEoXMBGsD3fx0ZD9vWS8dhsqOf36/V/JwOTIY295jI/4mI/i3/+7/4g9xKNp50objdF9tEqhITYVehfOLu5S7Tbr5QM2eyusc2gHXrx6kdvntzmbdnz6J3wiP/+XfBQkv65z3iHMXOvRN9tkJB0RKmpHBB+nSZmGsphxrUdaPzLsGqOxrg2TCUkzaCJDJAG6RVqXwnOyMFZYdbCOBUanKFQRrpeDZ7TL4HxqJK5YByzrtScFmm+HMccxl7VT6o6ady7J1M7Qjmnn+uqaaTp6VIfBcfU43FyMkiGvazy4Vzw8PPRFT5rJZe/JmdKwPpjmPYpxPB6Z5zPUVua8Ys1oIsAKZgw9cnl8lpILtZyTZCKh7OaZvh59OWg4myAnWu+OKScc19NMkj0iPp2s66CUyhUeWFaKINWhmj46u/0Om47ORkgTrQ2ulwHLyuiDml2KqeOKYZ2SE30VdvUcHSvaEjXtqCmxn2ZKFoY25prYZfPF3O09V5d+go195rgskBKtr5SUkLNClsayXGJyTW++nJWUWNfmjvbqlLqHQxkjM01nrMdGLYn9bPF8Zdqy0kZDJHG5Loy1c1juk+dGyYnrq2ty2jHVCxKw303Bn+40VZ69PL5q3fugi6SIXAB/G/hdZvZQRP4i8Mfw3uaPAV8F/Bcf7Nczs68BvgbgzhN37V3vez5AZC82bYG+Job6AsBGx9qgiUsIc1dWc/NdhhezZKH1FDxAXnyzSzfX9MZvplxC7yxYKlhOlCLMdNRgjETXQseT55L6BwFGa439PDFP2f0iw2vS6T+eDd66BU+PwFQHoyeuLoV3vucFHnF0JYjgMbehdkjmf89J3k467q+BPb7y5wSkibd+xFP8ii/9TJ54613e8KG3+ad//d+yDMia6CSQiunCoAG+JBrLNYPE+XQLyo7WrgF1bHZdOTzzHFe72xzu3uK5F48c5Jyr8pCpF3JqDDtwJZdc2xXrWDErWJ9DI99hOG9uaZ2u5txN69x/+IybQfROljPKtHdTizFY1DBdnR6Wz5iKW2RdX14x+spUC1WcRnU8Hkhzocug2ETJe1QSffjSIYkTq6374TrMIx9SfuQMhBEsaox1OVKzLwmTCK1dM5qR80xKhWV5EdOVrgceHR66PdhI1OmM4/FIzZHcWXdsgVj7vXMBa17JktGktH6N2cTxeMUyHjCGMNULnwSYuXVxweiNnByn1uHMgRVXjRUyklYMZZ7PWA7KXCumXthEEvv95fYMU2XvHpDhgTBNcLi8prVBzhnVhuJLm7YuXihLIg3H9w9Fofhz55njwtXi6p22+gLIODDla2qtLFcHUhL2F5nWB9OcyaVyOF6Tc2OqNRRImSnv2E87pFRu3y4cDlf0sSDAo0cPkXxEslKmTu8LTQ3JMw8fXbMeDwzr5HKLw+Ga48GTVA+PLjEGh+ORWm+TpgaslLBR3OXGsirz7s6rPlMfVJEUkYoXyL9uZn8nitzTL/n9vwT8w/jX9wBve8lff2v8t1d9td758edecFkdyW2elhVawxiuRhmdpXe0K5N7mLslGYB6Yl6VhKVEy3JyCcmmIQVWmikpFbIUCC6lp6jBlN1DUV1bH8agXmTVYuusSs4VXTuDTqoTB1tYW3dsa+20NujNOZZ9uPv0GIPejGeefsQLLz5A7eBj9fA8nCTGaD5Sl+wPvNrY+riXTddbBxmfQfxuOv05QXjz2x7jf/zq38vHfsrHUvOOz/6UX8TT3/ff8m3/+h30NVPrjh52a8TXUBVIyrpckjSz312gqvRxcPcklLZe8ej5F6lv+DBe/PEHfN/7fpgHZ8+y75VRLumrZ/Ac104yN6FoS6K3oz+w1bf0x+uVduxIEpZ+dKA+O9ZWaKR84OrRFXOBRR8CzbfseU+VM5dRWidJY02C5Il1wPWxMXXHo3S5ZKpn5DKBaiRMViRnzqYaJhygbTAOa2zB3a1JkrE7u0U7rvRutHUF8Wnj2ButX3kN78Zh6Vw3yDo42+0xcnAzjb4OVHvI7+Bw/TzdCKx0Yu1H1K4Zw7mA0/6WOxyRydk7/+XYKCnGVXzZsfSFllY3mR0N0mBpyjxgXa55eDnYT5VQrtLWHUL17jQffQlzbazHFznbFRKFMYxUEst6dEpUyYzeSEkYRdjnib6s5KmyrldMyXN1Wne5ZpHiLj/ZaW11ruzKxOHqCu2D8jD74lO8e728foTR2M8zNc90bVSu2KcdxzawDK1d0sc1Z+fnLEujpond5BSkPA1SKlwvR4YuGI1cxB3DijFXfx4lQdll+lTo7RprDynl6MY0FKTe4qJM7F+jEn4w220B/jLwDjP7My/5728KvBLgVwDfHb/+B8DfEJE/gy9uPhL4ttf6HqMPHjx3311AUnGjhNUxGuLEXEZnbS0iFZSSBr0oGfGcYJRVFCvF4x5opJzdj84E7Z5Tk2VBcsMkY+pZxlUlrKRc6pSSB1HV7ttpUSeJJ6kRct4Z6+T4Z1p9odOF1o3DcmRZAxvNhVImSqnIdMa7n72iLw+RBtaXCNT0TtRpGMSWe7yM++hb1lcDIg3JYXKRLrh4Uvldf+JL+fhf+HM8HlQGdi/zq/6rX873fu8PcvhxY6kLFPfSdM5vjPPD+XMHfUjKExe3bvPg4YoEpw4T7j94kWeee45kt3jPu5/j0flzVL0mpWvW3hgqmBXm+YxUdrQ2aMvKXGf6YdBaIydXFvW+MpZBTjPWvaNdh5FkeABZ7SzjPt2EkvZUybQikXFUmPOOYc6tTaWgZIYmZBSmqTJPe+fqTYWTjp+CSGHowWMQyORUsdaCQzegd3Q151iqclhWVDuSFno/OrYmhZwr++kuRS5cZVQS05Rpa2cdB992kz0/CFBd2E0Tu/kcsUzrcOvsgmV5RE7G2hMl75lKJZcwjdBOyjXkqYXjeuTq+sChPeTW+czFbs9lG7TjwuiXTAmObWGqmcO6kuZE6cKuFEyPtHUBU+Yk6LjihYfK2e4xuna0BQ2oD7LcdW9PM6bhSqaSKrb6hHQYjbMyM4ZhY5CnigyjNaWbYENY9BGtH9DWsGuh54SRmcUXYCBcjcHx+AKbS/qcCusqIdhYMI6sR1gXZarK8w+O1Glyff5QltZJaU9vlVL2ThvqxnXPzoEdyljguChzvaBMMPolmUzv11ypL56eaZevWp8+mE7yU4EvAr5LRN4e/+33A79ORD4On09+BPitAGb2PSLyt4DvxTfjX/Fam23wwnC8XhmsSCkUSaTu2tOhHenDSdmt+cMsHc2uGyZJLGQydfjofcywuRWvfaGIMEoQo224OwlKVmOMzCqJUicmxI0GkrEOpVlkZngtYtjMoRFcQpzmsK+0nJHd3vWgTEwGt0xRgd3dC97w2JM8fBq++/u+n/W4hsGFE4NTSoE3WpiF9lfEGbfXK/1eGZVSE+nW4It/3xfyUz734/j3yw9hQJmcDvXhP+un8ym/7FP4Z1/7je7CHIYWYk58f/lXNQ7HF7lV73Hr4gkeXb6I2yZBH0d+5N3v5J0/+APc+pk/Ca0HevYsF+84jZQT19dXjHbljjck2rrSW8KHEs/1liSUNHN1WDBdyGmQDSxNnO32pDLQ1ahSKGlHsomOqy6mUpnrjOA68JInZku0fiSJuNS0btd0Ra0xz95VJjqSG8u6oi2TZWIcG3Wq1FxZ1gNLu/Io490exoHj4QVKhvOzPUkKvbvNWM6Zi/0eKVM82Eo5v8VoE6VWRCamNN9APjlTywwKZ/M585S4PlTWxTv2/bQn5cTh6oFnz+8mrpcr1uF8T7WVeZqZ53tcnO2oeaIdHrC2xr7MmEGtE0Mzy9JJCzyyhWu94tlHL6DjmrNSOc+VLAOTmaUfIC0clgfMc2HO5yR5yHJo4d2ZmevErf0ZYnDMnevjNYln6euCrsYTdx5HV2WaLpA0kfI1h+MLtPaIs73bra2HhTpVRnKhxMX5OSklWmvUMiMUWnNK3GFplFwZavSeSUy0VdEsPHp0dJitJFe06YHeO5aO1FqRDqVnUjmj68RYr6mlcrzslDIjPEbLgzYOpHVQauH4/48zuZl9E6+8T/361/g7fwL4Ex/oa9/8BVjW5hko2n2xYtCzuCh+DHfuaWEOGnGnTh4Oc82cAx90QwozB6gHA0se7zrJxJw879kyNBF6x/GeIcg0cT4XSihSBB83ducziCEV7t46497tc6Z54uLOxL3HH2POe+6c3+HO+W3200SuGamOY17cvsec7/DV/8PfY32wYG1Bw4k71QJo4JZbobwpV6/ePfLyP5MzcjvxWV/xS/nEX/Nzef7yQYSTwQi6R007PuPXfxbf8U3fyXM/8JBmIKWgi1OB3r/0Dms8urzP+cXjTPsLlsMgiRvnXh3u847v+24+/OFj6N3GXHccFYqduVmCCIf1krZ0cnZDrD6EiUq2xNobtew4Pz/DxuDWbg90aoZKJuVKqROKuylNdWZXz0lpYm0jPvcgj6tRcyWlSmuLSwgxSi1Mu51Tfkz9UPKdjf+9dI5qojW3yqt148lClicZrbH2lVJLLNc+BL9BUxTmFcPIKfvXl0KtlbYe3WFdIeWJdQzmaeeu4NkdpHJ2tcphXZEk3LbHUDweNUX2EbyJqk5GuloWLLuLew43ege83fDk7N6T7ko0DB3dnf6xE4XLFF90Lpd0c0VZGQmxwVGPHIeTwUV+ErVUZPTgRbp70CnPCWHKhYpR086J5CSYMg+PR0aHPR1BObT7HK6fZ4xr5suCdqGqu9IfZHAYjdY76+pQx3mZOCs7x03nmWVdcLbBSiqF/f6MPIz92d676pyYc6ZgLMuBVRtTOWfq55zXu4hkjrZyGAt9PaCHg5unFFgaTiGzB9i6kmVljNe5VZoJdFFPFTS3aGoDWortdHCsNs10yoVcPAwobS6eNjgWHwnzMJbwW5SS3YOuhLVSqcgkpBl2YtQ6cX6+4/adc5547DZvfOwWu31hOt8z7yq3bl3w2N1bTHPlot7msVt3eOzWLQyY57vUeouUK51OsyOrHTla5+F6oItyXYx3/uCLfOO/+wGO6yPa+pAxjqTkphOjtRsDjfe/Li/ZYr/Wq14on/mln8en/NpP5MGjB9RyzgiFrR5c3jmxsHvjjk//kv+M//OP/B30iOf35IT2n/i9xSpqncvr5zi7eALkDsvViy6fNON97/ox6kjcPn8T57f36EWnysQ0TSjGsh7oXam5sCszc5k4KxNJxCksySk3aGOaJsfvhnK2m5DqTjUtTIyrFIpMAZ34wYW5qUPXgZAoUtGxc4s1KYSrJUtfQapf6z6wIeTwJGxdSeezcyt1UHJy7NQEnSfMdq6/1w4ykZgZLRZZcubE5RxWCur+irX4oqbKhJGofUW7QympZMbo5Op+n5azx1tIQXunRF5OTsW5hQitN26dOSnbvZCL043ELb96gp36WFwopxwZHe6Iv47VvUx7ZypPAkJfnCOpabDa6kbGacK6TxYlJefoWnOvAdXIqHH/+Zp36FBnRySX7jIg1YIuhpSJK72mL1duyKvmm+3i2OfhcCSVwdIW2rrGVAZtGa7bLsayXvuCzYxu/l5MV47Xjd3FjuNoHEfHjo3D9ZGWYHd+5PrFhwx9H6W6a1BfB8dlcH24ZL+bON8XLq+vkTzT+iW1rGi7pKT9qz5fr4siKZjHbLbOOrrnZqgxcqKUiTVcuDMVbJAK5OJhU4aFj6JHLcw5MSVPyNud7ZjmiVozt+/NnF1MXNy+4M6dcx6/c8bZvOfevTfxxL173L11xp2LPef7eGgSpFSxUrjUxe2utHItmYMmeu+sy/NcX78HlQY416uNAeWMpspuXyjs+Iav/36efddz6OULWO8kimceD8cj4ScWqZddH5Gw15dTl1AtQ85cvHHHL/+Kz+HTfsWnc50fcTHvHExP7lSk2ty8uCvT2cRnfd6n8UPf/J186z/5PlJ37p0VDUL0S75pbNfNBsfLFzi/uIfpBWO9RkR5/pn3cP/HXuSzP/8zoF5j6An6UIvsZfUsnV2pblulI7p7ZzEMa6g174CKE++TFHeLpjHVRF8bx/VAkeyFpblVl8pw7IuKG5gMNDl2msYgF0Bdr5uz23KZFDQDhEAgrSS8CKu43FTD9xDwg2t4h5tFUOtIcSMTzwz3ZYSInaIJcqnuXdqNPppnBtUSRruFkhLaYWVQ1F3lS8qsEtdPFZHB2l1FItm7t9QMtUQulRwha4Z6xpIatfhCsqt/jmYJM2M3JpDEKuGOPzoXt2YYHS0FlT2ihakkNKSu7vCeaepodMFx32FCkkwxz4AP/zHaGNEhJzR7NnzRRJ3OSQo5JW7dFoZ5oNtju9tI8QZghNRxHYMxEiXPqBja1vAfTX5I6WCXC0tbXWJbMnJ0We2IpezgyPVx4dhWhq0onkV+XDvXh4PTCW1hFseh2zIjkujieU6v9np9FEmDeYBY2Meb2xzlnCgp7PlTwoqgcybvM/PZzDxVznYz52c7zi8mLs73vOGpezzx5D3uzDO3bl1w++4d9udn3Lq4oO4m0lxBDCuJowltuJPI2o+8q13THz2i22BtjWqJnjKPliPXyyWZxFQmV+iYcFwO7sKcFdHmmKlUJMFyPAAry9XEt/7Lt9Ovr1iWI6bJib6j08fq85+8+mi9dZOn3xeQnCn1nA/96Lfwpb//s/kpP+8jyfsJyW8hM/ufD/G/IBQTplRYh3GWZ37Hf/1beefb/xDPvdv5fZury8vX6FE0FUZbuHz4PHfvPcn1ZeFweEjOjX/yt/4pn/9rPovHf/LOc24EmvXwSXQaVkLoffgCKQ0ouNuLKE2VXCoqbncnuTD68LgHUbKpe3WWRNPuCzgr9NYipTEz1BVFOSWaOmadh5IjNdID4PBNp3k2uWWPJE4p8k5Gx9R/BsNQ27a7K2NdSKLUlCF7MNsWduUjkAexlSwhinBMeYQVWs7+2al1Eu7D6CY0xSN/EYYqtbi5SJ4m54T27oFbrTO6xyMkyZ4XrZ2GkbIEl1hZVn8PZN/QYz4mS4OcfWLR7rGv1gdJlfXYqfNETUpb3X9RTIFOxpjmiokzALrrLykpo2NQ5xTBccZuN7GsCzo6KXckJ3fhVz90JDr0osp5LdQ0MZLDB2O4Q6pidJdMsbQjUqsndk7hNaTKMjrzriDZ1WQ1733CxL1idS3cufsEt8XVU/vZMWs1pY8eDkeD6+MVuVbW1XOikvhe4Ov5p6/4DL4uiqSJMKqn3ZVdoTCYdoVchNsXe3a3ChePXTDtd5zd3vPEE3d40xNP8NTjj/HkY49x5+KCadox7Srz2ez2Y+EzeVhWhinPqXF1vGKsiWP3zmlOFbeiaxwOj1BrrGPl2BaPlCDRLdN6Q9u1bzg3U17Flzy1euxDLH2m/c4t72VFeuUH3/4MT7/zWdbLF7Ch1DqTkgbm4qeo8Ooj9fsXzySJO/du8flf9On8xt/+67n7tjsMVs9vMaHKhA5XWrixQKbgLkWavPb9rI/5OL7ky34d/9Mf+/NoLNL954jP4xWWQ0NXHj58yJ1bb8QscTy+wDu/74f4K1/zV/lt/92v51AXVlX6lprYld59OdQ13q8MpqnSm5flq+ujF60BS29M+x2ig/V4zZSS53eXjPZGaytTrSfFSOu+7fVoAUX6YO1rhKV5kBhjIKK0oazHxQ84GawCzUKyqeY4XHhxum2dH8rruoAOP6xz8SUboedWAZoXJDzMDDZDDe/Qh+HGEZKABovLYEUqmpz0DbG8syheyf1PdQyGeZdaszDG4kIHSwzcoCUXP6zFLGS5hnUPuZpnJ+Rb+A/k4nGzkoRmypQDghiK9iU8TMWvRcQm5+b8R1J2O7WUOC7OWRURRlu9kz0WQCInyknvjgtn93rtq9OKcJpXSqFhV7dA0zF8pyCGJKPkBMMYvYd5hvM2d7N/BmtvzLXC7BEOWd0keTfvcEu87sbao4eJiXg0c/II3U6nlDO/NgwEXOL5Kq/XRZGsu8yHfOxTlFqYb+157O5tnnrDE1xcnPHkvbtcXBTuPXGXJDPZhN1+QnfZOZNlQkrl2T5Y1iPLC4+cgH70lL7RW4w0xVUu2bWsx7EyF09eOy4Lx+VIUl/ybBI291aesdGZ8kKqHgMx18LdO7fZ728zLDNNE1MuTClT9zvm3cQkT7Fcrvzdb/921hcP0B8hKDkry3rgA43Y22srWNsWfHc285t/56/kN33Fr4T9HrMzqpzRzEmywxqSE43Bln0ztrHQLLyljM/8Db+Mr//H/4Lv/KbvjiL9GuJVvID29YqHj57l7t3HkSQcj5f83f/zn/AJn/czuffR9xjDbQKE5Hrg5gFsmrwzkjZIsoTLi7uJq3pBs24s2qgCs81kFda1MSLYTLVyNTx+QiyduISpeLCZ58p4NyZDyTW6cDLQkDlRSnGzFAWRhIqbfuTk3cSUC2n2wqDm+uOci/89fBHYe6dk97t3XquQ00RXx1lFHDcHGJGoaaakkv0fPAfc1IPdxvAOkSRY2paOhmQfnYe6rVgurhVX9WXVGG52lsX9T02dxdBHaL3jMOlxApr4pGAeJM/RGmlLAE2GDccERTqWnEwv4aAkufhuILsSLJHwzPnwbcSt5bYDXcJUI5dCEqEPfw+dDGoc08J+v3dcM9Rkqkpv7uJTJte4pwStu+rG1D/JLAKLN1FDGmP4kmwWJ8hjRpkdyhpArOd94SeZmjJn84zUwlE9FtqGcr57nWOS9x6/zRd96edQA7SX6oYGOvxNqDaupomr1QnAU2/Yg4VlGfRx4Lg2sJVkRo7N5+V64Hg4YL3TlhWVhFqjJLfkNCDXytnZGW3t9N64desW09nMg8tLduWMs7kyzTvmaeL2ReV89gu5P5s5282UMiHiD2gxZ9WAY1I2jO/83h/iR/7Dj6HHB64FrhN9HBljdWzxNevSZnoh4bacKHXm0z77E/ic3/pZPNhXHN17GrOMpOp4oDq43sMXMZGjaAFmiLm5qp13fvWXfz7f910/QHv+4MuUaAJF0ssWSZtmPGWjtQdcPsrcu/dGLh/c4fDMI97+zf+R//xjPpM+Bp5QkZ2SVT3wfhkr+7yjTkFaJpFTUGQSMOzUTWcxzxYyo6nn/Fg3JDv25COhF1fMR7mUxKMzzGOEi2Qf4MwLUNPOUFdcqSmiEtdpCsstd/oRLAwxOBn4SiRZKSDDu7ycPGTN6VOKDc88TyltFzC+VqWW5GR2AVKiBARh2pmSF2GPHjF6GEqszd3Qixg6sndott0T/vU3IcEwcwK8+URi4CO9egGyzTs0YJt0Cu+KsDgdrGOFKux32ZVQNTG6SzVRsOEOPohgRZGukBLqnwg5Ol+M4CYO1j6wa7/PvVODMcQnHjqXl9du/VacFTCGCzGMBNfhrxrP6miDpbYT5W/0waPjgumRWiolu858FehrD99Zf2+qRirVp5/1mpyEmqAdupveZP/sR3v1J/F1USRTKehu5pgSowOjoZcPWVcH+oWIRlBjtdW5YikjljgsnWPv5CKc14nJZTJMySi7GZEz5sd3TFMh5cFUzJ1OTBgJprMd+1Q5r5Nn/1anPuymHbNU5+6JuX8f0MOyrGY3AlbxB3snCQ0HZIDVEv/mX3wPV08/oK3PI5JRlNZW4KZDfP9xerNms7jhvGMxSq285WPeym/6Q1/EYS+Mdu0YVBqQKpXtwTeqZDJh68Zg9IW1tVATuTlqkc7H/YKfxi/6vF/EN/zv/zj4jH5zvvLPFqNShsPVfRDhySc/EvQeUzrj3sUTmC0e4mQJCctej/4cSESBthbZOmR6uL87bScetLE41mdKwpgSaHWBgA4fo156/WwFRIMQ70UyCTDcFabkQg/2wBZN60U/lmbJwdQ+XLOt2SllxrYw6z6SxueRk5MlEkJO/viIOv6n1pHkB4oG/OI/q+er+HsYbntm4QDVFh+1zbtDETdzTilhPZYwYzNJVky9KKaUSFlQWx3WTtkPAfWFm4iQSqKbkiQzpRRCDPxrJHEoIUE2/76ScIx1rORs4ZOaGM2oUyw5emfkRs6JWjOqw3n6cabmnP3PqDsvbT7hBT8IzKdx7/ZTvrnXIjBNYupRFdY2fNyeIWfnRJK82XFoZPI8o36gZO8812XxBii+Xs6JMRYUOB6P9N58qtqC5sz9LOfyOh+317bwo+/54fCrE+bJFQnZErtaHE+xlf2ucnue0OEb7LlOIE4TOt/dYjdNqHY6g7nMnmOR8FjXoAQE+8O10zkxzzPVYJcmhOTb6ZQZQAs6xCTmp6VFVjPJyerm21yjsyRPYcQKmhsvPN/4//6L/8Dx8j7WV5J4/OrnmPHpwD8Dvu4Vr4YXyFp2PqIKCJW7b73Fl//hL+SJn/whTOHy54B296iKyOQ23ErNRY0OrFuuMJ+F/tw7sQnoHPiS3/5r+c5vfDvP/NCz4bnnbio/AQ0wFz0CIJ3j9Ys8+8w7efJD3sjbPuxNXIjSPDXef2YZJKu+CPESGMXCFTVmC1WNSRK9wlE7QwazeHysWYolikfVjuYmrjr01FR50dCAMdzkguwjl4kn+hnhUD2Gd4hRaLZ349eNyGpxNyeQWH6lUxBcTkLKQPax0hVcEgU1Ok6JGI2Nv+sXIuIyhCq+lNRYxGjyn2eMQSmFqbrXJebGwYiHEXhBNx/zA2Pc0hc9aNC7b0mJUutp6z7EKXKlVJcXjoE2x+BQ/zl6wEoJkDFI3WWy6MBEqGWmFO/c1/WA2baMcoPelBy2QNx8I51y4MXxUdwV/6ix6ZeCDad7gePAG3Mjp0KtOWwHhZonRIVSfTorO6HkxFRbFNdCbwu9HanVI6Trfn+CO4r4Z3RcVhTh7OyWwyW1huQ3IVpo6xo/zyu/XhdF0l1LLpnmmf1u4tbtyn4+Z19ndlOmFnF34RzJGuIUmoyL/i05Vaim4rhR3KS1+kUf2phqQXAck0hPG72zGjSDNXWXQ+I83YZytIWUxV2Shz80w5y+wvDljRq0vsZQm8h5jxblHd/9w/zwO34U69ekBGN0PofO3wDOcSeQX8eN4H17eQeZGWrkIljq/LSf+ZP5PX/8K/joT/1omg5qdgpDCrcdP8W3KPeMpsHipByPbcBTELd8kRAZgiY+/Ke9kV/6q38h/8dX/S0f2eymK3jZz4UbzOrYuszBsjzH/WcXzvMZE56Y6JOch1IZrpkewXMdGg+HjpCIenXKCXZSGcld3rV1x/Fw/byORJ136Mj0kdj8NM0izycNkrmbDvH5SOUm3kLNM9Gjg3CFTkGCb7kVuVzzzcjqlRrFu0BfRkQ3mbNvic1IEVFqwSgIR1JM3dWplOJu2bEQsiSknEjDi5Alj3HI2WEbCFf0wEzH0BiTvTuS7AeiLyc0DqSABFShm6uWtmAvw/Pow81Kuxc2PxC9UNU6kZN/to6aemiXqkcbW3Ciat4077H9jxHZD18v5j6ueKDs5sFq2bFxv6wBM2WPhti4z4qH+o1lCcJ8QsIVfl1WWu+oRBcfxtA6VkbvAWE4RKSWsOGH4EgeRTzv3TfUFHJWFKVmkCT0dWV/XmNqe+XX66JInp1d8HN+9qdSC16UTB1XCpnXsETONWgZvsWV4nQEG+HaLEbT5q0/0MfqhrLsUZMbh5fgpo2IcFhb5+pw7Y5BCLVMkBKrDqR1JGdynqnljN2cycmXL1KNIsWT9PqgJhf0w4Tlyn947gdY79/HtDHEtdWfgRdI4n8/A/i691fYCJg54fn8zsTnf/Fn8qu//HN54s2Pk/IZc37kWBU93veWeChBF0pkOtValM0cX9d5c96JeNEueYdwzS//ws/gX/69b+LHvv/dpJ5PONb7b7m3f/1c4DPU+Bdk/vGLR/7sH/1f2d97jI//OT8FLQ+xkA6+/C9GCFRO0VEKOc8uHGndfSsVNAkpVYo4LSSZQi4eB2qFtTvDYFOUbPEMgHc/ZqDmuS5pC4LzDJOc/VpYkKPlVJTcPCKnKBFxP22/P4YyWsSrBod3io6S5PxKI8Xvx1icfCzeiNKIBNd2kEXic/JOcTdPJ2mqqufuCEFDDDVZ7+5dsN0nrvF3QnfOfh9uC6eML3BInnmephnwQ76Id3t+wIQ1GUazFsF4s0ckBAVsDDwHXpXePbNb4++Tsjt6Rzrh5qSfkws4EtFJJ3GqDeHXmhKjQY/NMiKMDW5N2XF0s7BJ9NduP7Hq8KwdzOlMcc3G8M/UckRM9+HTYja0QE1Osgd3IMJ8WaU2mOou2BKv806y1sq9W3e9kyk5bK3c2XGYYcNPwmUYpwCj4V1daz7eFUs0M5bWydm5WK0P2mEAPooKbrbq45BQJSMM7lycx3jloai+0UygiTJVbCiTZOep0VHXnJEooIOz6m4/SQDpCJWrp1/E9Co6FQDjnwFfghfIK3zk/gkvg5SMPMGv/91fwBf8js+l1syKkNKlQ2jqppjesbgMz/Exp78I1V1ZcH/t6IECRHdT45oUrEGaedOH/yQ+9zd9Ll/zR/4yNmCVw+lnef/X58CpG/4S7fx6U/7h97yT//a3/DF+y+/8rXzOb/g57M6L44SpIElRFh97ty16DgwuxttS3OQkjc1g1p/OpAol0XTlsHrekXfrKZQ4QmONDJMEKYxiJ+fgiQb5Xsw1vSKM0elJ0eQac0YUrRwcyqHufN9H5FPjI2/8WvDR3AqBx/WAJxxT1BEQQCjBnGKmp+1ytRT3tkSz6oooVx+pFwwxREZs53209uWHd1dm5hZjw4fGfMLYkoswVCkWoz/4MgRAYxrqHY/O9eWUBSVK1JBIsHFowzmMSeppCcQJM1e/35PDToh3as4TBcTvPVV1DDWuZCqRubS5MFngyzlRUvJERJzb6PilOKtE5CYUUIWeM7ne+KyKCKKQanW3dFU06GcZ56JKUkiQzIsqahEXoa9Ie9ter4siqaqs6m4/7dBo4m25A+CeWFfMOX+ighpIFqj+8U+TJ8IVfENXyuQ3qTmW6Dm9ThtJSRAGKt2t1tRJxVkyWTNC9TFMlI7beA01eqpgzR8QE5JlcoruhHSKj1BTCsOJ4z5cxGkJX4fwhWZ8Bl4g/6HITXsWry1+82M+9afyub/xl9FnxUZ2uR0pcDCnrGyb61kCepDBFk7gnudAHCvuNeKLi0TcVGTMClNOfN6v/Sz+1df9K/7jt/4AtBzdwfv9bCL8ErOXdcOfbsrXDXj6h3+U/+VP/DmeevIt/LzP+3B6cQ3zthDQIbE8iUdXvBDaGAxrJ7VLjs5Gh3P1zII4nHyL6oU1ezwq+JgvHoy2dUeODfp9YuIUpG1hs+GYJsa6evQHopg2aIs/PN2jQXJkegtQSjkttbZObJhGN+OFM4tbk6Xo3jW27Wb59OclulqzyBmSre6Et2lgjBoH4IZ9iATunLyTzeLfLMWEYCNCzJJ3tym2810HkuIez+5mZb17iJgRZHTHbc2Mtbt7Vgoqk4D/nbhPe+9ePCNgjhRsCXU5ccrFO042GEBPHbmZejBYTBQnXFh8rPbl1vDERvNuP2WXQzps0oMb63yDlCKS47T8MbQ3x2kFJBX/8NVIRE64xn0Y93eykKK+fH/6stfrokiaKuuxxemamYaS1M0Dyjz5zZAz87SjpnIzOoI/EKZ00xgfBDM3w6jhKKTDTSTGWPxkyX5KdVKksuHcMNyxOlsmZaHZ5OTfkkilkrRh6s4vSUKCh5/2kopDBKZUE7JtfcfLC83XibzKwsZfioPuv/xXfA4f9eSH0nPz0DPZOjHhKAdHQK341jwwMbPtVPeNvO+Q/QT3YHjvKB2bdPwSnBLypjfd47f/7v+C3/Nb/iDrc6/Oh3j/bvgbsncvJsqD557mL/zp/5kP/ajfy4d/7BtOtCsJbE9f+rkRS4ccmegpYbato5SeBiOWD1mEuVSkK4YbZmDK6G6d50XHC9agU5K5/G8oa1e2lL+cc0wZiW4eV+x8vxG8Q6GkkIzKcAZDdDpb0bAYDSoZCWrOUDeD1u0AwnFFiYK64XUaVnRs9K9o8Iyb4ivEmG9KqvV0iEqM6Bt1zHOdPJJkU2X11hk0hjkemXPxA9xwy7o1Crz6XZuzK1o2LNEM+qYEUvNOOWaPUh2PrDn74Zc8YG2LnyV7O2CKB+XlDd6Rm42/2ssEGdujYTaCVO6bd+9E1JdT2qIjDuK3OCSXxGOiU/LDaIuZnXfT6dDwETOREi55BUoqN6Yn6v6y6SXQyyu9XhdFsuTC3f25E5/NO4+kATRjToYWd8w5jgMjOFkiKW48SMU13CJ+7xVV0thuRvcQzFsnIAMYbrZbd4iJj3C4vta7OWXKrhPOYkwommYoscGNTkBwSo1IdKrxUO9KPXUK/ynOPgnl4vaeT/rkn0WSmZmZJFcMWvSkhZkaXE//WXyLl0E8zQde8nAROJkM3E9RTg/OzQ/lXdAv+oWfymd81i/k7/61f+w3+4mPt30lL/JbN/wNwN/X7aF27t8PfM938FV/4C/wJ7769/GmD32MXAsiDU3qHp4Wo1Tgo8OGR/riCxkZPphl/GGopvScY5EB0hyT8hExQuDMYZmtU9MBXf37palQRsTCbp1JX0nmYWcoiMxImdyH1Lyjsay+sNENm5W4fo5RtuBlWhChLR78HvxNC3/QFEuI09CbQimT0ukzGjpcC67ObBh4o+0TTTrdQxYeo2Ze5CwK4WljnxLZdt4VbgsddYs7US/WSRK5EFt0d9vP4t3eZiQhCiZGLskdi4Z//nmaKDnT8S5f1MfnIZ4dpIErG4ll+Na6RJEyDXOaVL2oB+9S45ARIzbt7oRk2mOLueGyXoRF/T5OiaBSGb27NHXO1d+7GmTxhV4Sevemq+aJFHJRiWnVRg/T49d5kRTc3aTFplqGsfbVZXR5xkho61SNsVPxPGTJaJiwZkmhhPCHuyTvElOoJiwlTGKENHPuY3q51mR4H+cfXgDrUAPLTBTbdp8KOG66bRZDyoJFAS0lnzCs/9SL8dN/xkfyIR/61nhIOhMzQj1dK7Ps2z0xd2S3HAav3r0MOdJoIaFzPMjQiNrNZAOVMNYIPAmE3Vnmy3/nb+bb/vXbec+P/ThCBvNsaOyGF/R1IrFwiutnYSxhoK3wf/+Lb+L3/o4r/sj/+N/wU37GW9G0uGNM9s/AcGOLmLVISaiaTxtQM1fuaMAaNbqFnhJSY6mEsU8FxBMwS1eaeRhFZvLeI8FUC6giNhAdiHWsFKaU/IGWmWwVUcetDL82/p7G6ZpH5ThdLxEnNZtFfpH5WCilOPE8CO6bJl7Ip9HajTNi3Es5CNsgMsX7CZMk9FQASim+tcc/a6/ugWXikSYp5cAZHa/OKTOYosg2RNSNqFXpG26qTv53qo53vBsB3ztiNxMxdV/PpvjvqbpPa3RlmNGGUursWm0DxJ9B2Lo9F8BYdLoiyd358ee5pejAMep+iqVYcygsZywVRnOIRhCm7GP+VHPwVn3ZKyWfuliXPAY7RiDZYGjzeBUBCVf413pSXxdFMqXEbrejmDr+kRrT7NkiOU3k7PGkWVIQUPUlCxGAjdcmJw0rKNM0uZ+hQXf1vLf+MUI4drFthaOAxJ1spwuXohSk7Q5HGL60IeP+gobIS4qGGr3Z/4MK6Q/Xz/v5n8jZhTBkxCMwYozbwPiDsyDNsZsh2fWpsen1VIv4/pbi5gdOwyB4IEWPX3u3oww+6qe/jS/57b+aP/WH/zz9uHU7EXv7su7z5s1FaWPb5CZNfMs3fgf/9X/5h/jv/8zv46M+5g1QNEb+hKgLAUy90INSJFGqH0tqTiTWNpBNbILijCGNEdunBg33HYucbGyg/cCWX66s0elG95BKXJscP7OQ9Gb8SyIQkr4b1yVBhx+nEj+vDgmdeCKJd4AbLctvnXQqiGM4Jy+X6cQsMBtujBLj45QrpJfDEWY5sNAgWieNcdn/3eM1QrNtQmuDufoyzHcn4aVaCilNnts+tq02SCzAxHyRsplJa1xfjHieclDhckQg+89S8O4TM++Oxdm5SV2BttnqCfgCFAsCvFPMN+jCnYB88TqG0pYw6zD3Cs1B/MbEO1ndzAZ84aYjoBN178xhG1TgyimnYQXMkGFkKApzM3ry7/lKVoXb63VRJL0oeQEsZWL02ek126JiA6djHNrGFztdrBgrg1+GOAYxhmeCbGPSyRlH5ASgO8AeRGf8wYLNeiJvjWd8Y4lvt/2fj7gJ9ZshCutYhX/3bW/3v/ifWCjnufBzfu7HkXNHVeIkHi//rjIzUJDVO5yglpv4FjV7733SUUscCEaO/ysMZi+SEpgqxf89GZ//BZ/NN/zjf8W/+dffQ5JMktfe/t18CL5y0AGiwnd+2zv4yi//Q/z3//Mf4OM/+SN8tAZSmsi4AiUx0ORBb62tsQjxjq1mYdh6AvLB8UvTOJAEsEanM0ajmdNeREOIlPyAGMO7wC2qQUJJotHNId7ROe3IP2YJV59NnYP59ztpq+0lcETcH2LRbeILvGHuASnRFQ5ruMGyBVfRC+8Jj4z89W0rLRaKlYA+vLPrscsxtxDDqJMbXdSaKekl9ytgXUnJKNkloaQRogMgBBBidsIch3oEivik61xT8eK0bdqrKdLdpqzaiHtRkOQUp5ILKdcws26xUMve2amG6ipjquTiMMuwRo+LtbFLUhhqb/eEE47GidC/tBY/182Z7bhtYNLxHksp0fk3pPozn3tnJ4lmTmjfuKCv9PpgMm52wP8NzPHn/y8z+0Mi8mHA3wQeB74d+CIzW0VkBv4q8LOB54Ffa2Y/8oG/jzuhZBH35FPB7b2Dn6aOTyohL5Mg5bKNJhKpeN0XNWKIplhmRIlJ3kluYvySJVxctkZrexi8e0hxw2+Fd9DjpPRtocXaZus2k/lp+973Psd//K4fiC3oayZX/ITXh3z4G/joj/kIKjNVnAg8cPebFJ2gmNClI/iSShAmKQjQA60csSyJvxH0cTekzRR/q8TYBmQb7vNogzc89Rhf8qW/hu/6jj/O4WFHTD5gkTztqcRweonAML7/u5/jT/53X8tXf83v4y0fdhs7NeTKBsTLdgACkuPQcWsM0CM5Cxtd5dgLFO8enP4SOT3Esi9PMBeKyg31o24Hoz94FDAtvvWWQZYaP28odNJNp6YxqudY6ql6V5lD1WEaCxVTWu9+QCdAXFZ44j3GcskjLvx+9SgLv/skOtiXjkiybfTHiG7UP6sxXG5XShD4uZGktsGNgiVnavX3nMUJ0wkPwBsnOaGFzNY1zNq96JWU3WwiO00ui9JbD5w9JpdhcaiEeUxKdPPwtGS4L+vw6YjuBbD1DVbwyWC7Z5JVqgiSPEWyKLGBjs9QEsbAkpuFaMACak7lmkpFm8UI78+cj9rpVPwRGM0biFIrLSmsQsoV8qvf3x9MJ7kAv9jMLiM18ZtE5B8DXwl8tZn9TRH5fwG/GfiL8b8vmtlHiMgXAH8K+LWv9Q0EYRanP2wns7sgH2NjuOksN8B/O1UN1a2NjFPWXI6XMbDk9IKUSDUySE4dCIjY6QTyE72haetEXazlnVv8b6grwLW5jgIqw5z2kSmICu/4zu/l6R9/7oPovm662oBw+Pmf9rO4e+eWj+wsIfvaZHTxw0ZudDHfCLvLdHMXbQOoQImO8ub9xfyNMagvWQYYofUVH1tNBp/+Sz6JT/vPfjZf//e+xXEw/Ykb75eB3QGYi2y/Dm7guObt3/qd/B9f+3f4L3//r2cujhebuXRwk/Z5NXe1SJaMWHYcbHPcwcckTa739kWbvyPtxiQTeZpxxVF1iosaJdzrt65QzSBUOz5s5xOfb9hNGNtJWWWO+0qqYarhzuEpy4kALjmhJjdTg5r7jCJkvMsjCSl7xs4ICs0I0+VavFgO2yAL/z5NPTPa29bNWs0ZHBI0p2Gx2MEhBYuOXmRb0bnnZO+HMNNwKtg6XHnjHUSiDb+jS3F/1SSO7WHGPjpI8BF9NWVk77a1O83O4meY8+SbcfAYizI7WTuu/0k3raG1Nz9EhoKquHFxqiRGaNbF+a+n6c8vcUXYV2hxz2+STQ3YAMxHa3BoIwX1St0lSXtIXtWXrq8+bH9wGTcGbFFiNf4x4BcDXxj//WuBP4wXyc+LXwP8X8CfExGx16gYjqNFt2BspScciTVWEPGYZ4+vRPGuMuRaGafqmIqPc+I4iymndL5YgzqWVJSR+unUcb2nj9cJl1PVrTs0l0INJLz00qloxXPkTkDiXoHf+e+/m7ZsZOD3KyY/8fqSs8spa6l87Mf+dPZ7X2K5Y5FbRL10bDfcoCNJCvoLEBxJEbxY2w0O99Jvb4FZGDl+T4AcyxnvkBXjzu0Lvvy3fQn/5hu/i+efewSnT+DVP8NTDTf/F5OE2Upr1/yDv/XP+SWf+Wl8wif8VJKtbpwaPy/ASG66OpJfZ8Gdf/ogHpaEilOvavKH1sfuThKNokRAL14ge++UUmht6+jyqTO0GNt9URPFEw0erb/VlCWMWjeCeMISNB2kHuM2bvU2JOCcgG9aTDVZ/LMYw0CbwyJBb2ljMOXCGG56MsRedkElOII6Olm4oRPF52hhl/YyFVF6CfSU3GV9mMsjTbwQ5lpiuRHRFBBwV3Lja7zz6sOJ1rlUhkDH6N3HdRDvnIeR6glhDAlnLGmSv9dhAfzIxi4Rcq0eprZNMsmfIQtmwKYDHxCKJr+xJPnPWUju8h8+lNa8Rmzds2v13SRF8L+TMFD3IF1783usCLWUwKpf+fXB5m5nfKT+CODPAz8I3DezDfl/N/CW+PVbgHfFB9lF5AE+kj/36t/BSKKBPziJxQ0kMk5TcOzML67z2GwQ9vjuTGPxoYwe/DCFmoEcNlw5hWQxxp/kpORaOHWm2M3wnCy9rMAIQmXrXnwDJyYozrvKCKIZRuHt3/HdMeI73vVawOQG5G8cuO97xw9Cq8ylonQHzQMrs6hTFguKDSZNcb00+lsTCbNVv7abUQSnnySWURYcUeQUQ+BMTX/APvGTP4Yv+uJfwZ/7s3+VMC966T3x6h8nUSiD82baePZdD/gr/8vf5mP//B9kvpXipyjRySjZHPPK5qO4L2IgW1CWNLF2SJN7RibzhUTJOTCuIC2LH56qxjSFConiBSMKpagF7BkRAtjJRXv74U3dtUbMD3DEFwwd9aJj/uBp/Fkp5SSJ9YPRnfW3hZBEMSvJM867OT3mRLaOr4Nwkk+Cj8BJXG1jxst19eI6eXCxjm+3/WuRwgJOfRKz5ostM2i9YeJdG9HRqymth7otOu5UvGlYu9GH0jYuqfifm3JxOCw05ZskdBOCbLBKSr59dnNh182bDeY6kXKirY0eU6BfQfPlv3hzU4vfL71F1Ic63j5GFMCcPX8cocX/luL3UxJfHlkYo2j3/82p+C42RBr5Ne7nD6pIRiTsx4nIXeDvAj/1g/l7r/USkS8DvgzgTW97CnezcZ5hiRCk0LG7T+TW0hueD22xYTYvTpotZGhOg6l5Cv2wj5QD31hbSgGbOQHcMS0JDbA5bSi+r0Yh3KjXnEwi/PskyaGp2R6uzIPnH/HuH30fIZ04PTSvem0JvJTENM1cPjpg3TuCnHrww7woj6DaqAUEIa411q0QxlcbQXOXeE+yTYIxEvmCoZ/Gso0k7f347F2UCWl35Mu+/NfwLd/87/i2b/num0UGnBgBr/75OvJvOpFTQ/sj/vU//Rb++T/9N/yyX/NJvlYSd/WO3iiWamCnxEAgrZTkWSc5KaMk1lVQshuSaKLmQppSmNQ2Ct1lbCIsyxo8Wf+SpeRQwdjW8r7E2ENjGShoHEwnp58UclDzA5RkdIvi7IAEZuFf6ECrLwyMwM787RRxt3DMZY86Rqhm1DtXufnntDMyC3yU+NljcxzYhndfIMXxtW07nSJWOYk7APnFVQ8wE4/1EIeq/XtuB6ZsAIffM2M4Z7FkxwzrNNEPi1O+ZNtOx7UMQ+VhN5JBl2H6tlxMPNhPtvs+nzruYYPNIMPEWK25UmZb6m40wTjktBRf/gwYuqVeTnFNfKT2rbi6ostvXIb5k2ICkkuQ6199d/CftN02s/si8i+BnwvcFZES3eRbgffEH3sP8Dbg3eIM8Dv4Auf9v9bXAF8D8DN+1keZmMBwu6Sk6TQmZPUbQvGFhcvNBjmVwBiDs6YOmkt0CBvQ5xdUYsyx002lOPufGGizKFA8ksGBKXKMpJvDz7CbEzIGwng3KToF45lnnufFFx7i9JoPApOEeEodoH7bT3oLqXYkDUp2YNoNJzqEIcBW/NUs8lYERGOf3f39sPWL282/8QvDAUgUcD+9bTu+0VdENq0rvPVtb+IP/qGv5Ld+6e/l6fe+QG/e4X0weKuP+04wTqVz//6L/G9/6W/wyb/op3HrCc839w/JTl2EhezMG0wDsnMf8w3WlnI+fYYW/EsVV/2IeDcjhHwv47y4zSqtJ1LguCPswEoq7qKdExraZ0NPmmiLf5eUPTlQcAmcDjbvR18+hSwS7xhzmE5sRcwCTvAFkAeXCRLcwEwKFc0wX2jllE9FcVO+SHHBhQU3WDY2R5DJVZ3sbhIFDNtaW0brjH6T995jY8+AjTe0TR8JeYkTuneBhtHbyhKb7WBuRyOzLbtG0LrwQ9LcSAIj4AA53cMb5FFz9eudSyyphsMG0clvi9ckeNbQxg4wzyMyI/KM/Ncai0DJcU9b9p8FI4WlnjMezA/D/Nr3c3rV39keYpEno4NERPa4ec07gH8JfH78sS8G/n78+h/EvxO//y9eC4+M70IplZoqmUJVJ4w7R8v7pNE9e3vrurS7dGq0leV4oLeBDqM1pTfjuKz0pqBuAIvIaYwgBV6GjzijKdrc3VyIf5KbxfoiyAtQEnVYQGLcxi902sjKdJ5/8DyHwyGKEB+wTm5XxgK0/9APextTDY4dE8qEUjHc6i3FOsAxP3eb6eIBTyrGEPeLHjhOpgJDnPTbUZoMusS4gjLEycJDlMZgZWVhoafGMlYanZ/9qR/L7/zdv5ndRQXxnyNvCPprfKYOmbisrHUDabz9334X//wffXOMekdEu6uUUpjYbgeeeUphSVGQxB8cYyDZkEL84zSfDRseqqx9+Ii4UVrMyf0lZ1Jkwjgrwh+8ro1mnSG4k0zJJ2NbOb2brd/1DmSogBWwjHbHTbNUklSEglhhNKE36M1OFnNr746HyVaUgjsqiZpmshSS+RowxcHl+Tr51FGSvGsSjJuQIsWsIzJI2TDpQTlqYA10JdGZpupyy+jGJLnCabRBX5tn20cBQTWgnO1+jzjewDLbGC7kSE5mPXlEjuDvxowjactQ0+3ODJPiwdqOdF0xXtJ5Bi+ylEqZpgg2s9PnVpLDABVhXyZ2pTLlTC3Fo1SmiXk3U+aEpY7JcOpP9Z2CxsGRcsgf20o7Hl71Tv5gOsk3AV8buGQC/paZ/UMR+V7gb4rIHwf+PfCX48//ZeCvicg7gReAL/hA38BUacvq4Km5V98Qo+Gs/zKgmFMSRu+00RBxE9KNRwkF09jCiXibnbwzMBuo6M2SJjmcG9ovl6/lDWh2S3x3/W7++4ZjpZJjwXMaRnDH7c1oNHH/hfusa9uglQ/q5Qt7YZoKb33rm2ltQEms1uKS9zCviMLNZmYbh4icFNrxv/oSiMD/n4bri5N7/aA5kcSxKL6JzVxXzdzBR5RRlM//ol/GD7zzB/nar/mHfiDpq48nr/FBc7w+8Nf+0t/n03/pJ/PEWyYPt4fTz20kyJ77fFS/jilnX4S0jhRXVWSRE0WHoCiN4Q9fjw42yY3hrQ51Sk44SUnK26XxOFqL4VrEGRGBjW73p3ffG+4HG/c2h1GDyxAl7odY6Jh7LXrB9e6/d8cQZXNhGPYyn1SR7DQmXFPuxhZy02lHVyUx7RBO5l5zDQ1z2xt7RC/urYcGWrdFRujd+7Y4SSGhdPZDd56R3xvZpzffRhtDGykVaqn0boze2OhJ/lI2CawvYDIZY9URqjf//mouOdwO/d5vjDS2A2Hjh/r1j/cS14VgUGxa9401QPLvbWGfKOpSUI2O3pdY3iB51EONELNXfn0w2+3vBD7+Ff77DwGf9Ar//Qj86g/0dX/C31PHSgTnOKbsQrwk3jGNERSIXHyssOGGnxLuKKYnIjAE+G0xppoDL9voM1QRE1KJzqvfmNCa3PQMYzS3t0+b67fEreDZ0r6dzUF8915vOS6nDOsTZecDvCTYjGWCu4/fcU9F2VBR/1n8DB5064E6emkzE4a2eIA3QwM9LWpO3yPutJOU0hxH2oqkYzKJ7Sw063SUrt7lz2fG7/o9X8q73/U+vuHrvxnt+T+5UPpSYfCO734n//yffCu/6ot/gWPJspV0Q0enxfup2WV2x74w+qAQ4WBqJ0ccf7BuFknuV+jb5KEakRpuZGD4YtByjMcbnpg2ey7/LFKAuNvX3Zx1EE5ZMZtrudNKNjzS60pKJe7LfPq5xug396VILIrUZbZSPBRrKIjFQsgXkF4eRhhJeCHb8oi2LbNFkbAY83Xb3Nt2UscyJTBMXxZ6915KxaK73mzedITpcXG8eztYtnsxp/h5WRAJ8n8cvWoO8DqRf/uvQtfN/1ECIvJC6J/L1oVyekZlm1QsFED4DuHlPgg3RhW26b7TqT1wP87wbPBCbAxxVoHYzX33gQbd14fiBonRxk+Frh1UnFaTEkMSWnykGdowu7GaMkmBO8ZNV6vf4KOxdk9LRIwhCZMUW1xI26kctvySwGQ9mbEK4mOuuUxuY0W623ZYeaVYNuHmsDsqLQi3W7rhByqVTpIFQXnqqbvce/IxWlAXqlT/Xv4dUJPAYEYUs1CRZHe1IR50XzCxgTn+57YliblQWCPaQOKhMWtxAiewjOHZQgn3aMy58tQb7vJH//hX8uyPP8u//3c/wAaQy839/AHerUMcy7Ly1/73v8Mv/KxP4s5TO0BPLi4JLxxqgz5aLFCMUovbwXXvgNUMYsNs6KnoqGoUOwkoxDEH96+EkYjFlxfHLDeHJ+Z8Vx2Ob5rdFOBtkeKRGttS0bwq2lbAtsnFH1iLBQX4ciMnj9fobZyGjK6dmrfOx9MYR3SP3ln6PVpKCkzeDzh3GMrOE45aeAIEVF9S6G8kfNuyarNaE9yd3zSwaLzLqtV5g03dICKWyyjerSfJOIIVVCpJtL65pLPx2wNfddzQkSk54d0oTi0a3sAgcYAHNrrJNEosVlSVru00Dbpju7KZHPe++FKox32c0ul+tHDv9yYqzGzGZpknp/rzaq/XSZH0EQBRdDRy8m5ybDb526ibk28RcbwJIz5sAZwIPPoSpSzoOjk2gBodQdAtNu6WSvexMmyn1CLcHc8nzvhotxUexblnubgbT5ISI+Mgk08dw+n1AShAnquipJz45E/9ZG4/dg55YJJpQfJmw4PE3YxGnIq20V7iWliQ5b1pECxMOPxwF7YiJWLehQXemzS6Upx4nYGC54skKf6PJSQZH/URH8qf/tN/lN/2Zb+XH/qhd7uDCuDj4Qf6nPU0+n73f3gn/+qffwef9as/BSnuep08Lip8JHxBsU1xfXQnu4zK5szgm2gPnm/DD8MkmZyKUz7UOwknQWwKGd+gzHUiYWjz8VRKibE14ia0+z0ixIEUD1jE0m6HoIiQqi/8vLPePpPAu/FtiN+zC6LerYt5VEIu7rpuMTqX4p9TzkFVMsF0M2fBC7JutC51DHQbOf12OokqzMxpcsjpUHUvCiXnGzembhrTWfSLMRJbcvHEDT1nu6+8kVAVUqSDCm52vRlybHDECQYKd6yUQpEEboOm3nGfPDajY906394aBvTe3G0IpzbVySe7FJ3+VHfxc91o+l2TTXxFuXkGusGJ+ZGRkm7oX6/wel0USREvRClXhrhLh0h4aKu6U8eGLZ3GCN0OTd9SJ9ePbvQAY9BaCwDYH5yhA0UoccMMNWx0B/Px8W4zEPWRRbBcUDaK0IDkdk+JwkQhmZPIkYRo4vLRIcKNPkhAUrxTfeKJ23zRb/pV5JrCiMANa9341DsA2W409YdeJXt3TCeFUsZsOF8zgP4e28WTMSn+kMhw1xy/znE5h5GLb0lNgoZFQSxtIZDkNPiZn/CR/OE/+ZX8nt/9J3nfe5497Q4+GFqQT4advmT+4d/5en7xZ/5czm5XNKnbaKmrrUgCYcufkgsVEfElDl5QxCyceHwETMVt6xzKFMiRUKmRvS6FuUzuloRhY7DbDCS8ZaO3wVyLK0As3L7NnDit0b0R6Yl+ozmPVcM9fDOjyAlLiRqdsAebbVPKlqJo0X4rOTbrG59ykxXa2LislWE9sDqXL9ZSTp37GK4S8g/KS5NvyKebDbttG3mX563r0Q0iuIEZTD0aIUmihsmFBp1pCy3bNOYSdDqPCPYIiRJ8yW009/2PnTpLjWWVbEVf9FQUHbf1Z83FTeHzGPdVkRLdsVPzNuK/RjWWYIFosBtqesnCzcwn0GAWDAVJHotxyuh5ldfrokhuuMNLL+5ooc2V5FgEFiacrr/OgQ+Bd0UWHcrG+xNJpLopl70wlGDWx31CkOMwVXfBToVS/MKZgUqhlBsnaqwjWahFSdI4bJnWOKk3S+VHf/RdNx3VBwFJGg0hce/xM970pjtRzFKMElFYCAMB38neLAFGc0A9jUAt7cZItju2JyFrk3RDHC95dkqNjRMGKwaD4O5JZYkbtIqE67kX6Y4yUuMX/Oc/l//quS/jj/6BP8PDF6787X6g92sS9KyBjca//aa3873f/i4+4Rf8dJTr0wJFLeIw2OY3x8+ck3hTuAQhZyiTg/Kt9zAg9oUT4R86lWnbFfvDlAx3TY+UmZfm0SCkZGQKZpFfI04H6r2fOpvtrXrRcxzOOyqXsW5fb7sop0A0u/ELUNscjTyQakN4N7loimdju1MSGtiz65VryWh393E2o2GNLBox1rWdcNExogvL/mwonkedc457QwNagE3H6VnablC9ruuNpt2UkspLCpj/bKZ6Wnj5s/myXjI0hc61TPh2eQzHnzfK1sZm8cMybGZSclgrdNh9DEYb5ORwhgaspJtnJQLD7yOJw8/Cq/PG8Nenwyz+Gb/ui6TiLb+FgatbpG3mFcn9/ZIg2YmoxGmnBqnGaGLOlZtqjVQ+B4W3oqliHpieCGcbN49QdRJvydVPujZOWtoeYWQaH/qGmRj+oAxmJ7SrxioFfuRHfoytyNnNHf4aLyHJnqff+4Dv+74f5sk3vykKpEu2hjllYiNhGEIPKZ5JA1qQlQeq4kYYZicXmsQWRu/jsCnkqTDMuacmG4/SGZbbYquH+maSTIllCIHRUZyg8gVf+Lk898wDvvp/+Iss15GXHS87vbuXvtOMWceCrnL1wpG/8FV/ja+oX8zP/pQP9069uOHqhnX2cLXOuYRfZglz5ULBH8hh/iDVumNop+vicabinWbqkZceMsQhSk6EOiRcx8s4BXoZGrzGKJzaySV7IFd0ZFsG04ZfD24WAbJh1ttpKQJhSusLnlhShHLFs7L9fWxKGwIe2LBFgumBeddKyhGKlb27juLkMteMmcfUesEagbtvSwqnRKnGkiq21jm5M9JGoG9pkHJBu+OpGlt+M6WP5vg/ho522qZLThHatk16/swG6SoaCIcc/Ho4RKXcZGV7w7Th+q61tqGnQ8S7XkMCDkneMqNhDixxuBqecaM6QvUDOjobi2EMDQf/1+5nXhdFEmJVjzMBu23bODudSur2IUEOVTQNhmXG8JHS+qCUTFMQKSjLacM7hvpFVCGNlW5eZOYShrpDbnAm3NUaETo+pkFsjiW5OwoWGFdiWHMsySrHy8EP/ci7/ENKkFVOp9yrvlQYHFjbBc0OrHKFY4dOpvfAe70Z68zo1tChp8D7vlxj+GglImRV3Ocy0XuA4GpsKYNLW/xrZ8dpUvffE9s2+T6OG4amzrV257GGzM1v4Mo0Vb7sK76Q5599nq/9X/8WvStZMn14vg/vtzU0wkaegADG4Lu+40f5qq/6R3zl9Nl80if9ZNc520KWSrc13MknshR6ySTLLNGdZCmoerTq6IMRBH6LbkWGxtdzVUu00tRwq5biJhrdkt8DAeeI5NjEeoiV2+EFrzJst7T1lyxrFCJjHPDspE3NImHOm8CLnzFGd/MM9+GN5WEhj+Bnm/9ZleTOUzag+1ibkjCyuQIII7kvHTpq1NUt89ud/MEXoCOMarGGaqeNFUtEx2vONU3BHRx+n+Uyx4EQBTufblo2CSgGRdyNaBP1ujKon5Zc2zbHY1ScB9tT9JiyfWJucuGyDFc0bYkjLzWiMZMY/TNWPD3Rv6uQS97+QmDSwUJY16gjfg1PEJOApUxKnKhZr/R6nRRJIgfDaS7DNtebkBCak7odptq2dvjIpAZh86WmrK3FbsbHiy2oyRdAIcBPPjIsbWWqNbAdt7ZCLNybuy8uwwNRkpxoJ5hTZtyzUZ3CocrT73uWF597gGMxFjf7B5hB4wY4O59545ufZLAyYuzc/noyp4xsFnGtR9ejYVxg4UybCiVPsUEkJHFygha2zWHK/vAlM4/JMGOOB2QLV+oa2hwzaihPQCJAyiV5A0XOEr/z9305P/Lud/HPv+6bw5wWb1lf8b1vP4//TPfvv4fnf+yj+dt/7dv5yA+/y90nzmMZURFbkAFmzdVEpiysjFiErO2ASKL1G4ca2zxF1a28qhQkOZ44TTEtmFFyDfusoIRFAUrinUw5FYRxinkATtjctujY3HHsRDFzpVYSi7RFi4eXE26Wk5P7rWxJiIKNLbptU+iEDVly3FFKIVmhj9jSBxbeOESEavHPMm8b9lh5R0Gz+DxEQHKm5m2Jd5PbI6E2cymsL8vMnFmg44bsbeZF0BdXLhnc4ARiKehRDTlG9FjGnOg5wcKQkArjCzUZPuqYGR0Lwv6Ig2Ns6BhjeBfY1wZRxMU/KT/At30GAZWFNZyP+NFotB70pxt449Ver4siaaa0kEu5osBxlC1hQM3tqpIEY14cW0gpnfDHGgC86k1EpKQIwopNacmZqWSaKl3jw9wqrhAUjUEqkdcxImEtxfIE5bhcI4GRxeKcIQOzytNPP8Ph8shJzS1+g3zA9w/cvn3Bmx5/I4V9LGBuKBViA8meazNskFOlzFPI7tyoY7uBcegfEqdr4yRgd28ZfdB1+LbRPJOYDA29IUajjCIkqVjwKTENtUoUqOFbTAWmu8KXf+Vv4Hv/w/fx9I89RNfEFv702p97wewRLzzz7/nOf/uQr/8Hd/mC3/jpiBxRXZCSWdS5Cv4IBKYFWPfF3DTtmafJ8cjhZgo9fBw94+VmeeDAvn9vXZvbhuVtm22nacIn0I1IboHRxdWVG+/GUguqYWSxueIPxXXxGzXJIxM2p23/GUI2iQsi0GCpymaq4gTx4kwVHtzvPHrYaMuEauJ6XZjrzLpcYeU+FxfGU08+zm42RKab+1/ktGTZOK0WAoTT7tlcRigmpE0eG5DCiOuxavNil6Ijlbi3gy2yee34cLWxCDZyvXfaG47sEkX1MTwML3S7LurThfekIa41f9YcZtI46P3Z3aV6wx/NzlTemge2PQc4fCZuEpK2Q2O3wR9hqXdzKv6E1+uiSHqHUl3uRQcdvqQQxy5ScppEJTESTo0x5ziN6HCS9tBaB5nW9GT4uUVgjrFy6AO1FNth36qXXNy3TwfD/GGrOZHyLkB+RSLjpffVc1Pw7Xcu2RceGO/+8R9juV7Y7iHGBly/1svPvMPVNQ/vX3H7yScwGSRxUNoJsRJlwpdHlpbTtlpHyCajSG4W/c73zN4RhrxzW4KUUBjEVIimMJIlh3RTg0KVT5y6zZLLl2BQN4WC+Ub84z/+I/ni3/Ir+ao/+lcY3XmW/tuB+bzSVifCyQ4PH/FifZa//r/9Iz7x5348H/lTztwc2WaqGVac21nMOYwiiZEy9WzPaaEHEF1Dqil4eQEjUH30jsJQMwxbmeZCQyl4YY19WHguxuRwGn8dJ/NtqHfWy+rWSK1vHR8wDMZAJZZQ+KTQh0ZjPU7XwuKz6a2xpf9J6J29frl12/mtysXdvbMWzDCZmUpCdKKNSq4CssPo3jXHYQLQ+8aFDa6nhNP5pm+XGxpdlg0+6Cf9TNfu/FyNIhd4Yd4WQOpS3ezZGjdEdrxF1GF+bYN/vB0+RNfpdnfbyYXDFPF1erSOOnyJ00Z3z4agZDGC7xmfh/uQis93fTClyO6JrbzFB+zLLY+I2OzzXvc8ySSJuUz0YUzz/kSBAQkvSMG6yw1rKWSr8WEF1oRBdb6Vc8dycKJuTD1rZN2U7Esh70i9vRf8vycrjFTptlBLnGQSo24A4nW385tJNkCgxtAt/PAPvJc+4t/GjVfea768weD6auG5F57hbbwRrDtxPWgLLYw2xqZltxueHEQBihGnSqEUB6xJERLWXT/bRgvSdWdd/LRFfCli5lZUGtniU9pD1ugUFLQ5IR8nevuDV8klk1WQ3Pl1X/SZ/Kt/9s182zd+D72l1xxhYCsGxrpcsx4e8v1v/0H++l/4f/MH/9SXk/crkjSwyXHqXDRF4JM6BYi4JsOCvyo4Dqnu2ZhSRltz/8no4A5huJBTuFibxwBsDIkk4l0iRg/1T5YcfD43aW2jxSJqK64u6fT6FumM4l6HGgvJsKGKeTW4wUDB3ahad3ghSWJIcPxUqUkRuUSSK47MjKbR3YrQm2LpKu43O9GJwFy1mDPI5s6dHMsVv++1b3xhoYu6Osnc0KP37iN3mUimDm3lCNwzY10X//lzgezLrm5KW9fT0rVkhx5Sdiw3InHoYr58ST6diDl/0qL7zBJjcHTuOWVf2uGjsogbdJgprA0f6a9Z10ZO5QYW2YpoSUi+CYUrRU7FMROpkK/yel0UScB93tRzO3JKHiIOJPNTyqIj1NV8HCe21Ketnm9wJUT5FrRzwYHenAXt3S9wKaSp0BUYypQy0j3jV5N7EI5hIEck+8k1p5k+tszgIGjLFoErYJl3/dh7T8X9NTuol71uiMB9+OJBo7g61hJWabZheZma9/49NnK2D9gvQQC9ux20/1977x5tW3bXdX5+c8619rm3HqnK+wmBkAehYsLDBJrQYBozQsDQjSARRmvTDGkVRse2EU37wAfqsAct4qNVegCCQ+QpLdACIgRRDGgSihiEIgmQpB5U5VHPe+/Za805f/3H9zfX3reoulUxQN3KOHPkpO7ZZ5991pprzt/8Pb7f7w9vDS+Jkif6tKMFNAJkQLBAEDiBT92RklOGCDKDCSFPX0rwHjWIro1vAkU/+elP4Q1f+yf4k7/0Ru6+8wE6o2fy8OounwsZiU5nBW/QOv/ye/8ln/25L+VzX/dyYAVTn+eRipEXoKrwwPXVEGRw7yxLRBQgZZo26jW25XjxpO6DrVHyhOVQuenidGOEwrvk42TLlUdT+xDdS2368FQiPxnwgZxVeHNklJrX7VmmIQqaBnRFzzXlzJRmvGqNpRKf4TKyI6zIOU62ZKFDqgMw5aBM9izJwH7ICfce+X1GEYUNRWJRvLOU1LfcVeHHjFICPG8zu6zcaPe+4RJzmjZxW6U1PdJeBjVaSFjgJo++GA6Ghace8nI5ePGtt+19baRL2mBVOVtuNWBy6pLomBXmeWLQfFs7OBTedeB5VfrIwxEbaID2eCjcJFcHNyL8bV430nn3wKlpL6jCnETPkkEcjAIPsKjTTDJo3UfGZKJMhYR68zb3KOKUMGxOKUZKhdq6JiadCnPXTSF1z/FAjhgCgZNrDW6/9c5gTtijSUVqxImZUuLcuXMqzvmQLUuMIEBFGwHEzRXqeEj3J5LyOTE36kYX+MreyKSgliW15ewdYz0Kf5T32ZTOcUrI4jtgo0qPAP9Gonvd4DOG5NAazqd91kv5g1/+Wr7t734v/UFqcQ9lKJW0X9kv6nB4790P8A+/6dv4lE97IU9+zjmmLs+md3mNtZ4G/zyLtz22v+p5yqPFfKZo8dDyaDGqCa/LSilSjVEqYxBOlZdLNlS7ZSyGuo2uPw7ksW6Tii6VEFGoTR0M5UptuVkL/UfvMvCtK3wca3xZF+ZcKPMsMoJL1ss6EQqGKLJ5hPAGWb2g3MWv7i06N/a+3e/AHWIWgcO2omT4k0VfpOhGGYLVZkovCoM8MVkW1M21rlqXRJ2p0rUVwNAlyui24dlm0TKDt957ZzerN3xvgxsX+WJjw5GmlPAi5IIBa1TLB6KgtlOAaOWheZgmoTrc4+AYLAjifMyDFafGaT0ERq6kQ3BVGEkjMaVJndBCQC4XEd9rrQqJS2YqEl8YklqqpUgYQOs59Opc7S4x6F6lZuywrBKsKDFRCsWFCWvmrEsnFek4yjs9h1ymSnPbYEDjVE4+sHKd++67yJ233RXXdbAMj8RCGUYkpcT53bnQcVSOaPyldvS+1p2eli0xPRY/UdVrqKe4+lrLM+rV2e/3Shtk2wwzroWdzEiTPKY1TvvFl0hJlEjqa8NMqLs4bRErpSjlYV1SZ7bb80f+xJfw5p95K+94668cT8VD3nuIKrHsT9VWoCX+883v5ju/4wf5mj/75TT2NJy1raSID1J4GDmMB4RqUXNJiHlQ86Ip1pjfkRebz00yLFUbVyGFRdMt8H4KJv3IniR6kjvo0B2uaYmii8JiFQUkGJEilaM+485SFQJbykFVhGTOnEJ0oSp/WR08KQR103rtYYBAHr9HhCU1cU3iEOowoKLC3LjXEumB5sHnFrf2wF+2cDJQSWwIoIjSWOlro6fKamkzkslGEeoQBZkrjWU5q0UvRpFwOblkJqJqHbTFQR88dFsfbVsO+6WFpz7+nUfP8CoKsvCgkKzgoY2pIv5QNIp0lHusCYHsR4fGlFTj0AHx8Hv0qjCS7p21LeOgZ22Lqk1ByUtJ+nzZdLkSCOhhJxvN24ZzGrp7FoICrcsLyVMOg6LwYjZVvEYlbfCAW60S6ywF8xnrzoqquFMsHoW3SSrTLWO9ctftH+Luux4gUfAw9o9KmDZpUcznC9ddfw2pq6WBAQOcnEMhskXujT5SDJobPAj/KbM2SVM4HTcdMK3VgGMMyphLfceHZqJ0hUrOaqPgqGJphO6mvMVsCJTuVYdLySqshS8wecLTxDOe9WT+5J/+cv6PP/WN3POBB9RMPgXr4tizjO/Nnean5LQDT6yX4Hu+84f53Fd/Fs9/6ceo97Yba3XmfBLN6k0GHWcuGXUrVPMur0E79A5U5TR7Yu0ovF1EU1WuS7RL76vCxAQe85iCAdIRy2OEiIN4YLBJc9XseFuUiQhqXl2D5QJ0X2n9VN5TVLpzKttBnW2KWW4km6B10fBKhJ+RuBiAcOE+K73toQnbicchEVQdsy7AfCg9ae4n3aPr83qrJKsypj7KiJHfRxsy9RVPCUxKSpYUcWQD7408lyOgfFXROx0KPa2tONqPaYhiBBZzrIexVTz2ZO2V3oVocHc5OHkKB6kxuqiWrO4E6r8eiBOkP+rBqhmp8ZTK1s9IHjI6CEd4/zDj6jCSSGsuxUnW3SWqawSTYMWskqoKGH14WzEh4NE50MXF1mFJc5X2B4l/I/R3VdPJRa53jxxJDiiByyv1eEglGCYK6QL+gzr6lTJBz/zS23+VC/ddZOgxwkEd50pjbObnfMzTeeITr2PKE51NPzw8U4V9A+vm1EPSOaXIOR3yn2aq8mpfCEjvPnpKK4tpyyX5YB6VxOxUb5Qk6bceIVMKNZXEKIqAUNBSajeXVwDORU5pnkgl87mf90r+w799O9/17d9PHxXLXrl8Sny7RvdKaynyj53ffO/7+dqv/gb++J9+Pa/5A59NmSbSfA1TCk8WQ61rhYboAsFKfJiER28By4WpG2uLIlR32rrHUiVZpp1einTLSOY7ROGvRON7vG+5uwGsFsOxCxuIU1tjikIFJlJEjhYevYmhopSg6LKDJtrjEN9yhGZAYk6DHjvEX4i8q46sdVmZdxPJ5Glu+MMuiJhA3PEzk3c9qtse0U4LzOZg1eCor020a8gD8oO80HW9BClRa+htZj0vtdJN5GxbXn0oIm1if+6sy35kRjEr4fUpJN+eTVPb5h5FqB6kBh08LeZ7yLnBPI10RDgUWYK8PTjcKRl5ypHCAItUiirxne57rZ2r3Uh2R7i7lINFEs0HTBuUpI2ztlVeYRFn2oNCJdZA3wxcylmtIhPSouyNlEpgIjX5+y7YyFA7VgK3k92iKip1aOWsRIGKd+rEIvKEVHqDn/sPN7Nf9hDFiC0//SiGGezmIvXmoDNuCWcXFAeKTmgSjYRlwSLa4LObiaaVjO5LhGBD4XkXh83goTfpGG79fZSiMCTQkBz2i9palCSJteRSRLJBVbC0SfbL35dSUxnbYDfxx77mD/Pv/93P8p5b7pQhH6yXhxzyQsTKaSSfee+77uVvvvHbec87f5Ov+Jo/xDVP6LS60DyFoMNK9yrKpMEaeUVqpy4LxCFZl0aeJ9kZOnmKPjQIXkLSAbJ62w6GtgSioajqbTkOCIchbJtLEc3Opc6dHR0wENCWHj+TFzbwhzCyStHhLw2IjuZzXZUzG+FsbwcZOGCr2rYmA+s91KpyxkzC1RwFsce4SSEy42ddxmitlbaqJYMMRpfn70oLdIy1LrS2MKVM90x3ow5aY1A5u6sAmMzoNUQqIjz0oCIeRHVL5DH3Sg1FcSzyV9uqSiF7pgqPbWF199Dh7KoHCEI19sMo2IQOQIjjWEqRgnL60LdUdeyK+/OqMJJmptNvCyW1GlX1OzAZzDIlGuB1b9G0K6SPikCwQIgGKMFuwOitkULPrptjvZOTU0x5P68jSR6QAcBbDQ1KGQtzLYY0FrAnzBZOH6j84lt/OQDDH6YQbddDv/DABZb9nlTOqY1AhFduLaq/XQbT81asUV07cHCEZ22O28CSBWAWFSNIAbDt4qUPh4wEXnN4pYJWabMlYU3DM9DWCowp4YAHMygZ7JihG/u2kErmuc9/Ol/0hz6fb/4b37YVF44PDrNjwVPfxIMtDqr9pYvQE//33/7n3HHnXXzdX/xfePJTz4uGmqPRVZMuoRFQkZiLuUz0WkM0OUdfGGcqE1YmwNRyNidqFY1T3rnWTM5qMEZKW3pBQigLW5Ghemx88eVb09Po4VmPDphDtWqXp4OR0AwcDsSuZ3mYj5ABs2iZHDJkI4IYBtRSAW8hLOsyVBtGVHJmw7MbXmoPDzanFJRFD0qiVNRbpAC8i8RRAVY1UctmNGvkaYKUWNaKJafVSm3D0CsfmCE8V1i7U4rarshz30dU2MJ4RY3Boud5iJyMcFkIjxT3kwLfWKlVYtHdGslGw4jOZJmhHEVO1NawHlJ0EgMlJ6khHZqqPfR4RCNpZifAzwC7eP/3u/vXm9k/AT4buDfe+j+5+82mv/bNwGuBi/H62678V0LhBGXyS3IlzokiiEniaFMttgBYV7EqVCFU3ibHaeym02K08dIDCPGMksiu8D5bEpB8nrZFWRXQQgoedxfwfAovSd0XRY5POLfdeifve+/tDzV7jzS9Cr+y8d733M6bfuotfN4XvBqPPiBKJvdIQI+8kkeYIlm0Hht8tAUV+LmICmfylLt36RE2nZ6lqIlaa43qo4/yjCOjU9JESlMspq7QCPHrSRJdbV2HzSEHZSxtr8JTTjQTnfGLvuTz+b7v+mHe+847HmY2DukJNk83/PF2ysWL8g+/7zt/lLvvuoev+ytfxdNe+FTM9AytRAjpUsFpDjUjvN+UNzA3DR0qfYhYCIOaerQKtkI1eTO1VRLG6boPo+ewBkStyXMVZKiFune0BojCiwg6YpeYa6MmLJSH9P3W4tcOobRwhGKctQ5riDOoqUW0Uhj5vq4cstqlxoE5GoBt7CGP7TPaR8gTcw8FcjMhFMJwSdHMQt9SYX6PFE8p81h+JFP0tK6rmolEAWoop6fIpA/4FwYlRx6d4Jr3Gk2/JAc4TZPudTDc/BjIPiKlDiTRi0uiFLuMMtm8R74VlrZSpsKURZzoKZyXpMOxRVrAkjzPjxQCtAde5e4PmNkE/Hsz+9H42Z9x9+9/0Ps/D3h+fL0C+Ifx3ysMZ22nerCemFPRovCGd2OpCiMPRRnRmGCcUm3zANYWyeosNW5h3/TwWxfGK1VXVdIMihgIKSV1uHNEiE+E+lBhsgK9UHunFPUiGUbLHd71K7/OhfvVSOiRcZGXD0Oe3b137/l7f+ef8t989iu49vpraF3CBt2bdO9WdZNTBa9CinzLrMW79oa3wTBRWLW2xjT68nTJk1ErtTfyLsfilcHo/ZJCoJxic0pYQ9enuCmZCmLyoKeAmjQ6idQVZipX3JlMzeaf/XFP4cv+6P/AN/7lb6EujQEw3+TnAKUTJBwiuE1XvjPwepahni78xA//e5al89e+9et44o0nzGlmX1cBr2MjdYdawWsjJUFAvERxBuT5tijORWohuah1Je9k7FhIyVjXRs6FKRUVbpJzsjsRvq4LgdFirnprakqWOslb5JpDQd6DCx2FDIdgpgQfP+4z51HZNYZcGYGg8ICpTXlAvKCuSxiO8MojcoADjKZ3Aa1bNCIb7Se2jpO9k0uWo2BxrIdhHuB9+qK4Ln7PusgHpaQoCikpLmRBqE1FemcDtUexSH7t8N4yU5kRGeyQ62+Bd56zQPaRIKdzoDeKdgyjc+g0y6NvUslQ4y+ziKwCe+zgVRhnTJTmVkd67CPwJF0r+YH4doqvKwXxXwh8Z/zez5nZDWb2DHe/4+H/iDZNjgcnmENctBtTmRkSJAMSoQWYMEp4Spp06STKV2B4nTlv/XVBp+bAp7UAoePO6oIstNaxDtOc2U16iLWeKilPovdEq4ZROSlwyy3vPDT/+vBsJB3JmdXq3Hv3PfT9Hu9iMAhao8VRpiLwbu1MZbB8FEY3D75rUMWI+RNUpgtknxzPyj92b5zuB7c2gmiHnp1UilSYDLyvmLV4LlKlGSGkbfm0kMYyo/UCJj88mVOYaNl4/eu/mH/1Az/NO26+5WjzHNg4ESCj8HOF6OXo1MjJRUjXGj/7Uzfz5n/1i3zhl36WUi5eJLPnSrP4kNBKCn9TVrgsYoHWTU6wdLWGMFODud7Eu84uXK53Z10XlmVhnneMQuzaOz3YImtdpCsQOXGJPBMoAQ+oinLD2peKcmoT7g9nKxqZGeuiImJKWcbJ+8bn3jxVI6IZ5T1LTqy1SkowJWiCtFmkTtIR5rDH+tefDvpp9k0NKkdRJVkSUD0ZJQueJ4wnQCJ5VoHGpPDfaKQSZIfugUPu22fklFjrehTyN3JxMYVcikUptCFhHLRF+zauybsclkNO8+D8DB2Hja+/7S0xduq4a5cq1BA5EUpF4Hf1637o8fCExaNhZtnMbgbuAn7C3X8+fvTXzeztZvZNZraL154FvO/o12+N1670+ZQiKpFaQopaV0pm3mWm2Ui5YWkll04pME2ZUlRRm+eJkgtTia+cBRNJhWnaMU87LKkdSZkSu5OJ3VyYp6yvuUjxxxpranCSYLIArEr/UI3PVQxKSZ0N52mCavz6u9/zSLnfhx0p5ZBCa9zx3vv4ge/9MU7XymlfWehUROFae2PplcU7+9bZt8bSOvveuLQuwpiZig+YUevKflnovVFb5eL+IhfWi1zqC81C/aULRVAsk8oJTqFWwENFKBXcMssqAZJlvcS6VtZ15dJyiX1bWNoiumOT19u7uLq1w9qcnjo3POMavvINX87umrLlysZzB7b0gpOwdA5L14CdQ2d4GPguo7C/WPnn/+BHuPXXLjDP1wYgHHZTYTcXpmyU3JnnTCoWIdtKa7r+7itWCs2MFVh64/T0lHW/BOZxZAuVg5smrctpnpmnCaud0uT7Cn/XacteYP4IIZfTU05PT6mh1rSuaxRYFM4nhCtVbi2FVwXzNDNPWVqX3qh1DUMcmqclGEpdTBvRMTt5krhLrZW1CydZuyBj0V5GRaOR2QiWTi5hXCIHWHF6Mlak3zoogs0LzSfWnlmaIo3aBLtb9nvW/UKr6mbZu9rYulXKLFFkFddE/TXvJPPorZ20h0ukW5ByemuNpVVOW+Vi3XPaFvZdlf7BklEDsFViwq4umD248NsBLEVe2hJY6QC3C8JUIk/flEK6grv4qIykuzd3fxnwbODlZnYT8EbgRcDvBZ4I/NlH81ljmNlXmdlbzOwtd3/wXvBI/q4La1uobaH2PbXtaX2P+x5nofsptV3CCSkzq0CFtkBfoK14hAtrNDrfrwtrPaX1hVYXelu5eOE+LsXXhfvvYbn4APW+B+gXL7BevB9f9+rjvSx4r6QkymSti6rHtgerXNyfcscdH8CC5/3hDt8ojp11n/mRH/5Z7r+wsgLVYe1VFLki78KzqZlVSvRsNINUBk4NvA+6GwzXNucillCZmXY7clYhKgXWs6+Nfb1E7Zdo7SJeL7GuF6l9obY9tS+AjGmO8D3nDKWQpwkrWRlhc4lMMOE+08gsfaWVhd/3+Z/Oq179SjGWGEWbMQsNZ8HZ81f7BW7u9/MN5uT5OhnqXlB52SBVfuXt7+Ef/93v4cKlJn53CgWbvkJ3etuxLAnjHMnOQ9uR+gmZ85xMN3KSz3FNPscTpmu4YbqGG6bzXDftOMkTcy4q2ES1uERb1daVx5stMYfXlVyFjSmLHnuSZ7KjNrHhXffIKyaTrmRbVqzLAzcbcmI55MXESMlZ8KNpnsgBSxPVsWG5qTqfO9NOa6L1pvyoqYeM28CIesDaFD04IZeXgo+vf5JC8UqUQ98qxOu6sj895cL+Xi4u93Fa7+d0vY8Lp/ezr3uqH4ouvVVaX6U0Hnlgj3RRSqrgez8YQg8SQF33eNtvBUB53EZ2I61Ov7Tg+ypyiEd0CAEXFAQuxZocUWPOU0RSiTnPTKkwpxJtW4CR9uiAF1o1vH8E4fblG9rvMbM3Aa9x92+Ml/dm9u3A18b3twHPOfq1Z8drD/6sbwG+BeDFL32ezzkzpOdbJLcNKUpvKCgLlXHvUlohbEFUxuqiELx3x0umtlVNs7yKIhbtX3tTE6BOx0K+Pk8ThcwcINWUJiZ2zEmCDguVao5bpxKbMSUuVef97797q8h/uIbSe2XUOxe7xJ3vv4877vwgH/+kp8VJ54KPoLyNJaNGSkCA+g49k/KO3nUCV1ZScrDGvjopz9SemHrauO6ti81hOZNK5rwHCqDkQ5U4JRx9bmIItBIFoqLkfFAWU7IQKNaCdc+sJih19s71TzjPV371l/Hmn3kbD3zoAuvIc22WsvPXOOXPxzN9Sb8ItfL1+Vqm3Q5spS334jQurvfwY9/3Jl7xGb+H173+5aSSKRTSlCgGXqsA7h1ObMJbEUbPdiQm2rJIgChJh1S9jRiriRbNuob8vw01oN7JURkQFnWo4NeNKuvd8GlAdZogRH5oHJbnsi2RFNjekWvvLtC1vObQxiyhbNM9VLajRUlS+Ju69AO6tYDPKBxWOqQH88oCMtaV40yJXhuWxUTJRXm/lCBbpyf9HXmujpoYRQviFFColGE0iLNMKuK0t5a21z1IEA6iO3qi5MKcuuoMUVwqiFW20oUmqI1e18A5JkihGdp1Lx7tZ0tOKm52DwQCUZQL9EW0+VWxSSmH5kMDMyTTQt3I/OH9xUdT3X4KsIaBPAf8fuBvjTxjVLP/e+Ad8Ss/BHyNmX03Ktjce8V8ZBiI1irJCjntSFl1salEb5su2E2xRHb1YnHYcGuqrA5sWRiBRe1TvQefOdz1ni3yH8FuDoJypeHZWddVbSzNqe2UeSoyzKOvb5zsFpvq0qULXLhwgSunaa84v1vFvtbGXbfeya2/cjsvfP5zaKUzZKHGSZ9IrK4Qzkby2bSQa6uR2w0Z/lCdrn1PmaOJUng0wvllwUFaoycBjtt+jVyWKJgekCZPUZEdeaU40c31bFIXTk38Yh1rAvXLXVlxPvHTnserXveZ/MB3/LiEJFq9rND1uuEAo/9+QV/4C34v3dWUTDqBCbOVC/fezXf94x/kNa/5LPITLom86I73vUK+kqJgklib0XxhshAz8dEGVsYmlQO8pveQC0kH6S8VTyTiW+OZ1bZG7kw5OKwGaF4bOKcZ8gFOpmclRowq68PYHQoRgmolcb+7RYdK29qo5jyTGErohmotLcJQiVMo/VDkBPQVfI2cPRuOeF1rsNFkHMQ+0pqapqLorTWmaWKaJnKTZ70VeqYcmIuAnkVrX3nAEsX1OICV91aE05r6fXtt5C4wecqJtcnbTEHA2M0npJ3QFFEelU5ki66Vkdv1vLK0NZr8dZJ4E4HKiJ6i3ilTDkpiwUMZygO3q313DEX7rePReJLPAL7DRtd6+F53/xEz+6kwoAbcDPzxeP+/QvCfdyEI0Fc80h9wl4FQpSqk802V3xSMkhY4MlEtD3xWVbttoxctyyKIS0AS6E25iBTwEsQCEIQD9vtTVe1SYt4V5mgy7C7dwSGllpt6gKtIFsWjbJxeusSFixe26/mvHbI/xunFxv6+85xPT+ECd0pkImhp8iIAQiQ02C4lvGyCpdEDzrDsV1pf1Zs6PNaR2PZ1cNRdYgteY6GrWIUNFRoCciVBY1y5z1qrwORR0RgYS8yiEqmTvoRCjllhPpf4sq94HT/942/m/bfd81vm4IeAlxCIGOCHAWh4O43KtBa/YFqJG294Kk++7mO4z38dLxU8kwusNVOb8lSFQTl0ulXWfgl3FbuKZXpdWaugPpYslJ4ikgkOPQO3myKMjZ+5ubxI9OyE0w0URhiQIY6h4kwYCaKvzXhuBPOGJQ4lQywroWFbH0Dwul1fMuE7pxCoTqnIi22wtqpnkg2znTx8B0vCwlK0J0rK22GXk6BChjHlHdnUHsSbMbC/KuqoICUkQ+Abw9moVf2WCOEVPTJpK3jrlLIjEw0dOSUXgcTLlPCWOJlOJI0Weg1lnmm9s7bGfDIFAD+qo95JCWocyFNInQ00QUHFmdYkyGEpqvBxQHUPCLx37HC5DzkeTXX77cAnP8Trr3qY9zvw1Y/0ucdD7u+0tascjcU3gdPI7dRYoMlGTse2jWtthAMKpBtr9MwQdXDkO1rAVgaeMucSlW6n15WUZi342jnNC26JOU9MKVOHcU0IR9kb67qwruuVZpArPYLhRY6DbD65gXvvK5hfA13Vb8m7Hfp3qEpLiAVMEEo0a/T7sKSNaSkFi0mn9zyLeZNzIQezZ9C4lqgkl8gZSsgglGcAryu9ReN3E1RqF2rOYvd0Ae+NTeaLbJg1IjDHPfGyl76AL/ySz+Xb/8EP0Bbb5gDgL4Z1/APIQP5Fub5gg0Uc0CErzOeeCifX8c5f/U2ufepCuUHVVPIl9l3SctkhucI5mwqNztIWzCZInf1yqmINzul+z7zbHQo3sT5Gu1kdQlE46DGHwQKx6PEtWE/AmQxIMqLEOh1RjnJ28nJHuN27cA5jrZey27zMERHQO9MI3yPXSOQicxn0UeXXtmdCo7amKnwIZ+DObp4Co5hDLGRHMeELEpmBcltr3Z6rh+GRyEikiD2UzhNhuPLmGRN5SffIG6ceep0dSqSP8IhGCq0u4U1bdH5UMabAkQTimDuwLgdBnqEHTVIG1CJdN9IMuVioRgWn20RZTci7Ht76Q42rgnHjQC9BXu/OGj1eSsrqJhdYJ1WxOj0lLXZMD9TUvEqeYohacBJ9LUzkfDp5Uo+W2j3gH8Zut5O6Sa8C8aLcpafONF+7uTXVTIomSZ3oRmL47g9epC4IWmBJOUQAL+D1EUxkgOLHgkqZsruB//BzP8tNLzvhk17+XNJMdKpzIHrf9IWcCvPJLC87yPxGwvtC6Y1zu3P0PLH2WeDuvAbHFnpz3CROUatLdDcKCX3AsUoPEYUThZ+9UkqNcFvvK2TMKsEKjxSGgOtykHK8pgZexSbKrvJH/ugX86P/4k3c9p4PAkeL0+EvGfwlABT+SwTD5LkD3SVk6+1efv7N/5Y3fv0lvuTLPpeXf+ZTueZaqGsmTzW8J4khe5phXQSTCcxoHx5hKEJ5kj7kSP57VEjlSYesWBRgZDEq1MDVZqWBdruZ6tJzJMlAScHbcE+s60I2CfgaidUP95ZTUu7RIWdhNbFpq+j2rnat8v7Am8JZ30xHtFUYobv3jcqXcxIMLAofUxInvUQHwewdXyo9Gfve5JVFGiunRF4Tbh1PztKXDfAfOHMV/2o0GotcXyJRPJFSodlhLnNO4DNDCXxUq81MPXoC0iPyCDD0N11ogqyOaFRM4k0Ex96iDURKwCSMNMptGqZGai6dTnnnwsuuru6J/pHkJH+3xn5dKelwYpnpFNCDMMo0S2aqC/KwVnUMLNMsEQILCffAhS2tcXJyIjc/C1aREix1VY5y8zqDAGiiLS4taIBDVSXlEPvsIQYqEu8QiuirQ59EifIlom5TDtNknK80BpUSgL5y392/zr/+0few+u38Xy/5K/R0QcWm3oNWOLxrGOo0tauQIim5ib4aF/aV+eREvX16ovi0hW2WkwpiJiqa5cSU9LpMsZODtZGzcmO9zHRmBH3WXDWLSqNBXTuWVJQwHOuEpqcOj2bKzXVzPvZjnsULXvAJ3P6eDz3EjNjl/xzzE/8Zmoy1Lly6eD/v/rX38WM/+jae8oxX8QkvOse8m6CfY54KlnogHVSF1VIq1KzwsjdBdFIqXHNePYMUyTlbIZTIpHSXYIYdqK3SEDF5aM1pVV6pUciewrsMfrAZU551cAb9cYo+0ltfpshzSyQjSUHbTG0MikRpaxX2cJoKrlwMo/AnAxm/H/vAwztN0SyN8PySq6pduxhENslI7HKJ1EqkGOjsd0NoY2AxPbj/QQjoCnMl4ix2eO+CHLXmW/ShAmAi5RwQHj/kbN1Cu1VRokcxVrrhmsKKmqWpZAlTYHS7C6rXgjiiyleNB3fgxOdYW45A6J4zVi3y/r9N1e3fqTHyTbVWpmnSJPYWTX5sy+/kAQMhkcvMVAR8nUqJk75vWKmhPWcQqkIVDwyWGlhJPq2FmGsyQT7SrsizAnoUVXKWpFje1FYU7hafSXlH3mXskqqK3nPktPYM5e1HOwmO09tF1mXilnd+gFve9UFe/ClPUt6x6E2WoPV94MwyeVeYusI49eIxep6U43XlsxqCYlSUiynuTGZbIn+/7EnZoarI0sLL8mRYrVjtSqpHWJ+Iok8UDkousRGO1GyS0XoKjrkS/dXFl6/sVfjAPqxy18i96flPYJn14gVuuP5JvP3md/K8F32qPLi6yANKUTDoOVo6QGfCXKK7ZNHV6rooahl57KwUggc7pjX1Yt9ayLpEdTNEZANeUnjeMhCqX5UtleKo33fvamXQmmMuY0NgFavXyKGlYDmp+FbyAN/3LYStVUZxKH8PCNhAf4yKOsE8s2Ck9bZiKW9ajTmMSJrUx3u0UC4lqxgU2pTbez3RU7TMaC087RzQL12T0pTCcErFJ4pGXRSIvq6blNoQzDUfBzjBs4fBgrH4KmY0ErRKNli6UAVDAEQh+ID2tOj+GZ53UBq31F0cShZep6Wr3EgmwmUfZ6KjzTDyNa3RV5HWLRWW4HuWbBsmculrdMuLAkJVkaXkQrPBF23Mk+h0RGg1lWhA3xqtGXWC5FLzXrtCvHmODnQ2hCRybPDMC256Dt/8bX+eu953D7/4tnfzEz/+du79wJ20/f5gH68Qb1vMgBMQEu+U3Q1cXHb8zL99O5/4Sa/Fzq+am97xteNdKiwdWFvD96fgUpHEE3k6Eae1LpE7MqaS2M07LdiUogKqsKMwg63ycAD3FEYtqF/eBaKeJ9I0MRqtld5VTUWFClad9S1nUpmkChT1PrVB7TgVT5WTk3nLWz3aoVzUnt4glcbHfuwz+aLX/0F+7uffzNOe/Tx286eSkzGfS+RUJSpLdIlcFwZtLScTPXNdY/NltXnovlV1HUUj3jtlmuQd+cimJPkzwZ9uuDzlUU+M9BAeXY4M8EQh06Nq7nE4bwIYvUclPZPyJG68V/Yj555ytE9Gm5rRSsE2dj/oIkfByVz9xYeToD029kI/dNp0Y21G9xQ87CSlrdRJ3Sl5h+Jd4TFHDyq1j4CelFaR7CDRklaHRp5DRQvD0+gJrh5MoxjJOC4Hc2p8DePJ4Cw5a7SunTKkNAeNNJ5J8oNhDfzp4J8rnHa8uoDyQVNNETJc9S1lAXJSfwzR20K3zxAdL3s0mtd7i41uhQNlXzHvcSrJ6HpvIeSgkNRjQ8+zwvPBnCGpbUT3RJnKloNMiKeact5U0ntTnghX4r+3lXy98amvuYnd6RN58lPey4/95NvoPKDcpRUVO47cJYvmUKNlLEnK3/qjnZImpnnHtTc+gdtuu433/tr7ePaLbxArhlg8eZKYQ1MDKzsR4NlM5P1U8pZHawHgTRZdnauwpPshfhrezZQDauLg3ZgwdinhVNqyasE1cebTtiwT2SaFZgYUbd5aYyPmhewFqxyEF+wUS43z50+wNLjEMTfJSD3TLOO2J3WjlBN6qtGMLQuOcnKOGz7huazXnPAv/t/v4aaXPJUv/tLPETYUFPZaktanT4Ll5AmnUVIPEHJm9K+xJDB+sUnFiFRYW8W6IhsLKbbB+jGUv2ytkWJ9Hby/KIapOB3ydsLx9VBmEmW2kVxqRBZh6g5BWdQ8y+ieo8dLBwvR3xCRULpllUBJKqFzaRFLpq0QYQHtKjZJBQtdl0P8rSK2TGuhgyD1nmWRhFqZJmoVPbPkTMkh0YZJbSogdOqDlgJrmVQ87GoeVlCTMaUzomlbr+Q00bt0IjseorzyqlPgpv3oCOiu3OpkiYzR4r1iaon73upKyrZFkluzPEIIuS/QiKIbWA4jfYXD+qowkmZGKtMGVqavjN4rtTZqnN4JteY0JHFmRYnY7BPeE2TlO+p+IVnfFrf0JxWq1ybsVYctCd+VUcZjcRpAc3ZlCoBwwIa8MFmFtmfq51lT5wl+Hef7M/mVWy/xf37zP+Xeuy/SVgudmdOHuNuDHJY7kJryLwa766/jic98Bjd96st552/cQk4f4Kk3TpxEnmgYU4kACHRvKWHTpF7agOXM6qJqJYgqvkMT8c8ss/bGiqqdda8WuEvTKsm5SJG8JNYIu/scO94c96okgqsNQU8Bz4rUiPpQa1nlljGvIV2RSbnrv9P1fNzzX4jZvwsl9I7l87B7Ctc/5+PJpx/g/be9A7dMYyF1KGkmn7sB9k72yvqBW7n+2ht56ac/jz/2NX+I655YAkKDBBgStHWNSnTWZkSYQEuJtXVqVEYtZx16EZZZj9YegXpY6hqeIRCGphP9k1yQnmSZlNVMy80CgB0RjoW8XjsAy9073U5xpAOQskRdpinHGpWhEAdbyj29iwfewkC7jjAZqhE2B+xteKi+CsqTLck4RTpAK9GU+7QdZeBgLQpevZMnCUDXupf4RORxBTLUAVBGHyEg0KQhSZiOmD/BiRgQvKrPaFtfmR71BkWO0tMU9VjEhHB+cqeECEgLAedkYEW9p2C0tAh8bnR11DMzks0Uzo+shcDlRzbo4cZVYSTdoQfPUif5YVHUWqWAkwqpFE4CIxbK7Hgkvs2hdYmHWutMJRaiHzwsFRIzVjKpJFqVSEHOmdSdVJ1pnhXeYBHe9C3XUtxJDnO6ntKfRL9wjnfcuvILv/pubv3V93DrLb9Bu3QP5nNU5fZcVr0FoG9eHogxYRiUzMnuPJfuv4f/8tY38Rn/7cv4E1/9RTzt2dewOFu4gTmrixPdXRgyWw/YtVSyJL68hwhsCkMptgHZBBVKChpnkyyVwkmnLmvAQoawaw3v1WlJcJdouUaaJxkiIifqo6GSwpuVytqMa+xazqVrOO3XcGk/wzpz7cmLOTl5MpcufQC7/kbOPf2TqfMzmZ+wcuHdv67ckoUsRTNSmjj/hBs5vfcDPONp1/BlX/F5vPqLP4trb7wWsyr1JzIpq+tjr2LFrLXRIpdoKcGykEPkQLnTjsehnLBgY7HhG7V5REmNG90yBFsVGTUT23LZCWmc9iqRBY+CY5LBaGsUM4KqZ9Ghs0fHSGlLdgR9UtqoR3g6IoZSisDjScIntY4K8mDLlNhLchtrFxHBLW+po5yzYHWjNbOP3uSH++se7CuzUJmSroIEbaNw58FBj2vMWMivBNoBjzA8BzQJ1JdGB6+8YwHocy5M5URGMKn+oO6WEyqIK2ppvQmz7MrhWnjYrfStKt6S/uaUp6DDRmu9yJVaSlF3YHNcHmpcFUayI2OVLMK5pGpTjl69mc5gP2SPaqoJ4mBosY+ufQAllUgEE4luZ5oloFp70wJOcVKH2OpI3LY1ANNdOMgxdQ7sDOr6cfybn/kg/+nmn+Xdt76fd91yB7e/+xfwS7dSL9yHtYXRP8R7YjAwxjgcAiMd3clT4sYnn+djXnAjr/zsV3DTTc/jlZ/zcso1xoW6aMO6YB69d/bdowlVi6KTGlL13uiVkDOLzIE5FQHF3TuZwi4OGgJygXXMQ8gULbra69aAPiAHWAudwqIQLYWaipg7FvkmFataa+SeSf4c3v2+iXf92p286zffy8/+/Fu4/4Pv59473s/JjU9nsUa55qm0eWJtd/OB//IL9Pvfi5CVjWyZcq6QS+NZT1l59R/7Al7+ypfy4k9+AT1XaBYhbUadH1pUog8ahN4r7hlDSlBihDSahXJ586g4q0Chgxfm3YkEaTGyzTJawzOyEkYuQP6DiTQA+ilhNlPMufmtbyXnxCfedJM2cwEwXQdO6oQkXqQkYOuPrr1wKLVvVMmoCLXaWKPV6jQNgS59pRTSzC0QHEnhakkpOsX7ptaTewqRW2QsXXKDUy7hkaF8qWlPjqJIChREMyOr2iKeuoiBh/y15SB5WOShVUEXQ0ueo7vgaRYEksGssVFUJcfh4dTedF9HsCHM1EHApXBv7sH0qXJ4gJWGxTX7qhYWG67zYcZVYSQNyFbELEkGrtygGCYdz1o0kmUyJlf+pRAtHuLhQITWKgErTZYS3Qi4ipF6ZzfvtgZOrTaWdVGIaJk1PM/mLr0+y5SobPfyBH7oh2/jG/7G93DxgUuQPwR+Sj29Hb/vN5U3zYnueyy63gnoKvWgJz/1Ol7zBZ/J05/2DN7ylrdx++13ctMnv5Df83s/kRe95Hk8/0Ufw7ybZfQMltWordMi3MHk7U3mTMmVEywFD+1DorqfTWFNqyGqW9QgKbtvVMDRglYLcWgR9vA6VsE3cCiZKbzNvAGrZTcHCB8TJbH2NcQvjP2+c01/Jt/7/e/g7/2j/4+7774Xv7RnvXQ3vt5D9lPwBqmw3Pd+ysUPYW3FlvvprZDKxPM+4cl89qs/mU962YvYnTde9tIXcsMznsKpr3Qqu2hW31GzKVzhYLJ587qKIW5wqNQAdMuUFP2U6oq7qGopvIqN9uaje18OkLTAyK2p0dvwPHscYCN3pvSMMIbLsnD7bXfwkpd8kp5Tb6EIrryomZgqSrirAKaiTsatKt8ZzeKU5w71+NaxUiiop1Afv4eRAqgt9fGABHlStT3wxC1oq7UiKIxZECsy5o3UV0nIhSbrEDcZdN4hwdarUhPFXAQECLjVyCNGpR1VkD0AlspPy8t0P/RWV7EGFbdGB0SXbB0u1h3GwUgO6ACmOTQHakQ2EqV2h5pUX2hxPcMR6o+CKXeVGElhyDw5nkaukC10E/wH8YmJfCJR4EFeROs1HlyGpCyYV4n2UrKMDPH5SR6FmACJ3TSzrlV/P07nlAz9agYTdOWO297Pd33r93Df7W+B9RRcUv69nkK/BPQg37v2f+TSp3Mzz33BE/hr3/iVfMrvfQW57Nhf+kLuf+AC1197Dptg8VUhRgCVqw9+rJLL2SYslUCKSWF89ZC4byaAPAjfF3nI7gEyz6PzHcF4sKhAhgo1RJ5WHkeLEFJwCi3CTGJoNw8xjxZc8XVRzm7vq6h+fcHyNfyzb/s+/p+//4Pc+8F7oV+irRWvq+7KJERAMrqfRh90AdLLznjlf3cTf+Gv/q889eOfhKXO0k6xpNay2RKtV/ZNQse1S6V96JFOkwoluCAxqo9Fx+nI8WWMkgwvhnUB673bEfWz0eqqB5igVlXC1XSrk1KJPF3CEqwjZemaQ2sdQr7rs37f57Cbd+BFohFOUDgVMm4cZ0KbM1ggo/ifwpPLJuUnb4qofCoUK0KCWKF1yFmFqtFcrfihGp6iCu3O1kGzNoGzR5idgqqYuqiga4hwlE4Yt4rnoTrl4fEOmbxIJcXXWCtaL8E4ojM6fY78vFAODW89IEU6pLqPlRmf0wbt1ahdqYnjXKKoh/q8EQniR3+j1VjbMf/xuY8klH1VGEkMPKmlbHdJnCkNqNNkxLyWhpy8jFcaAbY7ySSPpIx9olIZ2SPvPXroaI5ri4cdpmMAXdV4Hqqvcu27sV8XLHXW1HjPe+7g197x05SL72etIW6AZPzd2taV0NBJbki09LnPO8cb/swX8azn3Mh77/jVgBAlSpl5/90fkLL6NDF6F0tyTELBKvILpmM241EhVWgmBZqpFHKecaAu+/CGjsRWkQCCIiCXeK4HtzUW9hrh5tAXxH3DzFUPwxkQma0aGJXxWqtwh5Hrcu/ce/Eefvwnf5r7776Vsl4M4VMjZVU82zjAW5hnM7pNWILnv/RJ/M//22uZnnSJD37w1mhF4SzrnmwnAuB7ZzdJmKF52+ilDtQWArJZ7RxseA0ePN6cWGqjmg7CGvS8NPr+uOiZpBwBipFK3hAJVizEFqI3Uwq018iX42RXoc3PWRwIUdaINJAl5dV769HAS/m3FIwmVYKj3YglemubkeytkZrTkhg4yQrYJEMfxqp7w5jUzjcih1pXWo/opK/qhkjFzQ8kAVJAeVbcO2uXd1dyCuUhCU2MIpePSsmRcdoEQjgYycGdPhywh1y9c/iZDgZFco2RZuJw4Nkh6klHmazxNyyeg55btOkI+uPxe4/HlUJtuEqMpKNG6N47FeUvdPIpb2ceyPwkMKmiirHg0uYtALGxo5gDbI3Wq3CEKSlPl7KpEX1MUEoCz0oxJanZVOssHao3Tk8vcPt9d/HMm57G6QPXMc03kvPppjpicyLvJpJ35ilx7mTiZC485UmF57/oevz8h7jl3Znd7lrmeRZoPlIBeTeTG+xKYVkbZRr6gvLuEkmStNbxFKDctdJ6lRJ73bMsAmdL7d7wKqqbA7UfLdZoB9BacG4twm+UgKcNGJFyllvTMwt8X7ZtoVvKcYhk9fruonze88B93PtA4wWveDa+29Mu7LF8DZ2JKUGyBqWTS2JOxslkzFPm3DXXcMON53jhTc8kn0/cduedXH/uhGmasZQp0y7wg1AmqdX3CtN8wlxm4RizdDJ3pqT8aRMV0wy15wjxjZScucx4GfS3UFlP6oGdKKy1qf96MDLkxHgoZs9CTkQue9Ui0qGJCnzH4eEawOdMkBsAop0vyGO0LbwM6TMbOo/Ks3rqrF30PwMRIWrFbY8NOEzkADexkXj/CEk9kBFSHwhPq+nQ6uFRDXHa8WWW1fQNvd76evS3BvvsqEiDb87M5jW2UeRSSNwHBM4PRamhMSq1ckUvfmQ8RwGViIZ+i3Fz23KgB5Vy/ZHRolbXdJjzfJn5fOhxVRhJOOSBSnBYh+KWMZLlxw2cHLciIdK+sq9VXptl5Rmb49YCXJ7xJI9CjYhEVQsOjz4vcnm16fTqZqx1pdXG6do4DRzZ0555PW/4+q9krQ1fOz3vyWWi+0Kzpr4pnjBrZHPmqdAqZJs5vzsh+UIOXrMXceNO5nN0LDCdIuznQQ9squ5angK+ITEAhRHyyIZ4wTgotMCLQtkc4XM0XzckflAyW/Jfi1KAXy3Ypvc3GVrzhhXlVOUtqCLb3ENtxoMS1rjUVlrtrL1zqcDnve4zeNWrP109ZQwWFzA7W4nqamGXjV1So6iTcyfkrJ7VJbjeFHmJOc+kSRzscBeY84R1sXDyEMotJXpeyxidD0EIQx5cNtP8dMhlDq9a6RcLo6diYAoap7zAAaTrPlhcETaG5wwZOoFrZIPbTDnTQvLr0rIwubbsGoUTQ+tSjrkMjZrgdSE+XB6eRXFkNQm5ZDcaGe/Cf2KrFIIYnRSj42d4Zb1H2G1RnMLoVQIcvblaf8QBrP1l2+GS4x5rd4jmccOooWXBaOMaqwTfjCTh+fk2Z8fR7RYab2FvAhrRuVxrM06xrU9NRD2DtTM+lyOj6S7Dr1d0KOCDMTdM51GK5AqW8qoxkmttqOupMbT2ureYELZb2x59hBWtVUFaLNGtSqQcV6GlN/qi38PENR4hjtqbBng1OOGjV47jEoNwYd/O5wkouJ+gabUNp1amwhBLTZbENS2ZtVZyyUwlYTkrUV/D2EfR6OABKx8FEV6lkZyPRlR5pvZThTa1Rac3KCWYQC7PCK9KO/QsuFNK1DAWyVV06d6pvantAKqyS7dwdJUMpEDw7rrXcHOkV+jR01lFnagqVmHjriEFv/naEBuQR2zRnmC02kgudkpJM6WcCLtoKSqYoj0WK3Fg5uihk2mm+qZWdiaZJN16eCcMyBBGChGSHkbBbGRd1ddEXlUKb8V0oERhpuOsBi3rAC1mWIDv+wAoB9PLG1HcsY3KGM4jEPhT03OovWO94fSgKA5D6XE1O4Wqpvyjh9BLHR4bErKVpymhYOXYgpJKtCgwNu+umh+8VatCFQVrRgo9HHCdPdJcDPiV/MLMuhlEd+FtJVE47lPhfey0g+GLERmMzatU3vEolxjXih+H3V1MOWzrfbPNeeztxKS5CKWh42vUVenLPBytrcjDAf0RxaSrvrrde+fScknwGBM+UcWTzFSKoFLuOOsGtbDwJlqTFJQV6D3wkEkeU4rcjqVEc72u3JGH9JjApD2iET1XLY08FfXwK5JOwwU2xn1TMDE1I8HsRDCKHL2fS8ER7S5HpTpbIs/QrSlPliSgaqHQLF6yDNCWBzOjpJnuRqLIwzCFI6oiF3Jy9qtSBDApZEoDRhIbwOZQuMmkPGsxWbRH8g5dSt66LfG9lekwBtIgl0zpjieTWhMGPfjfs+7zJM2MlgRmCbL6DOU8ydvzWe1qI/zTM3RGh0cjvDqUv9RmEngd1Otn5M5GRDGqqB4qQSBfRKkKNRPDwkPbSlP7+F150y3yyrRGDmhZ8+jC6fqwNP5eH2kLefsKnR2J7oY31YPrbi7rIF9QrWtVi5dwtLft3gCWflE9113GO8V7Dhv4kNMz63RbI1gcHpwwmvL44zeOvauqdIs8yVHlheBixWd08Kr78tHHOsRuhzcdu8R6eMLIaI11fGwgIYxm/HvYu8s80aOfbRnO0eHwUJIAavDFw4mqnYGFFkLhEF7L+I/OoZHbdFfaYuTbPfb+48FIgvieSvp6NCgq4MYSOROhTwzv6pcLYORYhCH0yaB4BYMhqSDj3mM2gi2h4io5GCIK7dPGwEmR/xxtPA3lLFNAFxSWpUNFMic8T8oThfLN8C7EC5bhTCXrxHVde+rGlCeEK1SvZ8uJWltIujXWuhdelIKqqqgiasZYeSWr41wQLwDh9pIr9ZBM6tvJBp+V0MUMvUkM8xziH0aZMplzUTWNirMbOcC+ysllsp0Lj3909ZP4BERot4U38bVZBME/YBTmPCqflW730IaUXVIFPLlMSw1vQOdjhHRpfGLAPkfnK6syRR5viR2o9IryfM17UPRMLJBeyeYSu4gDEQuOPHEv+h+Ce0dDKncGnz9t/lfQPgMPO3KOw0xbD08t0h1qiTBcMz8KI4837yhi5Nj4oQQfRwwRFus946A84Cq33N4Ii7d/H1T9dX/Dq9NoPhSEiOet3OAwNNhRuH2Uq7xsWAS6cR3dHuRNbtbRt79txlGYHs+tj0PDsDQKvMf3cvR3e9u+39J0cZ2jKHmgBjz8uCqM5HDPx01WR1CNFsR001mXho8RXqSemvKAyWRGSk4C17qS3uDiWGO0RfzUYoncB4kKiW9aUvHI5CEKbygcYIqF0029lke7TlpgBbswbL015jyRSt5gNBOCilhQoMTbrdCdkzKRsrHWhXlXcO+sdR0oB0BSZVCDoSAvLJdCyuPUhBnlIVs3UpHXm8hgAstP5YScd+IyhyCsIYxStkJGOcLuQy/RyV0eoUdIbpuFk5UYCx1CaIEwRuFxj8ZV8gk5eHKRvPch5hvg4ICg63cCbC9xVAn/Dv9rPHvtk5Gk0Pfd5SvCogjDoJNI7QiADViTfmjvgUF0h2ha1WkSeyDSBKQAIqdx99sYXQabdzKVHMkgwrv08CR9M+wjHDyEpaOmHJaGLUiMdMFlwwgzHC0f6NtBaTas9/DHhgs2ikNhPMIoj2d4tAu36zqobYUfNvQ3+8HY4B489vE8jl4fH3Q0YzYOcB8HwCj4jLncfqyjwNnWjhTE0YE55NnwiBK2I+hwj+Netlse5SS2KHCQd4bS12XG9UHjqjCSwgaqxC8IRImTfOQmZVTkRuUtpImAiZQNi8ZJvVd6r7QBxUiJkidJquH4SApb3hLIvWvxmUk6q+OiQQWTYcOQ2cCvZZxMmbWY1HwMsCyA+9ZAWHg9S+pOeBKQnu6jeq9Ob1PZsUmM4UR0QJ+VG7Xs7HbnSTaR0w442vAYl+vhCZp0DG8aoazyiSWMywpbj2udqBLeiPemYE345ZupyQ3etA/VR6aC1U12CwWbm9cFbJ4RNk73CLGDkaQ8V489rujALMu7ZATK4ongR8YxVG90D+KrNxcsSFkLGWF5V1E9Ng/xCc1bSpKYowvYrfvNeA/VHMUIl63ZFNAXoapiR3ZVtR2jpqhsj6fi4i8TSIJh5Dzup0eRaBgOZ+hVjrTCCAsP4be5aK2yhcNlFixneF/Nh4ngMqPksDFsaj/cW7IoJw2Ra++0BLnpGPOmdISwkLquNESxDT37I+91eLipRzELvS+HGIiFK9iAoQgieI+TYl67Daoh4YEShSuZr0Mlfcz3cKAiCjQVvkjKtcuT5MjA/jYZyehx8xbgNnf/AjP7OOC7gScBbwX+R3dfTP23vxP4VOCDwJe6+29c8bMxStpt/9bCjZyiu0QQ7DjcSSEqeqiUmTlTThgTtSe6T2FXJc7gIRGlkz3TzaL9JRGW12C6HDwmhwP9yg780GICtXtg3UqeFN47TFiwd2QskqOiRFIBoljI20Mot+Q46YfhO5zOKrKseOoUm5WoxmjbNoKIIS8zmi5AyhZeYRVnDbVnhcTawo0tLGRiULtiPbHZqxB3GAwUHQjxPHzkfBSApmBBJVB+La5xg2+4mBUCOAfdjqNqaBOV01IND3GErB23FtoKts2S+jTLcI9cWWurlJ4IGNWR13QwE5q63kfF2jc9Tu8WxkVYW9u8l8NoA8g8wnjTvHg8wd45eMuOKHJhJCGMvo/NP4oZippGFDFCZYfQZvytG3kI4dMM62zQnIELba4Wr1rnSV4iptyljwN0PIMRIdihcBMeYw+qZ4/vj0Nqt6O1Rniqsf5S0v0106HbAu4jzQKXI4QfeXS6lg2HyTD4gtT28eTdICeOUwkD7kc86cuHHVIocW1a6Cnu/bfHk3wD8MvA9fH93wK+yd2/28z+EfCVwD+M/97t7p9gZq+P933plT7YzJin+bKLNwwStBr6IuNkikVjCNqyGUnPIU5gJJswawdXvjs9+nTL4KlandOJjG8KKbFsCmvDI5pyIdl0ZCRz8GE12ZkZsxKFhxAQs+D1YihlNkR4ddXp6FkYEkYYMlhxtTQiFIzcao8Qdsia9bTGO3t44GDb50UosoU/stcWuoEyIz0WchRg1P0lPOAUoWvbDNuAZ438W1z88RPUeqOTTD5iR5muLcRkpC1kylRbj95CR/ANJzE46SFvjgee0K1FZ9PNkmAenHWcZgHN6VLeHv16Kn0LJRmPAg7pry3Uk0E+igzjsBv4xcMvDnGVzfhuHH2pasvjHW/37d8Mr3Cr4h4Fgx6V8VGpbyF0TBhJWd5Ix4SxjZwuEb56iiNpGLnoEmpbvtKFox2P0Q8P0iLE7h5G0i4Prb13xp87hvyMe9Rcau8ce6NjgttAkeBq/UEcHtsMHIyVjXn2vj2wAWsaFz88Vogcu7PN6VCwB98QKyN+2TzJ8bevYCDhURpJM3s28PnAXwf+tOnKXgV8WbzlO4C/jIzkF8a/Ab4f+PtmZn6FKxnJ9G07xZ4ppYSll1EclTN3yGVH65mSZrV4pW09NTYKREqkPIeuo6rPqrTOpGBKmB36CWcKyXaoetwpNHKacB/q2lq8lTr8GFwmNc7jtLE6osMGeIlaQii7XObW25ZrOwwR9FXJHoYjqouets+Cw6ZNR/lVgehHXgsZhWYbHW3QxQTVOXht3kb1nljkhw2yNaDfDjDAEn1jUSTwEv2ffYBqxIvdcoESWlXRa3B4VwlNtKi0h/esn8XcNpBAqsL65NHqNg5LogVDM9/QC5uq0Za3GJtM19IRVKZ3l0ebDt7FaFsQv6DXQkdzM2rDoMsNDVdR0CXNi6KIcZK4exweAULywWAK2NnWnyZxHCZ65AGHgRlallt+dXwNa98PUmHxhxltU8cYmMKtsyOCWbmPcDlyKcnjOB0efhj1bJcvYd+C+VirtqUxCLgSiB1jrhA6onwZqy0mGJ7o8E5lqMeiPE5FXHb4XHYdccBHcXXMi8cveRQZWxOqwILJxNH1P9R4tJ7k3wG+Drguvn8ScI+7DzrvrcCz4t/PAt4XN1HN7N54/weOP9DMvgr4KoCnP+spUjDxsZAFmUmpYFaUpxzhb+TxejfwxDSdSCjAxa3dQsYI+8YGH5TELZwH5TtIcvm9062DLZsH0SO3BXpoasakE9YsglqH5MJybYn2UCFRznRiFBbYTrlQy3Fwq5uunlngEl09a8xDQsuG4HC0CmDV94Fd8jgQmksAlgFO5iisMC33Hj/ILeY6ZOnWLn2/kSudKAcaWFdf7tF3RPc4jGN4eX3BvBK9p3RRm2GJX4lfrUdFlOHKDY9qgNqTabGnCLVH3ql1MVHcBgBeXopjG0RLIasL2B2FjM1QuAD5w9i1QCwML2MMSWrpqiMTuv1eswN8ZvhBiR7N5ZQ3NFeTroMDGQjM+J2xFrS5R7jYNiMwbuVwzXF4HRmBcYgeQnjC8B48S0IAxl2fncZh10cxZOSJ4eAxc0Q3hG6hhWDSQrCDc6i5GnlWczxB95XkQZ91zSZEO+hkagsyVuhYw963nPhhPcQ9+UgJdKqFc9+UavDQMrWUZNg3RIDBiHyGd9sj527lsHYOT4iHG49oJM3sC4C73P2tZvY5j/T+Rzvc/VuAbwH4pE95gV977XWbETM/x5RmcpqxpMrrYT0MNz48wHClD66z3pOIIk5M2HHSevscXOyB1sCbWqCSYtMIwLxlKEfENE6l8AISEjPVKTvoa1KlUYvcishoAySftuT4gDMMz6AFDEV5tA7pAG5PXVVYb76VEgxBIHpttDxq/0eA5uP8S4Rgm8zp8KQCqb9uPYWGDL4HiDvO+RD8OJbp8nYEGo7wc8Sp3du24bZYN3BrgBSSPMgBNo4txwk1HzO8OWsX4mDQ7FJ4aOJ6hzGzwCt2eSbZiQ0X7UIt8m1xrWnQ7hwZmBQBcISUuiAJPXQzrZHeNi+obVzgzfJGdfZwiNgWQh7mRvC1I0+QYdD1O+myFXoIqbfqq3lw7kdBTR9uR8a4h9EYv2/ZIto5SiEE7W+E7Qfv2GNuDwcbII1LG9FJIA+OrxMZ3e1efeQO9UeGVzxgUwR2+TJIDod86oM99nGXdFeRKHKjw3GKY4bR32ozkg8ygIacheHF+mbErzwejSf5mcDrzOy1wAnKSX4zcIOZlfAmnw3cFu+/DXgOcKsJOPcEVMB52JGscG66AWALt1ISi8S96gSI9/bggFo0ntKCcCixEPUhCm3iewtZrGOX2iOJ3CKkMVxeY+Cr1WZ1iuJoPMT4zWGkU+g6tiGe0BQGDtFgj3zZYOOMh9e7CZ/nTZ5Y2BkPRkZHp3JP4cHh5Obq7wy4zeFzaeGaDwqjqpnmEcpsHsjYiYftpDpcPaihAOEO466KZw1IkDnyYIfRI+bE/aiSGd0B6QGLUZ+gMeXHObHxveS3IsQch3/2jQ3Re3jGxtacLB8dhqOaOoQMugfAeeT0uvKZLcgIBwPfjvKJTs+agS1P5SBqYHh0o0DoIzzWRj3M67iOg2FjYAuH8XXRYfWmEdEcL0klaS4/yuMTNldcm3ykInT5CmtHeH8Z1Om4GLJdV0CbtrmAbQGO+x0FqfjbfeSHI5fd/cCC6Ub0Nz98gHsP7zvFOgsYDzEPIIGZ4fF6wLciFH7wsAi9zV0Mid6jGq772q4xrvfY6A+nSdTPzqZFPopmR2mRhxuPaCTd/Y3AG+NiPwf4Wnf/cjP7PuCLUYX7jwL/Mn7lh+L7N8fPf+pK+cj4I6zLsp0ujYv0JrdYeT628GTkypSqHMKdSWrTR6Fd7ceTaFjv2GEt0BE9T8srevGmFlSryPBFN7iNhjWmMyhuKaS1Wngy5qJWtjY8AP1Gb0SIHnnBoL+ZhVr08XVtoNwqbyaF5xxSaN2g+LIZhe7OHJujheE6JNd9WwKbR3c0jiWppERzRO2yOP2rBHy7cbn2nuzQ5qFHMkQeMtq2m75hGj2Lxu/H9SKIlB99porf4SW0OLgYzaTiX/2wmTyk8djC7zBG1sEzQ0lqy9+mJEiJHRmaCC3TcfS1Gb+RcT6E7IaLbsnBmBy3BnYfsLEw5mHMhsE4eDaxwXVCHQHRLx+b4TWL0PNIQ9W6QmDzTUFoXMN2LZtRHtcV2gBhvI+N5DCch8KQq6Xt5rMdTEqzca5qjSaOI65x7/qbXezQI6+xb2H/uOVj8Pwxc2egCNIWmhAh+4M94YORHJJ3fnQvtmF02Z7Lkb//sOMjwUn+WeC7zewbgF8AvjVe/1bgn5rZu4APAa9/pA9yd/q638Km1iV0llILLrIKO308FM0ydOUDcVjXw6mhSRn83Ag9RyEDtLAsMGTh/ZhlzIkGURH49OH5jDByGAUtNAHVYHghygTYFu7GY2OssbE1uodIgHlAZsIwET5RLJxkHoIZUVCIHGKPKneTvA89HZSQkkNLI3QMwzacCdi8NHPh5EbDtUZI2SNaokaIp5ryqyNqPqypQ26pRsDc3RAbR/2GhpeJGbWv4GK1bCGbh9d/9FxjezIoZ9vGMOhq/7gJS7jXqIQP2Io8vxSht2/h/6AJ+qFyy3BZI31CtF+New+FFRJVoZmJHiph8VGcGBtU6R/n6OCIAojFfY41sHnT27MYxYWxmcPDgkOBysYZE320kw4YAnyfTGIQZoSo7iGVQxhii4fn3qKQHpCe7QAacxq/N77fDNyhYLRBgW1gT2M9OJsKuMD0MnQyWs6WVkk6BD1SBynuO/nR/Jhd9uUQXOuD4dxM3IiaYn1mMw5b3kOtP9YJBI13GNUjL+UhxodlJN39p4Gfjn//GvDyh3jPKfAlH+bnsl8XmSNXPguckj007MJDSBZQmJj0kPLCfQPuqhiRcK+bD7A197IUi/iwQTb4zFikrkXUj8KK7dN9nMXpsrzLMHHDQPZoBF/pkTcTmHYI/ToeRiHCCONoYcZp7GwQjN77ZYveo9ObVLjV+jXZ4fdbqwHzOFzb9v9hcAqDtTIqqXHyM7zdUWTYeJDb+zSMTUlb5ihq99pcGXTwxKYST1p+bTZtuIbFQcDmQ1lMtEex5vK8FLQa6uJjM+u7QxoAPf8U+VZXE22UY1b/lWOQt8fceRjzlEb/JNE23VV+G6mSDcS+zcc43A7GDGKN+sjTGeYm4zF+fvScR3TrG6unb/c7ogXfZjqouBsGM9aC+4ZfPKb7jQhseGWDcXLZ2NAGvh3wbUDQUhp1FzZ/NDbM+Cu+zUOY2yOv7niex827Izm+OEi3yrkFhGmkuY9ysuqvY4f53P7u2CzDS+ay/x/Ph+FFxvo8/Lz9FjjTg8dVwbhxBNsdvFbfTmiFv6PixciFdBkIncBpeygQxsZ9O6YTKbq3xXEXCXmGF2GHaxjFgRFGXmYk5RRIQCJCkhaSZUMSvowqfGvU3nVPUwp1oHGq1iNjpFD0eFEfPNVh2NTrpNbK6HJHydvhgIWGXhybvQtSNApPdmTcNjEGgFGx9INv07fFLbD0ttwOl6OfxwmePIPnONFbJPfHtA6vg4MxNh0wNqBBETqO8PUQ+oy/e+Q1HK+Xo5AsMbwa3zaNh0ci7zs2v/eALBkP+jjgoNTe5YAc5tMF0nKZW0QKlBfE0d8r8ZnDI97yymHgElDGfh6H5OHk1WVuIYeud2D6hvKQPKrhWce9eNpynr13+lof0hAez+FlBmEzRNpTW4O0o3B72yQPM4Y3NspZwzCPnz1USKxja3NvlTfmYNR0BF9+rQ9eBw93LcMemI9D+7BmH+FWHnLYI6ULfzeGmd0P3PJYX8dv83gyD4I9fRSMj7Z7+mi7Hzi7p49kfKy7P+XBL14VniRwi7t/2mN9Eb+dw8zecnZPV/f4aLsfOLun34nxEAmKs3E2zsbZOBtjnBnJs3E2zsbZuMK4WozktzzWF/A7MM7u6eofH233A2f39Ns+rorCzdk4G2fjbFyt42rxJM/G2TgbZ+OqHI+5kTSz15jZLWb2LjP7c4/19TzaYWbfZmZ3mdk7jl57opn9hJm9M/57Y7xuZvZ34x7fbmaf8thd+UMPM3uOmb3JzP6Lmf2Smb0hXn8839OJmf1HM/vFuKe/Eq9/nJn9fFz795jZHK/v4vt3xc+f+5jewMMMM8tm9gtm9iPx/eP9fn7DzP6zmd1sZm+J166adfeYGkkTmfUfAJ8HvBj4w2b24sfymj6M8U+A1zzotT8H/KS7Px/4yfgedH/Pj6+vQrqbV9uowP/u7i8GPh346ngWj+d72gOvcveXAi8DXmNmn85BMPoTgLuRUDQcCUYD3xTvuxrHG5AA9hiP9/sB+H3u/rIjqM/Vs+4eLE30u/kFfAbw40ffvxF442N5TR/m9T8XeMfR97cAz4h/PwPhPwH+MfCHH+p9V+sXEiz5/R8t9wScB94GvAIBk0u8vq1B4MeBz4h/l3ifPdbX/qD7eDYyGq8CfgRxSB639xPX9hvAkx/02lWz7h7rcHsT6I1xLN77eBxPc/c74t+/CTwt/v24us8Iyz4Z+Hke5/cUoenNwF3ATwDv5lEKRgP3IsHoq2n8HSSAPVQZHrUANlfn/YAYg//azN5qEuOGq2jdXS2Mm4+64e5um3T042eY2bXADwB/yt3vexDn93F3Ty515peZ2Q3ADwIvemyv6L9+2O+QAPZVMF7p7reZ2VOBnzCzXzn+4WO97h5rT3II9I5xLN77eBx3mtkzAOK/d8Xrj4v7NLMJGch/5u7/Il5+XN/TGO5+D/AmFI7eYBIrhYcWjMYepWD07/IYAti/gXRcX8WRAHa85/F0PwC4+23x37vQQfZyrqJ191gbyf8EPD+qczPSnvyhx/iaPpIxBIfhtwoR/5GozH06cO9RKHFVDJPL+K3AL7v73z760eP5np4SHiRmdg7lWH8ZGcsvjrc9+J7GvT46wejfxeHub3T3Z7v7c9Fe+Sl3/3Iep/cDYGbXmNl149/Aq4F3cDWtu6sgafta4FdRrujPP9bX82Fc9z8H7gBWlBf5SpTv+UngncC/AZ4Y7zVUxX838J+BT3usr/8h7ueVKDf0duDm+Hrt4/yefg8ShH472nh/KV7/eOA/Au8Cvg/Yxesn8f274ucf/1jfwxXu7XOAH3m8309c+y/G1y8NG3A1rbszxs3ZOBtn42xcYTzW4fbZOBtn42xc1ePMSJ6Ns3E2zsYVxpmRPBtn42ycjSuMMyN5Ns7G2TgbVxhnRvJsnI2zcTauMM6M5Nk4G2fjbFxhnBnJs3E2zsbZuMI4M5Jn42ycjbNxhfH/A3kN6xQaSccTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUkAAAD8CAYAAAD6+lbaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOz9WbBtWXaeh31jzLn2Pufem5mVTWVbWX2DKvQsFgCSIkGKHUhRAoOiGKRlhRraiLAthR70IIbDCtkOO4IR9oNlO0ISg1IItELsbNNiFxL7RgRBFFgA0VQVCtVnZmWfeTNvc87ea84x/PCPtc8FUFWgCJWUD7kKFzfvueeevfdac445xj/+/x+Wmbx9vX29fb19vX19/cv/p34Db19vX29fb19v5evtIPn29fb19vX29U2ut4Pk29fb19vX29c3ud4Okm9fb19vX29f3+R6O0i+fb19vX29fX2T6+0g+fb19vX29fb1Ta5vWZA0sx8ys18ws8+b2R/7Vr3O29fb19vX29e38rJvBU/SzBrwOeB3As8CnwT+SGZ++n/wF3v7evt6+3r7+hZe36pM8vuAz2fmFzPzCPwZ4Ie/Ra/19vX29fb19vUtu/q36Oc+BTxzz5+fBb7/G33zjeu7fPiha5CJAZiRJJkwMWYkkRBABhgGJGaGm2EY7kYzcEuaQwKRwTqDEcmMQD/dAP09lUQbhhmYXpzMJCNI8pd+j+vX9nMiU+8jjVNCnklm6mXs6n0aRv3f6Wdi6O/qtc3qvUWQmcwZp+/TbamfU5//9LX6d3r5qHuXRL2XjMQNzBwzx93r50DW+9t+FmaYuT7xL3k9u/p81Odme+3Ufds+WN2202c63cPtQ99bvRh2dfO4+ij3vL759mpkAATJJDLqE9x76XOT9X62n23bJ6ae3+nLVw/j3p+S9/xd/vLXuPoefsnf5C/7KafbAdv7v+fh2+mrtbbz6lkmqe/OrGfK6d5raW33xn7J+tD3/7K3xa/4wtX9sO3p6Vvy9Hy3P+t5nfbIPZ8n0fdvd+/qVWoBZP1ku/dFfsnyr/uY9U+untX2hrLWh+mR6vfte+75YfoZV+uIe75ne/Lb/4/T1/O0cgFuvvrGK5n5Tn7Z9a0Kkr/qZWY/AvwIwIMPnPHv/MjH6c3Z9657OoPDhDs4bx6CiwmX01iPQDiY09vC3nd0YLc3HrnWuG+/0uxI642L9cgrdy546c6BO2uwprPORqaREyITd2hmuENvjlsjRzDnynGuzDGxNJp3lrOFtixMOutMjmMooK2pzRvQpm54uJEOjRVfOt4b5k4DehpkMi1p+86uNc6Whpux5mDOYK6Dyzt3MYeVldZ27Ps5jU4MLcdk4A67ZaH1jneYMchYGeuRy8PK4bgy5qBbY+k79mfXON+fs6NhbgQKqtYay25PW/Z461jb021Htx1mHdIhkpgDmArElnRz3HVgHG2SI8iAbEZ40txpCYs3OjsFCquNHxNLaJY0S5xkcTsdHu7OvnXoN5h5xhzGugZuRw7jDS7jDuHJyMmsTWIxGTHIMObUoUFC712HgxngOkQjTodU7TwyIEIHSzNw11YK13uKOsAisv5bgRoHN22obuBpWCgMToOwxszGGkEmeG3+JZ3eF46ZjNlxW1DMWEmfZK7M9UCsgxFBWOBm9Kaf3XvH+xaeGnMac0xiQsMJSzA9M4WJSRI4pnXTHMugYVjCjOAYqc8dznqYDFNgbgbNnGHJCGdMfT91D8KNMLBMfA3dxzTMGpiK1gSmKxFwtA9y1j3dglZz3ABLAoM18EimBV73ctDwmTAmGcEYg8hJa1syA80WSK/AP3CDpLNieHMWN2Y6lhPPyf/nR//iV75erPpWBcnngKfv+fO76munKzP/BPAnAJ564r5cxyTmxMxo7tjUjdmZce6JhR7SwWHNlWx1UDi0ZuzNyTwyA9KNdSTHCUnDvdHcWKcRUwGYNAwnMrHuuJkWPkMbOBOa12bWM54JhIE7GYGlM2MycpKZONrYbo4DkwDfDsUkYzIDHYmZDJIwI3eGeaNFsGayxiTXwNtCxIq70wJiXQmCGEbEAAt2u8aYQy9SQSB7I7JfbWQAa1hbaK5gHFlZmzkR8yoTCOokd6wyzkxtIAIsDbLTHKY+IdCYORmRMILIINKIDiMmrc7qsImhND8idA8rsGzJaqCTPyKZoe/PnPoVTjK5XI/MqjLGXIlMVkJBLwLqeSRNP+2e9M7MaKbnfsrO7slGMrQ+MkLB7ZQx6XtOQbKep/5B6hBsTm/Grhk7a/TU/YuEA3CZBtOJWfeTYG7rYgaGK5aYEWnM1O+JETTSDNwwd9L0XsigZ1UfAZH6sE4yYwULzAMsIPxUObgpmFoku+Z0QxWYXgLzxLrRbWGl1lEkafo8Eam9FFnZqOt77N6MrrLi1MFApJb+TGZmPWv9jFPlVg8ryKownAzd7zCwtj2MSc4kxiBnMDOwrYJE95LKxoG6Z/rvXrl7pilQb4v7G1zfqiD5SeBDZvY+FBz/MPA/+2b/IGdAa5BG80ZfGmTSaCQTi6ENaHqQZMfoLGZc3zv32YrHgXUY0xYudf4zgGaN3qCFbsa2oBLHWyPCGTjNBrDqwVe555gWDkZLx6bK0aajipnB9Il+WsWXCLQEkqGYSsewgHVOLYp66H40JsbRO1rLRs/GyCBNp3DPxOcg4pLsTuSOyMASIgyyKfuowGdhWBodJ9qCZ6O1Tmsdr0xtTS1CcyccGrCOScRKptGtikd3wR6x3beke5W/GcpLIogEm0bMJGIqgwkt0dmcg8GKnqc2RhQMoIw7qQzMgQrsbs5xXSr4g2WQOer+oTIwnK2qjtT3kLr/25ZoXGWNEXEqezNCAakCnwIyQG1mU4YWKaxHGaQyJGutamTwhJim4NKUWS8N9q51MyLImYxMZgYZdnqttMrG0Ht3jHDH6cTU68400hvKtaeCttWz2YJTqKStt0xmYBl0D9wUGMNgRK2VCkYzjDWDbHY60JcO3tsVxJUoQ0XBccxQFrll6huUZIK7tkw7cwuSqlgsFRDvfTpWD25m3QMMj3vhjSm4jSQa7NNwghbGHEGMeYK3EnTobDCYDRoNrGO+w7zWTRgxJ5OBt4nZ1CHyDa5vSZDMzGFm/zbw36L9959n5s9/438AViXkGKGN4mCR2BgsCZduNJJ9eGVIYH3SSBYzdgTGkbHC3TU5EkRzJo7T9DAswCYzVSYLB1l05qQTOcicVF7OPrfFnHiBIZGBZ+LeIFT27DAyBi0TD31vEpglbo2O4xnMCDyMNRJrzuLKyAzIMUgMz6C3hsfCzMR8kOkccc76wjruYLbHbAEL5npBtiTS8ey0VPazaztyGtYbtmtXmVAasUKzzpGjirTorJHQgrZop0UfenTRChPWQ3ELojbqQFhx5sQyCZtkyyqhpmARV9YWWaVSlXobLJmWFSQdsp3ur6Wp1PLKDm3odXPFbIWctCxMOpOgVe7gpO+YMZkZRKyEGT1ca6BDRJWeW3YzrHIOgQ9W2ZaiXioTDmWtmcJCfboy69MiVmYSqeol3Yim4BFTOJ9nw9MZsQV03SsnaF5Zbxt0W8jswGSmw6kkTLCGm1cprwNhzMQTPIJmKqnDAprTbKG7sslohodxHMla984zGDNoEacDfVlch7o54ZXi56qgNCGjERHMGcq6K2BHVxhXYb0BiEHkJOknfDwzmV7wyOkA2oKUFTCwYbJ1gpqOtln3LiKZU/s4CGbTPTRT9t6s0R0UNRqB9q6KT1WrglZGYedXYfmXX98yTDIz/yrwV/8pvxsHlZSHehiWMJPGjt53dILV62Q35WmeelCHY7D2YNca63pkPa6sGKslKzBHMMZkjEGOqJs9T9kQGVgksSrlb64s8iIPNBfYlJ44A0swBu6J0VjMtCkssQia0js8VDo0V4AUsDyVhZgpW62NkZHMOevk7WR2zIO+6OQ7HpKPPPVRvvcD38MLr7/KP/jZ/xrrwQgj5jnHaSqVClM1C7xB70aOyanp1DoeVpt0EmNAa1xGYjNIN3omZo1lroxUxhjhRH2W1iowFYaozV/V8j1lkTfX50nIVVhlNL8q5akA47bBgSfwPgncvLIJldokpNeJb4lZnjC/ZsauXi9SqBtpzFyvSmMKB52BcuwtSOpzRSCopj6VuZ6f1+YkVVpvPyNsnEo5JZ/CbSNgHY6HKyh1wRWn0nk7rdzwZjCVIGQYvXW6d8ydmTDS8QHtdKC4yv+tTsUYc141bhIyB96S3g33TqPRfaW1JK1xnMbMZE4qcCi7myQDBdnj1H7Z7XbMdIId6xrMmcT278dkFF5vbWuS6fldNc4Cyy2zjFPGDtCqJ3BqFMV2LysT9G1NBExlpbnh1WzVg56RudO66fCoEO1W2Uc1IjN08G6l9ZzJGAO6CVrIezCZX3b9T9a4ufdyg12GCiNzZWRmpHcyOmM4acaYg8s5OSQIllZqflgnr48j5w1gx8zgMJNDBsdU2TiHMY73PCgznYAZrMejwPR0lXOpTKv5pHUKY1STx1tiDFRn1T3HCNMp5kxiBJ5Grw3mW7uxQcxxagRlTkboRLNJnfzGvjeSlZbw8P1P8r3f8YN8+PrHaHcOfOzjH+P2my/z8898kkO8iS8Lx+y0MJh6zV3vuMHSHXIwpzZhJMype3CwyRLJHCtrc5aACKA1lUKZ5BxkHQKc8CIFzkwFEjeDdGUy6QyLU5c8VgU3waVXXWyrL5pXjZZWi/mezir6d601VIw0kmTOo0pJM1rfqTQ36K6sY0xltm6N7qlSisK2tudh2hRWzQHYMF2rIAXpoWYNqejJxE2MA2FsRyytgrmB6V5FGDPgiDNPGXMwojEqiG7ZjHkUkwCYiTNZqNfPpO+cXXZGDEZsnVhjFnZvbqR1qr0BTFrvpBcGaEZ3pzdn6TAjmWG0Ah9jex7WGKH7ZAk+tEdGzCIJOsdVjbAIWNfJnMnpUWWVydV8UeUwK0sW82MLkoEgnn7aN3l6r5xKZeH/Wdm2cqNkjqxnyOngz2ZYN1q/YplsDIHA1d+wKulnkLniqcQkYjJG0HvXPfkG11sjSALnPvHeGaYsrNnCQCffJcllM45pHCI5Jsw56bUBIyYX3TkMp5EcY3I54XIGhwgiIMfkGPoFVwt1oxBtD7ZX4PQIsEZMlYzuOuVb68KrSDIHbGVLdcgzKYxP4H1aCLOqg8p70mqjDIIZCUMYGLtOLBPLS26c3cfH3/f9fP9HfpBx64KvfeGTnA/jgfs/zu/63n+J97/7/fyVf/gXuRuXrKmSLiyUCRmsQzhhDJgzC9cBj8pcbRCtETPUJayFmrNwHhtqaDVldmla/LOyAyw5zhR+lh2vQJfNdR8iYSnAXx0vDDXIzFEmZY7boq9juAVbmyLTVSbNLYA6WDvRpdzbKSsNBvjAacKKmbgN3BozJsHQpiOVQ6YoY16NkjmjAn5grkaZsNqtPAyiabHNqQbihm3aRtvRTlcgr0ydoaCyYXJp64le1Zpynt6SpWu9tAwWBmHCyGfqYPXuYlxEZbEbhurO7HV/UNAPXNWMGeYLIwMPWMxpHVo6PZ2GEXFP8DK/yvJy0NJhNjI6cybrgDGCOWDk1miprM51uFt9ZVpBNqGMksoRqmUtHPZEDXO8yvDtykx8a3ACG67RXPfZTYyU9HZikbgFralZ6V4dbav3EqOwcgXGOYtSFFV+p/bFN7reEkHSHHZFk3FLZrgizagumkMVuYQ1WpV1cwYHkjVXYhrDd9gMWjPCxa9cIzgeZ4G6avk3byy96eYgQDuZtEj6BsHkUFPEqjPWDGsNd9f3h06nllU+G3qAYbA1TrZOJJAxyVQZ2Ztwvtw6atZxX4gW0CbX2oP89u/5F/jYI9/G3a98jWO+wpOPPYivxuXlJd0a73ngA/zLv/Vf5a/8o7/Jq5dfI3ww1snddc8xdrSEVmXOnAoYvTU1x0yQwjAI7zCn4AJTd9yjjnH10tk4dZkrl/NS5a8Z9KaAkzssRe9R1m1YE5wBdXKkMtI4dcoTaxvYX130LYAVcyCtkSwYrTaM0X2hUDfhWJ6nTHXjf2YUyacaIlmBo5kpD05RWZqpibT0XrSkosfU7/OElc0ThtZcQe7E7TNUBharoaWTE9aYxAw1XU6Zd9A8sF5Zc05yOrudst4lGx1jJng1K6aJTlbcl9MzOnEo18C6V9fZFaBNGVXPOB3c8zjprQ4bVxNyppqGgZ2aVFqtnRHGpNUBVyf/bJU8bL1gw5oXDVJcSkc9A22SPN0n/WztpzRV1+71HxXg59YnqMDqlWWLBQJLJSIqQEyHfsETZhNvDWv18yKYpoMm5lTJPmYdMLp/1n4l1/TrXW+JIAnGsIWd72ld9BlLgdNrJj2cXKnTtTPHYExlB7Eoc/GAadBbB6vTFsO8g19lU0QD7zTfaRFlEAjHwaw64DA9WWuPCwpRTy5yA3zr9KvOWiD6iI2EmDQCa9rAsZHNQyB8tQaq3KM67MlZ2/Hg9Qf5PZ/4Pbzr2hO88ZVf4HzcZul3+Zm//zfZ+f28+yM/xK3DBW3feP2F5/hXvvs38uLxLn/5H/8t7sYdDvMCi+AM49yh+cSaTuHWHFsaWGPG/pQhzhhFoWi01mhdv6v0LtoLgxkXjHGXMdQBb8vCstQiZdGiv7olNFPwmVmAf1Zg9ML8ALeGtwkFsGegJox30rr+OwvXCOGmW+mdJm6WslNtpMmpniWb1pal46YMRFSt2lyp7HXj+Z1kArYlQFbwxCzwDtzVHsrswpGr8eR94q7mQYbrvhWTYhTk1nOH4+TMU/aTlqwTOknEQdS2Jk5Hm5OVWaxUXVG46AbzYJV710afzMJmk5hZWbMrAUhh1zN1AMwo3Nh0f21WkLLG2LI/A4p3mRjpjqFDdgtYuf1KBCWFOJaKpFnMg3tI8FU+u20Ege1/WzTYdpsC74n6ZFbrpzq7GJYNtZl6oar1jFMV5kzt+xwDpqCnLSi6Ja2Ohen/41OA/ntdmcZl7Ih2jbbbidANHGJwEStrGtNVCqYbx1w5ZvGsMjhvHW/QunHWFzIm7ThoiOyczYgcelCu8446kVsEjcBtakOGSq/WnNk63hrui0610MOb1V0/AflWFN16ym3rbFc5HeHMUAPEqiFhW3nunbDJWV94/PoT/P4f/P3cf2ncee7zvPb8Z/HDq7zy8gt87lM/zXve8x7+3td+lIv1yHq84M0vPMev//BH+eAP/T72xyM34wAYfU5sabTm7HbO0kUk96UxvTKIPBfXkilIAWXK5g1fFno/w60Lt5pHIi6Y88Bcj6zHO0Qku3lO5xzbTQU6gXl6plQphuCOWIPIgacONTWvDCv+QbYr5c/RVSphzhIbEVmbKIprZ+Y6mAQNC0MzUb7CdMCJ3KyudoEGp8MTwAksJ71w1FPHPUUlWYuknGNAXB12WcFyWiora8bSRYhPV2PJgytKWHVVk0FkF23pkERLCQDmSrbOYBV8UyTgXnh2RrDOLErQRleiIB5nbDRsu0exk8kxg5ztBC0tzcWygxP8QP2dmASKdLMyPtsiX9Nem5W9WmaJLxKYzKZDIWdiBd0YdWBiVZrbaVVQGb0YJlkZ5wmWhNofmJo1Tqv/Ru/RO9CqoeNkVjY79ZSD4rKOWShIPf+ICtZXfFSrgLnhs1/vessEydXOGXZG93M1tIAjwZHB0VaGTYH/bWJduJhXCt/MOOuNs52xb0msMLszcjIyWBxyabTuxd9Kgqn2i+nkxISrqCQDmnAc915B2xlp2IRR5b7XaQicCLqSFDrTjBGGN6kXPLcSsQoTV7nR9g5t4Ybf4BMf/B7aK6/y0te+QF7epN19mWe/9Gl+8cUXeOWN4MWf+UVeX+/S2mS3Nx6875w7DwR//Wf/G+7mHWyC255dT/Y9OTuD6+dwY9+5fn6GLXuOCZdrcogdlweIcLIvOF1lrznWF2zZkeFYTOCSmEfm8cjl8chhrBjBEo1kr0ZBm1hspaC6jFlpZRJEiF4VUxzV7bWiAPWwpk5388oHjJYNs6WoHcroPI9a1KaMfcRgVDZ4nLDWmhqzSP4I/M8KaOmGzYJMlCsyC5ui8NrMZCYcRkjgEMIyN7L+FmK9q2mwmHHmUhhtGy8qAJiroVUwHd6TXdd6GCEi+VjhEJ2lLzg7zPZ4OgvQ88BlCI8UVNFOzTDM6fUpirJJ80ZD0NIgWENrkkhyAq4s9koamjrYTL9UGceJFqPPuvXxObE0MINWCUdBEWmCByZFOK9/l5WQnABlU/EdMU+Hzr1GO4kodtlc5Xzt8VlJL0hJY7ae3l1ukE6a2A2h0hoXk6IVGGDSNRdMJCjPM+nfOJF8awRJMFZbuFwTt8HMY512zpjByhARlwk2WRbHTN3hs6XRbLLzxrUOPaUDGZYcmcIYu9McjhmsIxijuFtZKhrvVQrESdLkLhWPtQVsIRABNU5NkKRbXmFhW3eOYKYzp4DxJTn9XWSpTywwU6bXWuDtnPc+9gGefugJLp/9Iq+9+Au88NXPcO4dX513PfIY7376GvOwcnEIPvfqc7QHrnF5/To/c8O4GysHF65FDnbLnrN9Y7937ru256Eb17m+2+HLjiMLh+ncPkxuW3AckHR6Ozud/Lb04kCqaTFzJTKYMTnGqPuaTI5EHklWnegmbY0CSYdYKytHDJmoTCxS3Y/KzNQRlVJHJNmGRIxdHFcXVm0OPkPcSSTR632CByOTGApuM0KlcGWaOvVUuoEoLMQs0v/2faIU5UgpOVCDYo7KjjxPpWVU1taa0xcFyV3rTIuqyg3aqgBR9BVL4dm9G2eLEqIxOyMmOY3D0ZijMdvC0hemCb/VfVmBgT5FZeomPC8zlCw00bOWJobDBLzYQROK6zoYM0kvVZsJQ9RJf8VhNNuK2S3/k7oLmwpIJoretE73hseoIGkMh+ElSTSr+454ifUnPW/RojJNuG/BIGQWburqATTJD5pScurDiw7VVa8HkxgTD92vmbPI6XHF24zcQBqoQ9EC0rVe21WM/hXXWyJIhiW307iMgR0ma6wqZ+fW4ZNEbCaY71g8aMvEWDGb2oiLqayM5DiDZSa7DfC2JswtjWjF6BihUxnUPCtK0Nw2o+cmES9MUliSNKJUhkjhdx2BYOour6ZmiYwBGhZSCSxtwTIZZkQXOXxpjev9Gh9657vZ5YJfe4jZnOdvfo3bdw8EC9d8oV3f4dfPGbuFs/c9xe39OWvfscMJjnSH2ZPmC60v9EUqo947u92es/1Cb3DNjGMu7Hvjeu9cHpPIRssdK8bB4JgKDJ6TEUcO85LLPLDGXQ5xm8N6SaZxEQNbnDbE9wxf1BxKZSdR4GRrBrtGDCcK9+reBT2cDpbJGEd8caCLXF0ZRsSRtCPYBbBiYmJL+WOdycByqHwOrgLVVrpFkDbUBKxqYEu9pisi5Ex8JMzi0GWZomTS0hgJLUsdpPYq3WEx2C1emJzoOWFAr0CTqYyYhi3GrjfOTFlnxxluHMOI6RxpjFhYWciuSuSYTZABBSsoZAFap8XNEGzQGq0ZrSmj7KvRYnKk1bofwmzTsVDwmFvDpuChtCDvbU4BtCwajVRSZiUzLerR4hDpRDZggk+iSnBC3FarYJWVZWZcBeXMPGXCBlh3rDead1pJfJWhC27ZNu00I32In1vBz2cUvFEHW31/5CzV2CQL4rHpNNehEFtJ+HWut0SQnGbcLYL4OA4Oh5XjmIyhhsPSqRsEfqKLbKWwYTaZzbhgMpdWXT6B26J5JRlOmyKgt6iOo12l+ZuLiPsVRysEphWxuLMCw3QCtaJRCDZWtpOhDNY2OVtMwuz00JxaOK1eu6tZ8cC1h3jn9Ye5fPUNDnfe4IUXnufG2X3wyGM8F0dursb0hdnOyLant4b5QgwxoL0Lnzvb7WhtR0d433GFu4fk9nFwtl847zsW6+xT0MKZO2NXfceYrGFchnPrGNyNwSEOzHFkPQ7uzEvW413memBdj8xpXI7kSGPaGedne5bliKU4eCoHN1KvuGrhRQfKVLntOpiygp5ZwhySUJqrYXPS0h+JvKQTdWgVDeSEFRuLddYYwvFGMOYUHcSyKEzqbGaR0DcHpZypn5EoGx21ATftiFFrQNQjLyWOOtTiD64uSsxqJZMrLXJrwqOzGikBLMtOGZqIBvIaMGfaXtlkbAqfyZyo2ZdNmGZ1dbf35L5l7hJKTJfeene2YzpMG+Q6wbri1wb+UUHJwHJIomoKVaJU+YngvamQuAd/V0UQHJHiTf+TtiUK2tgyPiiFUT3nLM5n1ps4yRXdoJqHXkwS2CAQ2LT4WcyL3SgGQCYxgzEPtBlqUgHpgnBmSko5Q4d/piAHw0XoV6r5DePTWyJIlpaFGSt3D0eOl5OLdXCcwd6NffMqg53F0abzDXRVwDuMyZqwzIlPdftiM5pwMO86cYeaKnidwZUtnFxIKoMgk2Y7Pcwi2uqkdNIkzOtYlUSFe0GlnsI9MlX2LTa16a0RppMrN55fNj7w9AfJyyNxvMXtN76GrRc89dS7+druDI53GCOZR8gjeFp1alfok8PinNmOxTvL4rgrc10D1mNy5MhlqkPa254b+4YbnGewy6kyrTvDkzGSi2lEW1nvXnCMS2KuzHHgcDxwuHtBrgeOMSVAaSJc+24PfQdNZiKbEVvr6kJaVdHZskrE2hhpJ0MFr8aJ5JlldKLcRoTt4o0M28j4FeiqhAavhoadOHBMkaqtmqFeXEu2A7LwZwNOyg9vBfhVM4diIbQ84VuEAm7vYiocZkAvTDWt3rNh3jCcsOIcZpayx+m9sSmvpjvTGmOazC8qbYyIeh9bzNokjF7STRG2MQXkQA0eVbkOU+wAZWhxwty15+oykD56kFMiCcsuZkZ908gpmGNOUZK29Z7BcHW8u7WTo1Z6Sq9ezy1NFCO/AjYFg2yAZESVu062zZCmqZGYnOhza7DhNiRH+tyEpFvzJ08BvUhS9zBRVFlccUOFw7QmxVD7xonkWyNIgtr1h/XAYUhLeozJcUr2VKJFugtzaFlK3Xoo4MyhoHTMYJf9CgdqrkzLF4ZPaaujcJ1EGURxHpWKU24qQqu8tVN5QZUogzKICJVW00yyvkrnx5jMOYg58Jm0NmltUTBgy2KNGIPz8wd539Pv5/jsa/hy5NbdF7nxwDk8cD+v3zqCnbPMI2SytiOzBX05Y7/b0VyHw2KNZVHnEsR/PIZ08JbB0YJld+Rsd2Sac32/cN6dyaRZsN8lrXfiGFxMqXAu+5FLP5B5ZIxL4rASF4O5yjorgWYl05mDnEdGLPToNFOgzOi1Qa0MNq7oH7md6BWEJIqYyj7mFFZUCiSzqcU9g9UEY7Q02b2FS1IXyXHIDGGsK3MMlevFi0O4vpoRedVE2BQxWU01XFJCSkKob5O6w91PMjs3hyluojUj5KSiZkVJEDf9+ZzF+03jMAZvzpWO6E9rNi5nZ9DEV52B+dTPsyHtea4kgwwV2t2qrI3N5GQwrSz9JuIMpzDFyA7RTpLZrACRIJcrkoFeh9Lg95lE75RdAdPVvKqjCBCP0wtPnqmO/o7ltAbDCyqpvTM3F6FQRjfrOW/d8IZjXTzojTdLZrkkxdUhRpl85MpxblxjcWOtpIibCU7cA7nonHSicTpMM2GMpC/3HBpf53pLBEnhBytrBscxOc7gMOSrSBhzJsu+E8MYsbJvhi+dDGV27o0Dg2OYJEepDexFnm3RsN5ZurMsMmuYc0JMKUUwYhY9IMELO5wIGG5FFE5TyTANLF168Jj0kM1ZzqMW3QzmNIhdyZ9SZUc9oOZ71lwx2/H0Ux8mbgVtwvHyLpe37/JQf5AvrJM3p5pFRxVAmC/sHK71zm5Zyglo0Ppkqfe4ppoHswK1WWdkZw15MR4OkwjjklYZ2Uru4UabtCbq9rJMdj1ofZBxQR4vsaPsyQaO50pbyrw3YK6D9XBQ1tXAl8R6p3MG3klfqkm2KZM2jEubsiHyb+YAemUeCFMi6SSTQdhgMIomouZYWjDduaRxyGTmUXQviv9YAdGzFTnZwEsCW2WaKoAizBPM7gr+ijSikFH8v6xubemdjcC9KSiUG9JMZSaGizM5rGShhoVxJ412V+sTjDGCMQVJmJcdXkCyMkO/IkZZxxleOnJxscUJnhHgJsXa4pzhYCsrkLMOiXQo3mdYEDQ5A+WRnEdsVvPKgkGchBC+pjDOZuxD5hQywjUOCQf0uRvBvhlL3ywGBXfEDFzcjiLo25aJFIVLypmtJtjgAO7xmZxZtnzltSAhR7BW6GxZOKk504A0eW+StFTjasTGrOi1n8W7HhEVB77+9ZYIkknqtMwrxn2zruZIlUtzihC6Q9nHimRkzTfqgbqjacINMdlAzXR2vsejlDa7xjEPTIrUPeu0Lrmi3pDOlSa9EhLRBBPJ2Y4USTcKpxmriOpzVGIVzKFFYh0uO+xicG1tWJe9mlnnrN3HJz76fcSLb7Lvydde/Rr33bjG4sZrd17mbgyGIVcgkQFl5LCustwyqWqa9+Ie6v1A0l2qEhbpdr2pqzlK1ZLrsQq0A5d5yTF27L2rS8yRrTaRZ2JWsAdDZPNdt6JkdUZO7q53Oc7B0lbG/pKzfefa/h30dq1oKburzqltFW2TwcNM6azxwp2tupIyvl3MJRLIrgC7cV1TaozjVClWfXZlEQZbP9uriwnVoAv5OJ4aAa7gjWcZwBbJ3beKQwtKG1xAd9qGabogHxUj1RxKIdApg+eNi75Zn+VMmk2sqWM8c2sQDchOMBjCI8RlzU1/rmcy5qbeQil44ZTiCSoATDOySdWjLHxCYY+b4mWOIeFDruWYpIrNWqNNVQiNYEdnV/ix2HJ1nIQyyTVl+9Ydzj1ZHPa7xoXDxZjCa3NyNJmrjFFNKN8kilEcXVn02bavtiwyKjAWjGChSnGEoJmSckgiGveU9XXsYdSaUVLFLBZCQGvSsx/Xt3qQjORwODJGYcUlpyLrhMgghmgmIyFnsNs3dumnLMA9aMWNOua97sc6aXteldFmrm5wwjBhJ2FRHU/hQKIKpOgnm3Kj/tfq3isQB+s8yiF5PVaMMpgFJo+BnzWiWzH8C7Oazrc9/R08vnuEW3abV15/gXW9y/1n58Su8dqtF6XusSEpY9FGzlovLFTOQQbFN1NG3ZqD1f0zKWw2/OuQUxsRWNoVOftwPPBqrOzawjqD28cjx6GF3Ux2cG6wFC1DAH+coIk5VjIusNzhfmCNM6bv8X4f1/vWapMrjfDYCiRWHc/QAdetbcZIonUwSm0DkHhTpWD1WdOs3LJVtinOlTFF+mmDbfpkBbXCrKoJ09yKT6mA52Ynz0dhXUoqN8w58RORXU5CprKbwlgR9cuqWbPBbqByPUMHaIBoFk3kZznR7LDs26aAVAlMTjIGVmbCcwwZ5oL8K5ufpJFmnSsncKvlrNI/K4uTrjk2gO6E+al1UVi+VWfeG4sr+Ox3CyevyG0FFM6ZKU5ws8nek30zdt3Yr3AYk+Ncsems6UTU91cDTPQSeZ0aCpLJvNKVR5bdGoIwUmtzFjzgteXylItuh0rdSpBSqNYFBQUA6sTnDsY3DoVviSAZAcdL2NS+rasjvK6DmSmaSzTWFBY4I7EWJataSrmx0qJByqV4rfIk5+ByHGjTaX2hWaejkQFrxJV4f8pbz65gDNlXAS1Ej46cNGvgddoxi7qS2NBr5RS9gBNAPNhdJnm+cNuDeTywb8aD++v8lu/9zeSbl9jhknG4zX3Xr9NmMK6fcZEaedCqw+oWdEUU0TQI0qZOd5eKpbUqLWdllahP0Vsnw7kcK7ZAYzIs6O7sTBnOxTq4exhcTuPuSGwK0+u9sVsW9t2IkadyKMw42iTmIMeR5MDMI97OsLawRGONBViw9EKOqtzNKhM3R/eEzXNSWUrgnvLJjBKbmRonPfdiv7lj3oBB88auqpFWctKJQRaVrPAnObCX5DJrU22NnO0QnSqtLZRVngjQ1VWN7fDcsrfIohj16uhS1dBGcNZzyFQJTYTKz8pAiSAqi17FFSjDBflWZgxRd+aRmGVcmxW8E1SHZQX4hi0l/Uzl4hujIiNPjuhCMgQtBc6ajRlq2nix4hJnphgkGsOxtce0T2dBC+HFcyQZhYdHTogDO3f2e2N1486qADeGM6fLU9VE8iYozFPwRdTajbyyWIvcDDXKNYQhGXJ6Ycxd9Czbgn19f92nLCMMpiiDRp0jBMLBzr5hfHpLBMkZwd3DlD9khjCpLIFRUzbJHMSQ8YE3UX/WCOmv6bTpxCw9aBxPeNdxrOrGreD9krYsdDqEK+BaYxKsscqFpXG66R2vWTgqAQsl0sJgk0JJvzvnuDIliPXEB2xWp+K6YityKmkXfOxjv5NHbzzMnde+Qo4D186vc3d9k7Pzzt37rrOnMXrivhSILTu3NRSYcfFL2+Lsu0pga06P4pQ6RRYu1xqCMVfGGhx96sS3pUq2xuDIYSSHcC7DWFIa+NYmZ3vn/jgjSS43nCghszG9rNhGIzHcJ27O2XI/y3KN1vY030nSZ8oibKsOZpXxOcVjQyYPuHGcq5RSSEefBAsL04uHuHH1Sh9ubdBS8EqWmN8wYaaV9WnDDHVvzU9mB1RzBtO9ulJUOaRrHMaWp5iyxC37o4QHFrMwPtO/yQ2+iWJMyN3bMqQeKhxPrZPGTMM4qmEVSQ5hiLOw8pO791byQnXOhYd6A1qQNPqG6dYBseGRSSN8rUNWNbfrTYAlyxSUtd1ta51pMJqs+MZQ0/IQzoYyzpHkbOLojlXwzFmy+FDzLh3fLaysXK5WMIb4peaqiNJVX1GN2lYH16x1q8Ni42hK8mnTGD6qDBeMlWwMhfrcs3zc3erwFeZcHVtl1nRBU291nmQkHFZZnw2ql52wR5KuJjYDXU8bI1hTDQimZGwx1spEaiMgcJkM5lhFv5kqUfE9bh2qA+kmSskSjb1pIFhaRw1S4VfUw5ompcBsRg8js4nIizqecxrH4wXEUYPFouPNiyMnE4S93cdv/N7fxOH1OzCPGCtLh5gr73j0CdbzRfb5MWomzpQ4f6wQWTxJ9PB7ZSTam6fO7abr3aYVbt28dQZ315XRgtmcMxNNxaIR65FjBMOaiLm9cdbPlGy5seLkcXB5uCSPR8k2u0ZEpO9pbWG37Lm+v8G+n7Hve1pfMFvwNEbZfMGGzZuggsoUNocey0HM4A66r5aNyJRhhm+ZGWxpUViKE1i/5K1QWhqHbYyEXtfkEF9EbLi6V5vt/4b1SQetrHOjCm0u/xPxCKcVEdujnISssMTK5CuztMImZJaxuQxNrSFvhA+aCYqRytJOQTwx8QeLtpZTwcKsneg1W2Jqqb+fTEEhAZoNJJw1bYiao5pAw1HMyewFBKG/t6aBcG7YKsOLoxsXNtVVnpWi1WyeaclxDm7nIDDO9k2O6Jbicy69sFypg6xCz2kaZG69awqK2dCvq2zSKE5tZZ6b2XOVIidl1SZQwMtVansOlQtbU+c7LDFXpZP/Y49v+O99JRynqDiWsPZyGEn5+vVWaXnrJ4wlLCs4Hrk8qjzBQmWBi1aSCd3kqh1jUHAE09a6u/10Exdr7N3oNrGpE0xO0Ao+mck+jI4xikCcYXqP1XUEURkWW0hWWgy89M8Tgf1J8F0f+QTveugp1ude0GbtyXp5SVs6j7zzCV66c5PjGKwxGRtonTUoLWFJZ1k63Zy9L3KGln9UeUF4aaSjcDBhj2PKqV0Gv4Pr07jRVHrFaqzRiZD2NUNKkqXv8N7BF9bsDC45Ho9bkSL3677QfMeyO6f3c872N9jZGYtr0uI8kbIb7vMe9UPx2KBgDpGj3VYSZ43qTk4pZWhF/M44bQja1b9dMxm5ZV7aaLOGXG08OgqP2/CrTQEieK3hzehVEQR2CjqblVhVuoVBbtC04Yv+BWQ1Hgr/wor+U6VzjsIAtV51yG4tBUeZp56hlZEzJq5l9ZYUSIsMraA2tV5HAJNps+CGTohWfeXjacr4N4GDOMNL7cMhvNgUlL38Gm9bpwX0mfhE1VQi2CFgMyOWqe3k9kz2BzjrwbIYPkKD+aIrALuC1KmVUmM7xEjQvpobAnyCFdShzkzR6qaJE1z9gcggZk3BNAXnLO35Fv/S0aC+6ktYkdcL0PyG168pSJrZl4FbCCkdmfnrzewh4M8C7wW+DPyhzHz9m/2cxFjDiTHpU8AsveztW7HiU4Oi5jaeJIMMl2/fCJjQfNBcFl3KKOuEOumtQ7ysWnibn6CncD3V5Qp2IqNyIh0HJklXIJUL2xaDdarsG6xY6bIFoE/MRr1GyIbMO7/tB34IVvCmjt806cr3N+5jf3Yf3HqTuY7C6WR/5UhZYwHZnLbvspUr6/xJ4iKLFT4lY2LJI1d1/LroJlHqk4uW3N1pnGgMEZm35lhryKAjHPc9y2IsS9L7FGRQnaRl1znfXWPXzrF+Rm/X2e+uc7a7RreuMpvEprIAZXDVUJl6+jr3iwTNxGMQLv14xmTJGp42FUQoXl7aVbnr5vUzJ8ygV3k/bXPNsdPfZ4U21fLKhFo3WtPY3w6MCsQWfoIDgspykGHylSmD7tOGzckBqIJ5Gjm98Nch+k1KRWQpHqOEIVkZ5jb6FWDbyMJfq/dMc9tIDFqVyenA2WbwpIkj4LYC6nZTxsFZDZ3Nqmw1mJsM1EudU4dHFnYbBlE81yYLLGV7vmXO+qyX1TCbR3W/dwEcC6+Mpoml9Ww2McgpEG63DB12ZLI7cZM3+7dQsz/0tV4ke8mOqUMw5VyUgUWNzuh6r15Bthdiq4EAfiLZf73rf4hM8rdl5iv3/PmPAX8zM/+4mf2x+vO//6v9kJhyvZ42Oc9Gj8JZEuFYhS9RzZYgi8AsZYkNx9qkNU0mbC3JOSgRl077aQJVWtZIklIwVNZpdlrX4t+Z10LRahmFS2ZSp55wp03GKHD/oAWYrvJoQTM4XJ3Yp594mve964Pw4k1mHLhcLzisB3Z0znZnXB5XLi4uOVweOUad6m3Qugjjizvnbiw7zdVZq7MbleksE3apGSsZxjTjuK7YHEx3RuE1lsbqgxjy0BxDpWXimHdGbjhZr6VUFCpb6L4QbbBrjfN2jfPddfr+DPo14BptWchsjACOMi8uRZqm9yGeX1gQVkEjSm+b5b84EsZg5MoRYZhGXFFuDGiTbdiamYadxZwwjjVqIIoWUnjVRu5OIORC74QMlbOacZUFAtr8DKZHzcuuLM8AorrIheGFaEeGloVKZd03spMRomuFJkaOKe7fjLXMBOwqpS5c0Fs15fpCusutmw06SBngRuoFK5tiqrFh1qCt1bgpUlXKFFqc1ZrcWGmxsFuDbGxEbuqQbkNa8xV9HCtZq8xhQms0km6tIA7JgieltsE4HoPjDI6hMt6b/ItmBdzILfuuoDfVRJ0bX6kOAYWyqOQvuKwDUGbXhUk2q9HJWRp6mWQMU/xQ+S0ILDMIW6sK+PrXt6Lc/mHgt9Z//yjwd/hVgmRmYLFWeixi9zQgpJgZNTFNDi/GiDIfmKW0OEyWNWouiZFMDpanoffWlA3KfHVzsAYt9KR5BWgqAJc7iaXGOGxzmA9QWYjC7kZjqd2pTjgiPqsMFeXF2yIhvhnf852/gRYLF3fe5PLuGxwubrNeXtDDuLE7J8fg8uIWwaXGMZizazv2rbMzY9caO6O67fVoU+U0VhBFlbfKRDUyNIBhK9OLYJ9N5ehBJyuEyi7bsbTOYkl6cpwHetPMcmzPbjlwbWfsbWHXd5zvzjnbX6ednTP8jJnnWJNBxLg8ntgYUbzC8KBblCriykJN91RNvAiYx5U8BGMObU4zelcmCn7CARUwZ1l0qYT3wq1EfUGnnhaaXqfI16NArc14dTNS3hRWW4ZeXsnidKJ7r856VymcQwejAEC5RRWzoVmNlKiDfa6T00Q/NJY2M4gj0GqqZUkat4rH3KFYFdv7qsdOVrNTHopSWHlh1FiwhgJ1b4s07bVnZkbNn69vNWWGlLO6n7r6zrC1SnQ5mg+3wvCqiVL71q0zKhPMPApKt41qpl9ZI4dbOnhiW8aO8OLYGrSGKHSVMddtUUWgc4RTK6aCq2+wyfbfHXLRz/XNKb0WWiWdxQSZp9z9612/1iCZwF8z8U3+08z8E8Bjmfl8/f0LwGNf7x+a2Y8APwJwdm1HZ5UigUrDUQYX8woyUFrtp7JHA9J1mqZN1irhnEk3kWIpKZZv4swTETkl8ypDV1JOKrGKKjGLoO7FEp6pwUhXRqGQiACrwbWlHplyQQEnMthbF+VgafTlnO/62A9w+erL3L39OuN4hzheEKu8GvePXOPa+f3cvXOsKXfgvXF9t9CbF34j0jTr5OTgYU5OL0yrnRxsxggmsKY6vMOrNKvOvYcz1llTB2H4Sl+ge2Paw+z6Xe67nhzzaYgdtn6NY1xyI65jfbDsOvv9nv3+jNbPOXCNY+4YrIyZ2Bpyap8K0uEwejA6Au+LYxdbNgYnki8Fj0RJDaV2guwblrbhgXJP1xTlraO8Y8NKxKsbtVoFj3TbinwZ9W6jTs3iqtkF4GX9ltVx3mSM1Rjb/BeFawt7jGmSpOamslHHFiCak23RW6vqaEuU2pSSS2YgldHTsKK4bL1bDTFTFjURDDA2jJMaVodw+ahOuG6spmPq/KgDvrIzbUhlYPcS480V1KYPKZaoMrwgnm02EbFWRuYaB5JRjZnBnApSJwpUIkz2JAlVwJ9DIxZsFnRQYHUkWHPt28JRsboXNJb0e+hWQUu9pz4b4ZNTjmiwmT9tDbvIbZaVbZ2dr3v9WoPkP5eZz5nZo8BfN7PP3vuXmZn2DQbaVkD9EwAPPHw9valeuXfSm/nuhEfGVDcwigohI1eE8cRgXTVLOCzprWRGFQC9TkQrIjKhEZNbRytmEXznINfif5lzaaHSNWEt6kBGkmPWvGFN83NPFhdh3U0UjwypGc7O9tjiRHeeePI9vPOhx1m/9FWYd4XNlcQPk9Ft9MbNO2+y7BYWGvtlYbeIFrPGVPe5JvZ5k1NOltfhgnGM7d6UwW2VO61m9BzDGbOR7iyzApg0cBzbOQ8/8TE+9v7v5cvPvA4Xf4n773uR1j9Hb2e0/VO8sX4nr7z6Ast4mb0Zy/mepe2x2LObO+6OHZdZXVcXJhvo3la3hbk1aUINCJozY8q158SZAzzprdzgc0rr29AcE4cowJ+QymkzXh0+2Sjs8sEsXNiEXYZqdbZO8YypeT2uA8aK4Gx+ZaRi00/Zy2Z6sdnreSJqzdTrJ13vK60aSXJtN0wyzYwaDOfQrHTQ8sPU4dGqo++QanzkhtcijC7iyitgo7udplHa9v6gh5+GwIn2pKbZzFSVrjkSmkcZCh9HLzPc0KwootfnEgXOwtgi3hau0hfhw3PowHMjkDMW5kxzhlFO41Sgs9NBOIYO1JYOEeozpPDY5Oo96t5VKhlUhsip0SO1lTOsUn82zJtSqHp13ajkpyCZb5JK/pqCZGY+V7+/ZGZ/Afg+4EUzeyIznzezJ4CX/ml+lllivTIGio3fAlzWZlgwW30JKwoGUOa5HqKtrAhzcKfmIxcwD9UBVs2Sw1QG5WTOUNletasIFDqNeqiEWUwluTrwV2oVq+DZ3ViaaAdyclEjgAWWXYMOH3z/Rxm3L7j95os4ygBmTo5jVYl7vmfYkTvrG/RF/NCly3Z/1mk8svCxGXQM3DbvBHH8qimyefRpFvbAbc/FzQMzz3jPR76bW8cLLu+8ynG8wtInfdmT+Qhf+sWVn/r7P85Yb/HDv+s1cj0wxoHpl7T1Lr/4xYd45InfxFNPgh0/TZvP09JZo9NmB9/DSDwXBhenLLFV9nEi/EaRek3eh1adypjK3qPNk0LJ0hiTqo8Sq4ZehAsIpLDb2WozF2a8EeotToEtUcbjreG2ILMRP0EWkoxWWW8mlRFoaFQZXGyYpbwZN16DSP4Khh3LTWOOJkp6q06quI7dW8FHyqw2l/1NreUmLDETeRhs+bLXCh1DJf2cck7azC+a7o+Z3HnMiulQ7yWNYmtAhtqPasholPMmDa4oUlWXV5MpVF1NU5Jw+q4yAgn1AZwsVVIlJ1bmwVaNRZxT8Dod+kpmYkxGlPQ0WjV3SrFlSTQN8FNmX5l+vc9t7npY1r6vVLRxlf0XxMYGyVCf+ZsU3P/MQdLMrgOembfqv38X8H8E/iLwrwN/vH7/r3/1H7bZaukUb9b1oHsj3eTwM9WFzVYP18A9RH8ZTnrgU/wzC40pxRHXy7QqtjLdI4imDJHyHVyRNtdRWRWpU3GEJiKenM2cIru7Tjsh/rSGVCJIV4oH2SsTQQax737iA7zxwsvkOBCWMvGIo0YjpHToh4s7Ar9Nk0cmakO6NymP5srmPdZqs9uun0oOUX+Sw5ysMWpxOZc3V378v/0pLm+tPP2dl3zvD/4w9z/y3SyPHxkXr/Hyiy/wmZ//KndvvUxcXrIstznf3zyVY7LSWvnH/+jz9PObfPiD7+Wf+4Hv5cb+jCXf5OIA1o2wJkeb3OMta/5JYtFEVWnKdN1DM8xd26wpyWJWALQRtJzk4iwxsBlEurTMlJNPbgdWnA6KDeTfkg0r/FYwTlFyDMw7TsezMYyaUaSN7wVcBYWfAVml/Kk0T2dajRROI6dpjpHJ4SZiCxCtgoTWTMkQlMXStE7Kp3gbS3x64wgqYnv9GKV3DmIesQqQc8oMpjVnepZnpCSKR4S9WjU93Jo8CTLZfDXX8n6MjfdZ9mozakSKyXmc2guCC4RtKjBNmqcGjmXRmCLlvVopWkQ1WyLIaGrYmVgnmykLkVs8Q9xQ3YpuRjfHujHcmNmwEcTUmsIdJxlD8krKjhCPDRCo/WOEe62NBAY1//dbZpX2GPAX6iZ04L/KzP/GzD4J/Dkz+6PAV4A/9Kv9IDPou07fOottUdpcHasw2c33oS6uYGQqoBbWZdtpGyo/GnhTl3vjyGmIkV5itqjB6MogT7ysyjY1VS/wkSrdvdi6dWVKTkYFUDxqKl3XhEIb9Da50Vxzi7jOQ/c/xu1nXqYfb5+A9nm8xXrxOpGNZsEbb77BcT0wc0oz2ySnMhM+uTM01c6l3vEurW6o5S6Qfx2MYVymrPR3LLzywpvceuMu+0g+/0/+Ia/dfpWnPvAJ2vVHOL9+zrh9nWnnXKzPwHrJe54srJc68y156bXg5ssvsTu7zbPzgk/1R3nyve/kA0/tWdZXOQ4NFkuTIxG5w3tjSdAgtiNQFmYOa07GWClefFn8K1C0BYylpt0VxWYafRgiZa+FTcoB3SQur40hWtCJSB/tVD62vtC7JHxQkw2NyqA2UrPoSrkmgyyNv6RXG9PB6qHo3tfSo4nOZUNlOGoSYlaGG+hzmFgH02Tkgjsj5Km4qWrUXdgyJIQ5xhChwqgJhkdmDMZYKwjVlEOCvnWCs/iEfg9cUJ816u+7IzqWoglb2pml97ai42xZtPZmlHNWTTOaA3o/efVuP047JlUJTWBC5ipslcpCk1Pl41YjJQQ414gT6dOlvqmDI1I0O/eSbE7SZmX8qrCUSLViP3hl/Tp8qEbV9DJJ/lZgkpn5ReC7v87XXwV++3+fn2UUmN7kApOpdF70AGMbv7CNAzXyVD6oy1nkWt8oFll1uW6II6WMmdVcEz1EQqc7liwmysWuzCCOM5jr5vYDuNN7r/S/UvQpfKdWHiLMWJWPTjOVvus8cv/1d3Bjf41XDq9zvLxJLsY4XHB59zUub7/BYp22XzisIrqnF9rTxIdMlDnqVMjTHPBuXg2CanqYqfzwXl3eSXjQzpSd2AzOEl7//C+yHwv3P/5BLvY3GHmX5ILdvnH37uDxR8f2PE/I07PPwjgcmTN501/nha99hZcPnfPz9/Ho7pLj8TZHzpjuhA9K7qKRzVH8xKLSrHNljcNpw7RSuGRoXGm3jltXx9olqQyb4JvjezUqkA2eh2NTvpWnUb8UXQh18jNR9ljFbZqmNJJb2zghVmY1sjIFsVBlelY2ZVTAxTCPGndQ86az1llT/rltfJXqm2qMU4vJMFGubJ5c2+XKbrV5a20VMVLxoUr8E8G9eIqzIJ6gDDNCwgbzE8toa1z2FDqxMV9EYK+GTEEWVvfsitFx2uRs0wFI7UfZqinf3oQNm7RYuvEkhyCyYVLXgXiLbRbUYCWKqJhVlfZpv8mfUr2IllQDaAj7buLRCj7RWhLZ3OuH1N0u3TZpBI6XD61dfbpfcb0lFDfbiTNSKTxjFRWnKBndJtGStZkGa03J2LaixQ3hdl3qku7Osl9oTQFya28p+yzIJZSCGtB9sjRj2Tn7ro5jW2EtzDIjReZ1V6Z5T5COVMbqZuUXSVlO1WIeA3pwfnaDN15+ics3X8biNo0Fi5Wb3M81v8l6XLlz5w6v37oJDvtlUbDY7Wm7pfhshb7YjijMLChq1D0wS+ybyLfTWAwWTx591wO86wOP88rnX4ABPpKXvvpZbpwP+rWHCc4Y3jnv14lrl7zz0TeLJ1maYYznnoMx7pJ0bt4ODp//R5zdephrfh/f/90PcPfOK9xuCwcTwd9Y0LzuY6G8G9dwss7B5XqAecBNLjdRPEhV3AudHeHq6B6OlydDjzzhjYGzsO+LFBhpdLoMGKYkfVbjiSXhU+DMU2pei6F+5ea+HduM9gpStVEls4JtRAWZGiHbe+GsCpJLiu+Qtim/9GA8xfXdmghuGzRQGGLOIt2iNVajbtmCYIay2pk0Cx34gTD3kiy26RryVUwRmcEnc5vBnQUfsXWj60BofoVbbpVXpZ5p91KFKA9OO+0/ZfLbXs6CNQqvDLQpJkXMT7Ilh3L8IRRcferPs7Jux+4p1afwTaMod1uiOUTSd+Hbzr68APKXBr0tSbQ82fRttKHm7fTf3+h6SwRJzJnZ1HUbAevgOJKZncxBMDWv2Xb6QGUiGjE3SFAdv2ayBeuF+time92comWiQEp3beTWcGW/b1w/XzhfurwG26AdkphHDuUmZIg/mdtqaRpR6Z4avJWmjrGh0ziMZGGkPAEvX/kax8PrzPUCu7Py/GcfInffybM3HuV97ScYMzm//wH2D9zP8sYdRgMfk7HogR/MmQfTBq4mhmbEeJU+s+AswyzYLc6uyfF5nsH3/47v4rMPX+eLn32O8eYFTzx8jd/7657mtTcv+ZlnX+GFdUePzm53jXc9tSPmpXIuV5b01eeOYNK6Ho5HzC5YX3mTz82f4n3v/s1433F5ccmxFu7Szshc0YxfDYpf58qMg34f4kF6HBjjkjVHudM0onXOfafyKauZ5kcWo4jCvTItAxu4LbTemWtIfFBYYKaI0NoEFQi2bLDKN3kpxommMuZQR3jjRRb3tkdANDUNU0FM0/lSXgDl89ZLQhgUFleFDWlc2qbPDpZiR88ZJRGPambc44aTUk2NubK9oZjBaeJZYdankOXFY0SiiqxDQUR9ZXSx1b0F1XiV6GGGFVtKY3GzuLbifbqLs5kW5BQOa5lYu5J5KvAVmf0eIn+4sEgNLYMmvpOodakOvChR1Zlvki9Kkbs1nsqwAn1tqaxRSbZjOU+0MmHHOkZbVwww07BAMNbjoHsnelajc+NC/crrrREkMdx3ZVoris26SgbW4BTINulTS8OjSVVRJ94koHVhQK2yBFk3w7aAUlP55DSusg1k17/bL1w/23Ft13RqLY03EC1ljJIzzrL26gVs56iRKKKzyB18O8Os5H8LhLCvOS85u75weecWz7y454wPkWPw6M2HeD1/gJ//qZf57X/493N45F38xM/8XXa37vJxf5ifvPM8z/oltw8HciSXau3QTFlzsy6lCaHP1Q1fGm0nL0k3x+dk90Dje3/wo7z3u57m+a++wJP7a3zgfU9w8XO/yAce33H2xoFbr73MA/fB3mFa57iueBoXd+GV1xvXb9zHfe94iDdvvsE6g3m85M6d13n+xWd5/Kn7uTy8wZhHejdiJ0GZQCorq7JalEWjWtcjxIF1HFjnoMWKhXMgOfaF3nai83gSfTLzyG5ZcNvRvNObFxl4kdJlajwbFN8vyl8Sjfqw3NQ3xcENCRCiZI7CyFL+o/Xvc4QMi1PcvlB+LPON2FQlVbaV6YQn2CzbMyjDizyV8Bt01PwKI7NmpbPfGh52KqXH0AyabSZ4UQfL8eiK/K0K3Ks5gsx7K+/KESfsFZSJ6rQQFj9THkDNTC7eaSfcMct+rzXwbfZ3atSE4ACvVV/ZZkFl7poCoHnu6OTY9OihiuxeU5aN03iV2GXxpIvOVfe0hfoQE/UshGnXfXCrERF6LXfN796wYDM5O5E6SMZYv3UUoP+hLsPofs5syhamZwUeo3nSfMOWapGFScGQW0dyMwr1Ex5oXY5Am8bbUg/J3Et+VlSH8uFz30kuVf++Z8PakWyNjKmhVKNsurbpdxnCxErFIopHdfxmMnIl2wUz9jz84ON025FtAb/GfOO7OA2WAnb2Dm5+9g5/6T/8T3nP7/qN/Mi/9O9x/InP8sCf/Ft84omP8uknJ//Vm5/iK2e3mHSOMTCDc2/sM3SY5GRpqJRHFCd3/encDeLINLj//s6DH3k/7+438ItLznLyAx//AV599UXm3edYdl/hwcU4RHBBsM7gueeN8M5v/31/gE/8xt/Af/x//78y77zBrdu3ac14/eabvPOJaxzXo+hcNIhLYBbn3U88PqBAsKjSJxg5pVEfKnlzMS5msIspInM5HU87MtbJbjFNqUzwUQqeIxCcAtZmErF1O2TXT3EY2UyE2ETKlsJGY9ui3oqbJ8lemA5QmWdUlZJg2U8lYBRFh4KKtkbPKflHmy4wsm0TI1dO4KB5DRCrTLlaIJkaZzwjT2wNazVjxxA1yrdebgNbilOsyknBVzh/Q671cxQlx2CEsrClofEQpBpKM04+lOoEK2BlokZJ2ZdYdcS3+wAK9JtvpgrmPDW4tikCmOh+ec/a2BqsMfPkokQFVMX48mLwqPG6htPJLtoU5cUgMaL+vGWicfpZ5SWQQ/f+rR4kdXT0mqMBtEHvBeJnuW+nWvwCZbSgzOVpFxTZN7IGmXs1Cuy0aNzsxJ8iJafKlNN20JnZGKF5MJmiFPUe4tPJSVSZUGtXnfLYCMLiwE1XEyin5tyMspSSucElr77yBfr6Gp+99d08evRfcReiX+eQ387zf+Hv88rf+Qdcf/AGP/Cpv8X58jgf/873sr7vXfy/b36RLy0HbKfFFUW1CEIjFbqzWAc6Fo6HnUoka52YA7MdcTRuvXmXa0ty8+I2F5/9SR5/5xM8/L5v48n3PEjfv8pxPXLn8g637t7hjZdvsRuDn//UP+Dxx67zL/+B38l918/40f/sz3Hr1sp6gPN94777Fta5stsveGVMYyhgQGrQFUXRqHJxpsrWSHAWmicXMZgNjuMoX8ca09f6YLdXcyabsbcdORdxCodeI0LWeNU2rfknRjaZHQCcfMSCYjnU+4ri623uD26F5QkS0hwkYaduwAQvulZ9QjUwTprkIuubsLc+REGbwDwrw7Ct6CGx1nCW4giKhmNV5oKyLtnPzWoImXidlieyvGzP+inwR0rep6ZKU+LA1cEh67TJYsbOVeJv1mXNTfJKqmE4hrIyyhWhgo6z4cVZzIsFsLoPCDap5x7b8D0q/s2Q2sY2qa0SpLo1Oohiy05L/TanGjfuuC9Kbujlfi93eSouJKVeb1Z6bR0cBMzjkcb2s7/+9ZYIkgbsWsOYwqPYkS3Iw5EckxkO0SrjUGnQ5oTjpqxRluFNksRWCy7SGJk0xKXCi6RrzmqylMLkfD6Ok4ulkXt1Qo/ygCKXzu64cNjJG6NHspTxRXeTZ2Srnmka60zGaqzDGOmkLcR65Bd/4ZPkcp37u/FQvsY6P87Snr7nDuhK67z5jl/H+Z0vYl/6af72R57kfa/c5qM/9zP8wK338+4PfRf/5e7z/Ji/DnPi+z0d2YHturN4I3zReNemhTxylboFI1bn/U9/B++MHfMz/5jr16/h3/XrOL/xANeWazxw3yOc3/8C6delyhlHmjfu3mlcW+7ylc99kq9+5HGe/tCv550PvJcb91/nzdsvkmNw4+waZjpknEGiqY2CDTWIKcZm+Foy0BGsYwinGkEgTLnj0mSn/p0yxoG7lBt2bvRt0FPr+NoUeGbhZQVUnziyRWQW7UWBbMSKzSgJn7KLVoE6TJzBrQEirXlJQUNmyjNVPSROlDEwKVyxQ5GeA6+gM+Yos5Z65jlJb3hVOEuu+tnueDd1bFfZjGVrtH5Gdxg1R34G9W/VmBIFqXi+QIvOtCmZakJUmbxxKDf8E7I4vpLdLi41F2OSzfBFazpcKpagpghk0qdBa/IESOTUlJplI5cmL5VNigo25b9gacpkqYMHEePHxlWl5q4XdzQNot1DweIKjqD06eZLwRTlX+CnnP2qoZoar5GFmWZbiTwyYv2G8ektEST1oWT66S1YwljoYtVHcjnqZCvddJo+KDVSMkl6geaBeJTkBt7mCVcxqC6lkatkU3gSPhkEFyPIwyqtKE7zztIn2ctMI6V4GNb1EExNcu+1Qcckjhp0FNNgyIl59cZLd+6yxl0eyts8+fg7eNdHG68cHyCfe4jdOn/FHbm49n7Wx27w4LN/n68+esYX39n48GvP8uGffIn/9Sd+Aw/d+DJ/o3+ZoxuXBDtT/TgqULqJsxcmSy5m4AFLdN772Ht4+OUDr7bOrbsXPPb+j/HEez/K+e6My4sXGXZgHC+J4wU2V1oEF6/f4X2PNl5/LTi89AwX73yUZ+2Sb/+u7+GVl/4mh/Vlenuah64hk4005kgOAXcNbo2JjXtoLAPmVAm5jpWccsPR8KpWMjtO7z0zmKzQduwSzrPh3plNgQiH8Gr+abakcp3NNSgQAXlr0JBEjBoxvFV02rzbjHcpgjacL6tclBpHVJVt00kya37aumo2zCRlPFot1VJENcObs7PQgd8Ma8FiJZvdMlVvRFvoQene1dzZClohjVtHuTC9VGZsdbAYwumiFROjAkzWVvDTTyt4EpXTZq6kI5O267qbIdd2ZmlmKhhHZeweIq1j93AVUweNSPaqxTM1iDKAXAeEydk9JnNOCT4Q1k67EgBIZqrM02paFJjMQFzuVUZXnyLR3COuSvZpm7zTymXIT7BZvPUbNwWwM0tREdWUaBxdjsZWm3xCGbZWKV0UhhlDYHU4jBqSXsz7zCSqZJbhhfS1rZwzWhnzrpEasZBOb430QesLvhPGFYn4moVzOJru1ryL8V8BOqEWmZPZWPoZscCbGVwck1sv3uRw5+f5rncP5id+C5/+yoe47+U77I7He+5IMs7eyavv/SEeeO7vs7t8lS89vOOL7Hnvz/8Cf/g3fR/vH4/y19rP8eW9VAvrCvswaInvQqYYJdVKGsfDkXXCj/3EP+T+W6/wGHe5exG8d3cfZ+06aZ3orzDnIOeBcbyDxYHjGwc+/vgNPvKe9/Dai7e59o77+Ozn/gmf3j3O7/oX/xX+xl/+a1w7a1w/P7BkYquBBdNTBrYeXIwBI4jjYOTgOGVIMqlDbAyNac2QFt1lRmszsSqRYhHE0Kyz+MLSdnhbcFuw7MKdETY9C3+LKNoOFSwiaiSCmhdQDjFbFV6bOEO6d0UhbVQFI7kmhVt5Feqg3mg8yqaiNNubkaxKbKORrvuS3fAOvSetyYlq3xQgY0zIDrg8OVVIia9bfNMgORkBWzmdI7xSQa4+dGqejjfRgTZdvIVQxGDz8rT6/MlxyElIUnST9diSGqkbGhfRYyvjr6CFHgrUVvxJ4gpnjExWjG1cx4zqPg+R/8nNfVyBeeNEboeqVUNmI503c9wWMUxak0Sxpg1kbBaGcpiywqdn8TCNCrRWo2+/Gf+Ht0yQTCKPzDww51qAKsIBU+TaFkIY1yImR/HLMmsOsEm22IqWkM1K0C41QItGWjA8SO+YT3YpgwqATJNDckgGuSZoimnDd0nLztlI1ihrJpJ0Z21NHo1F45hNIHFrKhkwjZ+QR21yPO9cXJ7x0uUdfvGlL/DUfuF7v+2Sn3rq+7GvDO57/XWWe4Jl9HNef/fv4MZLP8mNm18ALvnqtSNf+rkf5/12P/+LfsYzH303P3b5PJ/pd7nojTt7dbn3mewiBE1MlY3HCN79oe/g/e94Dy985qd48v6Fed/Ckre4u3aivaAMpC303Z4xDuQbxvsffpybr73Mu55+Dw/tH+ZrX/kFfuIz/5B/8sUvEHGLa+cu+VpMZjqXU5/hOAfrCAVxD45+4DIHhzyKBjRlEmupwWeRykbCjRXw6Xqm7kRz9tY02mLpdFvoucfYqyTOooTlDsvN+CHwuQqPbhSCXfieFdac6lDLkVty1aw0LU/0mkZWieleElcq2BZ8aaGAbnlFwbSqBseUa747KmUbsDPaYlxbnF2Tt+XcHJOG6DthFNk+yC6RhbrSwbQEk8M5JJZTh2IFS41OKL6kOT2N2QojROofN8o9qcjambI4Y8qQYpGqpWV1sc3JmjvVw+gRrOmcpKA1T8ZODa6s8lmZuoj+QZt+krxuHFDYuvTqRMPGldwatimjmdYgXe1J3zBVmXeIlzsLahH2OovaNc3V9C2s1hEnU8XGN+7cvCWCZDJZ17scpgBt2ZNxauEbs+yShF04Ln4TQbPSqtbQIkj2oZtgaDaxWGHi32mehWGtMgyXZ6W5M5upHBrBEkHkgaU1aJrgGFkBdUZtAlELZg4iQ76EJnJ5d6P3Ln25RZFYRVe6c6MRAdfmgf3Lz5DXrvEbnj7nsw99nJe+eB+7N29z45VX2K0VLM25/dj3sZ4/zAPPf1LZwfoGX7FbjMPCt/3Yc3z7jeTZD38bP20XfGm8yVfu3OJWu82aQ05FnBEz2btzdoBbL77Kfe94F3ktuLW+wu71l2H3IHP/OSJvS97mol0s40Eee+zb+dlnPkv6nhef/Spf+OpXuPXi68y+8vCDD/COB69x6+ImbToj4Pa4yxjBOgfHAXcPMKcR40DMyRoHLuOCjEMFZWdBioyO/CvDDO9yNektWJaF3jTIrWWH3EEWJlgE+9SiuYJWMlUab4qZ6qZuQgJrwss2/l1V1Krw3FTGVxGqTm2NESjJpiFIIOrgdhPUIDd0CjaQVHN4k9ChwW5ntO6c7RrXzyaLKWis0RjWVYqWH+W0hvks3u9C+GTGsbKkTZNeRhJow1spiqg52b2JcL/OVe/TryaAquQu+WXW4R41gMsGbbfDvdc9CXomLaEvDlGmzbP4u4ptojgZhKmZaQlYmYAEJ6mjm5FF+N9MOppvuvZii9Rz8Ux22bHWoXdZtqXUN4TcnjamBFZTV00CDGIyiXIHys0Dijw98F/ZSN2ut0aQTGpsZjCI4o7J05Gm1H0ibC2rcSM40cpfT0oEzc4QLaAXsTQsC1SvUok6zas76UU4GxnY3BxiVdJb6j3VmaaFVRxeK6VA5Io1ZbFg0pr2zm5p7Ba0kE2nZ+ak0diZAZ2bY8HWC67ffJU7973Atz/9FR79nvfwc5++n5d3C2ev3eT+u3dYphbP4f4P8OruQd7x3N+lj8uirEx+cX/g4tD4tn/wWf6o71jf9U5efPo9/Ey/y5f3d/nK4VWe87usrjKuv/N+PvTkd/LJT3+S3Z07TG7ReY3gOa49/rzmAVkwVg0fe+1VuPGu5IPvfj/jmRf4zBef46devUnYZLx+l6c+/EHe+cQ1bl/chqNwq7t55NbFkXVMRjrHo3GYyeXlsUwNVtKTXdsGQg2WMGVbaGKeSlj5AgofTHo7Y9fOWGyhuUbWZm6/lD25g/uEsFMn1ZMqLL3mOysoqkSuZkCiRVTB0LYBOoiUrsCTRV3JU0lrISMINu7iDHk81itu2VXr0JdkWWC3wP6ssevq2DdLeSkuanrE1vwgi35kld5W0KhSO5T8IrpT0aLMcBdnOB3wRmxO7SKzAUUYQBnrBlpGaFbQ5tLulgw7Sv/sLv5rbIoYTZy0Kb4us+hVViYeulFlC7fR9VTKq9ILYmyHjZWqzbazTQeNKSvF9Px6lu7enLXQNrmt19hdkswJzTQmeiOXB/XanEpuMrH+zUtteIsESV1VOmQyouzXvTGP5b1nNXDLahQCtWaYxdYXhxzTzTc0C0SHxCbbqpdJ6DW2cp1B5ApNygHv4t7NTGzq5As3ej2sskLV+NhqJvkp60plbbsdbddpvdSs0STAn0dGjJplvOf1dsZxrNx49itcXt7hweObPHz/s3zfRz/Ej/2jPYezHS/vF/aXR+6/uMsyB/PsIV577+/l/uf/AWd3XhQQTrJY8OUHjC/N5Nt+/tO866c7737iPcSTj3DrvU/xs+e3+NsXn+dn1zv8zb/1F/mxz//fGC/d5uP/0u+E++CWv473m+zGXeYYyphmcnhjYHdvcfHGMxwv7vDySy/w0y+8wps52F8zLC5533sfwfaXHKdqy8t15c3jgTcOg8vDZJ3GcVWDbQ55gq45aB3Olx1L2xE5ZXAy5eDerdFpkAuzyQBZWPUZi+1puWDhYAs5FzIWKF9AjQ8duKWGP8W26SFSh1tWtrM1FvTjRRWatY5sW3emcntbp1aWOskWtIoeBqV+0bqY9Tq2yKRh8aR3aC0K5pQapLfAoizjOuQArJGb3VhulmWTrdAX6Vs+BZW8cTKTLu11moLj0hZmSs1m254o7qD4AcLocjgxG6uNE+3Jp6v0btV5Kgy+tbIxS6sGq0q2Wfzl089PNWw8ER4ZCoiTMqvY3nsdiO4yq46p4BWVkasCEOZsynKEESNYTZWApK/GPN2DyDL3qM+8/Wf1b05Z74mn+nWut0yQNCu6YUo65uY0vLJE8a2s5mrDlhVEFbB5GpC0UQ6sgPUeVpzIMlqoE3BY8cVGkLaeOunMQT+9iCOPTnk2erbaEirgN22s3Fv0ENtuwfcLtjSZH2DE6MxpDAZruugmbtgwIh/ha2e3WO+8yt1f/BRv3vdV7r/v83zX3/jHfO7D/yrrwx/lcLbjpbMdZ5dH7r97lwV446nfxvrKT3Pjtc/W/agF11Z+4V3n/Ox0PvjyZ/mAPcn+9oN85NnP8KEnF378g+/gH+0uuf/xd3GRB+57+CkYz3I8Hrh2/U0yVwHrU6fvndeOXFxM3nj9dTwPvHp5k5fnSl/2PHjD+J7f9N088v77WOdFgeaDYwyOM7g8XnLnYnJ5TI5jkDmwMJa+0Lqz751zOmc4Q9wQcqlgZF18z9gxvDq2wI6FxRdkc9bJGpqhALk1WCoTTPk0qsM/Kz3hJFGjKpMMqPkPpbyhgqSVb7PVsLi8J8PZVm6V4ng1haoE3txxDNyd3pzu0JtmHclTIKrZpLJ8HJxjUhZ6IuRblB1iZbHi+Wrmt1vpsFXL0qzgn9iCn8rwOVyd9hSXVP2ULTuVQfSc5etYPORt7IJ8QMUBzprW2ZZN5inMs7mYDFH3xcM0N3wL6bkZ20o5ZdWI8avojk4TtJtN0JpurSASHTLlj+lqwrQ0HUb1y3+ZCLv5PSX0xmttqDo1KfBAzZ23vCwxgRUNWjeMFsJnRtECWhQpu4i/Xg4nXjhCJkQLmm3WVE0uKmU4sDWApiXTRW2YoZPMN4rHDDIHWz1u6bCoPJuGyrB66JhBc1r5QW0PXMFZN36UMmKmsY7BOjRDeyCnozaTs+msdL7IwvBrXG8Lu/s6va/w6iv84J/+3/Gzv/3f4PmP/j7AuDzbcVnB8oG7d7n7zu9lnD3M/c//eKUfV9euBV99/H6+mCvffvd1nnz/E7z2c5/kd3/hjB/4xEf5e08+zfkf+C1ce9dDvPLM3+fW7a/wwHKXGSLCb/fuzVfhxsPv4caDD3F842tcXohZcO36nm//+If5yK/7KKutZKihMObg7jq4fRxcrHBYB+txMmZ1Gr1jCXusOHaCJ5rJ9p9YCisUrWPmnp4C3KuSE15lO5JzMnsFrnnVbUG2Y5nGqMwvlfScAuU2qS+rS1uJ0MkwwsshR5aRsmHeSvZgSs3VSssftbnVLaiaW7xFM2MxZ5+CEmiNbGvpn2G9DA4h02abLhbGurDkwjGEd2vA6qgOeRA+he0Rpfm+Kh9lgWbI43SS6cwxKquLMuANzCb4os0XkOVkH1MMjzBjdHRAQGmqmwLTMCl03LDFGa0VhzFKqy20b7ARlgqrL0I9VpllzurXZEGClRum0TciPNQaabRWTZ1WmO9aWPAGk3memlTmKdMOii5IKfcUHsA2FdSmFHqrY5LAGpKMWXWthyXTRQexoU2rQ04dy15cqk3vNReVO16AsTrfRRdKzZwOkDWWJVa4JFY4SoQGYnUqi6gRD1UWaV7xqNEDm4GG8MbyQFYgHqu89czpy44MZx2rAg9GTzUjj6n5KpYrnTNeXo2HL+5yY7zGPi+5/sYFJHzX3/jPOXvus3zxn/93MD/DgMuzHccKlvf5+xi7B7j/ub9HW29zhbbp/3c78gv9yKcP1/n2D3w7d3/xF9kfjE/80X+L9cZDfOXnPsl5X7jYnbM7nzVlUDw/aHB4lOs33sHh1hvEPJBnTu7giQ88wZMf+iC3L9TY6EW8jnRGOGMkYzrrrGcwp9Q+3Zg9aL6j2wLWmOb0ZojGfUWmzuasc8cYHRvOSG0EWXHtCBopIFq4YjUOzK3YDAvORE4z6pKaaWiStXYKuhu9ZHrKIYmsgK3GFc3JGt2pwxkR3KHgHZOipHFSqngKN0tTdSS0vGJBKrM7ZmIjyZG4D2YeuVyddTR8isYi05IjkatIPhZyvRHLF28yICGTOafgh5r7IrcKZxuzNmss7sYdtLzCO8U/hLmirNpgLiYVWU/ZEKoVrn1aiYXWP4BocyPUXxiRxVwAsubJ+DYiN6srqzPFmv+STC4RVSeJ4iGbjIkdrCsh8aEkR+Mo5kZ/Vh3RHLPixNrm3AUQ5Rd7VTHMAdsMom90vWWCZGZgqxj/08v0tqL9SPHOZsJxrJCTWYdPNSBPuJH3RnNRJzSnN8sgVDffs6afNIHHZpCuResoBc+8xyG7L8IpMWE20cr0oLCNVlSNLFF9Ed/xxjI1OGLNlDoik0NMqIfuzfGWnB8X8M6bt2/zyk98gTsvrDz45m1SORMPfubH+OgLX+Vzf/A/gLPHyi7KOJztuTzbsbu8xrr/F3jouf+Oa7ef1+Y9BUuEI43bfNac44c/xtPHld0XvsLLN/82Lzzz0xwemJi9WSd7iJPaO81u8NCDH8ZZYU36buGRB+7nidudxz74fi58xzzC4ovK5VQTZI4VZtKoqXcAlixd971vXWc0CfOwqjvautOWZLdveNfQrBhnlKsxFB/RW8npNoFB4YkUH46kspjKLKOpm3oaCrxpeavaCylRZKYiOzGBvZOtrtZBKNVIK/9OLV5NCdxGUeRWhkY7vS013E2ewCStAuxwYyTM1Rip0bpjOHOuwJF9LsqCXcYs00eNJRAWKRaIKC5pIpxvowgirUyDQqqgrKx3I3TXvpNBRBIbZzA2D8wKYNpc1XwCepJWzZcRxKp7Y1artTDIWdDDNn0rmyoxyzw1u4rZzTZ7SnORtsQFNLq2OJHbqq7n0bIeOVkcZimv7DSldNZaVJZptdfb4mWYsRHutSL4JnHyLREkSWRYPdRZ8zlpsZl6Fm6SpWAYmgw3s8jirpPIrAML5FIVlTZPTD2sDefchpPnacOpgxl1CnVqdWTWn4RpGlmD76ecwWeclAFK6VWmrkNB1jswnaVHyY6TGMHR1URYMlnS6Ra0s8kacOuhh7HHYf+1Z3jRk6dIHmkNWye7V7/G7/7P/l3+uz/4x3jz8e9hokZVYlycnXE823Pr+r/Ao899kode+ieFc23NBd1jy2SXB54L+OKf/FGOT17y8NPO3dsrK8/xjstDYfNObzvmxTXaeeNw+y5mHT+/zqOPLnx4D/OhB3gdZ+CsESwpCVi3JqmdN7pP8dhajVCNZPFtKJ8z0sghr0O3PMko6ZPeZFpgSxHEcfrJtHALkh1SWRVFoVEzYpuHPZHGRfdJ1cBVk4YACzsZnmydUNjKZ6CaDpFyxJwlItjsnzFTdlNDp057zSVTtVTGsvkGzBjyoCwBg4oYZ+YiLutqLEPhC4AOkYNpMhrehrZttLZtsqCV12nkECZ7yp6jdM0ygTFfMYr5wSihAcLpei98dmAeWCv4Dqd50ptGKWyejtNgzSSGqri1SOLCWjfaj2CoTIrbzNXBWdE6KEhSECpmkoduA7pOLYJmjFaqpungciL3wlKsxoF4U9Dz1HEyYtUh3ZzFlFVNRHcyV6vn14RJmtl/Dvw+4KXM/I762kPAnwXeC3wZ+EOZ+bopzP9HwO8F7gL/RmZ+6ld7DRJylEICDadvaJHJvLSXMeusrnLRGTZ5GNB6p7eu08TKCbk4qpEUXaOkWpFgEsTXhxRGAXS8tN+148aUxTu1eGJSnlxaKKtK02llKzWV5bZ0PKe2TlfWY5H0ibDRJVm7ccaeacqwXuOcr77H+cBuz/Wf/CI3n30VEt5Bq45+8Jv//P+Zv/Ob/xBvfvcfELBfh6CZc7h2zjMf+M3cfPBp3v2Lf51lXl7dX6MyLWGw7768JJ9tPHfnOp8+/ALv+s43uDwuxZkzRhw53oT0I2sMOvDwQ4/SP/ROlkvnJ9bbLCzCbOOSw+p0Ly4iji8ND2dpi7AtnywR7PqOZdmRrWSktVojQ51oN5mbmDallYZ+M1mKRDK02hinKqQypMgkfdudIlrH1nAxmT440ndrqJuyXS8PRu4xmNVPkaN4iy37qk1FldE1DxuobJWTQ1CK8y1opombmwyZZ9SoChGFSmPvFItC5bOs/IJhg9mkRPM6+Ir+zeY0zjRm1cF5es5Ne2SdpNUclxpvIDpc0MsXMpqTS3X+V4kRrKqd3p1eqiBD3gUzhfkxZyUuALJYG0OV1iyz3CgzCbMax1Alrwj8saXfqK9SkyOtTEasOM7I9CKGEFgrjXzadg/Ae2HJTV9f16gxviVXMnRA1KGRAaMaR7GVFl/n+qfJJP8L4P8J/Kl7vvbHgL+ZmX/czP5Y/fnfB34P8KH69f3Af1y/f9MrURY2S5vtkVXGVitfUQA3abS34UNyPRHA30zcOLmAKPMYc9QJfk82RWGZWW7PRtXs5QVZJx8WzDiKGDzVhPGtQZp5kk6JgFubvEDxtC6HkikJF+gUjU1NYZoAuQ64sKBbVFfygK+TL71jx/n3v48n7ztn/dwLvDEHDxA8Nht3bfLU3/0zHJ/9DJc/9O/R2zVg0wwb6XDrkffx2fv/CB/+x3+BFrdOTkobVukYy5wcDsGjL77OD/Iot557jOM7n+H+h48072QEt97oHO7eYR2TaQuWD/Hgkx/l22923njmZ/nZ+470NTgjOfZdyQ01PnZmcPBGb/InhzMaQW97updrDwrYm9N8JBwP1EFntDaIJquMKMNAS2FjMSmMrilrGmpKjEwFgjoUFFyVRWWVllRlHqEGRi27CvDq2G7bVomlmglL8f4SHerbIZsNZtvcwOU96WWskRuX8hQUnOYb53dr9KQmHmYXZ8I1kiNNh3T6rA7suOIY5tYokcdNZs1bNOSkn2pk3jsfydrGo+R0GPSi08ycEj14yqsA8EWHnTV107fSe6bYHmFg0/BSFM0MPau5SXjLsCuh1dgU7TmD03OIYgv4CRvc3OcTSUYNqxNq0qZeezXto/AN2xQWCbDNwglTVts2FopFyYbzVA2EFV/016K4ycy/Z2bv/WVf/mHgt9Z//yjwd1CQ/GHgT6VW24+b2Tu28bK/yosQUW4lBD2K0uGzFqkaKNacngIjZiknWhO9YiN/iLbhQCdtnoTrlhu5V5mcsPniu5X5hTXZ7ntufLNqYkQU3UJ4l6dOwxkC0yfSdNcAEtzE1cJWYooWszlUB37S2EYGK5zwrQGkNdZIbu4mlx9+jLv7hRs//1XuHAZPu/Tlx+bsPv8pHvsv/12e/YP/B+z6U/fcTIXDubuPz/zAv8p3fOovs7z0JS7vX2iVt4AUE/uC83qDx583ePE93HzfnvOPv8a1s1fh8hp3b9/hrO94/OGHeYcN3vWX/zue/GrnN3zi/TzTvsbN3QrjBm53GSlriZkLI5zIgxZyl51VN/1OunoKJ7J0abZTHd51wJKNGX7CB2XcMFnqEIuUkbGV602WsiKnNiRGbaAq2XLj1aGM06TZzbyy89JaEF5qZlfrQERYlfCF60kUJxzPU2Yi6Vs2Etr8ZHF2K2DMwJs6zVco2z1Prpo+06rYr0pI61BNFKsEOaAaT3ZVutoGNpY+ucjgfenCiN3UIKuko/CEcgDXYY0laxZZXA36mgYpR6bmOvjTvYQacYIhRpHoNW9H79NTYIko7EHG1OcrGtYmD6R1ffaNtI+eg/LGmhe0qYOqRN94lspKy3yjbnaS+JB80l08VDcZEccYGm+y1fiVTX6j658Vk3zsnsD3ApqcCPAU8Mw93/dsfe1XBEkz+xHgRwD257v65LUwplwivW88Nc2+nvUQ5Xp21bXahsTjphnCrsaIpaYsqoO4SdXsCpXm3pNNwHzNahfvDWWHohgYtNAERhdHq0VjHEdlN1o0TpXz8p/W8nAZTUw4USG8SqSwclXPqU4+TfQgW7i5H/C+h3jvMXjlc8/w6bnyeCC3dAO/9Sof+8/+XZ75A/8+l08rYd8yRrFhGj/38d/P48/8FL/jr/4XfPa97+ZwQ7rmQE3Ga+Ycdx2vzOKhLx6JL9/HzXfe4Pz5F9gdbrI/Jk+tz/Ohm3Dfesb80Mf5dS8/yvnZw/ypwy/wSh6YedSw+zEZx+BwGEy/ck/vNSZgzJptPQv4H5NRB5hnSdRcTIc1gj7j5P5kzaA3hun7QFrmKDJz1DPbutwZIk+TsJlPWOnEN/aEVb2V1UzauH7bVr0Cxer7KmhpWuKs5mIypzJCs6BllEM29TSuKEi+xSiqiXK1H4R7V3D3yCJBO1g/ZXFSlGzBvPiPcCq75TWhNE8fqRRFi96bymSp0GyEHJuaq7Q2hA1X5rz9z7dPURpomvjCcWr0mAJllBluzemxNPUW0kviGNATCXzqp2dxoo3TAc6moLPt7il/37jA2+ffpiayHaQbt7VwtshGuLLg7cZ7lf76PADKxv0bQ5K/9sZNZqbZr2Kj8fX/3Z8A/gTAfQ9ez6iGSLuHaR8pyoD8+Car/PZ1OhXNQSl8sgGQXsOT0qbGkrozTYR0dSKVaea2Wrli/ce9nyK0KHLIvskwohvLrtOWpvnDR+fMFmwEx+OUm0yVOpGhOcy5Y2mFgdRC8BOOlMwyznaSxRt0w887dnEkmvFmDp75toe5rx959eee50F39pnVvEoGwT//l/4TXv7nXuYnv/P3UCSwq6zCkhee/l7+/L/2BD/8//2POP/qbX763e9mXt9h6F62X/b4PJJ3PBPE5SPcvrjBE1/6OT4yF95x/UGW+24wX3sN/uE/4bvyN/Fv/vrfwZ/78l/hs23QxtCKnoGFJkm6CefdhYwj1hTmnKHmV4Zsy6yqgrZr9F0nPRkZHFLNsk1fK/2ygpdXJrfNYhmhzeq2UVy221Cd58K/KmyJXpKG904SNTPJtqRRz9PlP3qVbmytMJW/rTtjTiI6aXYaq+BbN9iu0pSsHa+MMXVKUfhc5UxbtWOGgpFv2Y4weihIJ4NMP2WRRpX42NXURtnJSlrrUqJlHURRM3J8SluuOTVXsFTJ3MhMjvMINaYBK65llbLqj7YKkPp3mcJaxUmok0Gseh0cHbKUTG7G0joexc1FWOapkivljinCCeMMlchyRaqMM6vaNGebWZPV6HMXnWuoVKAXx9qqIbv9+290/bMGyRe3MtrMngBeqq8/Bzx9z/e9q772q1zCTnIO3IYInmZ1UmlRzTlF0A5hOdM0R6QvTThPZHWejzIZMJMrD5CzqXlQAm8z2a8Fk9isHAvr2WZkrDHwNfAIDhZYM6Y3dt1puw0zQpv0cDXkXDZZm9YYbCgwzHI2Ce9AysYJZZ1bFw/XZ/VMzrrRhnNsjbs9aR97gudvXvLEV15nR9AatGF0bzzk53z4U3+XJ575PH/1d/8vmf06W3akbZgcrj3On/8j/wE/+Nf/JB//yqe52Qaffur92I1rUn/8snojSzHywPmeNz/26/nxy5VPfPWrPPbGK/jlyuo3GZ+9zgce/K38W0/9Qf78i3+Dn7p8loumQfYWUlYcd519Pa9pcByN43EyjytE0N3Ze8M9WHZO37eN06XNMtQRHnPQ/Ywox3WLKXrXrCZJxaIMOQ55lYxbJihoTKXVsM2rWnrqzQqm1YgIbRwj+oL5jk1IoFA2yOo8D4wZnEQEeoRqgkhiF4TVtE0rdX/W6NkGxNB7i2TNScSZLABnjd6gkU3GHUscSWuMODLyeIJuskbVUi5ZS1rZtAkmSBNdZk4p1Ga2YndMiJWRAc3UFDPTQMgoepEl6aqUcq3miVnh7KrohhenmWraVWNQDWZnNumjLcC8qdljMjFpyAjGu4JxlGwxTYwDUa2yApzWQU8Tb7U1LPoJ0giT4ibqIIgaEeHCL1SNFoQRhRVnufmGXzXfvt71zxok/yLwrwN/vH7/r+/5+r9tZn8GNWze+FXxSECn4yyly+7EfdTNT2ZNbcghbEG5/MRqyFWzptb/pu3MooNQJwyJeWcT8FMniBU2BzqV0rK8C8s2itL8urDC3ncaddqUNeQU4I3JAX3GrLO7cEx3jf20QZQWd8YoPphA/jZrAh1OPw52XZnddEjvqM00eXMB+9hTvPrSLZ6+hDU0n/sJ2/NIP2cJ40Nf+zL/2p/5P/Hnf/jf4fZ9T57u7pYpZ7vG3/mhf5vnf/qv8Ft+/C/z/V/4HF/dr3zhD34/154zbvASmQsz9xy5VqYdlUOd7/jURz7C7gifeOYFHjzc5PY/+RS7Ny957L1P8298x8d41Ff+2vwityM4rgtj0SnV0rDmrG4cxuBwHJKYFrblfWHXjN3ZjtZlX9esEVNTIAmKBLRoa1W2PGeNfp2SoLYycZjVBFJ95WUEq4QsKWJzlWZXTB473S2VwkZS5sps0Awq6zecMEu44JLbuSvYSPUmaKi5xkhsJahEEVd4ow7MrFEfQ3r0VCXQkXdmlh3f2DKfGErM6n/bMKxplFzRTrXqxNQgqqmFE5lHZ4hvnAS5ZkEFpRePuMKCfZZ6SXCXuWFTg9LMXIO0iuwRaXU4+UlkYSBpZe86ECyusvg6xAzkgdpQl35smedWZpsc3sNO1ZGXsxboda3K+8JCsEzNOC9BQNTBcfLT9Dr43IQl/1oaN2b2p4HfCjxiZs8C/yEKjn/OzP4o8BXgD9W3/1VE//k8ogD9m7/azwfhC5q9sghbMWVhi5cxaBNhdZ2Fg1ieJIjm4uXNIp5bTTZUtSDuWmyIRuEYk1GGAgEb4jKnGn4hKsoIYRULclSxtlD0aEZ1ucWVbWUbP8lcRc3IDYA2LDXhLrwTdIJWjYY8YVDpXqVSQMrOf5rwVUfl9YHk8NANbr33EdbPvMI+4WHvfODsAa5bV/PIjQcu7/BH//T/hb/wu/8wX33P920ojzZX5Tq/8D3/Iq889j5++K/8J7x7vcPyF36Cn/y3fgNfHkc++MUDbd5m6a/RrzV8OIyFue4ZY8dhMf7eBx7FL1d+4IWXOXvh52l3XuL8paf4g9/2bTx03w3+7PEzrPskDoO5axybNMuy0nJ66yrDGyytsbTGflk4Pz/DOlLiRCNGsuZgxsCQznnOIgFr8Hh1dTfkRGUWsZXU9bUQ64EQv1LopQKjxjIUALaVxaeSVavniqsrmKMAX63T6vpuyhGX0AY1bxQgNixUATbKuNnYgDcxfaywV73nYJIt1Oxqoi7p8yTYqiTBFPxEq6lmR4zCxTk1OAwdIqRfNRBDBG4z9XmzHMG36iNzg6MKr63SNSMZHoR1Rujep0vjbbnJIyX3zOb0bZha3V0rJsrm8JOtlDJW/65WahaWHKd/22g0dfkptVPXs/FAIo9S02UF4rZlxL80oHFitmwQher5b3j903S3/8g3+Kvf/nW+N4H/za/2M3/li8g9pPcQaG9y01m8KYSVznolOZS7SqNhvRO9So4eVx8+i+ZjATXwSJ2swjVyI/4WWD6lyHUgimIwU/y8nSlzpC1MN9ZUtrLxL0cFy4aka2PKwhQTiG5ZmSRJZiscCk7OI2YML/7mGLRFQV9BdNCAYwx8wtJ2vPbhh/GvXRI3X+d9fp1HdteYI7HeGceydfPJD/+1P8U//M7P8cnv+8OYdU5rvq5Xn/h2/l//8/89/8r/7//Be197nn9sRz77kcYX35d84AvJB74kFt/YDWx3BO6IRhENGw32Cz9x4yH8Ded7nvsyj7z+IuOZ1/nBDz3Jt7/ne/jp9WV+jFf5XNzm8iwhBxvF1Jh0C7qJXH6+7DhbmmzT+kLLhZytSqSDNnhhYFlQmOANjY8VhBaMlgo4hUOyAf8R1WXugLDCbZjb3Lqs4n7BqXQXXmp1uJ5cfkqK2gr3dd+compOdLNSxWgTzhUFdUSan2OwltmKIAlj49PkNkfewM2UPRrsqkMswcQKlFGEPOFwM7rLdHZsY1vRUhPDJ9Uoy+2vpBLKapO7VZa1BSWrA969RDEBBH0zrI2k4ZCdaGAeBVuUajuEOQoT6nip2LakXBQcfcZ0GC3x2QWtzHqHdfIYVwE7kEzRutFaI3tlnCW877EFSR1u0eUcHyPrcMhKNHWTLUUx2hLQb3S9ZRQ3oowYZ02dJjMjFk5a53DHlgWrgGkIdA93cdTMZLo5K1uM9Yoegki/8o+Ulb4O3YQaeQqUs7E6a4HT9p25E0m1ddnEzxCGnQU0J8gZvTt729NWjawdWI2kEEnYzdjVZoks2kGNKTha4un0tseXHdbLbdmOMA5afM248MH60Bk3P/go7/3ULR7pTZDALFPizFNTwL3xG3/2H/HUC8/xl37v/4qxu5/TwWBaeOvZI/zpP/S/5Tf/7R/lcP0VAI474zMfNb7w/uRDn0/e/+WkzKxFs2gT2oS9AmfeZ3zq6c6d+4Inbv4MH/vsp3nPT9/Pe554N7/tySf46cfu8md5mS9dO+JtoV8GZguRB5aQyUUYRGuY7/C2V0YxO9Mbl9aAgcdKrOKtMoMI4+hguWIoo5cJcpwyCTn/oHJzBsmR1mTxH+h1Z2TRUNRc8xSGJpaDS5FTJGq54Ohwc4qrhwbCSRJn1RlWoIi5zdQxCI2oWOfKWm7sfZMuVga1W43ROtONvTlL3OMWzga6quNtHvKRJKo5JeaFhcm925Ke1eSq/bIpn/RZok7NyUwZYURUtu87Rk1ItLYQHITfoqytp0ujD0SOKpW71u0Q1h6p48tyltKnKsati2wKnBkmpkYEOVc5r/tmh+i4c+IWn2Q5DumNdg99J5qC4QavpdWzQNSxTaZsVy+PZXL0gG107Te43hJBUnM0qtuFTr6NX9aWRvMuv7gZzDFq7onXCbOdXgbTSgyjhRMmpYG1LTjI1FPO1OqKarqeNlPLIOYK5icME9OptXSV1RGTsRa/Dmm8fddZWqPNqfnQs9KdqSzBW6MZLKbfI6xmCULakd3O2e0b+yU086Q3vO+IqSyTRTKtRAvuc+/Z853/f+r+POq6PKvrBD97/37n3Huf4R1jyJgjk4ycB8gEEkhknhVQQaocKAcWaLcu22677Wpr9bIty7KWpdaySqsUTacCQVFUQBSSSYYUcoBIkpyniIw53oh3eoZ77/kNu//Y+9znTchMqMLqFX1ZQUQ+7/M+wz3n7N/e3/0dPnjAWDO9FqQJA4lB1XHX8DvsJtzz5Cf4g9/zX/ODv+dPcnzxxYD/7Lv3Xhf87Fd+J3X4F4g9PMtpmRbC+17VeezeyoOPJh54THbF8pOvnR80+0eN55cDv/zyy3zV24+YnniES88e86X33Madb/hs/unJR3lXeo5mnbEqUhslCYwZbQ60GxmxwS3pLMWANfMnvUh5UVMMx88cnxKyxQJDE02TU1lsXkDEgdVayPaiW5wxszg4HN48C9C6VbJl8+gdHWqV3QKYLp6eGH2oF0knRobAgDiUO1a9QFk3Sqs+2kfXdGrCWM1D3cboeLrroKvMuTw2v/G+nY4JqRLdUiwCzewW+zMvcMybcoiNs+666JB8e9cXcl/Df17JwUeN4cw5hvFlApu1eQE6d9j+Bvr9Jn3+r52M0tMsbbeBtn7L+37rlyKgDBW6q1B9YYsEW4HdQaDMUOysbrcglYctXIzaM9XIDFqpZ7jop3m9IIqkqDAsPAuGuGEl+SgjWSE7idutmtwQd6YHOBjbwbLz7CwueIAgks70oi558wevN7Dad3km88ky3y2qEoqeORFl9qfzjtVI4QvpQUtJOzk5hWeWGmjtaAMdnMEX9wXuzRImG0MiD4llYHK6SGjKdIMiAdx1XEKWEmQ4uQ32z51Hr02OxdIZ6B7DGi7sXs98FDvcFP6LH/ib/Lsv/xY+/tDv8Pf8k66Asjz6Zh6or6Dv/zg3Fjc5ThXZbtmM8KGHjEfvVy+Wj/NpOWVDgXRtoO0tGFnQZE16/GlesTzHH/uSN/D8lZ/lI3LMxiZySp7y2CrWGjoVcqrBfR3Ash+Spl5AzYtoxwOcUF/azU9GRj2CVZPTQMSgN1dGMV9/p4vtMtnFMbMZe4N44PDp3otk83XHDh/zP+/xfQW/H7U3whrfu5l2RjyfkRW6Yl39z5wdD+GbagglZaRnJ7H3ULjcyqVUAq/3r92bF+SuFrGxfj/3Wmm1hTtQ4HwplDwWZVY8W362CHPH9ahSVnfOYd4V+wiLSdjahklEdOCuke7zQ7IreMQ7Or+hLrQ5SyZ0c16XG2qM98Lso+DFNH6g+BkDYzX/3i0gENvpxdPZtwzutAUlMKnTqCRglFnKukt8/NS3NPBCKZIiLJbs8DrnM0JSI1JvMIIX2VyC5q5AfpO45GmmYPitXFsNLfCAdB9hU+hQ1fwiYLN2Fn/nte+s4UnuS5diqeKsh7aj6oB3b7057inqI7F2L8Q6QCudwWDIhmrEZTbQKp7ZnBTJGRkHFkNitRg8EVCE1jwIC9xcQEXQAXQALqy4duc+L7q65dA6Xb3jkeYYp6h7M4oIKVuMp52v+8kf4N1PfpT/+Dv+IOgYbz4gStEL2PQmhvJG/uATH2dx87189Ml/xDMPjNy4Z4/nbl/woZd1Hn1AefCRxP2Pn12u+bUdhRf/6rt4RvY5vzrPIi+QLMijndv+Y+MPfe7r+XtX3s7jixJqJfVwq+BMd/HNqwZ430IV462Dk6o1+YIBcWduU0UtIk/FjweNbs9Hc7/Ovr1OXiDbDELNnYgXSDEhWYd5Aoki5w2Lg5077bYFzSXoNL0TI6UDoa1DjfgI11L7BNFM6WQsRBHexTorY5AF5ETDDWZNI/qgzxugPldvDwzr4WoUIznx61IbdSpITogluno3lbsEXh6d82zgG4XY1WreGjtcAdp9oWjRGQqBoQIerJd8mx6tpY/a9knF3bo4QR6nlbnGXXdRtInAOncdPMFN9mefaIjmMccVc+F96beCJ1fiMIvXCqIT9Y45RbH1LxMTghiQdkyWT/d6QRRJVbxAWKO2jlTXrFVt4UDc6T1TSqduO73EqCyOG5kJIiH5j+IJZ+x6wUhjgNHdu4+uPYxCiS4VP+llABJdFR3Ebc3EffM8zjPGkEB3huQLmt6VgsRo77Cdqi8fSEYOcL2ErlsAG5ynp7mhC6VrbEDNIpNEcLWFuKv1AIzK2jpP3ZZ4dSs+9vdA6MU8i6RD1kxtxUnaQ3IKhcEbP/Awd195mn/zjX+curjkd3XS3c1ZLfOjd7+UN62f57MfXnHlnde5qdc5zEv277vAzXtHnr5HefwVA+lG4u6nzvCco4twf75A6ROnmyu0tMcwrlhZ5cL7jM9/+dfw9Es/h3/2zC9xo3bGYekxHXkg5QUpDRQyqTUfTedl25B3HU+SsDQTX1pYWGzV5A+5NkWaF1bBqUe9a9CE/AH1QhAYVXRiuxGMvlvsaQ+9fg9WRcx/3mzZ2TLMnNLVWjx4of5xjwENQnRQUYKr6QMh+GM8uyQp5LSjQKno2aRExbR5x9QaVjeUVoLeE2V+XkA1xzJbrV4EkwaEZZF7E2mEMU44IuGFM4lSI6xrflekN0zEjViCTdDnZ2A30fnmefYC9rczHqwUJroqO8VZ31XE2RBhPrCIZxh8hoslmODKnfjyzSravIGp3ZezavF8imvdxJzK5D9TYJXELsFu4Xt8BjwSXihFUoRlHqiVMGftTC1yZ0wpFMdXqtEmQ6q/m3OQkdiMOwavsQXXbZazDeqjmiRMB0xauPU3P5w0wHtxO3zMQHFZpHgGztQ7aoPjQLjES1IO6kKoBBCPklUfpVXUJYfJcZk6VSbVM1F+Et+2h0dgm3Oc4yapFpQV9SwUST4WbenczAG497kr8o5EVZ2EK/MN5zejmyb4+3b388/wh7/3v+UHv/k7uXb55RBpdXD20P/S/W/g6a//Dr7yrW9hbzrh6lRZPHKde5+5wGvec569TabmzsfuXfLBu5eIbnjd40cc5iVVBtZtYt0LtQjYCQsx7Fce5mtf+7v5yNHTvI2PMjCgw4o0LBjziKH0mrBi7q5EjNPeG5LSuLtnznJnxCliycnUKbbWQahlJhujwc+L98PZA/Fv2SFmwZFtuwep29m4B2cdkg/IPsIpQ4zj4SUandiuyFjy66QafeisLpHg9vmBKiGpzSQSrnG3PkcpNKxX6NWXI73SW/FMmbB5U01+P8eWN946hyJnSEm86zYlHIVmVDZ+Q40YiMDsDZyORQ/YITwQ4v1P3X8HgzDvrTP3CmmdORJW0u4IYsYq586wW48+L/43Qf7vPhXkHDQ9mcsa/lzHIdKaSwOKBtVPGiaNVAMrNaKkx6E4L9TA7Z1CpPLpXi+IIikiDDm7RtqMbW0u8xuUgRQGnm7Mai10lgZzVongvEa1RoqlTsejH+atr3XfSRb8zffAIt98S+DZhgP92ZpjIt0c3wlXIOlG7h66ldTHQdesRuuPjxEyz+TqxOdOZQOUnGm1khIMUfyKdB8Te4ca2JIazqz1kz0nN6GV3OlSaZqYYgBU1Z09/QxUI+wcjmawnOhXVD1SYL81/sAP/k/8+Bd/Ex/+7G/263DrRbHOoy/+In7gP7+HP/ADf5tF27JOJ3xi+wybcsrtiztZ5T0eegoeevyErp1UnMaSRBiaUhip00Srnd6PWT75buQ/voxvfMMb+ejRVU5ZucVdXgADtXZaOcUmoxaXGUpOjMNiHohdUudXzfNdYCcvm2k9Br44sXlEdtMFD6VqMZ723TWfXbLnz5nlcT3GDInu55Pv2R4ab+8MOzhuGjlI/iVDTeUncWDtCQsm+ayM9ms1uAm0OrVGLAfE6T9rM/eBtOBZtobTgFuh0hFNDC6+xoIh4vZuUXi6Q0V9h3PesrTRwEjtlvdwPmzwTrGZYOZRCaJCS8n13iZRDAn1kB8sCmfjc+dsmThT9OL9AS+u8TjvMN4WxjItoISc9Jbi6R1sxZ3AEj7uj1UCRvM3pzF4PpbVXUZSsk++T3p3AYm+0IukH2Zy9k8CyY77tMDZCCcPRIP64K18SnHAtPkixxm+I+b6PynQpNYrmEutztj3zp1zSzWnN3iXIeFsTpzOJZQ5QssaUq9Eqwmr5net4CNQcguV3hyLrGJMSUJvSljMO7DeW6dpp1H8pA9SfRYYNDEkQQaZ866w7gycveaYnj80njGDzrGn3km13kKJFOSPWCR4iqDwNQ//HC964HP5uct3u6vOfE26P8RH5x/g7//hv8gX7T/Fnb/yLh782V/gpFzlE/UpLskB55aH7A0LVzfERtEXW52xC52JNcpROyYfPcPhu3+Bl9z/dXzZwcv4qeEqCcFqYtthM3XYNnrxrrp0YsxNkUvth1NX3/oPUSQRPzw9twjvMoKUGTYYvvXvjVorc6zsfL/Mz4fJGZHZoRmn/Eh8/Ow14zN+zUW8GHXLPv6HC8ZscmExeoOP7o4bh5rIzg4z7dm5kOJhV6hneTc8nwkaPRmlzQ+3HxCW3MBXzZDqm/d5IvD70XZddO7erYpF2uEsW/G5OUbfTy6UsxgD09D5upFyx0gWIXpCNBHV78Pu4o4wzoplS0zg3cWdKRZfVb1gzZQdlyCeQQjzfzdrFF95+u/R++7nb9aZUnRPgbNKs/h5w5lLovCbseMitbOC+eleL4wiaTMXSiJLJAK95hbZ3y1MO5b95lBw30Bwx5QYoZo44dTMXP8aa3+aS7EcDXSvvdl5RIJ1Kyo+esZGENyNvDfHgVBX0hQXuoa0KVLyXLrhFIwAqZGOlRonqcvKMomiRokHaYjHGPOwqSyDc0TxLO/BYSqIC5xMsKYstkQX4uR4s50IjNl1HWYWgBusquTY/AJmZJTpwj28/MZN7liv+aF7H6T0YVcg/doIddjnZ5ev5TXf9SZe/bu+inPvfDeLd70Te+KjlM0VTvqhu423jFmltolmlVYn1mnLDTFutMpqbbzs6Y/T3/sB3vR5L+Ft5WlO8ohtC9sObJRS5gMn7Tq+7TRB8sNCm2DZPTytV9/ntMDJzCIKmB19BtwktreQt1afJCT5ekjUwnPUr4G0wLnFi5viztd1t67199Tp3fjoTMMY/WGbaWMmwdOFpnpm8MstWnNmf8u4Zt1n467QtJMNP0S6TxfucN9puG+j2fw++M/WgNSUliUctzW+dmijY6EhYc47F/85BiHj0NfsptP7XEB8ErHgPHXkFkmlL9J2st7eyc12tmU2j/Tmy5U0288xF++ozVEHNDrnbs5fVFEkZUpzjFrjXhZ8ynKZZ8Q3tBI69ljk9Boetd2jM1JIg2O8n5+BXZP2aV4vjCIJXphiBM3ZhUyGH5Vm6hpYwJJjP2IuSrcAhz2vV8/a7R3a4vhhj3FstsJyOoA/SH5jO1g9c8lUEq05Wcd6jQNKQr/j7bkz+WuEjcGcjucPYox1EZVaxU//iocRmXWPGQ+7+WzZybo4hcOk+TKgNw+VMqcvqPiIk5orFYYe4LzOllR+UmryG34YxjilLTBy31Qm8YOhXrgLAS5tC3/oYx/mh++7n+dYxnZiHsENbq5579s2PH90yjdfP2Z44MXsH28YeZaNLaEe06VSe6X0Qm2FZELvyoYNG4WNCFfXV7j7Pe/n7nvv5vUXz/MzwxGbkjkqMLbE0jImfSdVU0nQPYIjkXcuQi0iVWeCls3jdjjT+PLaP1atur1cdbpRU6XV4PKpIE2CPtiCLUFQq7wgeOAcQZFx6smM89mcxIj5hIL6TbrbTs92Yp0kPhY71onLDUNOa4Q1nHkgiHtKSlDU/D5qNA+Za67gaslDI1Lg5DG8MvZ5lLZgb1gcGjFYicSSPCamFB3efA5E0ZhhI1+ixZLTwQJSdwJ4x3+23I0+Ow0BPYeSJTigpt4pzs2OhN9mNccmd7pr6xD+rRKYrTWHlpx94PdwUzdgtvg70ltYDuKYQgfrJZakDqtZiEuca++FezbbnQ+aT/V6QRRJM6dMIIbmHhK/M/qERSbNfKY58D5jS979zXkW8wTRmdUK3jVYHDHur+dffyYU02M8TeBMsIzK6CqfOT60eYYzBq26DZiFL6XN4xqO3VB8c2kGVZqPF+ImBrONln8to8/+euYhU9oqTnR3AVoTo+WELFLohL07GktDcYzSDHoLx5PolHubx50A9Htzv8uUsS4UhZKMdvku5rchA7/nsU/w9uk5fvWBN8C8f52Pees8fXAP3yuZP/TDb2E4PUU5ZBTImqjNnX2qOSG09zmQK+IJOjxva+56/hHsQx/im+69i6PDE356FMaWyV3IkpGkTAT2VUsUiU6xGr9bPIz5jLs4X2sfYS1GbsCEybobsDb1pYr54ddTppnTtOYHvgXdZc7w7uaxwODXyMxNI3qfF0MxTu6WCoZZowR0I6bMYMe8gDL8oJvH8V3BDc12R51GHxShZLFlb5XaCq1MUFvEJnuR9ORDtwdbBOF6Gv3+vLVR6lEwnLyfdlnVZt45zgs/Aj/FnAa0O4QsiBR41zi7o/uNIs6T9F8Vdv+es6C8i4zdqv9sLZYmcVC0wIGjms9ndfzsQeHZfem+gxNEQHraTUDNDOu+yJwljl6+285n16MlonH4DPXpBVEkMe+eurlCgdYh7OssANz5Kouoi+IjkqDNumvR8NLt3nGlWXUTnLbuXWOD2Ba6A5B2H6ltDh4LV3ExAkRvsTUMgBfv0HYUihiX+swT6w6SR4Prxcl6LBvYGQVbmrOIAzahk1sjSXfAfVK2vTFRaUNGCb4bnYXBuA7uncSWb146BA2IIMHPHYJI8A7puJNOuEUf3L67QfxnFr705/8t93z4l/mxL/92etq75UJ5B3Ky9yLe8q3/V775J76Hlz/+MRKFbsnfe/UFmnRlcuY/Fy1zmUUYMTQmjjn40AdZXmm89M238db2HCUph7JPJTGkzMBApZM04oBndx/xXn4mHdf45RTdEcCdCzkvDoSCeiE072gkUiy9CNQdXWjOe7cwhRRzilGbox96dHfALGeVmB7mzsti3PODTtil/BFCLHyZk2S+j9j1gJKiGYgO6UyC6XnmpXdq99+v9+IS3Blbl85snDKpOSQuLooQ0ZBWzhnaXmhqjPOI84VVz+7FLridUcctSn0+9u1wQBVeaSzgnF0Ls9NkmxrM9KqgcDmrmIjiIAxvzXHY6FIdDyZ0Hf5c+jktEensv8ucfSPBm5wxTQcWeuwY4nlTv7c1pd1CSsFFAHzyUu7Xvz7zn/7/7BWnUu30CtWZDvFPbJjbHIHgT33HuxX3kqugFdOCUWg2IakiyRcprbn8qE01HjYLqo2Fh12n1UaZOrVWevdIT6iOuUiNndI8ghDznO226wIOBtdKK+5/WFvFamwEu9tLaTVmuaMM2Ttm5g1m4Iut0mujTZVSKptpYrutbDeFadsom07ewqymmM9akzN6g+M7MzYXTtmBaVk8oPnwTj904ioIIJsj+ukNXvLIh/j2H/hr7J08/UnXaf53TSt+8Gu/g7d+3heDbZkQts23/a6GyeRqmArHo/KUbvmorXl2WvNsXVNOrpIuXuZzH3gTX1ovcnnKTLaCNCB5YMgLdwMKPKs3diNzD8qHu9h4MmNKabe591/Uu+lmhIvTiOWRlkdKXlLHJTYOrjcegOSRpENe+OeTUJkdpsTZBDH6qvlGVVTDJi2dHZiwOxCT0xtptYcxb2y94/3328h2nXKj7/C4GUaqClvpbJIx0dwrUhxHbHHPVEIqKN7hlQTbPN+UcnbfCmgStxnD3f6NcJxSpUay5aw0k+SCB00ZTQnNGRkGX9qkREuJnjKWByxnd+2XKD5A2j0bcX/GPNjibpP42eY/9Q97KdeU3C1KdfeMna0fg/yVZKbLOvUtSVgmRrJjFu96k0X8i+x2w/P9Pu8t5uvxqV4vjE4SkJy84FmnZC+atcOGylhj1EnJydfiRF+xzhhQRhMPbvdAPC86qoksPgrV4HINOHXECbrqYLYo3hlUaIpR4sIohp/eufmp54Y0gWcJZ5ZTMd51s+CfeUdrEfXQMCbchEBzQgZBspN86cBktFTYJMdoWjiPIzFKr6ufrDnTBcbmYWgzObfRUMvu7tLPLvjOrcYgm1uq+ZYP6sW7fXSZD1wBrj+FJ4AI+8fP8Ue/76/wr7/223nivjf+hmtmKO943e/ksTvu59v+7VuQcKAZdWRMxrH4Emdo8KKAQ/YksceC54cFq9PnuP2XHua7UP7V+YF/f0kZRL2TnDqYssXJ9a13SmzMq7qDkKSMMPjDKAZSQLNfC+3usZgyKS3cpBUnHCdiMag1OK/TLq+FEAak2WRdYvRzIIvZTVuBnlw6qsGzlJhT56UMvUVER6KpYFmC0eAdUMN2G+VunS0OuSxFGBGGmaZDJWw12XaPJZivlx+RzipQMQYxRoupIM1GKcxjTSx8/H5Iya3/fHnk7luDJZIVTBw/75p20cVdoIVtoR9eSmtu4gsFkY37qWbdjdnMh1a02P4WuUm2rwGcgiQSQWsmQe4Xx0rnEmswO3kJnkDg5hd+YmgcjMRuI3arxOPEbCQ1h4x5sqTnYH2mpQ28QIrkPHKoKCklxrg4E0ap7tVI67RS6ENjUE8/2/G8IHiFjjQ48yG88uIiZZnlYZWkGZG+GzHm7VvukbssUGp1/DJuZO88XdKYuncyrmEOTk8Az/O9YXgHN3cfqEst85DJefBCGWTzGtwxwzd6NEOaO8S01iEr6wzSYWyNhRjZjK10FpJ3N5nP1OyUDTNhVnakaI0TWck06vkX7SRpM9Cj155yQ9Radsqlb/iRv8fb3/gRHv68b8Er4Se/nn7Ra/m7v//P83t/+H/i7hvXocOpVfbSwAqYMNbS2PTCs90NRS4fGYuPvwt94sNcvPgSvvJrvoD32DM8M1ym64D1LUhFe2atmSkVUu0UUSaFpIkxjZDHMJpwAat35UKK91dSRnQAcmiDzzA0E/Fi2ntgg+aKp8COqUHliS3rbAatZmR1Df6cL+0OQGea5jLj5fNYqpElYwK9UZmVH2fvv8N4Epidd2XguDI58E0XdMfWGExSxC/7w2yzz+RcmMQiBM1ZETWFOXSbHPKJQmnBhqhhSGz+liIYNQwO3Fl/3uv7l9e50zdFdJz54bgPVhDqYzlquzswul61XVAYNkaP2Jz6NC905u08M6bo93YTGOZF1KzkwWM4mllExXaf3FRisemL0B1VDCfqZzxi5NO9XhBFUvAfRDVy1czoWSgmDOGx16uF3KpRs1FnrCX+vk4N6xr4VGcumN4B1BgrZYeNiKgXyxRqiO4enrPGc4o3UvCL4aNSbLltViKcDapOG5ov4vzfndnZeh7PckqMg4+HrkXv0KqD9nFTpvgpUuAoNihp4UqaXGFVG0Nt3lnh3WvHxy/xBpldmzHjQ347MbeOXaFevPOTgHAMuPok1jpr1zkx9cZE5763/xDjY+/lF7/xz8JwLt5D/+0N2Ozdzvd965/nS37qLdzxobdzrW9JKjuDia3Asi+5m8ZF66is6Iu7YXkXtncvDz6zz3/2oof4/s1z3DhMMAzOEEgZ6wmTRJXm2UbiErmiikkiJyVlC9J6PBAkUspoGmLT4OR+EyX1KCQGZu7udIvFjde1ZNAr0mbDX9vdazOFCrvlPQj7u3mpYea0IpV5QCQUqoJ0L0K1uiGzG0/4si+FS32XBLNrFQYW3AczevXjYP577p5lvqFXD1KzaJ+6VZDMnH9jzWMOsE61AoovPIKD21qjBKVsNg1OFhZxYV/WZ6aJ+VSis8sWKfi5ttuMB1/KP0YQoOJ+PXtDb8FllV1Bm+lUNu8jgtLlnp3+BSRK8e7gIw5KiYKsM5wayynOKFhuq9j8/Zr5op/i9cIokuKjBewYOPQsLMNkoJlSqvtBEpQa5hF3/t0avgWXjlEibP5WYkTGJOG28oCkGJ3c0KLHNlZjSyyE2YK5Fjg1UAsNd1z4HckoOuFdxCVx2mNuquq/JBIcMSxG99ZQq6RWXY6W/IrmKP5NwYaMDcp+HnxB04z9U2Esjltpn8m+MZrMROZ58w7+MQl+WThRk/exxaH/aPEWmnXk6tNo9aejWWXbC2vzBdL4+Hv4wn/4f+Ed3/ZfUS58VoyhsvsClpb85Ff9CS7ccSev/ZnvYSXCAuWcDVzqCyQrXVY8h7CRznD0US5f+zB2Y4/x2it445d+FY/ccchP1mOmMdMn6KMwtETvmY0Yi+7whZGYunfLXQxtzU0zrJHx91JSJuUUnUY8GDOHNfBtQcCGyHbx38eXKuYuOTFZyKzUUUGTL2gsKDbYbMnlPVYLSzvCXXtuKNXcbq43DyKeu8j5387djU2sKCojWTwbmqq06g4282gvQZXxz/cmf5cF7hfe85YC07PYkM/PWpWKNIeffBLzzPTSI4+hzV+j+DMxr613rt7ebKjcIisUZ5bMtjQzeVs16DxhOycyPyHO/Eip7jiOvmOdubqzvgqGmcYkTkB33qhPbLSZ/+IbH23xviChGAr3n7hZzfyQytKx5EusT/f6rcQ3/APgdwHPmtlr4mP/H+A7gSvxaX/ezH40/uz/BXwH/hb/aTP7sd/se1g3aH6dhuQqk2IjYntoH9n2U+glTvoJibhW1E8vqe4W1HXCrDHTAPz090yZpo5deYIbzqUynLMVI7kl7+hanLZZPAqzYvRkaItONXKDKrOTugcoeYhXIwdZtZu68w+AJsfOuudFl+CDJVGaKmnMjIMyzMVdMzk7qOKGvw4fTB3GXrxIaooFBjirxeWYzoieb5jkY0hvu5NVNdEu3Lcbq+aTVm4857BGbAs9q7zQrFPEnM60uc4b/8n/g/d8w5/g+KVfDbCjHsXjy9XX/V5+/kUP8cYf/CscTGtOtHG1V0pTqhijwNLgggnkA/blMpVz8OFn+dpLr+FKgl9K18mqHExC0T00VRambKzdIhLpYI1a5+7IiLUsTTsqzq10l/nmUEur2JzGp1H0rKM60Fg4Kb4b4BnPY2tUqzMv/azzUcNaaLzFCS5I9814kJ5b6pQdxUAgeIQ088ybuQiYRxLPOn5Hwo2ekxe7aqBGsykkds0VRd0Qxt1Wu6uR1ClSEvIsAZptoylwpVHbFdoYxQ1kchaGzsyM7jh6kRqcws5goXLBe7g5LE6108JpyKQhlNh6y+4AEPHrNbu6uxuRNxvz8nD2GcDieRYwPMPHD6/gNjdcHWZuoO07CKWx5ZYbGkwo4risvw/doyY69O5cU+/OlXILjv/rX7+VTvIfAX8L+Ce/7uP/g5n9tVs/ICKvAv5z4NXA3cBPiMjLzM4sOT/Vyx/nwPmCxDSkSExkAFs6AO7e/26RJXjL2YQmNfSfM+IWD1BcnTlXI6vsNmEzd7XH6BUQBnFY0VTQBpjLA00ETRou2u5KPUSqY3Nwhtl5ud3CmZupEj5meNczU5A83lbcAWeR0aSuIlLf1KIg2hlHBTEfw5qxb8rYjY1VCgPjjqLSgg8qZ7pjAPHOw7PCvdjWi3f5YRKjkJnjkQEckcoArWCqVGm07lnfVTpZ4PX/7u/y9Gs/wse/9Ds9UZD4/eOa1jtey9v+6P/IG/7FX+Khq09ym44cWvYc9MCghrwkWUZPKqk969f/nSN/5E2v5/ym8VPjTboqJSXK4JSlQQd6AsueSugWTM5CEGmo+cEj4e3Y+uTjWddwFi+k3kD97/csGBlkdNs6w53ebfIDxs4iIggD3Qa0OoVcTmI30fx+tBTPvrl5sN9C/u/mVB6NAmcW+u8ZVpmxZFx91XvF7WXj2lmmktwrsvv96Zro2UEqWjHccQir9O5bdWILDAqt+XUwcdPZ2Pq2XuN3cSgrYzFe2+53ckzIyTfdfCGa5p5QcPpXPO4WLbRZw9zhIvBxx0otsNJ5uTNPQb27Y3ybC6uBr2a8zWzif4YYTtdz2pwrgSTecMdPpQu5dGrq9AQaXxsLA5oW3fhcLz7F67eScfOzIvLgb/Z58fpm4PvNbAt8XEQ+Anw+8B8/8/eAbfEoWQ0ljedYKONyRdIBtUyhuBW8dFA3WkWhiPqF6yU4in5C+CSisTTxbsJyCqqEn45eKHuMwGfcSsc0fPt8ZujsWKjfzE4/8eLj2lCXP8ykY7+xBvXT21L8W9oOF5nnVE0J37QlpuCKaqAtSbzr6OJ4W63GuG0sMI4i7cd6kLfFKLWQZfBkOfEbFFzi6MsHL1Ltwp0xLsdJz7y08S5mUlgbbKQzYUzRvh2YcjuZ29M+b/rAe3nmub/Kv/zGP8l2cSHGnbOXLm7jV37/X+Wpn/07vPp9P8sGwIQLNnJBEyupDMlYpJEkE8P6JuOTj7B4h/L7XnovcmngZ3iezZCwcUVvxlCjg88zTtXn+9THLJn/W2P0a9DMDRpaIXXHAXdGEuIYno8Hzk53+C7tcExB4z0OL1IRWnMO7RnaEEVvxt3wbhKz8Dn0lEVKhdbi4bcY3f0r9LiOGt6Rrbu5yo5bGV0PgaH7l3d1lUNHxHgv0Yt6dfZ73UfdLg5Cqcu9sOZwjSt/zox6CXpTisYDdc+DRg3HK43OT91RK5yWFDeJcQ5n4IXYjkIHcZD2GKNFzu7DIMG7HDIWKX2mFM18y+g2FQqNZC0iXvoOq5sXroZGzNX8d5x/2WLySfFVva36bRTJz/D6UyLyXwDvBP6smV0D7gF+8ZbPeTw+9hlfBtQWmI35aElzfXPriaQji2ykZaZWxUKNMZp3EhLEX7pzIiVkhrPMqgOihmWwwW+a1B3YNQtCuhngyW5igSr6jODbdfXPT8k38HDWRTopHTCnndQoKGphrKHzaQlYC5KyMPsIiqRQOwgmiSaNWgqqwXssgX91KNWYTjZoaxS1nU+i5/MYtTekBWE2QquEGXvFpWMpU8/fzk5FEVchXX/au5he2cpEZUJaZV+UizJwIY1czAugcdIKH29X0adu8KX/6M/xw9/6Z5DLr/ykIulvwoKnvuxPc/3uV/JNP/69nFNlLRPX21Ueb0oxj9AdDC6cDLz86kUW156kP3UPv/sv/zGmk+f46Q9/nDoM1E0nDRP0yR+66l2SR/T6wy3iQoMezj7NJiKS0g80nC41O/9oyliEaM2Lv3law2Ypnpsp9zBtEDP3T0R2I2us0ADz8T2oNxILQCMKW+v02pDWyLvT12gV32DToyutVK3BgLCdntoLnJ1NJsz+ow5B5bA1C9Rht4kGLz6dhmpU2KZAotbp7Pbszh3sCSSHKMFTyzBcyVLiPaDbjumBdnqq/g6YO1P5RtubjEEsTInZUfRs1ydk/31mUwzzYqZNogsNmWg/S7lM5hAD0Wn6L9DPNuD4Ztu6syHUhNy8JDIfHl4Y/Pr+xjt39/rfWyT/F+Av4fXtLwF/Hfhj/1u+gIh8F/BdAIu9ke2206UgWpy+0ZSmiSpO91FTVBvD4NtNlcwADsYm0D6QambSRLXtzsEFw0HjOUNHgpc/26zNoHsoVrK5lKmrg74i5k6vgY+oePbzDJYb3r2kiJigNYae6bNvpDRS+Em6jtXHPnASLw0Pfs/hc6nu40dgo03Nx5fSqN3TIvWkeu5fT9QEi9BsG5Vuk//cDBGbm6JAJufrWafuH9JiQ7l7TRs4ep7eKtUa2JY94EI6YF/2WErimMIzdcPz7ZSSO7clJTPwaD0h/dO/wDNf8e1cfPXv2mHCfqH9/5287Gv43ttfzO/5F3+N17WBl3IRWDFh0CeSFLp2liZsD084/YoLyMtG/rM7v4Th8jl+8lc+yMneglaO0GlA+qnbnql3AW5k4sXOo2fdwLj2RGZwx/FuTCIMITdV8RgPCay2U/2QtnCLiqLn3EknhftzLVjEqXYJk2cNWn+fJ4+zQ9H9CsWLUo8vhuuWtcUiQ9WjUcXJLtIaot0tfUJp1CQKoji2RldGc4mkhoy3SUeLk6eTZQZzMnwNz1Q/2AEzsnSnGc14AE7OlpyDCO4eBibmVAtTtCcneDehSmz9+3zP+oLMG47wEcDHYC+scxdOFLYeHzQXIZhr1aPSBlfTr6/E1Ic5Ybxbi049ux+DCkPz97yKr3p8sVYDqopGCpzXHLNcj+72M73+dxVJM3tm9wyI/D3gR+J/PgHcd8un3hsf+1Rf47uB7wY4uLBv26lR1E+kRW9ky1hy8LfiN3VKHsqlsIuNN/X+r2cHdsfiagKLYtgtFjPqfn4aLb5/VtzoGqNUd/7HjK+kwBNnDbMFg3wmuO7YWzHiExJB6biE0ZvT2DSrF6pwNwccjBejlQLd3Ztz4EMmQulGK249lWqM3b2zvzWGoDxIUo9RJcYteozYMTrO45W3zBidem6WIu6ouvTrT1LE0DSwLyOXOSQhnFrlSpt4rl3nxDacJ3O3HqK18cxYeM+LBn7+4h6PnW45fvv3cMeT7+ehr/zTjGnpA8z88AFy8WX8wB/+q7zjR/46X/78s7xUFiyGuyjjkoVsmcoVbt6/onzbF/LBc8bq3e/k7pclvuXzPo9VPscPPvwwU1qhPaPbrcNr8UAm3ODDTWHFuzFj56rtLucR+yoSeUoWkItfPxNfangGjq+7NYpaa86v092mWCBcZSqeTKjJc2Mq8dDP9BNg5vw1Ta7XJ8XOtgX/MrobY6cKmZeHBGTUxfwQlTbbswcOTeCIcTyZr4AFcV6ksrsWFjziJm7QGzXKb8doHLp1pymJ29S51ZvGsvMWIxbvceP7G0jyxmxexMa19+/rP4DvgCwkh/HHQdh3gj3xbM26ZLef85+zOY3HvOnYpXgGnik6Y/d9JxP2e5/dwrZaLMVwOeOwyzj6T8yTFJG7zOyp+J+/B/i1+O8fAv6piPwNfHHzEPD238rXbF3dPNQavTaG2v0mpNEkuwV8YF55JrfO/6dO7ZBqAdpnes5+wWcH4tiM+a2jqGYa6kqGMCTdbdnmm4e5a5iLXA+cKzSmc1cQo4d03HS045gTeMYLgAU/bOZ64U9DxZjM6Doxtgw5kwalZS/izYTaJUxkBbHMautyxzZbmtkZDcmVNwX6wI6nRgv6hSua2rnbY+sYhR1YPfckWYSWMxuDK9MRz/dTrtqWrsa5BC9LI+fayNPSeWff8rF7buM93/jZPJ+XaE+srpzyzGPPcP0X/gaveNN3cnFxh+OUofhJIuTFBR77Pf9vfuAX3sLXvPvn+exW2Ot3cnL7kifvvcizX3kf00sXPPHOX2V54SbXNo3Lzz7L1776c2m25p++7Z2UlqkmroSSAdkRkv3QqAItVbQVUi80asjtYLah8/HcOxRfkPidUcIgQ3r4c87+pHH5fUHWkTimZd4TmGEkmiql9+AtBt/R0W0Mdx+37ltrq9GJqd+fjRgdZ+xQohx1HJO0gdGWYQgRHViOUTkwcE8MdbzPAt8cYsTssa2U8Mv0W3c+3APjlXloVeatkqlASz6dmMb47IdN1Rra8ai0cVDPTOFZXkn8d6t156S+m/HVcX2QHeSq9IAtEr2fVfk5QMUZCHJGAfRBAiwaAnG3J483ccgj3eLdaeIYctt1kr+NcVtEvg/4MuA2EXkc+AvAl4nIZ8dXfgT44/EmvFdE/jnwPpyW+yd/s822/z0oteD5yltqGvxGqxXVwYXyUWRkt0l2nLYHXjHbtDsdIk5fUQcLg5u145ipm1jMWlL89qUnoTV3H8+77kPCb9LzO9zuP+4fBMHtruiey9JqZ4r7Ipv7P2onWv0euNBZDo/hm0RF3RcxSbgpC2qJpRlTK25AGx3ZEICzr4BglEShM1lnS9vxPH3xBBLGFs2aa8ov3sl8WaIJ4fmbH+d6fZ5tieI7COcs8bq+x14fSMvEdVvzwX7Ku3rjffsj+vkv57mDQ4ZpAb1T77zA8kUPMlTj0aOfZzO+ibuGB+Zm0r1CzdCqbH7Hn+AfPfBKPveH/zYv3zzNI8OCK/e/iLsXd7D3/HN86LmPcffzV7l59RrHN5+nbDZ83Zu+iEeffIK3vvsDnCYjN2U/L7GUKWmANJCT051638Dm1J1/MJq6/CzhHMTaLbwNa3Tc82HmJS3ho6+l6DxF3dW7R9Jm9rkw5A9eJII4njS+tsQyaI7AzQnpg9+S3cA8Z7qrk/vng9m9SP0AzSLklFnYQEkTNYvHl3QvwqahFandhQm7jbtDRHRC/xzSyLh/Fb9n577t1mfRYoKyKIRkv9fV/H2xXklEkKeERHhX4Ho8n32H4WJzKmmYyZjt4ni9KocwJCqeL2o0qFAQ7gbMoX3J3LCmx5QmOFwRqIdPR+Lk/PD/3T3rwpyTFKN5xFF8EkT0616/le327/8UH37LZ/j8vwz85d/s6/66v8UuYrMLrU0wLDFRuhYsR4KeiXd+g3gsa7iHOFOn76RPEtihS/MkaBDQ0sytArPCPA95QJSPqyre8Xg3EaNAQDZ+A3lx0S5YuAMlc1ig9RS8VvfBFDGkJmqKxMVOkJiD2yVOQCaF/tdtI1ERxqAlGcpClUXqTLGlTbWgYkySQAcGMcfWVHd50N4geIGYt4KVznYc6AcX/Ja7ZSwrT3+csXUOVLAkLMuC/bwiJ2XdJ57bnvAxm3hkgK1l7nvlg/zqg4cszKhDZ8sEDAw2UIbM5o4LPJo+zvEgvGT6rFhu+G8kWaF1Lr30q3nnn3wV//bH/jyrvavcv03sP/Mo7eZFxgKfWD/F3d3oxVjfnFheusTv/6ov44OPfJz3n5xS0siJKTKukHGFjgt39tlusRYwRBPM6k75U/CbPu24h75o2e1DBbJUh+ASdFGXpNbotpKiOY6nLrToEFXj8BZQxvh+MySSMR39awXepv4fWBnAnRAZ5sdBg/BiLptNOjCR6KNRWqOljorLZrGw6PGZHgRahpZsZyjsHPC0Y204lcaTFrM4O8LlubOxbWTOm2Ca6c2VR9ZKGHr7CI/NcSD+MVELwvZZI6HqJO6Kj9jzn8380N3MPX+NKFcapVJEnN5k3Z9bjR1AUHw8z0Z26iNgZ5Gn3ZU73S2AYqkXnajhC6I5MfI/9bj9n/4VW9boenurJOlYSoExVS+GKdNcDuHjTDUvOuIGFy1GWB+5/aa2UOy48a2Q1Amxs+BdY7wx5k41xumQ1N0CnIT1UtA8IqQJC2mh4R2sBh7pjw6CUs3H7xTjdjefHmalTMbjur2k7dYF7lCdfJQcuzAg/uBvN4y9ufVWSjS8CAw2C/pnU4A+0+aYvRL7pXuZ3aYDUaIfPceNdkw2o5hwRx85l/e4qY3nyhFbK5zg+M1tvXLprou89Yvv4vRAGZwFg6VE2U7+bPWMtARqHK8/wgeGm7yE17FsoQbBDwYDLqT7WH7z3+bKw3+Zk5vv47mnRtIF4+7zd/KJ0yd49rnnuFALtq186B0HvPneB/mDX/81/Pf/6ke4riv6uI+Me6TlHpaVIkLPmbYVrDa0d4ZyilR8cYHnIeUazj7aybHUsZgaJEU88cxKEF/89d3D7dtrT0LMNNzZo/u8T7IgQId5Qjeli3Mcu/qy0PoGTYJY5KqLL2SmoFYIxhJhxBP/dJ5KmmODc0zEHFcyU/kxodBBHJxKcQv3bgFtzkoZojAotRdSHOSm7qiaLCzMamzTw/fRojOsgYEmyeHO5blQ2Exu090EY9rck7M7rUgCuz8jms8YcdSCmTcJARf5mJxiGeqsIS+gRgUzWs3Rlc7NllIJa7V4rpV43ucFk8Q90Wdw4FO/XhBFUkTIg0dy9gomORxSCtBIHSQJVZpTIWZT2+bxskk0XFzwrkxDX2oOQpfuI3SPEDFvz88emlszTOqOd+edJ+A3qDddcWj7CR8QiNPgBVQ8cCjN9k7qFzwwdkzC8CK27EndENayomNmGDOa/cSvzWZCCbXjNIwuSClICVcic26YBUk+B77Y8YdOzH0VSUKjug3XxbuYUVE/foyj5z/OphXOyYI72ONYO0/W5xkwzvfEOckc5cZkhZde2OdtX3sfv3rfyMo6ieogfR3QJtRt8RCnMC9YmmDbZ/m18ed58fj5XN66P+Wu8RFhxTnuef1/zZOP/RPWT/w70lp56CWv4J67H+DxRx/l5tGTLKjceGzFo7/883zx1/1u3vWRR/jRDz2BHd7GkEckD8xds0lDpm0UEXcl79VJ5Br3TWnKZBWkxFQCWZU8zhnrBrNhrghJi7MdxeNT50G7k+iWUFI8bH4FdhI9saA1Gjm5RrxsN8yGEt3bJqeXiYZe2k9vE7CsgYd6SUtG+Ag4w2NmcDjFyTvQQbODLaYeqywwO/wjTl1DZ7JMRnCZZAtep0WcSTegbH1a6reY687FjbPiYvGbe5d4i7NWvA+5J3rIOf1U8O+lcMY4mb+AeE9p5tdl6HEE6EzcDzK5RVdqBGMg6D3cUhgl+oR4nxym8+/R4n2bg/I+3esFUSRBgjIhYIMrE8wFSSlA5taNWit0JWf/BWsLZmh315CaHD9x4nZzkva8sVPdYYHJguJqYRJAvKkqkVsjZ8uuuAcb84nj19GNBeLrhOQwKWgNw1fCKs3Y2bU1OjrMpgT+xcesyKjIUpDB1T098M0UWSRFOlt8HBLt7LfYdAruUBM9aKexDTZ0rBFIklxWaa7Ftot3zQP47mDYXn+chnJNK49xjYs9c6fm6GSEG6nRKbxElkwPvoh3PbBkOSVs2Umy9dGwj0g2bNOx5ksBNaeeOPS35aPbt3G8/zoe2N6xe3N9sSkkXXD/PX+YZw8f4uojb+HZa8+wf+5Obrt8G9dvTpwc30DTUzzynv/IPS97Db//S97M+597K08MB04IB1cI1QLbU9LxTcb1MX1zRKuF3pwMbc12Y3gRXxQOqgEHNDZM5A5jSmGnJgGbmedeB+4oFjgayTsk5/HEJNBQ32+T1fOzaxcEl0/WVn1BGYa1gh/EAu4d0DsywCSdTe4MQ6J1RSZFUnLH/oionaew2cfARNCcY4AQqqqb5hLcTZsnW91NFCoaXXAgc71jElg7YXI8T8R9PuACJYyvcRYV6/e6SY3PE8xyqMt054oV/PmznycK5I6N5POGb62ZF0NOB/T3OKa/Ht6dKPPyY763nSwj0XXOH/eOvHV/Johl5q9Pw7z19QIpkjM+kCKsp2NWdqTTTlgcmbj5g+mO7IsIJbhoswl7scY4nJF/Ldj4Jr7hbbCzjHc+qYR2RdF0hpFouLQExOE/J7I7pRwnF9Dk1memZG0BolfnzCnQWzja+BJFVBiClK4q9Jm0S8ciYhY1N+xoHZNKMTcrOFDhfM+UKNBDkNiJxcxaO0tiOyvq21TrdIuMlEv37CgQ/vvA9vlHmQQOuvAa3WdQ4bhXrmkniXeEt6G0c0t+8bNvZy0jHs/QyKoMkimiNCl+ONQaihKhDV58pHuU6lMn7+Jo9QCvqC8n49dxbnZIiTsPv5D1ax7g2tNvYXV8k73FQF1dZnN0wsn6Ou3KY3zkPe/izd/0rXzzmz+fv/VLH6COI2aNViZ0u0aOb8DJNbQc06YT6rbT++TvQas+ksaSTM2w7vheUouYYLBuZAl5QXLunR+K/pobGCEmhoBsvCupCMVx6OSHrohQ64QVI/fGtoWVmMZX61CwkNP6dtx6g+5O8knCZTymFO/IzgCaectrUai9mDp2OE89Oj8zEAZWjvHPoQfW/N7L0inImQVf923zrFUH2y26Zjf8HsVaBd8cS1DdxJd1FiPz3K+dHdLBE43fqRPNZHAm3XchyOVyRisSiaVL67uDyTT+Mk4RqrtMqn4LA8B13jNs0FuNn+L/GMXNf7LXvJIXxHmFEgsXE3ckAVQTYzhGx+/l/nDhUDPjMTTPftkmgq4gYeTgYLljEITLirLzxsMxIREn/IZXzq5Azt1oEIjQ5JreZAnSgA4LP6lrJemElS11MkovILPzTGzwkuuGsyZS9n+DYzu9OqUik1HNblpA4EwI55pw1yTUXhnMWCpghYySyWxVWDYPnWo43aVIZWsTff88Mix3o3YDaIWDm9e4TfY5yCNP2Yan2gl7CBdk4EBGVBNFMievezHvu3+PUxpjqTRLJF0xMrC1DuYdSY2xjd5INSPNs35Scqzr5ORj/OpwnZfnN7Lfxnj38QdKO3v9Lm7c/X9n/cxbuK8+y8IW6OKQ3rfotOapj36A6088wZe//CX83Eee4J3XJyd/Hx3RT05Im2u0k+uU4oTzXjupVFQjuCpCxbQ7B691g9Yik9t9RXvuDu/Q3T0nOb3Mw+nYuYdL8gKRWoLuS5BZZeXPoeHuI2Ee3Bqtmo++Wv17pXhIq7upm5p7WdLORls9W2RoivWG4jShzu5uTfEA+XiqqIWbj6jDEMFnNLIbVVuHeC6cKoXfi4FyCuaadzpSSvw4tmN3mEXcbRLvyiLcabddx7tG5zt7M+O/CWeYuOEdB7cWUaHaDKD4q7cSRc+LqAWcJKJOto/82vi2/jeDyuVhgNEQdWfDaIuqjETo36d+vTCKZGCtM9fKTyOgxi/qKz4/iYLX1Xtzqyjzm7LHF/Lns3kpy07w9Umiu2a2WyS1CT0lUncMM+t8OsbwYI6BOLYkcYyFka+5F6VaouZEHQaWMrJgoEijdQ3X8GkHqCvCYImKQPK8mprcRdpTDtldMOYuAkjiD1/WRFLhIGUuTI1kHUmQe40FUdzSOwa3IJLBsqsO1Mi338+E35xVYDBl/+qz3C4HPNPXPNJOKb1zKAO3yR4X8pILaaRZoV9Y8syr7uHqXmGcMhRjtIRmd+9stQfUNGfrCLl5l1WlOxSC7rii681z/Er6aR7a/wLuKOfmQQ3UC81gS+xF/2c+fvwT3PP8zzqvtXTS9ojNzZu8/z3v5Qvvu49veeX9vO8nf5EbfcH+esPxdJVeb0I9xdZrZnNkNWhTc7/HLkjgdh716genE+5Bq0QejVNXdPBxrSq7bql076rcldsYaCFd9ElmiEPamkSn7KPd1hxXTyZIU4Y4sCQIXcX8wR6Yx2mfepN4YqiqmweruWw1y+CHt4n/XqFMcZpPFNN4vqZdwiQx1sqOl+kbdx9K6iK+bw+9OH6IWO+xnY/OTGTH0nAljH9Pk8DEd+2tR5d0i/RGCe5i4P81nmsNrfgc02Idj3P2nzQ4zxaYfhjLxIg3S4zjt/Ka0id2fpvR6PgvFtSnMNk468o/9esFUSQRpcvgm0WxIGIbkiBppos/aM2MrELWhHTdGVKkjndg7ngKCBS3TbOeQinTY3yxnd424WA94mx915NKEDIAybGA8RN8iI2e56p4jAJDZhhGlAFlYJCMFAPdeoG3SOFTJx/nlD1RLvnp1sRH/do6DceqUix2JMadLIlqEzm5O1KzQpJERn0LaUThDtcZI/Akw6yhDYZhwenl+yhBxDmwzKCJeu1J3ic3MW3cwYIVmYNhj0O9zDgkpB5hg3H9IPHRixnDHQubuk7YTS8FrcZUOnQN5QvQvXMygxwj2pzGZ0Bvp3zw+k9y89xn89L6YNwM6nzK3tzB5eBreXTvAW5/7B9gZiy60U6e4eMfeCf3v/I+3viSV/Gqtyfe9tjTlLJBW8WmCZqP/NoI9gJgXpzQgZYGuibn/AU0YNW7qm2CyWBojb3mjkulw44Z0yNrpkZuEcYmOiI3ze3UMAeeo127dmqDVF3SN3NenctXd3zA2iYMZS/FIiqK5kzG934heLR4YW+xeEyBa/obHDi/uI3D7NpuzXFY/94ByzAvkILnaQ79eCPiuGAvxWGjKLrzaOtMirDhM9A5EiHAy/n5SdUYNQcOaGwDssJ84dqV6PSDBG2O+2JgNfrJuTATXagJRM7RmWqbHW7sOT1+SFqwF2YB+cwMgCFwz18n073l9YIpkpJXLryn+Y2OxJsFo/mJN+cFW2sOvlf/5WcVi+5QIlA1xNwfUkx2F5c4UbJJmFDNJNU4YWeMiXATTxH0pMrC/KMmyZUxoqyGJQNLSku+ka/VrfQdjadboiaja6JFiqHUTg66wtQnJBhyGUMyTi62CtpJaT4JO0mEVhs31BAdWPTgd/Xmu1ZJ8y9IUif+NilkUbZdaefvZkAYQsmxscbTz3/EKT+irCyzn1ckEqU+x9aEcTFSFpnrr7uXX9rfQBuQpkg1NwapDawgtdMnc7leXAVTH0upnbwt3n0oIRN096ZG5+nnfoH13hO8cnwTAzl8MnegL0t5Oc898Oc4+NjfoU3XGI6eQ64s+ODb38alSy/id77ss3j/Bz7EWpR82miT0atb0YGrqdr8kBDjmSpFQDQ5U8fBEy8sFcAv4tQ7vRhjGJqYCVaF2gSbengEQM2+GLTUkF6xYQhLvEj1TIr1zLj1X2utfq1rN0p4RA6tutIEsDZhfcUcVudVPrxCJVpDv9MxDXmpWKjDbhlbxf+KB8gGLc3CEag3sEYPeWyP52LZfZKp3SN6ey30MjmXV299ym4lzpx1jhpCjllZI6pstbtIIYp9ZsYqfQGZMJ/q4q2v3eLzb8F6RdAcS8qZssUt95sFlXC3pZkPkrN5zhdJrv3uEdXRmu1STD/V64VRJFVJy3Mk20JbY3VA6SSpSPPReDZkkObdUSf887r/8u63F9ScWUM6o+vhWuyu0ikoB9DdYdVdwXNwxGZwuDuFRnHFAoNQm/oNJ/PonUlpiaQF2sxPWyvUOtGthvnCED/ETA7vSE1YHyiiFPVM6A6kBLk6N9KGPNPWwXzb2KVzsmzcWAjbsuUgueGAJedtOhdTgvvnHXbKsEkTW8ksLtxNloGJxqkbjnH5+tPsqbLISzqdG+Um+7JgMVxmWB1g3Uj3XOLjL77IcT5CqzB1w2qjTolqlQJYgdQTtc1KHiXXzkT3B6s7BcXETVpF3MWp907thRvbD/DL43O86tJXc9BXoc6B2QxiYZdZv/jP8ujj/4D7rn+UoUxcef+Cp1/xKj73FW/kVf/hkHc/+TjT1Mmt0Jo5mV7N84rCVLXHIaTSGeMmGdIQCzr3o4Tuk0sRpGfaIhHsY2pXpuJZmpSGlYJ0h26mhbLGWJZEmjRiBnpsX4WqjU0gcblJGJc7bWdrQmq4wQoKNniXZZUmiSbd3bO7hGSyYAglZmnV5JzguWNvni0uMgSU1RhxknYxmKzhFJ3ZASt8WBF6ifxyTZ6x1LYeY5F86mK2K6TunNO9mVB8aeURFCrmXXpMedrcGq7b5FBW92XnGFxnElSaDyfxrDiQ1unJA9UGFTAPgrMZOY3nNWvaKdlojcFCzosfAhOVIYpnUSW1UKYnb4Y+3esFUSRFlLxcIlXoVEqaaHiOicZJ4SOzQsVtoVrb8Rdn3tOsQXUO5JkO2zdkwUfM2Qsc7tWIJkTd/dt5XEbrhnZhEaOONHPg29jJCrN1NKVggvkI3nvxcTLNlk/xY0HgnI6BOL5S/CL3jjSXVlYxLGIHunrnUgOtctPfTEW4vlQm6yQXudIHV3KICYsK1hNbUVZp4Thv2XJw+0vYpoF1r2xxKsvi5CYLXPXzXLnBOQbOj4cMaYW2Sp02lGGfxT0PsRkL+fQYMSN1ZRtG8XSYzB/1VrzzSx0o7lpjYqCeTmd4Cp80MMsR7VuZ82O25Tl+Zf3PeOld38Dd3BHafRx7BhJL9J4/wcf2f4zbn/o+LjwLn3jnL7JaXOZ3ftHn86Hv+yhTh1a29FrdATvy3MU8Y9Vmnmzr5Oydd4utrRA2/tao2shB+bLm/aB1V2y01qmtkLp7WxZzM91hm2L0DuOo5vekRXB1E6had51YmjpZlFEEqf7etMhpd9mjMzTmyAObnwWzULH4S3Gfy2Ab0q25E44qyuyvGpQXOn2X7tnBJr+IOqDmEb5TCulgq6FYM8gplL5OQ0raAn/2rbuLWqK0zYWqO8Qlvc2PZ1D2gkwe7ho9JkbtLvbAZkNff4AsiO7dXEmUhkxtjdyiY8SXrsHCZ26ld7BTfI718GqV5H6r4RTl8bUv8HFbVEjD6CdHG3yzK0PIxWY7JgmgGHrXKChhBy8QUhfmM2gW1s/ZJjm5xjV3IUdG8GxVImpomOISN5RgbLtvZBWlWWcgg3aKNfq0RXElxUITWYe4MB2jerdrRgrqA8zXyseaeXOo4jdGUqPn+DlyghyWbdVPRPGZkVVKZBIpDH+Zi+VUyXQukFnQMNmy7sX1trqgXb7PQ73E8dbBYLj2FE+3DYdNuJwucyCZZc1QMnVMDgssG0/LFR62LUfZKK2SJgkLfoEutOLYahUC8/TTWWM563Q03wQ7vy7GPZ8XvGOfpWSl8OgjP8TNe76Ilw2vQlMGqjvwRN+1f+HreHb/Qdbv/Vuc+7WHOdkmXvpFX87n3H8PP/n+D3m0gnRadxOMFiMs4c0ogW33OvmYaXMG9bzzD79DPyOxBtVSHH5zsXEcnObqrabCGPk7W4VijnWqJJr6wezGKTMdRTwaIZZHZi3krBEMtqOkRKhXbLZ3ueI41umNlVF7dHA2G2MYtMqOIKjOdOjNnyXt7Lo0ja7Mt+ne/c4aHolizS5CJJy4ROLNITjOjvPNJrqGL7463vW1XsFSaLnnBYpbvs2hZf6+aMBlQp7VPLhseCWec9VL2/WY/lydDf0hBSGJq/Ak+RbbMH/OxONMepSLmVcq+dOXwhdEkTTzE1o0I3lEh0U4pPiYFIkYcQPEls+EkiqzFEZxmyzoOwKqEUUyOJK7kdcPM7+pmmtLU2wtVd2WqdWGMfgFVCHHZtuxluZ4TpuwlElN3QTDmtMp4vT0B6j7OCyy042X5K7KquJdhAhDSpCMwdlASHIZWd09VK4SVqvcVRYc6oLjvnZuoGT318u+1FpUT3gs1mEYOdbO9QvncL9JYymJhLG99hj31xX7eekac504YkOyzlQ7+yyoo/DxezofXFUWZUUxsOp4puO92W21wlgh4YatLRxWVGLUVie9S9zGvqAj4hHwh1lm/2njuWfeQTl3g5effxNjGvyBCo9HQTgcXsHm9f8Njz76Pdz82Ps5FeUr3vgGfuEjH+GoTk7B6R3LM6fM3d81+djbzDO3qzSsNh8frWHiPpN+vwXLoTop2w8+XyC23twIxdgFgKEuQfRIrRbXzEfJjvMWheZxyF3I2RDtTK2SYlNMLFBm7Fxm8noUxlmI4MFarkhpuEE18Z4SEsysCdQR9yR53paxM6gWj/vQuasLKp0b2g4+Qg8+UVlWhuXg0sTqmGBF/P607idhbJ0xf1ZL75Q2oXO0q2einnEWJYpqqIC6EstWJ+qn5HimxcJGmoRE0k1cym77rzOBcu61HaVq7pcp4vp8Uw2aVqjSkvrHhoSOL/AiSWu042P6ANmc/mCaqCl5N9mAZm69TosMGe8KtakbigoeQh/UGe/qXIroVAcL95dMoqPVHXJIuBEAHdIAjQCrfeMl0YlKSpDCzD6wUFEcM7ISN12lSA07e+e7ZSQ8AWM07t6lph7jU/J/dFDvcBVK1p1p6ziMdCto85O8tMbRoVLykt4b21GgbtEhuflCV05qQXBM9GracqU2VpfuJSEcSGYMDMquXuGJBFe4wV6Y7F4ELsvAUvboF++Bhz6HF9/+Yg6Pfpnryc1CJqDXsN6aG+UkjoOacwCrJR+Tk+022iIzy03pJsFikIA8nNOaYjs+6Mhm/Ri/1q7y4G1fyUX2XPccGTEisNKLnL74/8T25ltZv+8d3HnfvXzBi+/lZz76ETbDyLBV+iIxVGc4eKJgYqwzRp2gBZxRffxc0hkMLA3UEVrrZFWKVh+51eVegyQYGqVVJCuWE5OZSx8jg8YEeuqgzeM3xNzBKbiRM91Ms4+d7sanJNLZ9UzJH+bYVjvBO4Uk0RU8XQ3NnggooRxChEJFaWjKTvyfV5Q2Y6E9MnJgIJYyqpCVQROaIK8UzQuGRWJYDbTWWU9r+lrQzcBUC3VWiIU3o1qHqbofJooF/9jrZ3SmoXIRoiMVnORN9mVX0IqyOrZrMW1Ja54XjkejzMuiGZaYp7ZuRtLRnZWkM4hnXM2RK1K6+z1UwbLCYmcv8hteL4giab3RT47oC+fc9Tph5maz1roDRd0lSM2M2t1U1wwvqDipfJdVE10hxLTRw21H8PEaIzyVfCvWXbrlQ0Y8+CZxQxlIp7fmm5WkiI5o99PXwqRVcyJloU3m6XwGsxO6xfiZxgEDammhqU2kBEMeaNmxHlXxrkLCF9A6oglLnZaEzZA5/Y4/xNP/8D8g7/4PlK5sXvwGFve9kunqCc/dfC/9o+/yzXhrrK1xsrdif7nnJroYVxU21vjRo4/wGu28ZFs4oDPICmPBdVmyyZ079ITltce58kxmuuD0DAuVkZgfIKiRhw6D7ILUPA4D76SDfpNyRErEaKSJuNmDWhwHSdI9j1UFSEpPWz50/NPcc+4Lua9egjzQm5CC/KsyYOe+gSeHh3n4536Wr/zqr+dDV5/meRVyzzD6Q4pqxLxmRA59u1kLfbuhbRvT1iMUxLLj3+PIkBNixmCKWiihBKfkSHJqUXfKVksKrdM2W2rfeuZ2EhgMGUJi110UkVLQ0loNyg9IimmiCaln8jD4waHB9e2uRpPkJhldo6hmRc1Li7RbSOzzVlz8QOrSSKkx9E5qbqYr4r6lFi5Ugm/985hYDAkdlHF/YHV4nuX+yLAaqGZMJ6fcuHnM6fGGca3kqdNmKKVWL8LJ3x+pHU2+LCMmQVWPDps7SjTwyl3WlAU8IswcSacAzUXQD9Z0S5EU8Wkl5eR8zN68uFbvcFMWb3RiUhSt0bQnZEzkxQu8k7Te6esjaB7SVa1H5kil1xL8tRlY7667FHW5EinE9D52YLM2u5+9gTjuMXvddcPJ6cltnlxRQ2BecS06hLe/FzsjXM7dsNd1sH1XdFudHAhvFWpDWt+pfWRIpIWio1KBnAUlOwSEkIcRWWR20q7kypXAm10VkkDHxIjy0Bd8Gf/u+sDR+Uu8QZQv+r99F8evegUnV4752C/9EsO//td88P3v5tWveQ0/8d53c2N1D1+02EeWS2xcsgUer9f40Td8Ge/RgcVoHG5PyYsBS52Hf+XtpJz5puOn+AP3vojHVie0VBGy/3wYKTvBeRAjK0wL7657N+oU2OyM6XHLgUMcTuE+4N2Ef1wQkiyi88Z/nlEYFwuO5IN8/PwDPHh8r7vBgEccRNHdX30Ojy3u4Z5PPMpXvPF1/NxTj1LHBQlhzAkTz/4TzU6z6Q2mDa1sKdOWk6lwOvk1cw9HZViMvgwxCWOSyIfpjW6hdOm+WKoi9NLYnJxw8zg4o0kgVSQZwzCQUQZRxmFwF6SmHltinZySQxGqpJbDgmzmWbp7jqYUixNx11sCU0PIHboKrc9kbN84ZxmwEbdXSwktcYNrBxlIOaPZDy0JxUoWIWchLRKrw30unTvP+QsHLPcXGHB8uiYvbpJXp5TjNX3dqKVTpy3Tek3vbcePtU5koLtWbcYje1z3GRobcnbiffeOM4sXNXFtaKCbsYyJCU+TnMVk9OhINYxykpKGjGnDasXiPdOsQfrvaGuoCD0D+QzX/PWvF0aRpFOnY3r1h6bvmPot9MszLaF4oTNQGV3OJaHQsR7B82ClOndsd5MJs6WUETpqDZ/zefnVNZYJgDnptnfHAztAanR1/qGmGsV4XqD4JtBao5etywrFQeKUE2mRSSuFIQpyV7SCVHzEJ8aoOH3d2c0xlKQ+bknuqBoXVvs89uhH+f5/+1Pc/Xmfz4u++kt571138/1/9x/z4fd+mMPbD/nO/+rP8L1//3/m27/jj/IP/upf4032IM8n5aEHH+Dc4XnWJ2s+8MxN1vffy/u6srpwjoN0jtsPDrjvwoInH3mM1QXFvur1/JtPPMJHDp/nZBAWJaC91HaA/EoTSTPr5ax6MMpWGNTYSKeUFg7YTrOYG4HALVCtrl4i7MC6F91hWLLc32d5kFgcjqTFAWlcc1Qe5/Cp+xjTkrbdYrWEOYGxJ3fwrmfO88V3Nd700sRTyVjKwDg4FgYE1aOTu9G2E1OZ2E6Fk40XSlVh8EvFMAy+PDOXMqIeZyytU1t3f0SDMlU2TJT1lu1qRRsLui67pMFhGFisVjgqJyyXI5o8zvV0LZTJOaTNDM0DqQ8omZRy6Pu9uLjXkPjYOCQX5Ih65929i0uTOU6oyjCMDIOQFl5AseCmJr9XkUSOfHfNEhtfQoGm5HFk7+ACB5fv4PLli5zfXzFI4vn1lr29E9YnE0enJxwdH3N88ybro5v03mnTRCsO6aSUQxnk3V8T50C6jwJYHLIkX2D5gjqWtKpYFkQ9r0pmhotJ4JTc0knOHzNk8AamWkPV0DGRkiKjL0RTF9ehNyFlpf7/Q5EEc1yvSoQd3QLAWvc3VWf6RfMWOVIHO825k3GizB2Mb668W3FlogPopuEwjcXY5F1Ln0+1Gf9EHFsjwPawqTeDri3GgETrBA46s8PqrruRlNEhkUchjeJRDuqnZUdi9Bro4fRsKbh1rcbD6djTkMCGU3o31C7zln/z73ny6Q/Rf/EKP3XjWZ546Wv48X/zD1lvnuKz7n81h4vfy/PPPYpwwvEzH+OBO16GlDUXVyMXD1f0wyV3nzvg2R/9cUrp6N6K5XDAo6q80wpXb15h1Qd+4bEjbrtwBy139pMG2s0uXyUlIS8HUjZWqXkHXI2c4v1NA7KBOnlXsSPyBuCek7rFW/KFQ2++yBAZyePAwcGScxfPcf7CkuXBkrQaSIPQXrLmynv2GZ5fUtZewGaW29AXvOM9mQffeI47XnRE0+QPTTKS+AIGcVqX1UZpjWld2Wy2bKctmrxISoeUMrtMle6b4AzQKtvNRKmV2o3TzYTUzEIzoyaaVRay9ZFcjMU4sHfuEFNhpcpyufAAsW3jaL3k6HRNqRXa2ju/kuhNwiTXfzMPqqs+nWiCLgxjbGYRpPr9u00VrRnNSl7BcoS08IvWq6CtU3pyj9NYB3nUcEhMtfuiM2AlYYHlA5bjIXuHh25Lt2gcDBPbg8K14yMW43UMZSpb0rT1YDurIZH1a669+UY5THqtRW6hDH5AqCIDYZLcKcHc8GbEi2Qx5wz3YBRoVw/EwzvN1CN2IvlIbT2mye60LangO/7sDmE4xi8pOVH+07x+K/EN9wH/BLjT73y+28z+pohcAv4Z8CAe4fBtZnZNvLT/TeAbgFPgj5jZL3/G74HjiFarKyNiKUJQISTY+2ZGSjl4Vv4XZwv3EssBjdk5dYLA3GgSmmvO9Ky++fY3qkNsZwP87X2nvuixWW+tuI3Z5AU7a0R3tlmDKr6x6041kKSkMaELJQ0Sp73SFIoKfcxoT4w45pSCFuRQpbfHln0Er4sJTZnUDnj6yc5j732SzbVjnj455fnnjvjab/gdfP23fDWPPfperj56g1/5hV9kODXe9RNv48HDO3jl+TvRvOQTTz3B1aOblFr4n9/63Zw+fwNdrJjqKWU0FouB1Thy+c776HVic3rIbS99GcerT7BZXHcDhugKMcfNbBD66J0jKmjqO9YJycgpMSWlVT/dp1rJKaIA1F2X0jw5CE4i14G0GFnsrzg4WHL5wjkOL+2h+wMp+T/33Ct85FcE3gvl9JTeWyhsHId67JdHLr7yRXzWFwpFjTbCsO2s0shWNzsxglmnbht1qrQyAeYWutW5kVNtlNp8K4wv4qxVBh1ZTxNTrXSENg3kvM8mnTi/j0THGMbMchzZO1ixXK04v1ixXC2oVqnbynh0RLqZOT0+ZSodsYFanHUhyTfYPdzsmc1thVjseCHXPJBGoadK62usQk6JYbSwYU9ozpg174SZN/AKkaap4ppwrAWXdISeqevG9uSYaW+fun+OnBbkDMPeQE+FZesc1M7JyQlHOtAs0+rGn9twWEeFGrxfsYa2htkQtJ/5qdSdg9Bsp4a5LJKAYzT40dbaTF7a1Q+T4EWUhrXgVeIHnLSAMYKDWdSnyWIFTHxbXn97nWTFc7V/WUQOgXeJyFuBPwL8pJn9dyLyXwL/JfD/BL4eDwB7CHgTHj/7ps/0DVSEISdK85uxF/MbJENTT0XT7qDvvCyAyOIN2/yxq2MSjQCsg86TfJOYVMnIjsbTkoc0zZDGTpYohMYTVILRH7ZkfXZINaG2hplf3Jmo63ZbQafIhg2GjDHqNOf2WEi7RBRLSjWHBZIEVARYVpq6emgxOI62vZl56tEjjq9s2RxtGYcDpnrKyY1r/K//6//C5fvu4miqPPvcc/yFv/gXkSQsnvlVvutVd/HqO+7kWr3M8fUjPvbUR7gxHfOeT3zA5Whli+jSVUmTb/z2VivG1YJrV6/xgQ88wb2vXzEurmFpi3VFLVGqL6ssa9zYzR1nxBzwR0g5UUdlm5Vpq5RWYcy+CTWHIxzjNRTn3jVR7zbyQB6XLJeZ1cHI/uGStDcgeYmqY2mf+9WZD9294dmfNOrRadBjvN213rn+wcJ7jhZ8/lcNHC8rslKmVhnIVDG6NjAlS4JxwOpILYVeGqUXWu1sp8J2crxZRalR0FsXptLZTJUpJJA5J/b395msUltBBcbFgoPVHhcvXmDv3B6rccE4jnTrnJweMbleiWydo3WnTY6Z19ogdaeizcXRjCxGHhQZ90mLgdVin6SZWhqn/ZQujoFq8sWGqfiWXBOVKcjdPgykwDvRxI5oJMkXb5O4Vr1MtLJhu95w48Ypm8kVa601Si3+87VGrY06NVqp9OY2KimL07qiU4RO6g0pFY//nc0qBo/yJfYFEv+uzp/tGcCnPK09YLEZPNvt/FxDZjPeTYS9uXRW1EIiKfTUaX2iS0VDlWPlt1EkIxXxqfjvIxF5P3AP8M3Al8Wn/WPgZ6JIfjPwT8xR+l8UkQu/Ll3xN76ie9KkiM2Fy3adoXYLN5Lo+MTzLRJOFJVWXYdtnUqjzrI3n+Vo2SCnXRfZstAS7PQywo7fheDjuIR2OzhkRryZ1lFSyNh6jJZe5AzX4ybNkBOWM9XCcFUyiIdCedCT13qnOvvWPgGqGUtKk86giVSVZx69ylMfOeHo2prDg9s5mW56kbeBnIRffcf7kHc+Almx2vicB1b8oYcmfvoLL/P48Rfz0pPbSQMcLM+zurjPE/wK5ae2aBoYx9GXRwK9FgodXS0ZNGFyivbOnl2k5iNOmCIPqEf/XZhNS10D7KC7CDBkkgzoMKLj4AT0jYeMrafJ30uCV6fBnWzCYDMtRGnV6JKYUDZmDM0Xb4ukDAz0OvDKV+xzeAE+8INb5OZM4HdyNl3ZPrHhbT848PXfegd1seXZ9XVOa6Uko4nHXoR3LZRO2VSm7cTJ6ZrNesvp6ZYyFboFdjqb6JbCerNlU4orRnKnibI/DBzuLWhlhZkwjksuHV7i7jtuZ3lu2IVrAgwKtYVliEHJwnTS6NtK23Z6bViq9OSwUg0CvqSBnJcsl4fsH55joQN1W8BuUjaV1jZ+/1pmkQYE2K4nyrpQS93FWgiR+00h5UxOI3lUquJO9kyR1b5lWyfqyTEybUnm9/9UCyfrY45vXOf45jVO1idM0ynGFpGGEaYx5oIQiCmxCtoi8lcVURdgdKLPCY9Iesdqp3fX+GvxKa+Hd6eoj92Ym34AOxx4jkZJc4GxUNz4OwjF0Cqh0+/oTr/0G1//mzBJEXkQ+Bzgl4A7byl8T+PjOHgBfeyWv/Z4fOzTFkkD6qiYuWmEJt1lH4tpOPQEp2zuJnebaH/AqgbrLLnvY7a5I0nIQtw01byx9604iLl0LkuobXBsQnoP7pCrWS22hdZTSAt9s+taU4vsEOf55SGRF2PICdW3qhLONpq8WCeJ+InonLtv7hDfoNaqSF5wfG3isfd8jOceP+Xcwb2uc7YNrZ6SNTGMIyebLaU2FtlYSGbdKnetOvcfQOoLXnryZiD41OaB9efqiznYv4tCQche1NVP3GnyhwgZOT5ec4Ur3PnsBQ4uHlLtOs0m2jS5lVdrtPnnds4Gou44lKSThyWiK7ItkFHQ3NhuC4N4Me/FjZTFxD05VUnVl2HJhFI6x+uJfLKlLzJj6TBklqMyZVc5bXtnPFjxkm844L0/8iT7J/t0zK29DFIeoJ8g73mcl7z6Zdx34S4evfEUTx8/w9V600ntBE2sGJvTialUjo9POT055vRk7ZxZgSENJE0MQ6a3iVIqtTk/0MTQASx1ksGwWrKdKqSR5bkLLM+d4/DCitZgW4p7WqaRw54wG30RmQeO2FC3W/K2YMUZFVmSb9gR0MwgQkoLxsU+w+qQfVnQdaJOjdPhmDUb6lQZFguUAaud003F1tWxT/M8JAkq3TAkz5gRz5ISn8W9kz6Z6MmwbaeMA2aVXtVltG1ivT5lunnE6fExp6en1LZmEC9RtbnBDNYiw0a9g2xCrS591NgZaI+gsmhkEtFNYi7QqM5druHmjpjvGqLBkmhUwKl0iFIjkzzrnN8DqsEa6eJKqtoiuvbT173fcpEUkQPgXwJ/xsxu7rzZADMzkc+AfH7qr/ddwHcBDKsBG330ygmsKtIKDfVIyDbLtJxmYubnjnVXrbQUeR7qXLMBX+unlJGcqAknMhsIiaRpjlx3fmIY8Prp5LiWi7Ri9R2bODdjCfX9DHuHW7QGWZaU3LhUI7hInFyNOs+tSuCmZrTaaLUECVbdQcY6eTPy3JNX+PAvfwg7bdx2+32kNDIOKzanJz7u14KOih7sMdbsUII6tvLWJ0a+9SG458aXc4nzu/dcxaWzD6eHWTMxSFCZ1LmdrgqC09M1t911N8t2yM2jm3zkwx/iweVF7ELntB5hJmxbRyR7cVGHFARjGBeIDeRFQtU7nq4r580lsLQBhNrX9OSdVOuuVBERlniUgjY8JXFTSacTshgZth1NGzajksfMIBu0C0d2TGsL7nntxMd/5sNcWrze8VFVSt7wivIwT31wxbVHP8Zd99/P69/0Wh46d5mf+Miv8uTNa/TkjjdlU6m1s10X1qcb1psN2+3W3zsfFUiDH7aLRSYlfGmkiuTMcjUwDonNegunJZRUS1arQ4bVChkPyZKZNhvoBRk6iz6w2CTqNrOnnbUKecz0caK2bZgrezVw7qkF48Ix9F2EanRW2oReDNXEIi/db1IEHY2yMawWhqSesU6n9u5UKvGUT8PlpZMYNMgbSG3i+OSEbRa0FaSJizqotG3FamW73SDmPExpeKpjGN5qbxR1GERlcBqfRKBaM4ZqDGmMyFtvcJIJJXYNs2+sqm/IvR33xYXiC7kkzoft5oqh1B0Ga+KNkUtIBI2GpJvLSvvWv1arnU/3+i0VSREZ8AL5vWb2g/HhZ+YxWkTuAp6Njz8B3HfLX783PvZJLzP7buC7AfYurmwUCWNdCZKioC1RvZnzwhJAr3dzFrdGrP9DVyr4g87g636SdzfJctAEAiy2GLP9VsNdFiXGAo0tNMwbdoQIGQtuVpIofM63nKMmmgWGqp7r0evkGGmKTWJ3zW3vjTpNlCmAeG0oibKFJz/4BE/+2qNQheXhIWlQ6nSElS0ihVEzkxWsGMOYyMMS7UYrDTGhVvjZZ1/PvXc8xDKdXWIV2LDl5/PPkHJCu2F9jgmVHYdts9nw+KOPc999D3L3XXeDFWw9khYLNqcTxcKM1gpOX4KehHFILPAUwZZGNA1+e6YFi7zw2NUm2NjotVH7BpGOmvix1J1YLl3dVKR2ajWmrXG6rqTcUGnoprNYCMvsm9KpNbY3ThiW+7yID3LliQ9x7r5vwcy49My/oie4ce4Obu6d8ug7PsKN9VVe+oY3cM+5y3zg2hPUKrTJ5a198nC5LInVODJmp4YpxjiMLBcDecgMwWut8zU378gW40BvCdHY7uoCSUumqlCUcVwAnlXtMltBU3P5YF8wJNjfF7IVJrlJ32x8S2sunS2tM3Ufj3UyFpOCCtO2UbadNrlsMK8GxsWIaGY5rKjDgpvbLWNJjOD6dnAzjNpRcfZHFZhS9+UibtKbu5G3lTIZqXph9vg0L4jz/SPdi5tFnney4F4iaG9O/xGjZCEX2SmBUIe/XFffParBlJx8AvND2Nuk1JMvYlolBU4yG2vc6quZ8GvonvCK9IQ2JfXmWOfWkFNDWkZaiyTUT/36rWy3Bc/Zfr+Z/Y1b/uiHgD8M/Hfx739zy8f/lIh8P76wufEZ8cj5+4RnngvaW0iunCLSk7dxal7kmrjsSISdK8ksUO+Id4o5k3JGsrLAu0+34IvgIIuihS9quv9RENP9lJIwvIBw1J5PbYGeNFQcaSe7c4MG1y1rM1ScI1hbIw3Vt/KYjwyt0GrxG1S9mLeWuPLUEc88fhNkj/FgRFYLTk7XSK/QZ08+54paa6QJJ8iaUKubF+wvD5DlV1HM+On2MQ5ZcF6WnNPMM4sPsG6+LHBO25aUEqIeo6DiNIvjm9d4369d48EXv4TbLt9GqvusRCnHj0YH4lSsnBM6ZBRlsVixZAVtRS8Les7UHQwxkEZoZaLWFNnbodyxGLBE3d+PhlrFekHbEqudVnzrmpJzFGs1ik6Ot9U1eTpiI8dsb7uLt/3Q95D3fpk7L97N3VfezvUXHbB/4RIHBxd4/PGnePxX38oTv/Y5PPDqN/Om2z6L58cNz9pVbpSCLZbY0BFpNFtQ65ZeO0PLpJAKWhJsVkft4gESTYyTydhMQmmJbUtIMW5uGmkylltY145tjBSyzlIypSamqmz7wHK5x/5iYEqVkzSyObqJTRusF5cctsZUJ+ykM9aE1sywWLEta45u3mRzeh2sMOQVOY9uFKEuRVUsvFKhRqPh0SYu0MA0XHIgqbM7BKO1glin0OjdvUgbYSPY3UR4kIGK6+FBGM1NNJym50+miRugYLOmOpqJ5AKNjvkYHM9SwlkfTRUbQjlVxOWEyll6Y9zLmOdsu++mkdQwNNQ8xBKn00tjmhqn8TwNolFwP/Xrt9JJvhn4duA9IvJwfOzP48Xxn4vIdwCPAt8Wf/ajOP3nIzgF6I/+Zt/AtaQhrIcwB/AOMakED9jttmpvbiEvTh8ZxkweEjqO3mV2owIj2WkNeLfRIwPYE97OihpmWBiOhtVCrMs0TIx97eynm4+kqr44Unx+7eEy4q5A6mbAzbsMM6FYJ1kht+L689ZotTp3swuiRrOB1oR2BCs9QC8cYGpUVdpUGcRVArPB8Ly1S7gVvqZMqwVNA1/+8m8EyaSnC/fsG8IEHEP91zzAh+BtB0zbCSmVLE6sbW0KN/REq855tF555uknuHjhAu9++AO89vPuYbSRbTn2Lil1NA1kyegwMKYFSQaMkd4GmjNqvBOYx7M+IWrkRQJd7NILxdx4wHrHo+yVToFekVrpZUJMPRpDKlCocuKGtWaMvbGVzPufuMb7n73GKh/x5te+Fhav4XS6yuMffpy+f41fffR5jreVL3j6Gl/66JO87Au+ns//6m/kfc9d4c/91f+edHCecbHg/G2H7B+OrPYW7O/vwTJjMjAMPi5SZsAlY72zxS3ZejNOTxtHpdNIbI/XPHnlKm2ERQEhk6fGKvDq7VS4cbThxsmaSTMXVucY28DQiy/6emLTrnkRCPPnbkZZr7lxsuX49DqSBrpV1kfXsc1EM6G3PTQ5h1WAVCuDVaq4MYUrdRwiajhsIKGzrz0akfi81GNjbL5BTjP/LjBGDRjACK5lbywZA6JwizNRh7KqVUpv1OTPvKZQwYgb5Fb1Z6kqTvBOZ7jjYCmeVYJITvwc7g6mxbdiBrRQJ/WQX4qZ59jU6pt56dRkDOZNVf7tuACZ2c8zc7t/4+srP8XnG/Anf7Ove+tLwGMyIYqkItnb+jEPWIrAr2HCtluovmFOOZOXmTz6wz2YY2NbE0QzKQkWyW+IunFoa+EvmeLUgTnSgdiaE7EJpsF/DCH+jmkp6oH2TmXfeQAqCcQXD6oOHbicyrBWkK0Dzr3NnnuCiHciTYTehLbZuByyd2optLQAGrUZlhJN/UHMw8B2mmKzDM229NR5w31v4tLqdgCeXQsv2nNFw6q/mz0+gADfcs8x/8PDbnk15oFa62606t1tyVqt3H77RU5OTnjmqcd4yQOv5KPvf4LViyp52ZC+QETIybeii8WKcbmHpQWlDaQ20DZC65Weti4DpNHaFuuFrC47JJYS1qBu3ZFHzRUaKkbtW+frNfEFkzk27emPccCZsEFYiMGqcts9d3NplXnFi89zvV1B7TaYCndcvI3nrxrcdpG3f+BjfOL6Ma976gZ3P/we3vPUdd73tl+gmLDK+0geaeqb0P3VElsN7F84x23nL/IFb34z3/T7fhe9d063W45OT7mxucl2u2VqcHU8YdGvsTk9Ya2F68fHbJ7oLFbPM8jAXho5N+6xkAXrdeXatVOuHN9kMYwc5H2kCillDg4O6evT8OWEIokmA9o6i+kEpbE+USbxhUSmkFPyTfj2hM16zfJwj2pbTjdH1FacEmRutyf0oEF5N6nmka3+PCaKuDmtSAo9+gAUV780IaOu0GkS04DHe2gafUGiYJa8GJuRpIXNn7A1v4aq5qIC83FZQzkn87KUhEhAWKJOEOkdmtIjUXXWxdfmY/aAF91JlNZSLHVCsBA7DFHIvZETyCJhL3TTXTOj1y1zyJbneGSyZEQzPXnYUGoZSRZu0G6akPPAmAd3l2re6S1IkYtjYQHlzisS3amzPfzUUXPsRBEs+eZacHlkwjXipkAXxzfixNN2i4diRFs6hUfoeBA76uOR9Y7UhhV3DPfTzrtXUSfEW2u+Bc+ugW2na3oq9JRBhJwHVDLJhJxH9z9MPsqouqrh7nMP8tq7P495COkIz6yV+/ee41x7q39PM77kXuNffnSP63Wk9uaUkpSd5NsmetuSuvD8U09hdD5+/SplU3n957+BxeV9bkyfQEgsSKS8YBgWLNISzfuYDrQ+YBYJkBXYTqTBTYZ7L0irJOdtIIMyqGKlMPWCLYTaavg8uluS9ExrHmQlYVZsMRe4eMppYcUar/miz+Y1X/x5yPqYn/jn38/64Y9w+2rF9e0pdXuTF62ES5cmXryfubI6x/Z1L+a9+5WT5ZIvvvdL6OsCGyjThtOj69y8dpXT42c5uVHZXBeOemIxXeOeC7C/v8f+/h5748htY2Zvtc+5w4ssV4eYKlMp1O7u7BZb3U0rfu+pcn3a8PyVqxyMcCoKdsA4XGT/4MA7+u2GY72JsUATDCaMw0TdX7IZ1rRpAiqSgn9rSsnOGNhOa65cfRzpBw6nlA2COTZHohnBGU4gbsPnptUwWori5tPTVirSK5o82iPXBB1GxPHBwLNDQxM6YW/1ZMdGcRzRrDNmdXWMueG1qn9e6yC9hvotzIe9ByACarEulEmo1dVwFqYYmJv0DuDpBJrCDDj2GyJIgqaJqspoQrrZnYc9JvJy8Wnr0wuiSAouYxJ1eozheS1eJBMt+8dM8EQ78HFbF0gePH2Q6thidxpBx0eDHjkw0u3s78bLkJ1O3NjtixyfEHWTWIlttmgYOOju77bwFuwtAsacTOKUoVDhzHky83fcBZHRw0HdQ51kW1DrDPtCTZUyuzw3/3i1xiAjgw5uKSVKGhfedaowyoIvfunX+1sz61mBZ046L9UfRnKE0qqSVfh9L4e/937XNA85MdFCgeI/X6sbhjSwWu3RauHG1Wd5+Bffwed+ycs5OLhEkcY4LEhDZlwsGcYFmpd08QesNsd2pVRXN0zOcbTZrVrcGi6JksXoCcZBqLXScpgW905plakYdTtR64A255h6wa+0+XCyxpicb9jE6Ms97v66r+WRO+/i6PFnuf7ks2zv2WfzyPOcnr+HS5//ebzknoeYbj/HJc2UkknjQBpXrBaXOVgu6dsTTm8+z9HpNW70U8Qy53Ukq/EUwtUbT7G+ckyZ1kgXWoVM8rFws4bqUb+ajeX+HovVitVqybnlHgfLPfYPznPfYsXLX3IH4/hidHnA4eocB6tzdBOm1rn5ins4vn6N9ekR6/UJ146PeObq81y9foPT0yNuTDdZm1Bbp2+39H6MbRzKKUVJp6ekZQLrfo3UZX9N4ohRIcuA0uhS3A1L/ZCzIGUv+j4LMZLtsRJlayfUXoP61HZUuTOQP8QS1m/5uC9qDBcs5D4Tffy5ME1o7QzNmSst+WTjah2jiFFxvHrSQtWCTjWofEZSZWXOuewLZbuKwD3EpxVc2ZVTQsTIDTflKMWNQPIL3JkcovqnsJeSRIo3XlJwEA2yuBLHdPRoVMlISmxjA20aZiC1QU/uFhQ2UjkerN33Ew0SkEXqyJw2J4F3ON+SKJiYMhSlmsvULNxgnLLlZroiXgSkdbS6yUBqzpXuVShNdrEOsf/xyy7COLmQbZEMaRPnlvu0Xlz2JhNTnUA6mhvoPmkcscljCiTBF3/W17Ea9na/n2OuxrseextHe8/zbQ/JrkMWhC+5r/KDjwnPnw6+wGHrkEDrWAXRTuuVcdzj8l2XOdkck8bEzaPChcvnyWND8uA2bsNIT4ObLNCcXtE7vW3c/1Fxyof4+9skojRaJ1nDhgXdPGe51RLhqkarxtQqWoRKIdnghrjqWt3WHN+iFhbWsDQiQ6aulpSNks8dcu9XfAm2rdy+dfXTdFpIq4HCkiucIKdHoAODDbS1MewpB8s9Lq1uZ9yrnCwP4PnM0fEVBhlZipCYWHTlOAnXpbCVDSIDeTkyDCv2GUnLQ8q2MVnlue11jm48TbuxoYhrsHszRBbkUpBtJ7FA0x53XbybOy7cw8H+JZIqZdrQe+Fgb8FqGMiq3PvAvbz6NS/j3GJkNYyUvKQ0fFSuxxxvCuuba64f3WQ9VQqVdStcv37M5nTNZjplXdYUcx+BETArbMsJVQqqriZT84CuvXTIV3zRF/D6V76eg5T4R//sH/OJK89gmqndPQw0vEWJhVBn9mPFVS4i0LyjdPljyFfNOarehLqZrgYVCBUkGU394JbSyVJhKGStqN8ILiNWpS0AgTy6JJg80CUD2WlC6pCNtE6hsh0LZgVVGF/o4/ZcZOY8m0Gy5wujO8wqiSBJ3Ea02eyx4MqK5ttqZ+obZu4qkypkXJqY6D52JKGESHpOjfPyaWFZBcycTBl3BHRRdW/AyOn2OMse7kKwy2o054W1TmRwqGek1OrLGgvT3eRKD8PQqflYb5W9YUkejeObV0lpBTmR+sjQFSuNTZkYV/tky4wyUFLhVXe+nnsvPHjLttKL/FM3PsF7n3onjywWfNNLOntxtd0Ov/G77r3J33//CrMCZU07OYUpHMezO0Nv1ic07uQlL3sD01Q4PH+ZvKr0dESSEdFE6VBbdWPgDrVNTLXRq5/wtQWNKhh9JPHiZp7TkrauoS1TpW4LZfLo3K4CRdiIUDcbp80kt9ECqNY4mbb08HQcholFHxmsImnJurvZSLXs5q9bYxLoU0P6BkOpVukCua3JcoE7F/dw9+X7eOjOOzlYJI43xyxlxbTeMpQTFuZStmpbdLvFTk6xtnWDllUmtcz54RyHB3voSjiZ1gxDgpPGyXY2qVBWqnQd3ARkLPSyYFoccvHygzx424s5d+4ebFhw9eYVnn72GR559glOTp/0znqzpdQTsjUO1Q+GnBYMTh+AlFgMCy7sHXJ46TyXzh+yGM4xPnAfB/sHHC5XLHRgqpXalYP9A0wKb3vPu/gP7/hFmhQkd5Yl8/I7X8FXfskb+ehHP8xPvfXH+FN//Du57dwlPnrzeQaDvQ4lBR1vnpLM2JmLSvCI6aTmnzd56+8NSkBkIp0+NYpWrDr1R5I3BYN6wmPLBW3G2BKnCFo7qQrSPBYiJdBxcNOVhU+lYhmzZfwMeMc/gGwTo3r6ZKEw5fJp69MLokja/P9j1FWZCwixFCH0wU7/6B1fkqh5Cp7NaR1BObfOaK7qsGYwdGz0h04CtJVwwBb6LghqdliRoBhZ94wQ37w5sNzCTt5aiR/Z+T+Om7jFliAeMtRcBGWlUkt1wrZqdEK+uElxWvvcMDBNA6Z7bOop9fQKKSnLvQPSoJRSaa3SNlfIwwVUCpeWF/ice9/MXKrn5demrPnZj/97yMqmKz/6SOZbPmu7G6npjS+9c+L733fCldLRviCNS0wbdTvRa6GVQqvGY49+nP3zh5y7eBHU0LQAmXxca77ksdadtNy9ky+9U0QY08CgEg7V7geqqkhrvm0UaHXtxPpubLvzYdEB6Zm0VWqf2Jjbe4kJQ/Y8oW2tnK43Tg9TYblaOIRiSteNwyDmDlO+nHWYAsm+QEgeRma10Wpj3B+57dLtPHj3nbzktj32c2U9XWBcP8D0/HOsrz3OIjq2jXZODY7MH7ySR8a8Yj+d4+L+bVxeXqRvGzlfZ5sqm77BpJL6hPRGFs/FcYfxARv2OVzeyaXzD3DPfa/iwuU76Sh7Ny5COudoT1sj0zG2BHpjoXAx7zOOgubEVI3rJ8q169fZbiYnWO8nDg9WrMaB2oxaK9J9IZrz4PBRF172wP18xZd9OdO68+STT3B1e8IXfsHn8JKLt/H3/vrf4cOPP8xr3/h5vPXn3sbNekJe+WLVPRUcYzXcYtAhfIvnwh9klb5jryTpTNrmUcpNRqRTB6gpYcVt23TM9GUi94lcO+tBkT6Qe2bU7oT8am5U7OoNp5ZlxYagadlAaiu0p1DSufGGqFKl0CTTcqWm3yaZ/P/ol4h75KXADWQH1cYobDiVoCutdspc1NQ5ghKLC89iADCKWgRCubX9jt1jfY5SBxxjVo0g+XiAkVgNdH/wBTc1bbKz5WUOjZe5MOCekI3Y9FmM2d0VCfRGEsKKSl2Pq14QdFRaFwY5z9Wrx2xOK3kcSalTNqcc33yeYbFksVggkpimU65dOyXpwNe87ltchRR5ITOg8HMf+zGO6wk56A8//tjAN9w/scrgXn3C0I1vub/xd95ZaKOw2j9g3F/Sm1E2J5TtOrwBt5TphO0Wjo8rw3SBnoxt7dC3WKm00ikaSYlBrK/JFRHZBg97w7ltuUUcsBiNTtls2a639IbTbFImN8hNqcmY6sSmbOitkE2pKWOqHqewdboYSWF0TLpUHKNurhUu2nwkRFkOS87nAw4W+6Q8sGkbjk5PmdqW5XDIhcNzXD6/5OLK2KNykJbUC+d49vAyz15/lkUtjB1UjNXU2Deh5yXDsGKRDjgYzrMaL7F/cAcsO309stHGppww2Sm5KUNrTOpUmkFiAkkH3HbhNi5cup3D2y5y6c6FT1B7l1n3zHa7Zru5Su3FzX+tsUoD+2mPkULOmfUAJyeNlWSG0Zd6aRw4WCxZLLJj9erO/q11h5NSptTKB5/+GE//q2e55457eePnfA5Th8cf+Tj/43/zV9jcuMFDr3slJ135vn/7Fg5uO0QXXgQjeYEq3pHTnaKzKznmDUGTyqRO1anmaxgVcS8AZraPMGR/FkQFGQVbxPJOK0WArp4Nrp0BXN/fHNtnNvRQcbI9A7kvGGyJNKcmVRpUaKXTi5Amp0IN/dMReF4gRVIFRlE/lWpz/bXzSneaYJcjigvga6eWjiQ80DwLbsufSCqe68z8BcKlfCaVmg99Zp1aO7SzXOSZPO7CJcWaUhTcpj9C46snv7XeguBumAkWcQQNH8Pp4l531nZBSZ6mGQVbfGvdUwZtLIYF156ZuHnlCNlsQAyVgeXigN4mtpuJ9bYxjguyrCjTKZ/7WV/GhdVlwM64nwgfeOqXeeL6R0OwDaDcbPAjjwz8vpe6wasrj+BrX5b41x8fuLFJlKnTWuFg75DlxQNaXVO2Gydvb+FwvMgDd93LcTl2t5dSKeWEViqluFIjd4ssEdxI1czHMfWEOpPO0JzyoSJMU2HaNqapU6eKZkNHf6+LKdN2YqKx2ayhRPcxDshySU4DIu4NmHJybTUDrTvxWM0zfwaBxbDiIO1zaXmO2w4uc+HcJczgdDrl2cUNrly/TpeFm1ikTpcEzb0Ox1xZtU4+LfQba2QSxuH/y9yfR9t2XfWd+GeuZu9z7r2v0VNnSZZl2bLlBnc0BoPBmNiAGzAESAgJBRSBFCG/jFRSSUYlVUXaqkqlCJUaaX5FRn4JkGTwo08CNp3BYEKPe9xJ7iVb3evvPefsvdaa8/fHXPs8CVvCNWr8xtDxeJb03rv3nmbvueb8zm9TOCqV69NIQDjRxGgDh8NpYj5NG04TV4FhEFbtmPVmZFCnhx1I4EpWjIFBxHmfw8DR4WlOX3cT69MHnD7jTcCchIOrK9brI8bVAWmX3L27RJK67n7UgNTkNMq0IQYXMBCjG78AVRo1BhckSKNlwXrOeIiN2Sbu317iEx++j3Bv5JyNvPsPPshLvvRrmNvEqRuUq/PDrFJkG2ZMg09WzRuchZEQh7iXDrsSpkcn97vKvAchkr27NyWKC0K045FREiYNwbF9jRUbCitNNAkMFsjRMB3QbEjqskNxKaRDXwFawmfr1OlIitUAVZHmEskkmWYNqU9yTNJEKAOk5nzAZtbzKBqRCGSKNcccW3MOlRaidsUGkZwcqwrdSMJ6xoVn/5pfQP3UNutysh63EFUxiaQ4eIhXjsTW40gXwqq5YanLuArWKpj2k9Q7xyiyT3jDrNMWlould6BaCHLoYycuy8o6wHzI5fPn0aqsUmaySFVPi0wpEOJI7Sax2hrPfMrn8PynvrRvo9l7OF48eYC3f/Qtjim2XsT9KObnPrni9XdWDqOPSphTkL7p+QM/9IcrX1wFIYjjwjEfkoa1m3jkkbOHT+VQbuLK7phtPYEyM9fJCf44NGIKGoKbvgJVZywok0S0qV/8CYYQiTheOc+z52O3igbDmrGziugGqUaZC3WuUF0uNxgcDCtX+uSRcViRc3QH6iHTrKf4RY/lTTFzZjzipvE0t5+7mac+5WmcPXOGMs+c7CYOLl9iqpVLZcvJ1UtcuHoLN60TGTdq3Z3sYLMlbCph1xhDxKqxjpnVKIQAFWEVRyxH4jjCcAA5IzIRxwRDBssQC1P1bKbmHUCfVIQkIwfDmtWYCbExZkijOT0rRIeUWtf8z5WdKZcsUWojlkIZYydKG1ttKLWbQ0BsCavq0SjiogxdbOLaDFIpOmMIY1wz5nO8/CteR8mJh07u4+HNPcQ8IbailU467w3A3LyBcX2idS9HaGFx+pdHmdQsj9YXpi6jdFZIQLRPdOYNSQ1OFQo2kloiitueiSa0GUNzpY0FCGSnxhnE2ViCQUpqfk82g1p79rdgITKLq+Rsevz69KQokk7cjmhnw6s6cVvM/e2cXm7ETkmQvv0KBqHh2dhq5NwdhLim43Tg2OkGS4aHtAZFYA5QI1r8w9ChE5cJZHMXn7bwvAxMi4PRatTZqLUg6sN7yolkAyFH97hU6355HnMQoxAtouYnnNnOmREhczTcwOVPTsRZ3UElBOd7SXIeW18SpWhueBoyL3/26/y1wX55pKa85YM/y2y+W3TltFOirBnTVnjjRwe+4a65Zyr517/6aY03fTxzfut8w5RcNphC7hEBkKVx5dIDHKwK83iBqVyCJr1AWte+7ttZrLlAoDbF2q4b9lZIEHNEZSBZQFHmOjksoY1gGWmB2vAlR620uWLVid01wJAS9JD6NI6Mo4d2aVdmeTCWdJ9Kg+y68jOnDrj5huu449YbOHvmLFOpXD6Z2IhycCFxabrMxUsf46P3jSRu5YaDFWG345EL57mkW3YJDg/XSIpYC252kSbmsu2+hkLMkXG14tThERKhzoLEiOVE2/nhrOb8Q2PqB2mj1Q21bKhtojVjKp1ArgGxSqs7pjqxaYVZK6XO6K7Sdo1Z/bW2VeRYt2ynSi0uzdutgpuh7KpPNG32JQmKEqmW0aYgvlREYGZDGbbc94m3+eZ+zK7o2nmEUxA3i9DmjYlffw4dtT6pBXPVTOsNmrhRq2+poXtMenENrePkhruaFy9wzaB1vqVp7Co3Z62Idp5nC32Kks6v9JFe1Q97Ceb7AQwris3ivpSqxKqkqkjxkfvxHk+KIklvj1tVWjWauN17HHycEHE8z6InwhHBsheuQbph7yiUHmVi4m+09m0a4uO0dYNNreaj21zcUqvWnkDXeVzB8SuT5C7hdPMDPCZTWqPWSimuJPHgqiWbxqkLoS/ILbpLTIyRRT1QbWaIDWmVw9WtUA55+P4H2Jx/mBhminoXKiIdbHbcNHRzjj/x/DewymvA9rQaNeN3P/pmjucrmISuoAkkCbQQGZJveH/5k5nXPKNy2DOAwBdHr7n9mB9636ofUupBWNEvqGhCMt+Ynly9Askou9rPiwWD6l1zF8FbcEPhUiutFGSumFX328wDUZzC0YJSY0MyiBhZPU7Umm8+S9n2k9+cTC+uzpMk5JwYhkROQox+WDRThz6QbtMVSShDMg4PBtbrxBiN0+tMXa+oJhwMkVUEbMdm+xAPXYhY3PLgqTOEzQm7yw9xnCbs7CEjA+s00nTHyI5crxKPL5C1MaTMehxYrVecPlwRo9KmgSvDSIwrgmakZTf50IJqwSUr6hSc3QnzdstuVykl0awybSJaZqbdJbYnx5Tt5EYmNRHUg79kOOp+rLYnhAc80IwaSHMiNp8arHnTEc2cU2wBo/Y8p4SJiybuu/IQQ+rRCXNE2wA6kmQJ4ws9J8dXpo7hd/qPGaO/9XQDI7dD0O5aZFDF+Y90nwMLndjePR4bYMWvD1/++DY8kxw2a7qgcF2EZWCD7w2sSy6D2/OJeghLqZVikDC0b7XNfKE6tyd5WqK7Uw8gxd1IgroUSQIhat9YuiWXZCFHgcGLQ0ZYBUgposkLEebaU4sLdcv13MF6qHpz7K02lwAGoWs396BlTzr0TpCOMzZRoCFWkDhDc/mgmLlZcKsdyfZ0xWgOB4yjJ7c1iTQCtVRiyCSOOJKbed8H38/VKw8RdaJaY0frvE3vkEWVYoaEwIuf/nJuOrpteef6L+Vj5z/I+z/5NjJO0G6OTzsFRisBhyJMI7/w8cyffPrsEkTzU/srnmr89D2RR+Z+qmcvrNY8+e/S5as8844Vza6ym2ZXQpXaGQh+k8Ru5+X+mA0tM7s2U7UQaiMZWDOkTD38y+MsNAqDdp5lEGLoprxWffzqQV8pDsQYSZJJ3VcxS08osECQ5HZf1jOHmuuOA55pQ/AMpVKUMlffdJYZ5pmoBtUo046TkyuE2Ji2D2O7gu0KMQQOzp7jIBxyOB6hdUMpW8bNSN5URiqrvGYYI3mANMCQhIMhsM4rVvEUa850h6OAlRna7HpvnQnxkNQCbVeo2wlp/v7YRmCaqPWYVhuxRbAVQ06cOn3EwfqI6w+vR6KwqVdJm/NouUidjgEwSQRNbtBCw3TYL0ukuW5fQiKYdxiqffnYoM5KYPTlZsf1VVPXkbsZRrVCMFfsKD7NlOLu5y1K3ym46csiIfR1QXIM0xUaNAGJyfX5Hhy1n3T8kZyyJ9GHaKkd4/Slq9sPxv6atWu8e8xL9ANYQsdgu8tUbLjMNynkJ/t2OwjDeuURAK24Y3gvfIK57M8gaCQk982Lsrh+BLDGikQQD1svzbfHpYv5PYBK9+HtUgNMYE2YzXHEkBIpJyR57EAiexxBcCpQ1/S4EoRKFt9SS1IyvqVcPCqtqTefOJ6YcX2zpUQjIrkyVWM13MwDn3iI4wvnCUykMVFaZIjCPsskBLeHAq4/uoXPvfMV3UD3GrpzMh3zX+55I1EhBUMVkriX5SwN8AtptkJQ440fTbzmli1Dpz0sUPdr79jybz4wOLC/awwpsh4OMIwyT7zjnb/DTbee5ujOQw+gCtm7RO2xDRZA3WmwGWAz1Ml16OpehVYVWa46cwcZqYIWQAKaPbZgYSk4xSrs5YcxDeS0IsfkCX/Wb/a+eMvqeDJ9MWUGzIH5uHLhwjHr4QqH6TSEAauN46tX2Fy5zLyd0ckoFHbDhmSVtD3GKkTNHKRTHKyPWI1nWB2eQucdu91lBqus8wadZ0bLrGQgiTAkIyc3YV6lgXUcGTjqsnzDLGO6ItZGbTMyrMk2EG3Gyo7QVgQTBqsEnR1+0oBKJuTE6aNz3HrDrZw7cxPXn3kKTSsXrjxIvnKKzQTTRglN2VWjhEhOgdlmQnXPyRiEqOINWcigPVDMuoDDDFojyYBbVxTMGhZDL5rSp6OO+5vbnBkG0fy6Fd3DNQPXaHqeSy+dLtdTTgN9eeMTYqyQBE+37EqdpF3cEQyXiHRJ715h5rJEV9l0P4goyODc5KRGUodj6qzINGOxENAnv+JGYmA8fUAZFKoHl/ubHKmlYWX2ExghSEVi87AtYrsgMgAAxgFJREFUE7/JtI9aWqlSmBnJHcej45pmrtAwFbTz4hTvYsKYkDERUiQNEZHoNm3qm3HEC7O7nblGNJn0MbqRJTLK4JIocwPXZgYSSBYJYWBIA2H0jaA73wQuPXiJB+9/gFqOwRpTH3WtdBdmmm9kEMY48IUv+BNoa320xS8EM978rp9gtzl2k9/uvJJjcDf2QMd7fMNorXDhuPJzH1K+7tnaG2cBMV79dOUn7zUuTkLImd1uyxBHzp09R20NrROf+tSnuPn6p5BvPHAH+Ow8QTNzB2g89CvWxtwC2dFgqnjmtEQfkxORNruixkQoASwYCUVC9O/VHb+1OWVJo/sOhhhYxcyYEkPMxMVYVqXz4TyEy4Ibo1CMrVY+9dAVNlPi8rFyyw0T65CYji/zqQtXuXS10KZAViWkyTv4g5GMYGXuUEGEYYTVmpgipjtCXjHmI0rZEKogRaB4F21LuBbusqMGal3KF5MvFooRdHBTFBNa29HqjDWnm0WZIBRSCgwpkWNktV5x2613cOv1T+XGczdw0803U2pl/fCavF7z8PYSly8dQ22MuXugRsiW0dQ6hOSfWcxuBu3yXT9oEs766OxDagiIJUSMFoSIIdo8PVT8s1Uc5xt14UcKoRdAAVKnBrmRtduTBdVO3VmKGpTqVm2SBIJ5DGwGoZE7Hc+WX8H5mBboih0XAIvh1D3rfz90n0rzttIM79jLTC0QiYT18Lj16UlRJEMMjGcPCTvDau4cKy9+zAXZ+YVnqs4HjAbBxxbp0sNWvUCVCEUa5v5czs1D+yahORnd3Lx1UPEYhI4lSYJeVTCrgOuyDR/Th2YO+Gr33/OZ2jXjsdvJ10Ct/ufuZLNC4kBIkdyVBlKUti089NGH2F69ylw2aCvM887Dx7q3HxI7Nqm87Dmv42g8cjUMC85jvPNjb+X8xY9hrTEDjJCj33x+Cge0ehfgK21fpvzUvZWvujOwTtcGmiHANzyj8SPvHiitYHHFyckGbYHVes3pw0PuvO2pnKSrmEo3EghEyYCSo0vcqI3NduaqJqwmd8OWyEFKjDmQkzjfVY1ogZ01WodUkgTWObIeEjkmWlN2xZjVYAiMq5FT6wOuPzzicHVAzsN+uZXMDVJK9ZvRv2fCBqG0wGY3cXXzEPdfvMx9569y7ugMUibOn5xweTZMMqkZ4aQR1bmICrTS2E5ulCuSCGlktsK2GaSRPI7obmLbKse7Ew63V9lujmglM293lFapIlhedZGBp/URjGADUhoxZIpVTo4veWbM5dOshsD2+ArT9jK1nmC6IwZllTOnD05x3ZkbuPGm6zl3Y2Y3C1M9zfF8QsiRGl16J8F1ySEJA5GaQvc/XTi12oUb0kPwvIvDcBhJQHLcs0LcZV87La5CdG/IaPSFiO2hsd5XuuUhoZsT95E5GGKhwyFgEijNkFmxKq6ii0JeJY9jDqmnHdJLZDdq9lKBCJQlosV81A6d19ysdfaJ86Nbj6geJyGUSEQ4XD3ZMckUGa87Q9hFtPgu3tSNVluKpCCEefauUgV6rKiPkEZpjaKVNoNFH4sV51P5UgXc8UeIzYjmWu4YfJOcIoxJYJVo6vLG2iZX61TtF4f2LbPL9jyJDtdCSe6AcaEZTNpdgWThjHXjjWa+javCvC2cHF9id3zFX29wV6MYoh8OneYpIXL3bZ/Ls2978X5zTL94H7z0cf7g3l/Geodr4gYRUXy7UVvr+23pFJLWzVXhShV+7kONb7y7Y7HdNu4rn678x/fPXGqReXdMPDxNtcLJZmZ39SJ33nkr8cyKi3aVIO5OHaO7FCV8hEICqTQH0DsxeBwSh3ngaMjkBNM8U1bKMDfiZC4VjYHVYeLsqYHT48AYE6U1TraVTTXimDlcZW48dcDNpw45XB+Qs1/Cznbwm6iYZ6E0E6qCkSktMO6M7aaxLTuuXr6AzjMxwGyz3+w5+ueLf1bzZgetECQym6HTJUyEIUZKDsxasRTQAXZSKGZcrcecLiOb7UCtiWk6puoxs51QdYcFt+cyE1ozmnQzhwSzTZT5EttLn+TKYMx54OTkKpvzD1OvXEXnmezBjsCOGAoxJ6ridndWmOYTdN4SdAab/NrNA6wSXVlLV+UCrqlX6YYsCxavsEhzDcNkh0UIBEZLNNE9/1c6yChme+PsvujGllA3vwG9wILzg4NT5pJBLJUWhNIpXrX6HmA1ZFbrTMi4wMO6RPhRtDcR72BRpcbkhsF7G0OnEy7opqh1U28jxoCM0Q8/4Ojgyb7dlkBcHzpjPwnZ/AWVWHxDHWfn/aXRc5fF12a1CZWKaqHNjaIGtdG0+IbZAqFbjdUuXaSap66haISUcJwzqcshu5NMUTeySEWx5oHtJD9RnUsY+sLAXZlR74Rqa5TmiwITl+y11mgtOp/OnJyz2RzTMNLgJrewLCikg9F+kZ5Zn+Nlz/gqX975/4EZpc38yrt+AmzRJplfKLN3tCGK01KsEXGtuZpHUlj0E/XnPhr52rv8gOjnCDkor33WzL97D4Q0Mu22xJQ5e/oUxxcv8Zu/+ds846XPYbeeScyE6MsgDGIeHSfCt5cE3BA5KutVZjWMDGNmlYVRM6U1dttK3ERqgzCuGA8z159ec93BwBiFaSqscuFIYVitOTx9lptuOMd16wPWg2fKIOYb/b6YazagFijNl0oWs18vVXA70kyIA4qPdJkAIsyzdd9Co8qMNCXbImWLEGYIW9SuAmtCbJjumPWYqleo5l3vrmROdhGtiVq2lHKZppeI8TKYG5JYazRpPlY2YzZhVw6YdomTC5XLuqUOaza7LZuL55mOLxK0OK3JZrbbC1y8/EnSkNhsj5jKlkcuPMSly59C52MOcmUUo+RKWkXiCCl66J2fJX4/NDGq0KGsbieoXoGW2NYkzgn2Li06b3Y/jDvrxIItlybg6SaY0IOcIXiBCrKUTS92GWEYIjWI5+IMiXmu7j60Gsij7Lf1UZ3hgeJ59yv3aJDqfNAhpN5lepH0RAajWoXWujQVFqH3YQxcWfm9cLB+klulYXTGvPMlmdxm3TNbzHlx4sTgnJ2zFWSkzEIKFdETJFYkbNEmXUjf3NgTdZzMOuNf/QRUB04YTMgoqfmmtmilVY/aTIpvwYLgDAFfKAVCP3kDMQ2k5PrXWjN0krvLsyq1tR7TQMdYILTM7tg339pHHPfUS6iaO8VEyBZ45d1fRw5xj0N6TpnwOx94E5typZN1dX91mjVqmYnRqTbL2GSCY3YEJvEN9qWd8MaPCF//bN9wON4TeO0zjJ9+/5aTGh1X2s2k6wI3PvUWLjz4MMcPP0K4KbGhYHlFTsIRgcbMKkSsVqpWjw6tAUkDIQ6MkhjSQDxIDAnWIqwmZb31RVYcB45WmbNHI2cOMkka0zSRD0ZmE8aDNafX7rd4uBo4dH8I57iJuA7b6Bp/68FtfkDCTNXA6mBEwgqR7PhXCFSEFJWy23oX0vPUQzdskKBu7DwEbJyY0lVUZpptmaeLUB/mbL5CsMChBFdEteihV20LYcN6nDk9zMxhdly1Gy0XM1QDgcgRjRQ2NKtsphmpa7bTzKxXiOsdR2kGgfW4I+dLVI1cvLjlysmIZGNXLzOuH+Tmm5SzZ1ZAQUJGUiSuVpCSxzXZUiStMz88k8aC0dRVK+7F6ImHMZ5Bay+UgLXZObzqUFZTQfEdgqk7fRsuCQ3dNi0kx2VFhFarCxZCdLFI89TGgvripkGOgdi77mCRQEZFWUL78I/J1XLqYWA0l10WcxpZEJwnEpzKRGuYONtDVKEeUPWsL6JEeLzHk6JIRgJHqmx1phRhM8/UUrDq1l1qkFNmlRLrNLBarSAObHdGtAmrPdFdA6WDw03d/bjUwqo6k6t/wm5nFuVa666eD+Mmn0KsRtKKSiRk37BbdHIqFrzzC24RlWIkJedgBhloRbEwodm7mmIzVUdWlskhk5IyHUemzYw2Acs+7eo1xToSiGq89I5XcuPpW9nzIMSLwL2feifvv+9te1eh/RjeH601bDszmJPrl47BDTyCh2lZwFrlZz8Qed2dxvAow+Echa971si/eccM3R9ymnacu+FGzq6PIFWqwYObDQygK6cRVe0mpk3RWSnaXGqKUAgeK9AxqTRmDsYV8Sgwt8ZmrhiR9TiyWo2EIbrSaNiRm1BJ5NXI6YMzxPVIHEdCBonuTWl9ISDimFTSiKp/poIX0oFIk0CMIyKZ1gylsGtb1GZ0dMK69Q10MCWFnuQ8KMPayHGHWqU2MJ1ZDxM3XSfYkS+QcgisVjPDamIcgWniaC3oauDsmdMeFhdgFNcSN/CteR5hMs4cHpKCgTUOVm4HeBNnKXIGVSXHTAqRFDMxBixeIKRAHhPTvGPgOpTT7MoWgiJaiXGgNGG1PiIzEIm0vmBUXGYrHetXrVB9ATKkRAqJotEVUdoAqHjipSqUsqPWGSwQQ6JVp2qV5tG107TzLpUVpRQUo1Zfdq6GEWHJ1QnM1TnCqLk5h3lsxxAPMMuUoKQQ8C7RCFKd0lfUY0OkOXGfnq2tTvMKsTu6tkbKTs7PNKIZlciSI/+PHqc+fTZBYLcDP4znahvwg2b2T0Xk7wDfBTzc/+rfMrM39q/574HvxDmhf9nMfuGJiySc1khsme08M28KOk9o57yZGWk1sE4Dp4aRg3GghuhZvDlQh0hriTpFanHvumDCUH38XpQEjp00mvlGNaYBM2WaZ4IldweSBLUhtTq3ShxPSeL532a+eROJRMkMOZOys/zJkdgasRW2c2MCVBtzLWAj6yEzjpkHr0zMWz8IBDwEqdMofBEiPOX07bzg9pcBvUnsZNrLm/P8+rv/I7U8vrWTm/w06lyIQ6TlSI3uOIRWVyz073l5W/n5ewNvuDvslTsIvOaZwk+8t3FcN5SamLcTu+3EudPXcdutt/HA7hEevnKeIMUVNC1RZHQ6RnUmsRJQ8VyeNhe2g3AmrrlpOOC68YAzBwfEFJmlMs4Ns0wO3plbcJOSECMpDKzHA/I4kvOADZkpBGpopBBdB54cVmm0rpDquuboxPok3Qg2RMwcQ4159M9nEmJcYVqw6uoopLF31jaXUY6rFSkm5jIzDCuM7okoBjJ7LIh2l6EwkFMi5AMkGSf1nMfPdjxuHQNSC02V00enSBK4vGtIjpQ6Ya1ysD5yCGHB1toENHLMXmjS4IXOJkQaMoLIBmuNNUZplZ24cmkgs86KtC0pZ0otRITx4IBSfXOdghdPSckXLVMhEEh43o6J585LEVZ5jYTIXCdCiEQZCZbIee2TixVydud7j3r259NM2U0T2Tz0LQ8D47BmlMi27HrYWPPsdQmcHB9ztDpHkgOUShJjricYTmSfa+F4s9tThkst0IwxpH0MSMQbBzUY0uBlqc2E1ojZmSDBE+Q/4+Oz6SQr8NfM7G0icgr4AxH5pf5nP2Bm//uj/7KIPA/4ZuD5wK3AL4vIs80Zvp/xIebJb7WsmOcd49QoJztKmz2KNUUsNiw56KHFTW+dj+hyK9tnYgRMelFoBqLsEqh0U4WuqQ5iWC2YBVJIruH2nrHzIisxSzepcy/LKB40JikRg2OkB+sV2albiOFehkSqNVqIqHknkrNw6mAg5SOuXLhMmWeE1kcQ6++djw/reMCXP/dr9wWvZ9tiqrz5nf9f5rp74k9M/H2oraGtEmMkm7mJRzcQqVYIQGuFH38vvOauA4YUfZtvxpjg6+8W/u17KtvtjvUwsdvsON+ucvqscnTDzaTdh4kZ8sFAygOn1iviENE6k1n7ZzE3NBmHhwdIHjhz7jrOHK258boz3Hr9DSCNKUyUBpmRtTbyEMmDa/cR88zx1QExD5xNa+JqTSkuBxwGX6SEGDu52A+TYJkcB1Lw/JZIL5Li3aORSfEAI1DaKVICteL+hl6WMISperEcUyL1ELCpFO9Gu8ROm4JNIN6xb8tEnU+YtJGHxG4+YSoToq4Oy3lkM9f+vQObE08j3NTG7rh43K9BKie06pNPa+qmKkHQph5eFRNIJSZFW2G3ndi1SoqpxxoIp8bM8bRDUuJTtaCzMubRnbTMODjwjbtY4PTRaYY8gDb3ply8U+sOFWWqM9O05Xg64fD0GTabye9DlBhH6qycPXU9VpQhJo8jkUhOTmA3jJNpR1VlRLh08TLjuOLo1GmmzcYXSD2zfr0+AAQrcH4LhweNqW6odevWem0iESmtsJm3WDRyjIzDmiENtDoTUkZNXWufEqUVYvZIjaqVPCQOV2ta2XkD8TiPzyYI7FPAp/q/XxWR9wG3PcGXvAH4UTObgI+IyL3AS4Hfevyf4SdAa/Pe41GlUVrFqp9c1vWacXau3DYYU23sdoXdSWHa7ai10RrQnFpQRbAUiZ16qjjWkmt0o83odJ6hJpTElp5tE4HYQAO5BVp3Vs4mrFImRkFJxFVmNWZWEjvBW1FTtrUyVJBdoxTnDsbcfCTaZI7PXyHVyS96Is2q0yG7/PGL7v5qxvHIR8UlNtOM37vnl3jo4n17OeHjPTxlckXMlfEgMM+K2kBLEY0OcgcrTj+RFVd3hTfdW3nDczIhZlqdAXjtszI//f7K8TRxfHKV9XTMuDriwYc/yh3PvJmXv/DFHOXEuI4c5gNuufEchEoKwmFecbA6TZI1KVVWq0SykfV6jUQhpURerZlloupVwIiSncYTVuQwEhG20jD8vSoWOWunyGlkQ+FSuYzQ3BUe95e0AbTtOEhrNKyYLFDblqqVotWt83CT1zpvWIxfY3fNLnVHKRNaC3m1Yi7VLeqa0jbeFYcQiMG5hHMtbOcJq42UR6oZm3mDtdkL2fFArYXdvIEopDyy3oxo6bZdJNA1RqG02elwY0ZbxcqmLylxjp8kai/EtAq7iRBdqtmqMu28M52kkrObp6SQCGFFLYrO0hUtjpPrPLPRigyJoIG4O2augVrUPTtNyDFj7YQWjSsnl9E6U6URtsb2ZKKhjOPI1LbMtSC25fTBaVRGjo9PmOaJlBLz7JOS4kshMaU1zzmvlydOTi6SxwzBG5VLV8Eko7OTyoccmOZCSgNaG9pmskSubDaYKFEaBKhtJAbx4teUajumcoKhHg1SGocHZyhTYzWuWa1X1GnH2cOjx72f/m9hkiLydOAlwO/gUbN/SUT+K+D38W7zIl5Af/tRX3Yfn6Goish3A98NcHT6kKtXdszVLbeKjVSdqLXzyRCYlO2VHVNWZEhUrexa85yKuTDVSi1GrT6a0Ff92g02Y/VOEXWaV5bscHUQ5pAIURjMpVcQMR2YUULtXV4TSvbTexwHTIQxRg5jYkwDzczH9hDJw+i0ljD5oicIqoEohzzwyR3HVzegQiS6hMp8IdOa8uynfh5Pu/5Z19x98K32/ec/zDs+/Fbo/70USrNPPwKDgcmO6267jrte8Gze/vvvx67g0bRjprRCqz5aha4h/rH3C195V2A9JA+Nb5VVMt5wd+KH31nZli0XTy5y+vA0B8OKV33+F/ClX/lsCP6+jnGFMLLRnW+2iUjYIWTUdgg+ThMiBWUyY6M7St3RbINRmOeCyYCEETQQoScBukValcQjMtFmmLUxtwlaJSlkETbT1jPapXE4FAInzA2meeNJgebbYcxxzGmupLwix5VHWGilacW0sjnZMAw7j+ow2M0ehxuTkSLEee437glXtld80RNGYlp7cqYUrDaGcY1i7HY7xvEAtZkxzlgxiggwgxlNdxzvHibFRE6HBBkIKKtxpM47Xw4aziaIgVIr1iox4LieRoKsEfHpZJ4bKWVO8MCylATJhdDcRX61XmHDztkIYaCUxmZqMM202sjRpZjaTmhWSTFQZ2GVD9E2oyWQw4ocAuthJEWhaWHMgVU0X8ydXnNy7CdYW0d20wQhUOpMCgE5SEQpTNMxJhtq8eWshMA8F3e0V6fUXWlKa5FhOGDeFXIKrEfr91ekTDOlFUQCx/NEmyvb6RJxLKQY2JxsiGHFkI98WlsNnT9dKao8fPz409lnXSRF5Aj4SeCvmNkVEfmXwN/Hccq/D3w/8F9/tt/PzH4Q+EGAMzectU88cL6DyF5sygR1DjT1BYC1ipVGEZcQxqrM5ua7NC9mwbrWU/AAefHNLtVc09v/MMTUw7gECwmLgZSEkYoatBaomqh48lxQ/yDAKKWwHgfGIbpfZPeadPqPZ4OXap2nR8dUG60GTo6Fe++/wFV2rgQRPOa2qx2uP7yRL7rrVdeoPjgxdypbfuVdP8GjjaYe/3MCwsBT77qJr/+u13LDU89y89NP84v//veYGkQNVAJIxnSiUQDjeCP8wj0Tb7jbkyqRgKrx+mclfvr9hc08s33oEU5Wp9mePcUjF3ds5ZCTdIWhJmIoNNtyIsds7IS5zZglrI5dI1+hOW9uKpWq5txNq1y68pCbQdRKlAPSsHZTi9aY1DCdnR4WDxiSW2Rtjk9odWbIiSxOo9rttoQxUaWRbCDFNSqB2nzpEMSJ1Vb9cG3mkQ8hXnUGQussaox52pGjLwmDCKVsaMWIcSSExDRdxHSm6par2ytuD9YCeThgt9uRY0/uzCuWQKz12rmAOc5EiWhQSt1gNrDbnTC1y7QmDPnIJwFGTh0d0WohBsHU4ZOqlRlXjSUiEmYMZRwPmLbKmDOmXthEAuv18XIPk2XtHpDdA2EYYHu8oZRGjBFVD4wbUqLMkxfKFAjN8f1tUkh+33nmuHAyuXqnzL4AMrYMcUPOmelkSwjC+ihSamMYIzFltrsNMRaGnLsCKTLEFethhaTM6dOJ7faE2iYEuHr1ChJ3SFTSUKl1oqghceTK1Q3zbkuzSkyn2G437LaepLq9eozR2O525HyaMBRgJnUbxVUsTLMyrs487j31WRVJEcl4gfz3ZvZTvcg9+Kg//1fAz/b/vB+4/VFf/tT+e4/7KLXyyUcuuGM3wW2ephlKwWiuRmmVqVa0KoN7mLslGYB6Yl6WgIVAibJ3CYmmXQqsFFNCSERJ0LmUghAFhugeiura+m4M6kVWOxcRVWLM6FxpVEIe2NrEXKpjW3OllEYtzrGszd2nW2vUYjz04FUuXLyM2hZ6Jk/Q5qlwLfLlz/s6kqROXrd9UXzLu3+Kk+nK/kJfHt5F+u4WFuxauPX26/jffuBv8MIvfiE5rnj9F7+SBz/wP/C7v/Y+6hzJeUWtO7DWi7GT3X/yfTOveWZkTJ3GYcYqKW+4e+A/vLtR5hOunr9IvvlOLn7yMh944CNcPniYdc20dEydPYNnN1eCuQlFmQK17PyGzQJi7DYzZVeRIEx150B9dKwtUQhxy8nVE8YEk14Bim/Z45osBwSLmFWCFOYgSByYG2x2haE6HqXTMUM+IKYBVHvCZEZi5GDI3YQDtDTadu5bcHdrkmCsDk5RdjO1GmWeQXza2NVCqSdew6uxnSqbAlEbB6s1RuzcTKPODdXa5Xew3ZynGh0rHZjrDrUNrTkXcFifcocjIjF65z/tCin0cRVfdkx1ooTZTWZbgdCYijI2mKcNV44b6yH7TBSgzCuE7N1p3PkSZmPMu4scrBKBRGtGSIFp3jklKkVaLYQgtCSs40CdZuKQmecThuC5OqW6XDNJcpef6LS2PGZWaWB7coLWRroSffEp3r0eb65iFNbjSI4jVQuZE9Zhxa40LEIpx9S24eDwkGkq5DCwGpyCFIdGCInNtKPphFGISdwxLBlj9vtRAqRVpA6JWjZYuUJKOzemISH5FEdpcMf+x3l8NtttAf418D4z+yeP+v1bOl4J8PXAe/q//yfgP4jIP8EXN88CfveJfkarjcuPXHIXkJDcKGF2jIZ+Yk6tMpfSIxWUFBo1KRHxnGCUWRRLyeMeKIQY3Y/OBK2eUxNlQmLBJGLqWcZZpVtJudQpBA+iytW306JOEg+Se8h5pc2D459h9oVOFUo1ttOOae7YaEykNJBSRoYD7nv4hDpdQQpYnXqgpneiL7371Vx3dEOnsng3A/CHH/8dPvbw+58AhzQkeq5zCEcc3aj8lX/4Xbzky7/Q40GlYeci3/Dffh3vfe+H2H7SmPIEyb00nfPr0bsXTuBN91S+5m4vPCklSi187d2Jn/lg5XgWLl2+yEOPPEKwU9x/3yNcPXyErBtC2DDXQlPBLDGOB4S0opRGmWbGPFK3jVIKMbiyqNaZNjViGLHqHe3cjCDNA8hyZWqXqCaksCZLpCTpGUeJMa5o5tzakBJKpGlAWmIYMuOwdq7ekNjr+EmIJJpuPQaBSAwZK6Vz6BrUis7mHEtVtpM7JkmYqHXn2JokYsysh7MkOXKVUQoMQ6TMlbltfdtN9PwgQHViNQysxkPEIqXCqYMjpukqMRhzDaS4ZkiZmLpphFbHiTFEErt5x8lmy7Zc4dThyNFqzXFplN1Eq8cMAXZlYsiR7TwTxkCqwiolTHeUeQJTxiBoO+HCFeVgdR1VK1o6Dag2opx1b08zhuZKphQyNvuEtG2FgzTSmmGtEYeMNKMUpZpgTZj0KqVu0VKwjVBjwIiM4gswEE5aY7e7wOKSPobEPEsXbEwYO+YdzJMyZOX85R15GFyf35SpVEJYU0smpbXThqqxqdE5sE1pE+wmZcxHpAFaPSYSqXXDifri6aFy/Lj16bPpJL8E+Fbg3SLyjv57fwv4MyLy4n43fxT4CwBm9oci8mPAe/HN+Pc+0WYb3Fprt5lpzEhKJAmE6trTphWpzUnZpbhNk1Q0um6YIH0hE8nNR+9dhMWteK4TSYSWOjHamruToEQ1WovMEkh5YEDcaCAYc1OK9cwMr0U0G9kW3DADnOawzpQYkdXa9aAMDAanTFGB1dkjbr7uRq48CO/5wAeZd3M3uHBicAiB269/Fs+7/QucL7noUYGLxw/xW+9/02Peq8+EQaaWSTkQTjW+7W9+C3d/7Yt5+/RhDEiD06Ge+bnP44tf88X80g+91V2Yk3QA3Ynvy3f9yfdVXvOshFBcapgyKwpvuDvy79/l9nIfve9e7v3QPZx60dPQvKVGz3Ixc9A9xMBmc0IrJ+54Q6DMM7UEfCjxXG8JQgojJ9sJ04kYGtHAwsDBak1IDZ2NLIkUVgQbqLjqYkiZMY8IrgNPcWC0QKk7gohLTbNzP1Vn1Arj6F1loCKxMM0zWiJRBtqukIdMjplp3jKVE48yXq2hbdltL5AiHB6sCZKo1W3GYowcrddIGvqNraTDU7QykHJGZGAI4zXIJ0ZyGkHhYDxkHAKbbWaetpgJ62FNiIHtyWXPnl8NbKYT5uZ8T7WZcRgZx3McHazIcaBsLzOXwjqNrprKA00j01QJE1y1iY2e8PDVC2jbcJAyhzETpWEyMtUthIntdJlxTIzxkCBXmLale3dGxjxwan2AGOxiZbPbEHiYOk/obNxw5np0VobhCAkDIW7Y7i5QylUO1m63Nm8n8pBpwY2ejg4PCSFQSiGnESFRilPitlMhxUxTo9ZIYKDMikbh6tWdw2wpuKJNt9RasbAj54xUSDUS0gFVB9q8IafM7riS0ohwHSU2StsS5kbKid3/E2dyM/sNrtGZH/144xN8zT8E/uEf972vfQFMc/EMFK2+WDGoUVwU35o795RuDtrjTp083M01Y+z4YBfpmQPUjYYFj3cdZGAMnvdsEYoIteJ4TxNkGDgcE6krUgQfN1aHI4ghGc6eOuDc6UOGceDozMC5669jjGvOHJ7hzOFp1sNAzBHJjmMenT7HGM/wA//rzzBfnrAyod2JO+TEwXjIlz3/az8Nb2xaefM7fwx94vPFO8wYkdOB133vV/EFf+plnD++3MPJoHW6Rw4rXv1nX8fbfuNdPHLPFYqBpIROTgVafvqFrfGLH2q89llQWyHGgRATX/Ms4z+9v3A8V062l3jfB97DM69ch54tjHnFTiHZgZsliLCdjylTJUY3xKpNGMhEC8y1kNOKw8MDrDVOrdZAJUfIRELMpDyguJvSkEdW+ZAQBubS+ufeyeNq5JgJIVPK5BJCjJQTw2rllB9TP5R8Z+NfFw5RDZTiVnk5LzxZiHIjrRTmOpNy6su1O/ALNPTCPGMYMUT//pLIOVPmnTusK4Q4MLfGOKzcFTy6g1SMrlbZzjMShNN2HYrHo4aefQS3kNXJSCfThEV3cY+h+yk2FxE0hINzN7orUTO01R4I12MemmKKLzqnY6q5oiy1gFhjpzt2zcngIk8jp4y02nmR7h60z3NCGGIiY+SwciI5AYbIld2OVmFNRVC25RLbzXla2zAeJ7QKWd2VfiuNbSuUWplnhzoO08BBWjluOo5M84SzDWZCSqzXB8RmrA/W3lXHwBgjCWOatsxaGNIhQz3kMJ9FJLKzmW2bqPMW3W7dPCXBVHAKmV3G5pkoM609ya3STKCKeqqguUVTaVBC3053jtWimQ4xEZOHAYXFxdMau2TQTSwmbW4QkaJ70KVurZQyMghhhJUYOQ8cHq44feaQG647zVOuO8VqnRgO14yrzKlTR1x39hTDmDnKp7nu1BmuO3UKA8bxLDmfIsRMpVJsx2w7dla5Mm+pomySce+HLvLW37+H3XyVMl+htZ0HHoXElz3v6xi7y/ijH7/9/p/n/NUH/li6D0A+Ul77XW/gi//0F3D56mVyOqR1ha1uXd45MLF6yopXfcef4Mf/7k+hu54VHwNaH2s4+hPvrXzlMzMpGK3NxORywa959sCPvtelog984uPkFjh9eAuHp9foUSXLwDAMKMY0b6lVyTGxSiNjGjhIA0HEKSzBKTdoYRgGx++acrAakOxONaWbGGdJJBk6dOIHF+amDlUbQiBJRtvKLdYk0V0tmeoMkgnBlzvWhNg9CUtVwuHo3EptpBgcOzVBxwGzlevvtYIMBEZa0S4yOHDicuxWCur+ijn5oibLgBHIdUarU7xCirRWidn9Pi1Gj7eQhNZK6nk5MSTnFiKUWjh14KRsM8gxOd1I3PKrBlipj8WJtM+R0eaO+HOb3cu0VoZ0IyDUyTmSGhqzzW5kHAas+mSRQnBFjhX3GlDtGTUuNshxhTZ1dkRw6S4NQk7oZEgaONENdTpxQ14132wnxz632x0hNaYyUea5T2VQpua67WRM88YXbGZU89diOrPbFFZHK3atsGsV2xW2mx0lwOpwx+biFZo+QMruGlTnxm5qbLbHrFcDh+vE8WaDxJFSj8lpRssxKawf9/56UhRJwTxms1TmVj03Q40WAykNzAGX6pHBGiFBTB42ZViPaPWohTEGhuAJeauDFcM4kHPk9LmRg6OBo9NHnDlzyPVnDjgY15w7dws3nDvH2VMHnDlac7juN03w/BlLiWOd3O5KMxuJbDVQa2WezrPZ3I9KAZzrVVqDdEBRZbVOJFb88hs/yMOfeAQ9voDVSiCRUuYFt7+MW697On+0QH7soffzno9fY1GJSLfXl32XkC1CjBw9ZcXXfe/X8IqvfxWbeJWjceVgenCnItXi5sVVGQ4GXveGV/Dh33wXv/PzHyBU595Z0k6I9p93fmv80ocqr7kr9I7ER+83PHfFf753w7Yq5x+6n0sfv8jrv/HVkDcOE3ToQ61nL2sgWWCVsttWaevdvbMYmhXUindAyYn3QZK7RVMYcqDOhd28JUn0wlLcqkulOfZFxg1MGhqchxdaIyZAXa8bo9tymSQ0AnSBQJgJeBFWcbmpdt9DwLfSzTvcKIJaRZIbmXhmuC8jRGwfTRBTdu/SatRWPDMop260m0ghoBVmGkk97jeFyCz9/VNFpLlgQECid2+hGGqBmDKxh6wZ6hlLauTkC8mq/jmaBV+8tQEkMEt3x2+Vo1MjtIqmhMoa0cSQAtpa11UnmkWKOhE/4bhvMyFIJJlnwHf/MUprvUMOaPRs+KSBPBwSFGIInDotNPNAt+tWp5HkNLbWpY5za7QWSHFExdAyd//R4IeUNlYxMZXZJbYpIjt3vGp9KdvYsdlN7MpMsxnFs8h3c2Wz3Tqd0CZGcRy6TCMigSqe5/R4jydHkTQYG4h1+3hzm6MYAyl0e/4QsCToGInryHgwMg6Zg9XI4cGKw6OBo8M1N990jhtuPMeZceTUqSNOnz3D+vCAU0dH5NVAGLNroFNgZ0Jp7iQy1x2fKBvq1atUa8ylkC1QQ+TqtGMzHRMJDGlwhY4Ju2nrLsxRES2OmUpGAky7LTAznQz8zq++g7o5YZp2mAZyTlx/eBOf+4wv5Y8WyM10lV97z08/hge5FEd/s0BiJOVDnv782/iuv/V67n75s4jrAYm3ERn973fxvyAkE4aQmJtxEEf+0n/3F7j3Hd/HI/c5v29xdXk0qPIT7yu8+hkjTs9Tap05sx75+uce8cNvPybGws//2C/yjX/qdVz/jJXn3AgUq90n0WlYAaHW5guk0CDhbi+iFFViyqi43Z3ERKvN4x5Eiabu1ZkCRasv4CxRS+kpjZGmriiKIVDUMevYlJjcp9QD4PBNp0W32IseSRxCzztpFVN/Doahtmx3Z9o8EUTJIUL0YLYl7MpHIKNqIUXpogjX07duhRajf3ZqlYD7MLoJTUL78qKpklPCDOIwOCe0Vg/cKpVWPR4hSPS8aK0UjBClc4mVafbXQPQNPeZjshSIMfq1WT321WojqDLvKnkcyEEps/sviilQiRjDmDFxBkB1/SUpuCorj649FzFWq4FpntBWCbEiMbgLv/qhI71DT6oc5kQOAy04fNBaQ7rGqbpkiqnskJw9sXPoXkOqTK0yrhISXU2W49onTNwrVufEmbM3cFpcPbUeHbNWU2qr3eGosdmdEHNmnj0xMkhAVXkjv/gZ69OTokiaCC172l1aJRKNYZWISTh9tGZ1KnF03RHDesXB6TU33HCGW264gZuuv44br7uOM0dHDMOKYZUZD0a3H+s+k9tpppnyiBonuxPaHNhV75zGkHErusJ2exW1wtxmdmXySAkC1SKlFrRsfMO5mPIqvuTJ2WMf+tJnWK/c8l5mpGY+9I6HePDeh5mPL2BNyXlkyIlXfM4bPsMobfzqu3+SXdnsf+eP/p0ggTPnTvGN3/oq/qu/+Gc5e/sZGrPnt5iQZUCbKy3cWCCScJciDSAKn/uCF/Md3/1n+N///j9H2171uK+RZsbDJ8Yvf6Ty1c9M+9+rtfJ1z1nxs/ee4cLVi9z7gQ/zb37wh/lv/qc/yzZPzKrUJTWxKrX6cqiqOsYkjWHI1OJl+WSz86LVYKqFYb1CtDHvNgwheH53imgtlDIz5LxXjJTq216PFlCkNuY6u5tLDxKjNUSU0pR5N/kBJ41ZoFiXbKo5Dte9ON22zg/leZ5Amx/WMblEj67nVgGKFyQ8zMx5rdoPnOTFtDvXQIHJZbAiGQ1O+gZf3qn14hXc/1Rbo5l3qTkKrU0udLBAww1aYvLDWsy6LNfjWVUa4+iEfOv+AzF53KwEoZgyxA5BNEXr1D1Mxd+LHpsci/MfCdHt1EJgNzlnVURoZfZOdpcA6TlRTnp3XDi612udnVaE07xC6Bp2dQs0bc13CmJIMFIM0IxWazfPcN7mavTPYK6FMWcYPcIhqpskr8YVbolX3Vi71W5iIh7NHDxCt1JJ6aCH6DUEXOL5OI8nRZHMq8gdL7yJlBPjqTXXnT3NTTffwNHRATeeO8vRUeLcDWcJMhJNWK0HdBWdM5kGJGUero1p3jFduOoE9J2n9LVa+kiTXOUSXcu6azNj8uS13TSxm3YE9SXPImFzb+URa5UhToTsMRBjTpw9c5r1+jTNIsMwMMTEECJ5vWJcDQxyE9PxzE//wR8wX9xCvYqgxKi89K6v4PTBdZ/2PrzzI7/B/ec/9JjfW7bZIfhptzoY+c6//Cf59u/9k7BeY3ZAlgOKOUm2WUFioNCcFoTQlrHQrHtLGa/9c6/hjW/6Fd71G++hu0l+2vP58T+svOrOxBL/oVoZwo5vfMFpfuQdgd3umJ/+8Z/n89/wIs49/xytuU2AEFwPXDyATYN3RlIaQabu8uJu4qpe0KwakxaywGgjUYV5LjQqUQKqmZPm8RNiYc8lDCm6K46IB5cZSFNi7l04ESjIGEjJOahJQSSg4qYfMXg3McREGL0wqLn+OMbkX4cvAmutpOh+92ZKikIMA1UdZxVx3Byg9URNMyWk6L/wHHBTD3ZrzTtEgmBhWToaEn10buq2YjG5VlzVl1WtudlZFPc/NXUWQ21d690Pk7ro/sUnBfMgeXZWCEsCaDCsOSYoUrHgZHrpDkoSk+8GonghJ+CZ8923EbeWWw506aYaMSWCCLX5a6hEUGMXJtbrteOa6t9HVanFXXzS4Br3EKBUV92Y+icZRWDyJqpJoTVfko3iBHnMSGP2fHKgr+d94SeRHCIH44jkxE49Ftqacrh6kmOS564/zbd+19eQO2gv2Q0NtPmLUC2cDAMnsxOAh1qwyxPT1Khty24uYDPBjNg3n8fzlt12i9VKmWZUAmqFFKR7LkPMmYODA8pcqbVw6tQphoORy8fHrNIBB2NmGFeMw8Dpo8zh6G/k+mDkYDWS0oCI36DJnFUDjklZM9713g/z0Xd+HN1ddi1wHnj5s57PXbfe7UFZj3o8fPl+fu+eX37U73Q5pEh3Ww6kPPKK138+X/MXXsfldcbRvQcxi0jIjgeqg+u1+yIGYi9agBlibq5qh5Vv+p5v5APvvodyfuvLlN4Eirgj0MMb480fqXxV7yYdG6286rZj3nz/rTxy4Qzbh67yjt98P1/9gtdSW8MTKqJTsrIH3k9tZh1X5KGTlgnE0CkyAWh93AeimGcLmVHUc36sGhIde/KR0Isr5qOcL8G8KGozkkQf4MwLUNFKU1dcuR5e+vs0dMut0CWg1g0x2Bv4Sk+yUkCad3kxeMia06cUa5Dy6F0gXZuMkUImp+Bkdrd7InUIwrQyBC/CHj1i1G4oMRd3Q09iaIveodlyTfj3X4QEzcwJ8ObQioGP9OoFyORazKuIs0A8vKuHxWljbjNkYb2KroTKgVZdqomCNXfwQQRLilSFEFD/RIi988Xo3MTGXBu2mR36AVKE1sQnHirHxxu3fkvOCmjNhRhGgE33V+33aiuNKZc95a/VxtXdhOmOnDIpus58Fqhz7b6z/tpUjZAyZoHdvCEGIQco2+qmN9E/+/b4plpPjiIZUkJXI7sQaBVoBT2+wjw70C/0aAQ1ZpudKxYiYoHtVNnVSkzCYR4YXCbDEIy0GhE5YLx+xTAkQmwMydzpxIQWYDhYsQ6Zwzx49m926sNqWDFKdu6emPv3AVUbar61RXwLG8VYSUC7AzLAbIHf/pU/5OTBy5T5PCKR28/dyMue8XREPsxJPcfVcj1GpLaZN7/rx/0G7tZs1i8471iMlDO3veCpfPv3fSvbtdDKxjGo0CBkMsuNb2SJRLqtG41WJ+ZSuprIzVGTVF78Zc/llW94Jb/8b9/U+Yx+cS7dq4jw4+9tvOoZvil0fBRWofCycw/wVnkO6DmGcMC5oxswmzzEyQKC22159GdDehRoKdVTd4nU7v7utJ1+o7XJsT5TAsYQQLMLBLTVPSVqeY42A6KdEO9FMgjQ3BUmxUQ1j99YomkX2aepL0tQ73ZMKxqdUmZcOxRa9zR1YwsnSwScnYD/eMffrCLBDxvt8Is/V89X8dfQ3PbMugNUmXzUNu8ORdzMOYSA1b6Eab3QqWLqRTGEQIiC2ow1kBD9EFDP0BERQgpUU4JEhhC6EAP/HkEcSggQzX+uBBxjbTMxWvdJDbRi5KEvOWqlxUKMgZwjqs15+h2yiTH631F3Xlp8whN+EJhP497td1UXsA9Mkz71qApzaT5ujxCjcyIJ3uw4NDJ4nlHdkqJ3nvM0eQPUv1+MgdYmFNjtdtRafKpagubM/SzH9CQft+cy8bH7P9L96oRxcEVCtMAqJ8dTbGa9ypweB7T5BnvMA4jThA5Xp1gNA6qVSmNMo+dYBDzWtVMCOvvDtdMxMI4j2WAVBoTg2+kQaUDpdIhBzE9L61nNBCerm29zjcoUPIURS2gsXDhf+C+/8k52x5ewOpPCyOtf+IX9RoXDdIGDdImr5Ube+O5f5crmfH83vEDmtPIRVUDInH3qKb7n73wLNzzjDobu8ueAdvWoihB6l6CYedflxck8vmA86Ppz78QGoLLlO/7in+Zdb30HD3344e65524qC6f94Y3xqx9tvOpOx50AEOW1d0688UP3cOq2W7j9zls4EqV4arw/Z2kEy74I8RLYi4UraswmshqDBGqGnVaaNEbx+Fiz0JcoHlXbipu4atN9U+VFQzuM4SYXRB+5TDzRz+gO1a15h9gLjfRX4+8bPavF3ZxA+vIr7IPgYhBCBKKPla7gkl5Qe8cpXUq68Hd7CIvHZQhZfCmpfRGjwZ9Pa42UEkN2r0vMjYMRDyPwgm4+5neMcUlf9KBB774lBFLO+617E6fIpZRdXtgaWhyDQ/151A4rBUBaI9Quk9WGiZDTSEreuc/zFrNlGeUGvSE4bIG4+UbosE5AHB/FXfF32jf9krDmdC9wHHhhbsSQyDl220EhxwFRIWWfztJKSDEw5NKLa6KWiVp25OwR0nm93sMdSfwz2k0zinBwcMrhkuxTFwREE2We+/P5zI8nRZF015JjhnFkvRo4dTqzHg9Z55HVEMlJ3F049mQNcQpNxEX/FpwqlENy3KhfpDn7m960MOSE4DgmPT2t1cpsUAzmUF0OifN0C8rOJkIUd0luftM0c/oKzZc3alDq3IfaQIxrNCnve89H+Mj7PobVDSHAq17wpZz5IwJRQUl6D89/SmGzO8MnLlzuHWSkqRGTYKHy3Bc9g7/+D76X53/J8ynayNEpDKG77fgpvkS5RzQ0JifleGwDgdjHbO9Te6HRwDOf+xS+6pu+nH/3/T/mI5td6wqWx4/9YeUrnh73XWZYn+LcTTfy1S+4ys994GMcxgMGPDHRJzkPpTJcM906z7Vpvzm0dYmoV6cYYCWZFtzlXUt1HA/Xz2sL5HGFtkhtwQsJ3k16Z9cI5m469M9HshfGWqtjoNjeYcYVOgnpfMulyMUcr42sXqlRvAv0ZUTvJmP0LbEZoUeUWmcUdEdSTN3VKaXkbtl9IWRBCDEQmhchCx7jEKPDNtBd0Ttm2pr2Mdm7I4l+IPpyQvuB1CEBVajmqqUl2MvwPPruZqXVC5sfiF6och6IAbQZjpp6aJeqRxtb50TluGje+/a/j8h++Hox93HFA2UXD1aLjo3729oNqqNHQyzcZ8VD/do0dcJ8QLor/DzNlFpR6V18t8fSNtNq7RCGQ0RqAWt+CLbgUcTj2n1DTSFGRVFyBAlCnWfWh7lPbZ/58aQokgcHR3zh530JOeFFydRxpS7zahaIMXdahm9xJTkdwVp3bRajaPHWH6htprWCsUZNrjm8dG5a6xEOc6mcbDfuGISQ0wAhMGtDSkViJMaRnA5YjZEYfPki2UiSPEmvNnJwQT8MWMy885F7mC9dwrTQRHnG9YdIv46WR9PC+asPcGY98qrn3sWnLl/ldz/6CS6euE3V4ZmBb/y21/JN3/O13HDr9YR4wBivOlZF7a/beW3LWC4SiFSylV42Y+9onDfnnQjOZYsrhA1f9y2v5ld/5jf4+AfvI9S4x7GWUeiBY+NXPtp49bPWpNM3EcYDAP7ki8/xc287zz/9e/8X63PX8ZIvvBtNV7AuHdw/zIAeAhVD7yiFGEcXjpTqUaQKGoQQMkmcFhJMISaPA7XEXJ1hsChKlngGwLsfM1DzXJewBMF5hkmM/l5YJ0fLvii5eUQMvUT062n589aUVnq8aufwDr2jJDi/0gj9z/tYHHwsXojSiHSubSOK9M/JO8XVOPQlSA/LCl5SxOgdrI+wc3ejX3izZk7ojtGvw2XhFPEFDsEzz8MwAn7IJ/Fuzw+Ybk2GUaz0YLzRIxLEb6TW8Bx4VWr1zG7tX0+I7ujd0wlFXfARgws4Ar2TDuJUG7pfawi0ArVvlhFxjN4cNgjEvrG/pjZbrQdmbZ61gzmdqb9nrflnarFHTNfm02I0NEEOTrIHdyDCfFml1hjyqrMlnuSdZM6Zc6fOeieTYre1cmfHZoY1PwmnZuwDjJp3daX4eJcsUMyYSiVG52KV2ijbBvgoKrjZKh37yxIRGmeODvt45aGovtEMoIE0ZKwpg0TnqVFR15wRSKCNg+xuPx7SVREyJw9exPSkdyrw//6V/8hrX/wlvORpT3PVGcbDVz7phOX+uOXMKb72Rc/jww9f5F0PPMS3/LVv5pv/0teSc2RGCOHYITR1U0zvWFyG5/iY01+E7K4suL9274E6iO6mxjkoWIEwcsszn8bXfvvX8oN/919jDWbZ+hN6VEH/yY+d5qtfdhuPrn2nBuP1L38WP/qW9/A//Pm/z5//y3+Br/lzX8jqMDlOGBISFGXysXfZoseOwfXxNiU3OQltMZj1uzOoQgoUndnOnnfk3XroShyhMPcMkwChG8UOzsET7eR7Mdf0itBapQZFg2vMab1oxc6hbOrO97X1fGp85O3/LvhobomOx9UOTzimqK1DAF0J5hQz3W+Xs4V+bUtvVl0R5eoj9YIhhkjr23kfrX354d2VmbnFWPOhMe4xtuAiDFXSo1ykrLtEoX0aqhWPzvXllHVKlKghPcHGoQ3nMAbJ+yUQe8xc/XoPDjsh3qk5TxQQv/ZU1THU/k6G1DOXFhcm6/hyDKQQPBER5zY6finOKhG5FgqoQo2RmJcNux8cohBydrd0VbTTzyLORZWgECCYF1XUelyEfkZPhOXxpCiSqsqs7vZTtoUi3pY7AO6Jdcmc8ycqqIFEgewf/zB4IlzCN3QpDX6RmmOJntPrtJEQBKGhUt1qTZ1UHCUSNSJkH8NEqbiNV1OjhgxW/AYxIVgkht6dEPbxEWpKojlx3IcLb/1N+c9vfytv++it/MkveBXRHmQu2097LwThrpvO8eLn3sHLnv8sZpkxW7vcjtBxMKesLJvrUTr0II0lnMA9z4F+rLjXiC8uAv2iImKWGGLgDX/6dbzlP7+F9//OPVBi7w6uXTj3P/AIv/7gs3jlU65xODH4+ucmfub3zvDgRz7G//kP/xk33XgbL3/DM6nJNczLQkCb9OVJv3XFC6G1RrOyV7vE3tloc66eWScOB9+iemGNHo8KPuaLOebYuyPHBv06MXEK0rKwWXBME2OePfoDUUwLlMlvnurRILFneguQUroGN/ROqpn2bsYLZxS3Jgu9e9e+bTeL+78vvatdTJV7s+UYd0h9oumNwOLb1z+vEHyTb0s6YPTrwHoj4f6o3t2Gvp2v2pDQr/HoblZWq4eIGZ2M7ritmTFXd88Kncok4F/Ti0it1YtnD5gjdLaEupw4xOQdJwsMoPuO3Ew9GKxPFHtcWHys9uVW88RG824/RJdDOmxSOzfWoaoQeiTHfvljaC2O0wpISP7hqxHoOeHar8N+fQfrUtRHHf5/9PGkKJKmyrwr/XSNDE0J6uYBaRz8YoiRcViRuyHsvvKLn2rVtI8PgpmbYeTuKKRN/ffa5CdL9FOqEnoqG84Nwx2ro0VCFIoNTv5NgZAyQQum7vwSpEvw8NNeQnKIwJRsQrSl73jsCXX/xU/yL375P3DdeuQL7nwqTznzR23j3Sz41lueQv3DB3jkYxe55eUv5rrPebqPMQg72ToCagmR2IuBn+h+qvtG3nfIfoJ7MLx3lI5NOn4JTgm55ZZz/MW/9l/z1//8/8j8yGfmQ/z7X/0Ar/gztz/GEOOKrnj6576C9//qz3D5kQf5F//4/+Dpz/4bPPOFN+9pV9KxPX3050ZfOsSeiR4CZss6Sqmh0fryIYowpoxUxXDDDExp1a3zvOh4wWpUUjCX/zVlrrpP+Ysx9ikjUM016M73a513KKTgeLdKcwZD73SWomF9NMhEpFNzmroZtC4HEI4rSi+oC16nzY8sp/Cw2IBiXCu+Qh/zTQk57/EZ6SO69K/1XCePJFlUWbVUGoVm3Us1Jj/ADbesm3uBV79qY3RFy4IlmkFdlEBq3in32SNlxyNzjH74hdBNsj0ShejtgCkelBcXeEeubfzVHiPIWC4is9ZJ5b55905EfTmlpXfEnfgtDskF8ZjoEPwwWmJmx9WwPzR8xAyEgEtegRTSNdMTdX/Z8Cjo5TM9nhRFMsXE2fWhE5/NO4/Qs10UczK0JEDZtS2tc7JEQr/wICTXcIv4tZdUCW25GN1DcKGxBGlAc7PdvEJMfITD9bUepq4M0XXCUYwBRcMIqW9wbYlXcEqNSO9U+029SnnfKTy6MLhetfLISeVN7/kAt193hi+486mcWa/63zBSilx39gxCoJ4UPv4Lv83Db3svt7/yxZy54zZGcud6+nPxLV4E8TQfeNTNRcfJpOF+irK/ca49Ke+CXvnlX8KrX/fl/PSPvMkv9j0fz//SfQ9e5C0feQpfcWfmvnKan7n8bH5ncytyVhhu+QC7T72fe/7wbXz/3/4X/MMf+Jvc8vTriDkhUtCg7uFpfZTq+Giz5pG++EJGmg9mEb8Zsik1xr7IACmOSfmI2EPgzGGZpVPTBlX954UhkVqPhV06kzoTrLqTlILIiKTBfUjNOxqL6gsbXbBZ6e+fY5Sl8zKtE6Gt3/i18zdNu6lFX0Lsh97QlTIh7D+jps214OrMhoY32j7RhP01ZOZLEU+19MM0xnRtYx8C0VbeFS4LHZ38XlEv1kECMdG36O62H8W7vcVIQhRMjJiCOxY1//zjMJBipOJdvqiPz008O0g7rmwEpuZb69SLlGk3pwnZi3rnXWo/ZMTom3Z3QjJ1NgNhwWW9CIv6dRwCnUrlSrAYI2PM/trVIIov9IJQqzddOQ6ELheVPq1aq930+EleJAV3Nyl9Uy3NmOvsMro4YgS0VLJGHzsVz0OWiHYT1iihKyH85k7Bu8TQVRMWAiZ9hDRz7mN4rNak4fgFImgH1iF3LDOQbNl9KuC46bJZ7FIWrBfQlOIew3qix8cvXOK+i5d5zlNu5CV33MaYEqdOHbI+WPUvdvL19PBV7v2xt3L9M2/jzle8lFM3nEHF3JHdYjd49e6lyY5C6RI6x4MM7VG7kWig4mA7HU8CYXUQ+Z6//J387q+9g/s//kmECObZ0Jh3eD/8lnt4Z/4G3qbPAq69fwcvejXzg/egxfj1X/kN/sZfOuHv/m//PXd/zlPRMLljTPTPwHBjiz5rEYKQNe43oGau3NEOa+TeLdQQkHzNtX0dEognYKaqFPMwisjgvUeAISdQRawh2hCrWEoMIfgNLSPRMqKOWxn+3nhoVdtfn71y7N8vESc1m/X8IvOxUFJy4nknuC+aeCHuR2s3zujjXoidsA0iQ3893SQJ3ReAlJJv7fHP2qt7xzLxSJMQYscZHa+OIdIYepEtiKgbUatSF9xUnfzvVB3veBcCvnfEbiZi6r6eRfE/U3Wf1t6VYUZpSsqja7UNEL8HYen2XABjvdMVCe7Oj9/PJfQOHCOvh74UKw6FxYiFRCsO0QjCEH3MH3LsvFVf9kqK+y7WJY+dHSMQrNG0eLyKgHRX+Ce6U58URTKEwGq1Ipk6/hEKw+jZIjEMxOjxpFFCJ6DqfiHij4XXJnsNKyjDMLifoUF19by3/n2ECMu2so8rweK+SNr+jQu9FITlCkdovrQh4v6ChvS2UfCTrBb74ytkf6gZ7/3UQ3zo4Qu85I7beP2z7/DT/lHQuz8b4/yH7uP8hz/BU170DG77kucwrDNNoutT+6bXUy36z7fQL35gPwyCB1LU/u/e7SiNZz/vdr7jL34T/+jv/HPqbul2euytwQOXKxf+y9s587JnP+rdh7g+zfq5X8rmD3+doIHfeuvb+O/+X9/H//JP/ibPfsHNkLSP/AFRFwKYgop/VkkCKfuxpOZEYi0NWcQmKM4Y0j5i+9Sg3X3Hek421tC6ZckvV+Y+vvbuIaT+3ngXZQhBr41/QQS6pO+a65KgzY8D6c9Xm3SdeCCId4ALLcsvnbAviK05Jy+mYc8sMGtujNLHxyFmCI+FI8xix0I70TpoH5f9v1XVrdbMMBNKaYzZl2G+O+leqikRwoCaZzA5LAXSF2BivkhZzKS1v78Y/X6KnQoXewSyP5eEd5+YeXcszs4N6gq0EGQfu5sG58w6Ad4p5gt04U5AvnhtTSlTN+sw9wqNnfiNiXeyupgN+MJNW4dO1L0zmy1Qgd9bTsPqMEOEFiEpjMWowX/moxeof/TxpCiSXpS8AKY00Oro9JplUbGA030cWsYX279Zfazs/DLEMYjWPBNkGZP2zjgiewDdAfZOdMZvLOgTBnFpPPsPlv7jlv/5iBtQvxh6YW2z8Pu/+w7/ws+yUILbRb3rgQf5K697OdfTeOiD9+834Y95u0x44B0f4aH3fpzbX3Y3t37e3e4iI75Fjd5773XU0g8EI/b/JRqjF0npmCrJ/zsY3/jNr+eX3/QWfvvX/pAgkSCP3f7Nn/og0wP3MD7lWY95XutnvYzdx9+DXr2AqPCu330ff/V7vo//5f/427zki+7y0RoIYSDiCpRAQ4MHvZUy90WId2w5Cs3mPZAPjl+a9gNJACtUKq0VijntRbQLkYIfEK15F7hENUhXkmjv5hDv6Jx21EH/7uqzD2Uz/3l7bbU9Co7o14f0iAjwBV4z94CU3hU2K3gwnHWuohfePR4py5HYD0XripUOfXhnV/sux9xCDCMPbnSRc3TXpuV6BawqIRgpuiSU0LroALdmq33y6ZhjU49AEZ90nWsqXpyWTXs2RarblGVrNMcskOAUpxQTIeZuZl36Qi16Z6faVVcRUyUmh1maFWp/sxZ2SeiG2ss14YSjtif0T6X057W/NTtu2zHp/hpTSr3zL0j2ez7WykoCxZzQvnBBP9Pjs8m4WQG/Doz97/+EmX2fiNwJ/ChwPfAHwLea2SwiI/DDwOcB54E/bWYf/eN/jjuhRBH35FPB7b07P00dn1S6vEw6KZdlNJGeild9USOGaOjLjF7YgneSixg/RekuLkujtdwM3j2EfsEvhbdR+0np20Lra5ul2wzmp+2nPvUI73/3PX0L+sTO4n/0ccczb+ZFX/g53HjLDVz8xIPc+5bf5/ID51lK8/IwDJ0bH/u1P+TBt3+Uu1/xedz8nDtcg03njrL0jtLp425IG0n+UuljGxCtuc+jNW6+6Tq+47v+FO9+2z9ge6UiJo8pkgDH7/x5hhvvRLp7ivXP8OjFX82Vt/6If+9mfPA9j/A//08/xA/84N/ktjtPY/uGXFmAeFkOQEBiP3TcGgN0R4zCQlfZ1QTJuwenv/ScHvqyLw4wJpLKNepHXg5Gv/FIYJp86y2NKLk/367QCdc6Ne2jeuxLPVXvKmNXdZj2hYoppVY/oAMgLivc8x77cskjLvx69SgLf/ekd7CPHpFk2ei31rtR/6xac7ldSp3AzzVJamlcU7DESM7+mqM4YTrgAXhtLye0LrN1DbNWL3opRDebiE6Ti6LUUjvO3ieXZv1Q6eYxIVDNw9OC4b6srWOr1QtgqQus4JOBVzgjWCaLIMFTJJPSN9D9M5SA0bDgZiHaYQE1p3INKaPF+gjv95yP2mFf/BFoxRuIlDMlKMxCiBniY6/vRz8en2Z+7TEBX2FmLwJeDHy1iHwR8I+AHzCzu4CLwHf2v/+dwMX++z/Q/94TPgRhlMQquL1/6G+Q2Q5lB8yE4GakUEGKLwPUNZxzrcytULVQdaYy+dO2iSCFFBt5Fcir5I49Y5c/Re8O/JdhVlCp7i5i/n0aMyYzMBOx/Xvpg23DmGm263/eEFXe96738uAnH/m0wvIZX7vIXswvEb70FZ/L2TOnMDWObjvDS/7cq3jO676Q4dTYDQVcIwzNC6AJ0+UN7/7Pv87v/rs3cuG+T7mDN5lIL4jeI7uWGsNoZGlkjGRKtBmRiRAKIRYsTbzqK1/KK/7E5zl22YnKj/7Vji+y+cBvPOa1GDDccAfjU5+PafNDrW14x++8i3/3Qz/FZnZPQ2kNWlfV9IMxBF++5WAMMTKEgczIKh6yCoeswhFjPsUwDASBHANDDKQQCRoYZOBwOGQlK4awJucDJK5IeU0e1sS06iYgAW0J1aVYRQcetFH7+6paKT2hc3EERzKqASwSw0BIniFOECQFNEXIXsB92wAhCXlIzoZIkTyuGIY1KY6ENKKIK6tiRiSh5kKJHtXEVEqPHjYfZRfDhtCvV4RqgerHDdWMAjQRmki/RgO1NnbTtmfZz9RWmOpM1dqxu0Bpi5ImkVN2aluKSAysYyJpXwl2XLlFaNkxyYVmZQhjHJySJhnT4K/TEpMJO+0shhj2iQLaM9DV/PBxtfCigOlsjZB9+RoTIXucyJgGTq3WrA5WxNVIGjPDkInR1UApBeLgyY3uoG6k4J92UHUyel2icT2g7PEen03GjQFLlFjuvwz4CuBb+u//EPB3gH8JvKH/O8BPAP9MRMSeoGI4jta7BfPSo7TuSKx9BdG7lejxlSjeVXa5VsSpOqbi45w4zmLKPp2vr0EdS0pKC3V/6rje08frgMup8tIdmkuhGtK99BaOV+fimetEk7hX4Lve/h7KtJCBecLNmWthXU6ZU+aFL3we67UvsVLwBdKtz7+Tm+9+Gp/4/ffzsd96L7VUljn+mrLFuPKpC7zzP/wKN999J8/+ss9lfeZofwg8+ud5oYz9zwS/dADzDlkxzpw+4nv+m+/gt9/6bs4/crX/vMd+hJsP/harO15IPDz3GFTh8AWvZnrgHrQVzGZK2fCffuzNfOVrX8Hnf/5zCDa7cSq6f24tuOlqC/4+C+78Uxvd4CGg4tSrHHzs87G7EkTd/LhfTM5b9a1nSolSlo4u7jtD62O7L2qsKzK082j9pYYo3ah1IYgHLEDRRqh93Mat3lrff4UO35Q+1URJSN+oooW0yOdao7TGEBOtzf4eyKPeX3HM0PN5KlGuGZ4sn6N1u7THqIjCo6CnYL1IuDzSxL0uY059udGjKaDDXcGNr/HOqzYnWseUaQIVo1Yf10G8c25GyHuEsUs4+5Im+Gtt1oEfWdglQszZw9SWSSb4PWSdGbDowBt0RROOKwd/nongLv/dh9KK14ile3atvpukCP41AQN1D9K5Fr/GkpBT6lj1Z358trnbER+p7wL+OfAh4JKZLcj/fcBt/d9vAz7RP8gqIpfxkfyRx/8JRhDt+IOfWG4gEXGagmNn/uY6j80a3R5f6Ziuv6m188MUcgRit+GKoUsW+/gTnJScE46nLB8CXiSDhccUGEHIvfHujDC37MJ5VxFBNEJLvONt7+kjvuNdTwRMLkD+woH7wPs+BCUzpoxSHTQXQRPc9bIv4vYXvIgP/cbbuP+d7//079X//8EPfIyH7/0ET/u853DnF30OeZV7EdiXVn9PrHNEkX0MAWjXORtf8EUv4Fu/7ev5Z//0hynzp10ToJWr7/h5zn7Jtzzmz8LqiIPnvoLjd/2Sj51aePgTl/k3/+dP8sJ//j8yngr9WaSO2yrRHPOK5qO4L2IgWqcsaWCuEAb3jAzmC4kUY8e4OmlZ/PBUNYZhMeVIXjB6oRS1Dnv2CAFs76JNvw5M3bVGzA9wxBcMFfWiY37jaf+7ktJeEusHozvrLwsh6cUsBc84r+b0mD3Zun8fhL180s1IHE5Knfz9mP2CuE4eXKzj223/XoRuAafOjrDiiy0zKLVgIr7gM9vzBkvt6rZOrQnJm4a5GrUpZeGSiv+9ISaHw7qmfJGELkKQBVYJwbfPbi7sunmzxpgHQgyUufSudhEwmC//xZubnPx6qaVHfajj7a31Ahij548jlP7PlGLH4rspcTdG0er/jCH5LraLNOITNDKfVZHskbAvFpGzwE8Dz/lsvu6JHiLy3cB3A9xy+03dzcZ5hqmHIHUdex/JvBD2ZatvB+nOzBrRaF2G5jSYHIfeZTmvrOEbawt+4blhReqYlnQNsDltqP9c7YVwjwbuTSL85wSJXVOz3FyRy+evct/HHqBLJ3jiErn8uReCYRg5vrrFqncEMdTOD/Oi3MwYDgN3f+XncftL7uLeX3s75z9y/76/u/ZPV6B85Hffw33v+iDPePkLue2FzyQkv4l8weA8tcUk1mWDkcDoXZQJYbXju7/nT/Fbv/n7/O5vvefaIsOvCUSE8uCHmD75fsZbH3tJrO96KbtPvJN28SIxFLRe5dd+8bd48y/+Nq/5Uy/1tZK4q3fvjfpSDWyfGAiEmRQ86yQGpaXAPAtKdEMSDeSYCEPoJrWFRHUZmwjTNHeerH/LlGJXwew3co8y9tC+DBS0L3H2Tj+hy0HND1CCUa0XZ3xZZtbHYQdafWFgdOzMX04SdwvHXPaorXXVjHb459qv/c7IrOOj9OfeN8fi84x3XyDJ8bVlOx16rHIQdwDyN1c9wEw81kMcqvafuRyY0g94/OJtzTmLKTpmmIeBup0Ihneq2vZXn3RD5WbXJIMuwwx9RyYe7CfLdR/3HXeztjfIMDFmK66UWZa6C02wH3Kaki9/GjRdUi+H/p64lNm34uqKLr9waeaF0gQkpk6uf/zdwWeDSV67oc0uAb8KvAw4K87wBngqcH//9/uB2wH6n5/BFzh/9Hv9oJl9vpl9/rkbzriBaRMimdAXLtVc1uXnsONBRnKDi5AIaSSlFSkORPWbJvWRrK8i3cbfhGTOc8x0Rxzz8QctSGtEawRz5C5KIlgg20CWzCCZLGMvyt2hhLTc1jiFxgvnQw+d5+KFKxj1WvF8gofvnPzYbarc/rTbCLkiYSZHF/p7oa9gM2KFII2jm07z4m98JS/6hldyeP0ZlhN4KTrLr7abueeXf5/f/Tdv4vy997trEg0TFy/6FtblWXuykzipnwBPvf0W/sfv+6vcevsNTtER6STna6/h+J2/gP0R11JBOPWi1+CGD4BULl26yP/nX/0HLj58laYTZgWz6vhl79zUwxhoRv/sIwWhRYEs1+KDg494zRzMV2mYLFQlnx7cexGwQq07ms7MPc5XtVLbTNGCCMSUGAa/niTkPn5mQh6w3qFJcNFACLH7N/qMHZbPybqsUgIpJBcZEPajvdbFUdw5mtqfp3ewveg3N5ltxbmI1pYFS3eZSslhJRaziNTz0QckJFS7IUdTSmtMtbKrhYKxLYXNbruXFtbmSxp3ZvLitODzQfo90prfHzgLWMvMtNnQCZbQTSqiOVfUJej9ug/0LrXro/ElznKNLe+L03wSQ3Yj6xASMWZij5jYtxEi5ODu4rEvbcVc5eN5Rn1Trt6JXjNk7qyGkIjDwGq1ZsgjYxxInYMZw+O3Mn9skRSRG3sHiYisgVcD78OL5Tf2v/ZtwH/s//6f+n/T//xXngiPXG6plDI5+LIhqy9vrDud7B1fatt3XVpdOtXKzLTbUktDm1GKUouxm2ZqUXf9wTuTZYwgOOVoMYhoRdHi7uZC/xXcLNZMu/ywEUQdFpA+buMXVljIylTOXz7PdrvFQ0P4ozDepz2Wd8YJ0vD0O29nyJ1jx4AyoL5iQbqRlRvZCybKuWfeyud9+1fz7K/6QtLBiD3mf0vpFE4uXuHtP/UW/uDH3szFh877uILSxAtME6XQmJmZmKihMLWZQuXzvuSF/OW/9p2sjjKIP49HX1S6vcLJ+379015bPnc7w9Oe51vXaiCFd/zeu3nzz/1mH/V2iFZXKfWIXT8sKpinFKaAa3KluVqNhkRDEv2X03wWbLipMtfmI+JCaTEn96cYCT0TxlkRTo2qWihWaYI7yaS4N7ZdXuXCLFg6kKYC5oemVj+Mo2SCZISEWKIVoRaoZTns8SVjLX3EdnzbzUcCOYz7Azr2zzqI9HyduO8oCd41Cca1kCLFrCLSCNEwqZ1yVMAK6EygMgzZ5Za9G5PgCqdWGnUunm3f/LpGnejth44X9OjyGZpWSmsu5AhOZt17RLbWw/z83pWwZKj5fWS0blLcmMuOqjPGozrPzotMKZOGoQeb2f5zS8FhgIywTgOrlBliJKe+mB0GxtVIGgMWqh+eWZDsOwV1/MMNTkzRMlN2n+6jsDw+m3H7FuCHOi4ZgB8zs58VkfcCPyoi/wB4O/Cv+9//18CPiMi9wAXgm/+4H2CqlGl28NTcq6+JUXDWf2qQzCkJrfrmUcRNSBceJSRMHdBHxE+gENzlxBoqem1JE/wk7tovl6/FBWh2S3x3/S7+54ZjpRL7gmc/jCC9W/APMHDpwiXmuVxr7D6Lh9PDhGFIPPWpt1JKgxSYrfS3vHbzil64Wcxs+yESIze+6E7OPvc2PvHb7+P+3/9AP1CWp7BI64wLH3uQiz/0C9z8/Gdwx5e+gNWpFT7qdl8a8x5UzdzBR5SWlG/81tdwz70f4od+8Gf9QNLHjifbe36b1R0vIp264TG/f/SCVzF/6oPYvAVTdpstP/Kv/iOv+qov4obbBg+3Z3m7uvo5eu7zTktfoERfhJTqG9fgi7WFokOnKLXmN18123dDi+GtNnVKTneSktC5gobH0VofrkXccKFjo8v1aeA31v7rvGzGbtTgMkTp10Nf6Jh7LXrB9U+jVscQZXFhaPYYn1SR6DQmXFO+sB8WvG5PZxPprkHKnriOod3c9po9ohf3UrsGWpdFRte712Vx0jtkb0ncas3b8E4A73G7ZjQthOBb8Fo9cnihJ/lDWSSwvoCJRIxZW1e9+c/3cEfZH/q1XjPSWA6EhR/q739/Lf19QXUPq2A9eTEGZxzg7k9FC6IuBdUOS/gSyxskj3rIPcTsMz8+m+32u4CXfIbf/zDw0s/w+zvgm/647/tpX6eOlQjOcQxRyPS2vBc4bQ1icnG9NTf8lO6OYronAkMHv20ZUx142dNXVD3rJRlN/EIxwNPlrvUMrRW3tw+L67f0S8GzpX07Gzvx3Xu9aTftM6wfzWt8oof0sT0NcPb6M+6pKAsq6s/Fz2CnqbjvuJc2Hy9c4heGzNO/7MXc/KK7+Oivv5OH3/vRaz+jX2nL+/PgH36Yhz/wMW7/wru5/QuejQzdtbyfhWaVilLVu/zxwPgrf/27uO8TD/DLb/xNtMbHFkpTjt/xJs5+6bc+5rWF4YDD572S43e80Q8ma7zvPffy5p//Hb7h277MsWRZUF+Pdy399eToMrtdnWi1kejhYGp7Rxy/sdgzCHwE9W1yU+2RGm5kYPgoZjGwJAwu3ZHbc/lnEfoCaPm+i7MOwj4rZnEtdwecBY/0uhJC6tdl3D+v1uq161KkL4rUZbaSPBSrKYj1hZAvIL08tG4k4YVsySNatszWi4QbdEgvBtcORr9Sli2k9GWhd+8pZax314vNmy70rBQ6jevaZAJuy6dNaUyIdPJ/n/HUHOB1Iv/yu0LVxf9ReiSvF0L/XJYulP09KsukYl0BhENsj/VBuGZUYYvuO1zjE4tB7J4NXoiNJs4qELt23f1xg+6TQ3GD9NHGT4WqFVScVhMCTQKafKRpWjC7ZjVlEjqQ2y+6nP0Cb4W5eloiYjQJmIS+xYWwnMrdll8CHgMbwv7NU7JPHf1UbEv/Zt3KK/RlE24OuyJTOuF2STf840qlk2Qddb3pprOcu/E6SqcuZMkdpwuOY5mg5i5H0pm4AcdnaqdZqMF46pDnvP6Lue1zn81Hfu3tXLnvEX8OfmVCv+RbUT76G+/hU+/4EHd8yd3c9Dl3dKA7Yni2UMA9GmPM3HTzWf7eP/irPPzJh3n779/DApAvhXd++CPs7nsPq6d+zmNe4+oZn8vuY2+nXHwQw5immR/5tz/Fl7/upZy5aQXo3sUl4IVDrVFb6QsUI+XkdnA1AM1vxr5hNnRfdBzb7O+6GaEHgrt/JbTghS704hjl2uGJQaAvdsSdo5ZCuSxSPFJjWSqaV0VbCtgyufgNa31BAb7ciMHjNWpp+yGjaiXHpfPxNMbWu0fvLP0aTcljN4wA3cHIKU2yn1r2gEB3RkKudcPIog6yvdWa4O78psKCYaeUPESLTnUKQl8uOwbcGkEijmB1KpUESl1c0ln47e5taZ5j7siU9ANCQXFqUVs2R10B1zWVi0wj9cWKqlK17KdBd2xXFpPjWidfClUnyy8wgnf17t7vTVQ3s2mLZZ7s68/jPf5vLW7+//dYVDCGtkIUBam0qEzi8a5iTqkYYmAIEEUJuENPJwggorTqmRfTvHGn8ihubmF+4lq/2JcoAdWKhE5Mb8VJ6dYo1pi1Uq3RaL4g6OikimtVD8OaU3LAKCNZBt/QtfrYlyZPfEp5rop/cF/0JV/E6esOITYfFWhd29zxIEnksCLG04RwgJC9WGoAdYcd16Q7qH/61ut58Z95Fc97w8tZnT1iv8557N6F6eqWD/7823n7j/wqFz/xECHCGAdWklmF5L9kYAiZZ9/1dP7xP/573PWs20k57sep5QQ/edcvofWxfCFBOHrxa/vr8K7hPe+8l7e8+W041U6ZS6E2xxH9xuuHknrxqrUy1dK7LZftuN+iO9LMZaLq7J1Yl6qhvZNIw/5X6MuAcVwxDCPLEiGlzDCM5GEgpaFPB90jUrrNlghjcqu1Iad9VxhzYhjW3XbL3w9PUnRd8BJfsZC56dvjSPAs8BDd9UiUlKSraZJf24ZPONodrVQJzYudb7ETKWSGtGJMK4ZxzTCOpJy725B0X8bUX/9IiAM5j27/Fz1AjBjQIFSMIur4bA/Kquq8z0XPbgSQiGpCbAXqlmwxLgeqS4pjFzHQR98YhBQhJOfhqxZUuzBClqWj46sLZlxL8fTFMlN0olEwaeTB4zZSFnIWDtYrVgdrVgdr1usVQ84uNOhsgmtdbcDqMkU4Tcpli0/yIiniGNOQMjm5VZmbyjruIA03zTVn5Fu1vo1z3KbUmblVL2ytUrRRtbCdNuzmHVULIShoQXXGtQg+epVWqa30fONrBqKtVay6IkENimovoK5ACZIZZMVgmRWZlWSiJY6vbvfhRp/di/dO9dwNp/nWb/8GYvbR2/G1glr1zkS9e4hAVndij5IRGQjSb05RjLm/Nz66NDPO3PUUXvBtr+b2V7yAMKa9pZX/0/pCC64+cJl3/ehbefdP/CbHj2yBAWFALNKaUFWwoLzo85/F3/mf/yo333bD3sLKl6OG7o7ZvPctn/Yy83W3sn7GS5BgGJU6NX72p97I9sTAMhoSpIyFjBJdZdFxZvdjcru8ENy1m9CQ4OoY1wYnch4Z8ki0rlCProopOKE5IRymgYM8kgBpjVVKHAwD6yGxHhIJ4yAnVjmxHtYcrg5ZD4PrlhQGiWTp/EVk/0/V7h7ejTVSjOScWQ0jQ8792s6eu9RjVFPqZq+ijm12ruxC7/FYhkywTLA1qpmmkaYJVTetWFyoWusLq3mm1tZNG4wYBkRS38O43ltR5lLZbDbs5plZK5M6vDJrYzdX5lIJTYld374YbSwu3j4SLxiub9ndDDnv4SNg/3O9vhu1CwOuXf9Ovwk0cjByEFIwwuIp0DveJUQsh+z4ubk0NHT4wOuId/GtFTD/fmOEMcIQPHkzh+A+BgZIT3Z8lKnwZ3o8OcbtjjssG14wWunaXAmORWDdhNP117HjQ2AObOPg/PKhiQRCXpTLPnakzqzX1j9CJ8dhqu6CHRIp+RtnBiqJlK45UWMViUJOSpDCdsm0xkm9UTIf+9gn2NfHzwKSNApC4Nz1B9xyy5n+YYU+StA36d1AwM/Xa0uAVhxQD0uXa9eMZKtjexKDd1c5cOvnP4sbn38Hn/qdD/LA2+/djy7Xnou//+c//AAXPvImnvKiZ3HXy1/C6uCgE6/76igUvuyrX8Z/+8h38/f+9j/hyoUTf7n99W4/9Hu+xDlz82O+/+Hzv4Lpk++DacJa4fd+4x289w8+wed/2fNQNvsFilqPw9jjaN6FOCcx7D8PQYgR0uCgfKm1GxD7wonuHzqkYdkV+1garLum95SZR+fRIIRgRBJmPb9G3NC51uqE5kd9tH4TOw7nenS/eZfvt7wp+0A0u+YXoLY4Gnkg1fJpLN6RXbm3/3QC2pdHrlfOKaLVJZUsRsPas2jEmOeyx0Vbc0pa6MVY8TzqGD1Z0brJiwgsXpeepe0G1fM8X9O0m5JC4pq7kj83645CHsi1vEvX4B03WZCeY+Pb5dZK76wXbHeJJYElPjiG4LBW12HX1milEUOXRPo2C9VHNVdN2Uc0d0jOFjaBGYvtXRT/jJ/oZn1SFEnFqN1pWXo352C6A/EqigVBohNRUQ8CUoOQQ1/SeGD8kHMfpR0UXoqminlgeqA72/h4o+ok3hSzc9lKIyc3Hqg9jEz7h75gJkbpcHrnTqr2VQp89KMfZylynw1PEoQgax781GU+8IGPcOOtt/QC6ZItH/PdIXr5VbsUz6QApZOVG6qCWLfu73KwwBJG72RkS4E7X/kSbnnRs/nwr7+d8/fet78Rl2frnWXjE29/Pw++98Pc/rLP4amf+xwWZSfJB6lv/pav5ZGHLvMD/+u/ZNr0vGwAU66+402cfcW3P+bSC/mAo+e/iitv+8+YVU4u7PgX3/8jfG/+Nj7vi5+J0CC54eqCddbuah1j6n6ZqZsrJxJ+QzbzGynnFU0rVSePMxWnjoXa89J7R9NEiYGuDumcwNT2gV6Gdm5iL5xaicmhhdq7qCWDacGvG9cWAbJg1o+y9KKb0lrnDAI+Bra+eGj+OhalTQ/J2WOLdKYH5l0rIfZQrOgUqV6cXOYaMfOYWi9YrePuy5LCKVGqfUnVp4nY3dwXAn0JjRATWh1PXbiOZkptxfF/HCJbID2JoYe2aW9+Onm8H/D+0n1B6O+Hpy66Wck1YxHb4/rNscOm+0PEl2uGWNtjmUQ34F6GaqTfK+rTZuiFWVtlYTG0pt3B/4n7mSdFkYS+qseZgNWWbZztTyV1+5AuM1I0NJpFWjO/cGsjpUhREEko016K15r6m6hCaDPVvMiMqRvqtmv2+Yq7WiNCpXQyN73guqGC4GA1BJoVECVZZnfc+PBHP+EfUoCosj/lHvehQmPLXI4otmWWE3yt5FidB977BRc617NaQZvuA+/rtMGgj1ZCVMV9LgO1dhBcjSVlcCoTehR42td8Pufuezr3v+VdnDx4qW+//X1fPCnrtONDv/p7fPLt7+fpr3gRN9x1m38OwQ0Fvvt7v4XzD5/nh/6vH6NWJUqkth3lwn3sPvZO1ne86DGf8+qOF7P5yNto5+/HWuPdb/sY3//9P8dfHV7PS1/6DNc520SUTLW5u5M73ltT5P/X3rtH23Zc5Z2/WVVr73Pv1ftlS5b8wJZtbBnLBowJEByH0IbQ0J0mgUAnNE2HDgNG6E4nJIx0Op1X90hGEvJ+0IEEGCQQQkiAQAhgExLefsjGD2RbfuhpSZale6V7z9l7VdXsP75Za+8rpCs5PHTFODV8rHvO2XuftWpVzZrzm9/8ZvLMJryTbEXE8KioaIRqeXgr1np8nqpaiOTMFGrVVrJqvD1pDSBit1mOTKyaWBGYpIypDt4+171kTYfoMQ6od9KoZrEQ500g4+e0VtWWOAdKnEyYoZTegjub6JakPOUNKvKFk9GyqwIIJ/UG1ultCrs6en5LyR+UAG0hVIvPIeCxxRPh8QoD7im4g03rLJd1HAhhsBcNiF0JKA7FpEY0inpVGVSXJNfI5qiNiniwNYWPaeOJJVoP6AJVNI2OI2ZDhlDJn1IKvWe8qHui/qqRSx5vUBliND7r223YEc2hxQFH4K4psVCznmhcJEaS6IPRI0kSwhBECaGL1J1MCifEQvKuDm+EzFePBICiAIUXo1FTM7nwOUQlu3c285bVNHBQSVthHurNVYnL0EC0ZAvtBPfAUeRBJpOE/v0fe5CHP34acBb9y6eKuWMBnDy15rk3XEtjS4uwc7w9uSgjQyJuruH19BAu8FCmTYWSV5FBJEribIEWRuYwZW2+5M4Vz7uWy7/iDZx5/z3c9XPvYfvY4ZIJDTtJNmN75izv/5Ff4P4bruFFb7iVU9dfRaNjJxN/4s98Ax+5+y5++kd+PsRpESXoV3+K9Q0vI00H593yZa/5/Tz85n8KNB555B4euvOV/OD3vI2bX3wFV1xzSoRsmzDfYA3cZ5UxemfDlmZi823nQ8wSc/S0lrDq6HciKa/JCpbUGmG1imjBnZKnkM8KSlgYoNGNsiwGoS1tHoDYoD1oaBGSomB4hMk52nj0MAB5fJhrw+cU6jlldEI0vI3WbbF2esiQJWH0VgrJC7VFlt4lqDJzGC1Ui1qe5JFhj5T3EMAI2TEzsJyl5sPu4NS1RQOwLB5wbcJ7y1QiyTloOjKCPSTMcsoLnCD6z2jVkCNE11ra0XPEwsCiVBgUqjflGdxVrybCfouDow10jNbkBdbtLAZDCM606IE+ulF2AioLaTiF+OFozDXoTzt448nGRWEk3TtzZP1UURDlbzrMdaoRnpQYHcIcU1rwx2kBlnctIi1FIyxTmV3JmVXJzL1TezzMYXGNoGg0UomERosOa0kV2tA52pxTZjIZOZLFzRruE/ff/wCHjx2xVHObFshT3j9w2WWXcP3Vz6VwQsRx31EqzBuW1demeSOnibJeRdmdhDp2iaKQyEosc6PkjNRbWm2S/zLJvk2pwLpw5ateyFW3vJD7fuV27v7F98lz5/zrN3fO3PMgt33vT3LlS5/HCz//1UyXnWJ1hfENf/J/5L3vvJ377zxD3yYdettznH3Pm7n01i8+737L5c/hxIs/g8MPvg33R/nEA+/gXb9yhh/74Sv4yj/6BZgd0fsGK5lNF+1KWyAwLcBrY55nVqsTrFcr4ZFNYgo1dBzV48WX5IHZOLigb2e1Vs2BFYZhSUER6wuR3AOjizmwnXZjmZQEy0FgHmGh6uIHNUktE4bStq4hyiZRQQRdhm+orVsQxIuYKpx+pPLomZl5s6L3xLnthvW0Zrs5i5dHuOQS57prr+Zg7ZitduvfVCoLLJxWjwKEhaLtYlCYmzLMFsUUkfRLKbHts4xdCo/UYm0HIX5o7Si4iuKAhVwvT3vgyN1bJFzDy4tI0Uw4sbfGaK6XEb6T04CZehz02rsHadrxR7OYysN5YOQ5QPCZSSQkjUPjYMAfOljL7lT8deOiMJIgHp6bhG3pTUkKE3aRkprTTyRagjnaQ7oZLeSjUq/kvVae7n0R/BwtMFvbclgb3cWZ7JFVL7lQcoLeaK7NNuVEygcB8ncserzUulXfFBzvcvFbFGDdfe+dbM5tGGuINoDrCw2deYdnz3HmkbNcdu01uIkS4Yp8ow2BJOTwiqfNko3rLWhJYSSHRL8yj5nao+eJ9yUJUqLCIKJCepKW31Qmbnj9LVz1yhdx98+/lwff9aHzEgdy0fXvh99/D6c/dB/Pec3NPP91L+c1r7mZr/lf/gB/8y/9M1oVzxLg8I63cfCCW5muvOG8uz75it/D5p730jdnOTzzKA9PD/K93/nv+czPfg03v+ykxJF9zeSOF1F9iovDaJZoKTOdPMGS0AMIryFNKXh5eq0xKfQOwzBlaL5ltS7MdAoyrJEPk/EkIocl/O0LT1IqUonNVlSnuQ6PDyQG2egWSSgUKdTWI6iIahFkgHvv1HlmdP8TKuBhvyTddurSiUuuOCGVKXfc1qxKwvqKuU3kycAOcKq85jhMAGrdyxIHBU6FOmGojeBMRrY4JUEY8Zxqr+Ln9jBygReObLxoSIJZsKGKpAAaV/19rTvh4XH4EF6n5O7GyYVgivicGq5jb0rizK2Sk6qSiLnOZiERFw353BTf1cYqRe+ermIUjwes5JZaRAz5vAvxJC8KI5kssS4ranNW6xOx0ZVxUnG64VXlhlMpZJ/iYQXWhMOkCofuPcrKYkKC6DtFr5uSlRSSRyr33tDPkxdamqi+YSpxklmEugGITwcHWkyDhMoUQbfx4Q/cR23xXdtp5V1wyMHg3NkNH//EA9zEc8GrOIKubOgc2o9t1LK7soVjwZkZRIgzWaEUAdakaBJWVT87tzlI15XtRqctpqSIu6Sous/YCl70xlt57q0v4c6ffSenP3wfIwxk5yfTq3PfWz/AQ+++kxd89iv5yq9+Ez/zkz/PL//n91DnxCile/QdP8aVb/w69mn1qaw4dcsXcOZXfojt5hzbwzO8/7Y7+N5/+P38+b/2DeQTWyz1wCbb4rn0ECyw3nd9TUKkoxvRZ12E5d4E2Pd5lv5keHCHrhabOYWKtasNwGBIJDN5iTg1qn+yBR80OmPObUZ9y4dxVUmn7Ft0ZzRpHfY+lMctXudLeSBAQWpUcxW8kCzRrAek1JlSx+wxLKniyN2Ze3i3ZtS54+lsrDd5tj2MjzSTM9hQ507Cck3rvkdmt5tRras6ydUDoNaqkLusSN4FbeVouOfOdrvR9ecCWcmu6p15u12SriULekhZWG60xKFa0HySiQTuovt6eJ/ZIgwOzz2nrKQdCpXNEtUDRtjOWp+cY7udQ/Sj7Moak7RmLe+awpVii3HMRFfIJxkXhZEEpPMWPK6ckpqIA8l1Snl4hH3rCseJLPWS1VMG16Io3yVehSGgN2ej16oJLoW0KtQOtM4qZayqx29P0iBszcGOsKyTa53W1DZ6BkdLCBstcA08c9ed9y3GfXh6FxLc1bDlNbUp8dDDuAprCak0H9hiZson9DeC7ylN6X30U95tY5aKTEmUPNGnNS2oESADouygEkCtz5itSckpbqyvvYzLvvx3c/rDH+MjP3Mb5x46rXkm/ljc43y04Y63vJ1TV13KH/+ar+T29/x1Hr7/MTqqF64P38vhh97OyU/59PPu/OCmV3H0kXdQH/oYeIPW+Xf/6t/x+V/war7gS18HzGDq8zygGHkBygqPksAaggzune02IgpYVHVkuGzBeHERiFtr4vVleY1qy6o5aeExp8C51AoXzXXoUNamD08l8Mng7uasxJsjo9S8Ls8yxeFNGtQVPdeUM1Na4VVrLJX4DJeRHWFFznGyJQsdUh2AKUfJZM+SDOyjXDBKvAmNp1hrg0ViOYxUSupb7srwYyZieym4rVhnYaPdOz1uIadpEbcVrOkBexnUaCEx8O29L4aDYeGph7xcjrr41tvyujbgkjaqqpwFWw2anLokOmaF1WrH0xxcUVGKdOB5laybhyM22ADt2ZC4SR6S7hH+Nq9L0Xn34KlpLyjDnFSeJYNo4YIr69y700w9UroPxGSiTIWEevM290jilDBsTilGSoXauiYmHYlz100hdc/xQARwL7gPTmtw7933h3DqCEufxogTM6XEiRMnlJzzFAmXtLD9lbSREoy5Qh33KOciCc+JuZEEV/AreyOTorQsqS1n7xjzXvgj3CeFh+w4xURJceCqF17L5X/kjdz/qx/lrp9/L9vDI+FrcQMWwP/Zh85g/Qxf/yVv4J/+wE/x4Jl5mYez73kzBzd+Kml18rzbv+TWL+ITP/VP2WzV4fD0w4/xj77tO3ntZ7yMa246wdTl2fQur7HWI82NZdVtj+0fyVbJvvmCUwG0PFqM6nrrdqYUqcYIyhgFp8Llkg3VbhmLoW6j5913BwVKRnhrVEJEoTZ1MJQrFQlJwT4WRt1jw89tt8a385ZVLpTVSsUI7qLtdCIUDFFki85KZpCnqMRRfXVv0bkxMEmtraCimUXgsKwoGf4krA+iG2UIVpsJXhQHeWKyLKqba121rv4xpkzXkgBDlyij24Znm1WWGRVKvXfWK/WG79FSVoaLiGxyzG3Ci5gLBsyRLR+MgtqOAIX+4/lM05BMi4NjjwtsgOccVXFqnNZDYOTxgi3746IwkkZiSpM6oYUeYC4qfK+1KiQumSn6hwxJLeVSJAyg9ZzCfVe7SwxVrDTxyLazBCtKTJRCcXHCmjnztpNKU+jlHfcTiLZRaW4LDWicyskHV65z5sw57r/ngbiunYUchvRJR7w0pcTJ9QkZsDQKqPTV9l7XutPTdgGmx+InsnoN9RRXX2t5Rr06m81GsEEe7ADNcwsyc5rkMc1x2m99G5BECVAfrrn1Rq595U3c+8u/xt2/fHuEauc/R1LnxTdcxx/+3Nfw9g98lLd+5G4OtzO+PeSxX/0pLvv0Lz3vPeXSazn50s9i+6F3q61AS/zqbXfw3d/1Q3zTn/lqGhsaztxmUsQHKTyMHMZD2VLHmktCzFElTjTFGvM7cLHViUmGpWrjKqSwaLoF3o/AuuYuSfQkd9ChO1zTEkkXhcVKCqi8MQWUoz7jzrYqBLakDoFK0jirFKILVfhldfCkENRN67WHAQJ5/B4RlnQa9eCHUIcBFSXmxr2WgAeaRz13UqZ+qV+2cDLQcTd6ybu7GuvNjZ4qs6XFSKah2Wm7KMhcMJZl9RwCo0i4nFwyE5G1jj7iFkmqXbf10bZlt19aeOrj33n0DK9V780jm13w0c1SlIBQNAo4yj3WhEj2o0NjSspx6IB48j16URhJ987ctuOgZ25bZZtMWeWUpM+XQ+NXAgE97KTqqgfPaejuWQgKtC4vJE85DIrCi5Up4zUyaR68uFarxDpLwXyFdWcm04EpFo/C2ySV6ZaxXnng3k/w8AOPkSh4GPun0whM3SCN1cnCpZedIvWyeGmDnJyjAVgL7E1Jic5QUcGj4D9l5iZpCqfjpgOmtbrUVy94ZnbMh2aidIVKzmqj4KqeVVFCGjMkvM/g+Z/zMp77qpv4yH/5NR58X/BCw1FLllgfrHjhi2/k6GjDi665mnfddR/vve9jbO68jflFr2W66kY995iCUy//PB6+63ZoW/DEfAjf/90/whd84edx86ufr97bbszVWeWDaFZvMug4q5JRt0JXeFblSQsTVf09PTFL4QDbZinZ9K5EgKvFREpiBXjMY4oKkI6qPEaIOAoPDBZprpodb1vRrqKVQZ2jygXoPtO6PPAeme6cynJQZ5tilhvJJmhdvZxKhJ+BBA9CuHifld420MTtxOOQiFIdsy7CfNT/a8In3aNHqWCrJKsypj7SiIHvow2Z+oynBCYlJUuKOLKB90ZelT2ifFXSO+0SPa1J4DmnTBqiGMHFHItgbBWPPVl7pXcxGtxdDk6I8wq4VUKsZHUnUP/1YJwg/VGPqpqRK0qpLP2M5CGjg3CE908yLg4jibTmhjhDd5eorhGVBDNmlVSVwOjD24oJAad48NRaHYclzZXad1dFTx8crK5sOjkEKXpgJDmoBC6v1OMhlagwUUgX9B8y5hIGpWfe8673c/bMOYYeI+zUcS40xma+6fnP5aqrLmXKEz1OVL0gwP49rptTd6BzSoE57fBPM2V5tS9EpHcfPaWFYtr2UD6YRyYxO9UbJUn6rUfIlEJNJTGSIgCZE5dfzkt//+t53ms/lQ//zG2cufvjyvEHVnvNdVfxnIfOcM9d9/HaF9zAp95wLb/ykbv4yDt+jCt/7x9jP4lDnjj5qjdy+LYfCfyx87E7H+RPfeNf4Y//ya/kTf/t51OmibQ6xZSkkC13WBqb9NCDzIoIOgmP3gKWC1M35iavu3enzRssVZJl2tFhwC0DzHeIxF+Jxvd4X7C7QaxWhaPEK8CprTFFogJTUUSOFh69qUJFkKDKZWVEBs66qzQbFTerNMpjI8ETGPBIPs3bmdV6Ipk8zYV/2EURE4k7fmcW4vc7kRdcmq05iOy9KpmVSoZo15AH5Qd5ofN8CClRa+htZu0RtdJN5GwLrj4UkRaxP3fm7WYgo5iV8PoUki/Ppqm2vEcSqjfBBTp4Wsz3kHOD1TTgiHAosgR5u1cZzmTkKQeEARZQijLxne4brZ2L3Uh2h22rymRbovvQNtQGJWnjzG2WV1hCFj9KqFQ10BcDl3Im9RzyZ4GfpBKcSE3+pos2MtSOBeB2sltkRUcrB4ua6AC0iROLwAmp9Aa/+PO3sdlukDnZq4N9GsMM1qsi9eYoZ1wAZ0+IZFx0QpNoJCyLFtFGPbuZyrSS0X0bIdhQeF7HYTPq0Jt0DJf+PoIoDAk0JIfNtmNkqUAjXDAzvBRTSOOJK2444LV/+L/hwfffyR3/6TaOHonGmgme/8Ln8YlPfIJzj205Ma343Td/Cq88e5Z33/UrPHrT+VKk6+e9jO1H38n8sQ8Ia/YVd37wNP/vt/4zPvqBj/G13/SHOHV5p9UtzYV3NZcASI52EnPgitRO3W4hDsm6beTVJDtDJ0/RhwbRS0g6QGZvy8HQtsFoGH2BchwQDkPYNhe1S1ASo5AdHTAQ1JYev5MXNviHMFCl6PCXBkVHBniObpgjnO1tJwMHLFnb1mRgvevZKIyVcDV7Qew+b1KMzPhdlzGaa6XNVWvdg/GRPEJZzelct7S2ZUqZ7pnuRh1ljVHK2V0JwGRGr52Rb9LBnsJADaNZAsfcCBqK5FjgV9r/eMBhwcl0W8Lq7qHD2ZUPEIVq7IeRsAkdgBBetpQCgnL60LdUduyC+/OiMJJmptNvCSW1GpX121UymGVKNMDr3qJpl6SPKCLBAiEaIIDdIHhUwhDNTOTZ3snJKSbcz+sAyYMyAHiroUEpY2GuxZDGAvaE2ZajxyrvfNv7gjD85ADwEw01t4ezj51lu9mQygnJVEV45dbozKgSIWkDRLJGee3gwRGetTlug0sWhFmUjCAFwbarLn04ZCTwmsMrFbVKmy2JaxqegbZWcEwJBzxI79e+7Eae85IXcPfb3s8dP38bbTuriuj65/DhD96lu3Hn6lMn+fz0CT7ePsq78/U8ymqZi5Ov/gKOHrwDa6LvbA7PQU/8w7/1L7nv/gf4lj//v3LNdSdVhpqj0VWTLqERVJGYi1WZ6LWGaLIaU3l3pjJhZQJMLWdzolaVcco715rJWQ3GSInuVeFZF092STJUj42vevnW9DR60NKGqo/Wc2edp52R0MrfHYhdz3IX9oUilUXL5FwWdamBN4reUsBbCMu6DNXCEVVnxuHZDS+1hwebU4qSRY+SRKmot4AAvKuIowLMaqKWzWjWyNMEKbGdK5acViu1DUMvPDBDeK4wd6cUtV2R576JqLCF8YocgwlSaiFyMsJlMTxS3E8KfmOl1qZ7skay0TCiM5l6JUX6m9oa1iVFF2Kg5ARt9kV16cnGUxpJMzsAfhZYx+v/tbv/BTP758DnA6fjpf+Tu99m+mt/B/hi4Fz8/O0X/iuhcIKjSgMXcE4kQcyEjY1ww4JgXVVVoQyhcJscp7GbTguGUo8DEdLkktT4CxFoS06wmpZFWRXQQoo67i7i+RTBgrovDi1L55677+euO+99otl7qulV+JWNOz96L29581v5oi/5Qjz6gAhMHhp7A1fyCFOaMKDY4KMtqMjPRaVwJk+5e5dsftPpWUpRsqo1qo8+yiscGZ2SJlKaYjF1hUaovp4k0dXWddjsMCjDbcOVr72BW195LXf+4vt48B0f4vobruO+e+7n8OzR7qZ757qzd/H7rpi5wy/nfX4VWzL51FWceunv4tz7/nOIERxx7pz8wx/47h/n4Qce4Vv+4tfznJddh5meoZWhESoVnOZQM+L7TXkhc9PQodKHiIU4qKlHq2ArVJM3U1slYRzNmzB6DnNQ1Jo8V1GGWqh7R2uASLyoQEfVJebaqAkL5SF9v7T4tV0oLR6hKs5alzxf79r4KUoWF7yvC0NWu9Q4MCMRM8oSU3iDKshQOO7BLfTe6WYknBSGq7uC1hyqOUYK9oNRymosP5IpeprnWc1EIgE1lNNTIOmDT4tByYGjE7XmXRJlrUlweZom3euocPN9IvuIlDqg1tC5JEqx80omR/M6d9i2mTIVpqzCiZ7CeUkW8miCBSS313/DFKAN8EZ3f8zMJuC/mNmPx+/+tLv/68e9/ouAm+Prs4B/FP+9wHDmdqQH64lVKloU3vBubKvCyF1SRmVMME6ptngAcwuwOqtXibhveviti+OVQo/SzaCoAiGlREnq+9y3Kv6X+lBhsgK9UHunFPUiGUbLHT74ax/m7KNqJPTUvMjzhyHP7vTDG/7e3/4eftfnfxaXXHZKun+uZkmWMnXu6iduSKo+Bd6y0uKde8PbqDBRWDW3xjT68vSmjoa1Unsjr3MsXhmM3g8VAuUUm1PCGro+xU3JlBCTBz0F1aTRSaSuMNOSsTo5cfPv/TRe8Jqb+dCb3839H3uQO95/53nVFf3wUdLJy3jJCl7Ao7zPr+JDXM6Jl34Ohx95J+3c6cDiFOrWoy0/+SP/he2285e/41u46soDVmklIV7bKUh1h1rBayMlUUC8RHIG5JG3SM4FtJBcpXUlr2Xs2JKSMc+NnAtTKkrcJOdgfSB+XRcDo8Vc9dbUlCx1krfAmkNB3qMWOhIZDlGZEvX4cZ85j8yuMeTKCAaFB01tyoPiBXXehuEIvDEiB9jRaHoX0bpFI7LRfiLlXWOxXLIcBYtjPQzzIO/Tt4rr4n3WVXxQSmKI5WIEsyDUpgLeWUjtkSySXzu8t8xUVqgYbIf1t+A7r7JI9gGQ09mVN6rsGEbb6Gklj75JJUONv8wisgrusYPXEC82lTS3OuCx34An6fL/A2hiiq8LBfFfBnx3vO8XzewKM7ve3e978j8iI5XjwYnmEBftxlRWDAmSQYnQAkwYJTwlTXoWVZ8Ux96gCoz+uqBTc/DTWpDQcWd2URZa61iHaZVZT3qItR4JlCfRe6JVw6gcFLj99g/smn99cjaSjuTManVOP/wIfbPBuyoY1Fxei6NMReTd2pnKqPJRGN086l2jVIyYP1Flukj2yfEs/LF742gzamsjiHbo2UmlSIXJwPuMWYvnIlWaEULagqeFNJYZrRcw+eHJnMuuvoxX/fefx6U3vYA7/u+/C4dDTi2SHacfYHXNC5is82n2EC/xM7wrX8186xdz+ue/D6cGJhchXWv83Jtv4xd+7J182Vd8niAXL5LZc8EsPiS0ksLflBUuq7BA6yYn2Ha1hjBTg7neVHedXbxc7848b9lut6xWa0Yidu5Spk/mzHUrXYHAxOdZkmFiCXhQVYQNa18qyqlNvD+cJWlkZsxbJRFTyjJO3pd67sVTNSKaEe5ZcmKuVVKCKUETpc0COkl7nMMe619/OvrDZKdHUipHUiVZElE9GSWLnieOJ0AieVaCxtTSt9FIJVJx3YOH3JfPyCkx13kv5G/k4qoUcikWpdCGBCIpG8rvcU3e5bDsMM2d8zN0HJZ6/WVvqWKnjrt2qUINkROxVER+V7/uJx5PXrC4N8wsm9ltwAPAT7r7L8Wv/qqZvcvMvs3M1vGz5wF37b397vjZhT6fUlRKpJaQKq0rJbNaZ6aVkXLD0kwunVJgmjKlKKO2Wk2UHIrPpTDlLJrInlq1JbUjKVNifTCxXhVWU9bXqkjxxxpzanCQYLIgrFaB/VnkUwuC7WpVWE0TVOPDd3z0qbDfJx3q4aza5PvuPMMP/qv/wNFcOeozWzoVlXDNXe0ktt7ZtM6mNbats+mNw3krjpkp+YAZtc5stlt6b9RWObc5x9n5HId9S7NQf+liERTLpHKAU6gV8FARSgW3zHaWAMl2PmSeK/M8c7g9ZNO2bNtW5Y5NXm/vqtWtHebm9NS58TUv5NO++ov4pbvu5uxmy3KS1C3t7CMxE85Jm3l9+hhfeMNJbnjxa9AZHga+yyhszlX+5T/4Ue7+0FlWq0uCEA7rqbBeFaZslNxZrTKpWIRsM63p+rvPWCk0M2aQEvfREfNmG5zHgRYKg5smrctptWI1TVjtlKZUmvh3nbbdiMwfIeT26IijoyMphIcQhxIsCucTqigTtpbCq4LVtGI1ZWldRlsKGeLQPC3yenpXpY3KMTt5krhLrUOVX1/NI/xGTI+FeBFVOrmEcQkMsOL0ZMxIv3WUCDYvNJ+Ye2bbFGnUJtrddrNh3mxpVd0se1cbW7dKWUkUWck1lf6ad5LpwLOUtIeLYCVh6Eq+bFvlqFXO1Q1HbcsmFOhHlYwagM0SE3Z1wexRC7+IvYwWGtvgSge5XRSmEji9VO7TBdzFp2Uk3b25+63AjcDrzOwW4FuBlwOfCVwF/Jmn81ljmNnXm9lbzeytDz90GjzA33nL3LbUtlVPkLaJRvYbnC3dj6jtECekzKwCVRy7voU24xEuzE0Tvpm3zNGcvtUtvc2cO3uGw/g6++gjbM89Rj3zGP3cWeZzj+LzRn28t1u8V1JSyWStW2WPbQNWObc54r77Po5FnfcnO3wpcezMm8yP/sjP8ejZmRmoDnOvKpEr8i48m5pZpUTPRjNIZfDUwPsod4Ph2uZcVCVUVkzrNTkrEZWC69nnxqYeUvshrZ3D6yHzfI7at9QWvWOQMc0RvuecoRTyNGElCxE2l8gEE+4rGpltn2llyxu/5LN50We8kn9723t5x533CBYB2mMP4UtfIBmoq+2IL331zbzhlS+Xoe4FpZcNUuXX3vVR/snf/X7OHjbVd6dQsOkzdKe3NdttwjhBspPQ1qR+QOYkB9OVHOQTnMonuHw6xRXTKa6YTnLptOYgT6xyUcImssUl2qq2LhxvZYlVeF3Jo51GVnnsQV6RHbWJDe+6B66YTLqSbTtjXR64LXJiOeTFomdOjvYPq4kctDSVOjYsN2Xnc2daa0203oSPmilKt8ER9aC1KXpwQi4vKeWH/kkKxSuVHPqSIZ7nmc3REWc3pzm3PcNRfZSj+Qxnjx5lUzdU3yVdequ0PktpPHBgD7goJWXwve8MoUcRQJ03eNsIBhkrNhnZjTQ7/XCLb6qKQzyiQwi6oChwKdbkiBpzniKSSqzyiikVVqlQhjDGgD064IVWDe+/gXD7/A3tj5jZW4A3ufvfiB9vzOyfAX8qvr8HuGnvbTfGzx7/Wd8OfDvAK179Yl/lzJCebwFuG1KUXlhQFirj3qW0QtiCyIzVrULw3h0vWb1ryOBVJWLR/rU3NQHqdCzk6/M0UcisgqSa0sTEmlVS574tlWqOW6cSmzElDqvz4IMPLxn5T9ZQeq+MfOfWDrn/wTPcd/9DfMrVz4mTzkUfQbiNJaMGJCBCfYeeSXlN7zqBKzMpOVhjU52UV9SemHpaat1bVzWH5UwqmZMeLICSd1nilHD0uYkh0EokiIrA+RAWTslCoDiaL3lmNlGps3cuu/wkX/eNX8Uv/Ozbee+9D/H++x/k1puex0ufcw3tzIOUK68fM6KvnMnXPB97/z1M6zXYTNuexmmcmx/hP/zAW/isz/40vvQrX0cqmUIhTYli4LWqoLPDgU14K+Lo2ZrERNtupVOZpEM6ZSnVDw+39VDO9p1AgqLfTo7MgLioQwW/LqWy3g2fBlWniULkvoSDeVWWJZKC2zuw9u4iXctrDm3MEso23UNlO1qUJIW/qUs/oFsL+ozCYcEhPSqvLChjXRhnSurLnlWJkotwv5TUYK8n/R15rg69IdZGhLiO4C8rJA+lrqKa9tbS8nOPIggHlTt6ouTCKqkfD5FcKqiqbKaLTVAbvc7Bc0yQQjO06148DtiSk5Kb3YOBQCTlgn0RbX6VbBLk0HxoYIZkWqgbmT+5v/h0stvXAnMYyBPA7wP+2sAZI5v93wHvjrf8MPBNZvZ9KGFz+oJ4ZGyM1irJCjmtSVl5salEb5su2k2xRHZoY+KHXUrGtg1uWRiBbZecWo965nDXe7bAP6K6OQqUKw3PzjzPamNpTm1HrKYiwzz6+sbJbrGpDg/PcvbsWS4M015wfpeMfa2NB+6+n7t/7V5edvNNtNIZslDjpE8kZlcIZwN8Ni3k2mpguyHDH6rTtW8oqwImSkarwsByyaKDtEZPIhy3zRxYlkowPShNniIjO3ClONHN9WxSF09N9cU61kTql7sy43zqZ7yYN37p5/CD3/UT1Ob83B0f4X33PcDrXnQjzz95OWm9q+tuwHun62ntUbofslRFecJs5uzph/kX/+SHeNObPo98+aGKF93xvlHIV1IkTBJzM5pvmSzETHy0gZWxSWVHr+k95ELSTvqL6ELpTQJwEreYAzsTBocpE6s+1FKJJ+/oZHpWqohRZn0Yu10iQlStpNrvbjJAwUIAyHlFYiihG8q1tAhDJU4h+KHICegz+ByYPQuPeJ5rVKPJOKj6SGtqmoqit9aYpolpmshNnvWS6JlycC6CehatfeUBSxTX4wAW7q0IpzX1+/bayF1k8pQTc5O3maIAY706IK3Fpoj0qHQim/bCwHY9q7tpDj2CpLqJYGU08QG8U6YcJYkFD2UoJYtS7LudIM0TjafjSV4PfJeNrvXwr9z9R83szWFADbgN+OPx+h9D9J8PIgrQ1z7VH3CXgVCmKqTzTZnfFBUlLXhkKrXc1bMq221LedF2uxXFJSgJROtYZYMjkxu/8w6bzZGydimxWhdW0WTYXbqDQ0otN/UAV5IskkfZODo85Oy5s8v1/NcO2R/j6Fxjc+YkJ9O1nOV+iUxEWZq8CIAQCXXVwJbwsokqjR50hu1mpvVZvanDYx3Ats+jRt0ltuA1FrqSVdhQoQmqmUnQGBf2WWuVyk5kNAbHErPIROqkL6GQY1ZYnUh81dd+KT/zE7/Ag/c8AsDD5w75ifd8gOc/fMTnvP4NXJYq9M4777qP+976n5dstI9JshY0rcSVV1zHNZc+nzP+YbxU8EwuMNdMbcKpCqPk0OlWmfsh7kp2Fcv0OjNXUX0sWSg9RSQTNfQM3m6KMDZ+5+byItGzE083WBhhQIY4hpIzYSSIvjbjuRGVN2zjUDJUZSU2bOuDCF6X60smfucUAtUpqbOkN5hbDWfcMFvLw3ewJC4sRXuiRNMiJeBEFTKMKa/JpvYg3ozB/VVSRwkpMRmC3xjORq3qt0QIr+iRSVvBW6eUNRkZSzgiF5HEy5TwljiYDiSNFnoNZbWi9c7cGquDKQj4kR31TkpQ40CeQupssAkKSs60JkEOS5GFjwNKrRw8KEcXzrc+nez2u4DXPMHP3/gkr3fgG5/qc/eH3N8pVI+d0Vh8ETgNbKfGAk02MB1bNq61EQ4okG7M0TNDpYMD72hBWxl8ypxLZLqdXmdSWmnB185R3uKWWOWJKWXqMK4J8Sh7Y563zPN8oRnkQo9geJHjIFsdXMHpMwXzU9CV/Za8265/h7K0hFjABKFEM0e/D0vamJZSVDHp9F6tVHmTcyFHZc8o49pGJrmEDJ2EDEJ5BvA601s0fjdRpdah5qzqni7ivbHIfJENM7Vyzah66NZXv5Qv+4NfwD/7Bz9I29oyBx+9504e+rW7uPmay3nBuY/w9rf9qqgfJLBRRRzKMFZYnbgODi7lA+//GJdct6VcoWwq+ZBNl7RcdkiucM6mQqOzbVvMJkidzfZIyRqco82G1Xq9S9zE+khhCHQIReKgxxxGFYi1vmDB3jOE/idJRpRYpyPKEWYnL3eE272L5zDWeinrxcscEQG9M43wPbBGAovMZZSPCl9bngmN2pqy8CGcgTvr1RQcxRxiIWuKIeeCzGC5zbUuz9XD8EhkJCBiD6XzRBiuvHjGBC7pHrhx6qHX2aEEfIRHNFJodRvetEXnRyVjCuxJII65A+tyEOQZepRJyoBawHUDZsjFQjUqarpNJaspFIMe3zl0f1wUFTcO9BLF692Zo8dLSVnd5ILrpCxWp6ekxY7pgVoi5eBQhSfaOYi+FqbifDp5UuP32j3oH8Z6vZa6Sa8i8SLs0lNnWl1CELuoZlI0SepEN4Dhhx86R90iaoElooMWeAGvT2EigxQ/FlTKlPUV/Pwv/hy33HrAK1/3QtKK6FTnQPS+6VtyKqwOVvKyo5jfSHjfUnrjxPoEPU/MfSVV9jxHjS305rhlJDfmEt2NREIfdKzSQ0ThQOFnr5RSI9zW6woZs0pUhQeEIeK6HKQcP1MDr2ITZV35o1/z5fz4v3kL93z0IWC3OB97z1u4zRSWMKgfDN5LCDK4hGy9neaXfuE/8a1/4ZA/+FVfwOs+5zpOXQJ1zuSphvckMWRPK5i3oskEZ7QPjzAUoTxJH3KA/x4ZUnnSISsWCRhZjAo1eLVZMNB6vaK69BxJMlBS8DbcE/O8JZsEfI3E7Lt7yykJe3TIWVxNbFoyur2rXau8P/CmcNYX0xFtFUbo7n0p5cs5iQYWiY8pqSa9RAfB7B3fVnoyNr3JKwsYK6dEnhNuHU/Otm/DeBNFDSj5V6PRWGB9iUTxREqFZru5zDmBrxhK4CNbbWbq0ROUHhWPwEIZc7EJsjqiUTGJNxE19hZtIFICJnGkEbZpmBqpuXQ65Z0rQpldbUr8N4JJ/naNzTxT0u7EMtMpoAdhlGklmakuysNc1TGwTCuJEFhIuAcvbNsaBwcHcvOzaBUpwbbOwigXrzMKAE1li9sWZYBDVSXlEPvsIQaqIt4hFNFnhz6pJMq3EXWbMEyTcb7QGKWUAPSZMw9/mP/44x9l9nv5m6/6i/R0Vsmm3qOscHjXMNRpalciRVJyE302zm4qq4MD9fbpieLTErZZTkqImUrRLCempJ/LFDs5qjZyFjbWy4rOClGfNVfNItNoUOeOJSUlDMc6oempw6OZsLluzgue/zxe+tKXcO9HP/EEM2Ln/3PMT/xnaDLWuuXw3KPc8aG7+A8//nauvf6NvOTlJ1itJ+gnWE0FSz2YDsrCaikValZ42ZsoOikVTp1UzyBFcs6SCCWQlO4SzLBdaas0REweWnNalVdqFLKn8C6jPtiMKa90cEb54xR9pJe+TIFzSyQjSUHbTG0MikRpaxX3cJoKLiyGkfiTgYz3xz7w8E7TkLULzy+5stq1q4LIJhmJdS4BrQTEQGezHkIbg4vpUfsf/ei7wlyJOKs6vHdRjlrzJfpQAjCRcg4Kj+8wW7fQblWU6JGMlW64prCiZmlKWcIUHN3uouq1KBxR5qvGg9vVxOdYW45I6J4zVi1w/9+k7PZv1Rh4U62VaZo0ib1Fkx9b8J08aCAkclkxFRFfp1LipO8LV2pozxmEqlDFg4OVsyY6Two16FI5yTmT1kWeFdAjqZKzJMXyoraicLf4ipTX5HXGDpVV9J4D09pgPLWR3J8Ex+ntHPN24vYPfJzbP/gQr3jt1cIdi15kCVrfBM8sk9eFqSuMUy8eo+dJGK8Lz2qIilERFlPcmcwWIH+z3ZCyQ1WSpYWX5cmwWrHaBapHWJ+IpE8kDkousRH21GyS0XqKGnMB/dVVL1/ZKPGBfVLproG96flPYJn53FmuuOxq3nXbB3jxyz9dHlzdygNKkTDoOVo6QGfCXKK7ZJWr1XmrqGXg2FkQgkd1TGvqxb60kHWJ6maIyAa8pPC8ZSCUvyoLlOKo33fvamXQmmMuY0NwFavXwNBSVDkp+Vay74jgEcLWKqM4lL8HBWywP0ZGnag8s6hI623GUl60GnMYkTSpj/dooVxKVjIotCmX13qip2iZ0Vp42jmoX7omwZTicErFJ5JGXSUQfZ4XKbUhmGs+DnCizh5GFYzFVzGjkaBVssG2i1UwBEAUgg9qj0SB0/C8o6Rxge7iULLwOi1d5EYyES77OBMdbYaB17RGn1W0bqmwjXrPkm3hRG77HN3yIoFQlWQpudBs1Is2VpPK6YjQairRgL41WjPqBMml5j13hXirVXSgsyEkkWODZ156y038ne/8czxw1yO88+138JM/8S5Of/x+2mazs48XiLctZsAJCol3yvoKzm3X/Ox/ehef+sovxk7Ompve8bnjXSosHZhbwzdH4FKRxBN5OlBNa90GdmRMJbFerbVgU4oMqMKOwgpslocDuKcwalH65V0k6tVEmiZGo7XSu7KpKFHBrLO+5UwqE8VycNNStEHtOBVPlYOD1YJbPd0hLGpDb5BK4wUvuIE/8JX/A7/4S7/Ac258MevVp5OTsTqRyKlKVJboEjlvGWVrOZnKM+c5Nl9Wm4fuS1bXUTTivVOmSd6RDzQlyZ+J+umGy1Me+cSAh/DocmSAJwqZHllzj8N5EcDoPTLpmZQn1cZ7ZTMw95SjfTLa1IxWCrZU94MuciSczNVffDgJ2mNjL/Rdp0035mZ0T1GHnaS0lTqpOyWvUbwrPuboQaX2EdCTYBXJDhItaXVo5JUtIiyeRk9w9WAayUjGcTkqp8bXMJ6MmiVnjta1U4aUVlFGGs8k+c6wBv901J8rnHa8uojyUaaaImS46FvKAuSk/hgqbwvdPkPleNmj0bxeW2x0Kxws+4p5j1NJRtd7CyEHhaQeG3q1Ung+KmdIahvRPVGmsmCQCdWpppwXlfTehBPhAv57m8mXGZ/+pltYH13FNdfeyX/46bfTeUzYpRUlO/bcJYvmUKNlLEnK3/qjnZImptWaS668nHvuuYc7P3QXN77iClXFEIsnTxJzaGpgZQciPJupeD+VvOBoLQi8yaKrcxWXdDPET8O7mXJQTRy8GxPGOiWcStvOWnBNNfNpWZaJbJNCMwOKNm+tsRHzluwFq+yEF+wIS42TJw+wNGqJY26SkXqmWcZtQ+pGKQf0VKMZWxYd5eAEV7zkhcynDvg3//b7ueVV1/HlX/EGcUNBYa8laX36JFpOnnAaJfUgIWdG/xpLIuMXm5SMSIW5VawrsrGQYhtVP4bwy9YaKdbXzvuLZJiS0yFvJx5fD2Umlcw2kkuNyCJMXSMqi5pnGd1z9HjpYCH6GyISgltmCZSkEjqXFrFkWhIRFtSuYpNUsNB1OcTfKqqWaS10EKTes91KQq1ME7WqPLPkTMkh0YZJbSoodOqDloJrmZQ87GoeVlCTMcEZ0bStV3Ka6F06kR0PUV551Sl40753BHQXtjpZImO0eK0jqlTOTqszKdsSSS7N8ggh5L6FRiTdwHIY6Qsc1heFkTQzUpkWsjJ9ZvReqbVR4/ROqDWnIYkzKwJis094T5CFd9TNlmR9WdzSn1SoXpu4Vx0WEL4LUcZjcRpAc9ZlCoJw0Ia8MFmFtmHqJ5lT53K/lJP9Bn7t7kP++t/5Hk4/fI42W+jMHD3B3e7ksNyB1IS/GKwvu5SrbrieWz79dXzgI7eT08e57sqJg8CJhjGVCIBI95YSNk3qpQ1YzsyuUq0EkcV3aNIWMsvMvTGjbGfdqAXutmmV5FykSF4Sc4TdfRU73hz3KhDB1Yagp6BnBTSiPtRaVrllzKvUpcmk3PXf6TJedPPLMPvPoYTesXwS1tdy2U2fQj76OA/e827cMo0tqUNJK/KJK2DjZK/MH7+byy65kle//sX8sW/6Q1x6VQkKDRJgSNDmOTLRWZsRcQItJebWqZEZtZx16EVYZj1aewTrYVvn8AyBMDSd6J/kovQky6SsZlpuFgTsiHAs5PXajlju3ul2hCMdgJQl6jJNOdaoDIVqsKXc07vqwFsYaNcRJkM1wuagvQ0P1WdRebIlGaeAA7QSTdinrSmDB2uR8OqdPEkAutaNxCcCxxXJUAdAGX2EgGCThiRh2qv8iZqIQcGr+oy29JXpkW9Q5Cg9TZUeqzAhnJ/cKSEC0npd8HAr6j0Fo6VF8HOjq6OemZFsReHkQC1ELt+zQU82Lgoj6Q496ix1ku8WRa1VCjipkErhIDhiocyOB/BtDq1LPNRaZyqxEH3nYSmRmLGSSSXRqkQKcs6k7qTqTKuVwhsswpu+YC3FneSwSpdR+tX0syd4990z73j/Hdz9/o9y9+0foR0+gvkqsnIb9rO3Gn3x8kAVE4ZByRysT3L46CO8921v4bN/9618wzf+AZ5z4ym2zhJuYM7sqonuLg6ZzTvuWipZEl/eQwQ2haFUtQHZRBVKChpXJlkqhZNO3c5BCxnCrjW8V6cl0V2i5RppNckQEZioj4ZKCm9mKnMzTtklnEinOOqnONysYF5xycErODi4hsPDj2OXXcmJ576GurqB1eUzZ+/4sLAlC1mKZqQ0cfLyKzk6/XGuf84pvuprv4gv/PLP45IrL8GsRpljJmV1fexVVTFzbbTAEi0l2G7JIXIg7LTjcSgnLKqxWPiN2jwqSY0bXRCCJYuMmoktWHZCGqe9SmTBI+GYohRzjmRGlOpZdOjs0TFS2pIdUZ8EG/UIT0fEUEoReTxJ+KTWkUEe1TIl9pLcxtpViOCWF+go5yxa3WjN7KM3+e7+ukf1lVmoTElXQYK2kbjzqEGPa8xYyK8E2wGPMDwHNQnUl0YHr7xjEehzLkzlQEYwKf+g7pYTSogramm9ibPswnAtPOxW+pIVb0l/c8qTGvgRrfUCK7WUIu/A4rg80bgojGRHxipZhHNJ2aYcvXoznVH9kD2yqSaKg6HFnmyXBiipBBBMAN3OtJKAau1NCzjFSR1iqwO4bXMQprt4kGPqHFgb1PlF/NTPPsSv3PZz3HH3g3zw9vu494534Id3U8+ewdqW0T/Ee2JUYIyxOwQGHN3JU+LKa07y/Jdeyed+/mdxyy0v5nPf8DrKKeNs3WrDumgevXc23aMJVYukkxpS9d7olZAzC+TAnIqI4u6dTGEdBw1BucA65iFkihZd7XVpQB+UA6yFTmFRiJZCTUWVOxZ4k5JVrTVyzyS/iTvumvjgh+7ngx+7k5/7pbfy6EMPcvq+Bzm48rlsrVFOXUdbTcztYT7+3nfQH70TMSsb2TLlRCGXxvOunfnCP/YlvO5zX80rXvNSeq7QLELajDo/tMhE7zQIvVfcM4aUoFQR0mgWyuXNI+OsBIUOXlitDyRIi5FtJaM1PCMrYeSC5D8qkQZBPyXMVhRzbnvb28g58am33KLNXABM14GTukroLAUkAUt/dO2FXap9KZWMjFCrjTlarU7TEOjSV0ohzdyCwZEUrpaUolO8L2o9uacQuUXG0iU3OOUSHhnCS017ciRFUrAgmhlZ2RbVqaswcIdfW44iDwscWhl0VWjJc3QXPc2igGRU1thIqpLj8HBqb7qvPdoQZuog4BVHyvKq9KlyeICZhsU1+6wWFguv80nGRWEkDchWVFmSDFzYoCpMOp61aCTLZEwu/KUwWqtGgSYRWisFLJgsJboRdBUj9c56tV4aOLXa2M5bhYiWmcPzbO7S67NMicx2L5fzwz9yD3/l//l+zj12CPkT4EfUo3vxMx8TbpoT3TdYdL0T0VXqQddcdylv+pLP4bnPuZ63vvXt3Hvv/dzympfxaZ/5qbz8VS/m5pc/n9V6JaNnsJ2N2jotwh1M3t5kzpRcmGApeGgfEtn9bAprWg1R3aIGSdkdomJjtKDVQhxahD28jln0DRxKZgpvMy/EatnNQcLHVJJY+xziF8Zm0znVb+Bf/et38/f+8b/n4YdP44cb5sOH8fkRsh+BN0iF7ZkHKec+gbUZ2z5Kb4VUJl78kmv4/C98Da+89eWsTxq3vvplXHH9tRz5TKeyjmb1HTWbwhUOJlstXlcxVBscKjUA3TIlRT+lOuOuUrUUXsVS9uaje18OkrTIyK2p0dvwPHscYAM7EzwjjuF2u+Xee+7jVa96pZ5Tb6EILlzUTJUqAtyVAFNSJ+NWhXdGszjh3KEe3zpWCoVEKmIPDIORgqgt9fGgBHlStj34xC3KVmtFVBizKKzImDdSnyUhF5qsQ9xklPMOCbZeBU0UcxUgQNCtBo4YmXaUQfYgWAqflpfpvuutrmQNSm6NDogu2TpcVXcYOyM5qAOY5tAcqBHZSJTaHWpSfqHF9QxHqD+NSrmLxEiKQ+bJ8TSwQpbQTfQfVE9M4IlEggd5Ea3XeHAZklAwrxLtpWQZGeLzkzwKVQIk1tOKea76+3E6p2TorRlM1JX77nmQf/Ed38+Ze98K8xG4pPx7PYJ+CPQovnft/8DSpxMrXvjSy/nLf+PreO1nfha5rNkcfhmPPnaWyy45gU2w9VkhRhCVq4/6WIHL2SYslWCKSWF89pC4byaCPIjfFzhk9yCZ59H5jqh4sMhAhgo1BE4rj6NFCCk6hRZhJrFo9SieiS6MiXkrzG7js0r9+hbLp/je7/wB/r+//0Ocfug09EPaXPE6665MQgQko/tR9EEXIb2sjc/9vbfwf/6lP8F1n3I1ljrbdoQltZbNlmi9smkSOq5dKu1Dj3SalCjBRYlRfiw6TgfGlzFKMrwY1kWs9257pZ+NVmc9wAS1KhOupludlErgdAlLMA/I0jWH1jqEfNfn/Z43sF6twYtEI5wo4VTIuNQ4E9qcUQUykv8pPLlsUn7ypojKp0KxIiaIFVqHnJWocjruleK7bHiKLLQ7SwfN2kTOHmF2ilLF1I9wYA4RjtIJ41bxPFSnPDzeIZMXUFJ8jbWi9RIVR3RGp8+Bz4vl0PDWg1KkQ6r7WJnxOW2UvRq1C5rYxxJVeqjPG5Egvvc3Wo21HfMfn/tUQtkXhZHEwJNaynaXxJlgQJ0mI+a1NOTkZbzSCLDdSSZ5JCH2iUploEfee/TQ0RzXFg87TMcguqrxPFSf5dp3YzNvsdSZU+OjH72PD737ZyjnHmSuIW6AZPzd2tKV0NBJbki09IUvPsE3/+k/wPNuupI773t/UIgSpax48OGPS1l9mhi9iyU5JqFgJflF0zFb4ZEhVWgmBZqpFHJe4UDdbsIb2hNbRQIIioBc4rketa2xsOcIN4e+IO4LZ656GM6gyCzZwMiM11rFOwysy71z+twj/MRP/wyPPnw3ZT4XwqdGysp4tnGAtzDPZnSbsAQ3v/pq/uf//YuZrj7koYfujlYUznbekO1ABHzvrCcJMzRvS3mpA7WFgGxWOwcbXoNHHW9ObGujmg7CGuV5afT9cZVnknIEKEYqeWEkWLEQW4jeTCnYXgMvx8muRJufsDgQIq0RMJAl4eq99WjgJfwtRUWTMsHRbsQSvbXFSPbWSM1pSRU4yQrYJEMfxqp7w5jw1pbIodaZ1iM66bO6IVJx812RACmoPDPunbnLuys5hfKQhCZGkstHpmTPOC0CIeyM5Kid3h2wO6ze2f1OB4MiucaAmdgdeLaLetIekjX+hsVz0HOLNh1R/rj/2v1xoVAbLhIj6agRuvdORfiFTj7hdubBzE8ikyqqGAsuLd4CEBs7kjnA0mi9ikeYknC6lE2N6GOCUhJ5VoopSc2mWmfboXrj6Ogs9555gBtueQ5Hj13KtLqSnI8W1RFbJfJ6InlnNSVOHEwcrArXXl24+eWX4Sc/we13ZNbrS1itViLNBxSQ1ytyg3UpbOdGmYa+oLy7RKLhJOt4ClLuXGm9Som9bthuRc6W2r3hVaVuDtS+t1ijHUBrUXNrEX4jAJ42aETCLJemZxb8vmzLQreU4xDJ6vXdVfL5yGNnOP1Y46WfdSO+3tDObrB8is7ElCBZg9LJJbFKxsFkrKbMiVOnuOLKE7zslhvIJxP33H8/l504YJpWWMqUaR38QSiT1Op7hWl1wKqsxGPM0slcm0D5o6ZSTDPUniPEN1JyVmWFl1H+FirrST2wE4W5NvVfj4oMOTEeitkrMScCy561iHRoogTffng4B/E5E8UNANHOF+Qx2hJehvSZDZ1H4ayeOnNX+Z+BCiFqxW2DDTpMYICL2Ei8foSkHswIqQ+Ep9V0aPXwqIY47fgyy2r6hn7e+rz3t0b12V6SBl+cmcVrbCPJpZC4Dwqc75JStiRRQhtSEhSL8RwJVCIa+nXGzW3BQHcq5fojo0Wtrmk35/k88/nE46IwkrDDgUrUsA7FLWOA5fsNnBy3IiHSPrOpVV6bZeGMzXFrQS7PeJJHoUZEKlWLGh59XmB5ten06mbMdabVxtHcOAoe2XNuuIxv/gtfx1wbPnd63pDLRPctzZr6pnjCrJHNWU2FViHbipPrA5JvyVHX7EW1cQerE3QsOJ0q2M+jPLApu2t5CvqGxAAURsgjG+IF46DQAi8KZXOEz9F83ZD4Qcks4L8WpQi/WrBNr28ytOYNK8JU5S0oI9vcQ23GoySscdhmWu3MvXNY4Iu+9LN54xe+Xj1lDLYuYna2EtnVwjob66RGUQcnDshZPatL1HpT5CXmvCJNqsEOd4FVnrCuKpw8hHJLiZ7XMkYnQxDCkAeXzTQ/HXJZhVct+MXC6CkZmKKMU17gINJ1H1VcETaG5wwZOsFrZKHbTDnTQvLrcLtlcm3ZORInhtalHHMZGjXB62J8uDw8i+TIbBJyyW40Mt7F/8RmKQQxOilGx8/wynqPsNsiOYXRqwQ4enO1/ogDWPvLlsMlxz3W7hDN44ZRQ8uC0cY1Vgm+GEnC8/Nlzvaj2yU0XsLeBDSic7nWZpxiS5+aiHpG1c74XPaMprsMv36iQwEfFXPDdO5BJBewlBeNkZxrQ11PjaG1173FhLDc2vLoI6xorYrSYoluVSLluBItvdG3eh+mWuMR4vQWgHwA4YN46uHOz0K0STlxMk9Awf0ATastPLUyFYZYarKkWtOSmWsll8xUEpazgPoaxj6SRjsPWHgURHiVBjgfjajyitqPFNrUFp3eoJSoBHJ5RngV7NCz6E4pUcNYJFfSpXun9qa2AyjLLt3C0VUymAJRd9e9hpsjvUKPns5K6kRWsYobd4oU9c2XhNiAPGKL9gSj1UZyVaeUtKKUA3EXLUUGU2WPxUocmDl66GSaKb+plZ1JJkm3Ht4JgzKEkUKEpIdRMBuoq/qayKtK4a2YDpRIzHSc2aBlHaDFDAvyfR8E5aj08kYkd2wpZQznEQj+qek51N6x3nB6lCgOQ+lxNWuFqib80UPopQ6PDQnZytOUULAwtihJJVoUGIt3V8133qpVsYqiakYKPex4nT1gLgb9Sn5hZl4Morv4tpIoHPep8D522s7wxQgEY/EqhTvuYYlxrfh+2N1VKYctvW+WOY+9nZg0F6E0tH+Nuip9mYejtSR52LE/Ipl00We3e+8cbg9FjzHxE5U8yUyliCrljjMvVAsLb6I1SUFZgd6DD5nkMaXAdiwlmuvnwo48pMdEJu0Rjei5amnkqZCRt5EkS60w331RMDE1I8HsQDSKHL2fS8FR2V2OTHW2RF5BtyacLElA1UKhWXXJMkALDmZGSSu6G4kiD8MUjiiLXMjJ2cyCCGBSyJQGjSQ2gK1C4SaT8kqLyaI9knfoUvLWbaneW0iHMZgGuWRKdzyZ1Jow6FH/vdJ9HqQVoyWBWZK6eCrkPMnb85Xa1Ub4p2fojA6PRnh1CL/UZhJ5HdTrZ2BnI6IYWVQPlSCQLyKoouFIqsxjy+jpbuK98qZb4Mq0Rg5qWfPowun6sDT+Xh+whbx9hc6ORHfDm+pR624u6yBfUK1rlYuXcLS35d4Atv2ceq67jHeK1+w28A7TM+t0myNYHB6cOJry+OMd+95VFdwiT3JkeSFqseIzOnjVffnoYx1it8Objl1iPTxhZLTGOt43kBBGM/497N15nuje7xaEc3Q43KUkgBr14uFE1c7gQouhsAuvZfxH59DANt0FWwy83WPvPxuMJKjeU6CvR4OiAm5sAzMR+8Twrn65AEaORRhCn4wSr6hgSErIuPeYjaiWUHKVHBUiCu3TUoGTAv8cbTwNYZYpqAsKy9IuI5kTnifhRKF8M7wL1QXLcKaSdeK6rj11Y8oT4hWq17PlRK0tJN0ac92IL0pBWVWUETVjrLyS1XEuCi8A8faSC3pIJvXtZKOeldDFDL1JDPMc4h9GmTKZE5E1jYyzGznIvsLkMtlOhMc/uvpJfAIitFvCm/haLILoHzAScx6Zz0q3R2hDyi4pA55cpqWGN6DzMUK6ND4xaJ+j85VVmSKPl8QOFLwinK95jxI9UxVIr2RziV3EgYhFjTxxL/ofontHQyp3Rj1/WvyvKPsMPuzAHIeZth6eWsAdaokwXDPfCyP3N+9IYuTY+KEEH0cMERbrNeOg3PEqF2xvhMXLv3eq/rq/4dVpNB8KQsTzFjY4DA22F27vYZXnDYtAN66j2+O8ycU6+vK3zdgL0+O59XFoGJZGgnf/Xvb+bm/L9wtMF9c5kpK70oAnHxeFkRzu+bjJ6oiq0aIw3XTWpeFjhBeppyYcMJnMSMlJ5FoX6A2uGmuMtlV9arFE7qOIColvWlLyyOQhim8oHmCKhdNNvZZHu05acAW7OGy9NVZ5IpW80GgmRBWxKIFS3W6F7hyUiZSNuW5ZrQvunbnOg+UASKoMalQoyAvLpZDyODVhhXDI1o1U5PUmMpjI8lM5IOe1aplDENYQRylbISOMsPvQS3Ryl0foEZLbYuFkJcZChxBaIIxReNyjcZV8QnaeXID3PsR8gxwcFHS9J8j2EkeV8O/wv8az1z4ZIIW+7y5fEbaKMAw6idT2CNiANemH9h4cRHeIplWdJrEHAiYgBRE5jbtfxugy2LyTqeQAgwjv0sOT9MWwj3BwF5aOnHJYGpYgMeCC84YRZjhaPtCXg9JsWO/hjw0XbCSHwniEUR7PcG8XLte1U9sKP2zob/adscE96tjH89j7+figvRmzcYD7OABGwmfM5fJrHQXOsnakII4OzCHPhkeUsBxBu3sc97Lc8kgnsUSBo3hnKH2dZ1wfNy4KIyluoFL8okCUOMkHNimjIjcqLyFNBEykbFg0Tuq90nulDSpGSpQ8SVINxwcobHkBkHvX4jOTdFbHVQYVlQwLh8wGfy3jZMpKi0nNxwDLIrgvDYTF17Ok7oQHQenpPrL36vQ2lTWLxBhORAf0lbBRy856fZJkEzmtgb0Nj3G+Hp6oSfv0phHKCk8sYVxmlJONHjbjfUO8N0XVhJ+/mZrc4EX7UH1kKlhdZLdQsLl4XcDiGWHjdI8QOyqShHP12OOKDsyyvEtGoKw6EXzPOIbqje5B9erNRQsSaiEjLO8qssfmIT6heUtJEnN0Ebt1vxnvoZqjGOG8NZuC+iJWVezIrqy2Y9QUme3xVFz1ywSTYBg5j/vpkSQahsMZepUDVhhh4S78NldZq2zhcJlFyxneV/NhIjjPKDksFTa17+4tWaSThsi1d1qC3HSMeRMcIS6krisNUWxDz37Pex0ebuqRzEKvyyEGYuEKNmAogoje46SY126j1JDwQInElczXLpM+5ns4UBEFmhJfJGHt8iTZM7C/SUYyety8FbjH3b/EzF4EfB9wNfA24I+4+9bUf/u7gU8HHgK+wt0/csHPxihpvfxbCzcwRXeJINh+uJNCVHSXKTNzppwwJmpPdJ/CrkqcwUMiSid7pptF+0siLK9R6bLzmBx25Ve2qw8tJlK7B9et5EnhvcOERfWOjEVylJRISkAUC3l7COWWHCf9MHy701lJlhlPnWIrAdUYbdlGEDHkeUbTRUhZwius4syh9qyQWFu4sYSFTIzSrlhPLPYqxB1GBYoOhHgePjAfBaApqqASCF+La1zoG67KChGco9yOvWxoUymnpRoe4ghZO24ttBVsmSX1aZbhHlhZa7OUngga1Z7XtDMTmrreR8baFz1O7xbGRVxbW7yX3WiDyDzCeNO8eDzB3tl5y45K5MJIQhh9H5t/JDMUNY0oYoTKDqHN+Os38hDCpxnWWag5gxfaXC1etc6TvERM2KWPA3Q8gxEh2C5xEx5jj1LPHt/vh9Rue2uN8FRj/aWk+2umQ7cF3UeaBS5HCN/z6HQtCw+TYfBFqe3jybtBTuxDCYPuRzzp84ftIJS4Ni30FPf+m+NJfjPwPuCy+P6vAd/m7t9nZv8Y+DrgH8V/H3b3l5jZV8brvuJCH2xmrKbVeRdvGCRoNfRFxskUi8YQtWUxkp5DnMBINmHWdq58d3r06ZbBU7Y6pwMZ3xRSYtkU1oZHNOVCsmnPSOaoh9VkZ1aYlUg8hICYRV0vhiCzIcKrq057z8KQMMKQwYqrpRGhYGCrPULYIWvW0xyv7OGBgy2fF6HIEv7IXlvoBsqM9FjIkYBR95fwgFOErm0xbIOeNfC3uPj9J6j1RieZfMSOkK4lxGTAFjJlyq1Hb6E9+oaTGDXpIW+OB5/QrUVn08WSYB416zjNgprTpbw9+vVU+hJKMh4F7OCvJdSTQd6LDOOwG/zF3RuHuMpifJcafalqy+MdL/fl3wyvcMni7gWDHpnxkalvIXRMGElZ3oBjwtgGpkuEr57iSBpGLrqE2oJXuni04zH67kFahNjdw0ja+aG19874c/uUn3GPmkvtnX1vdExwGywSXK0/iMNjmYGdsbIxz96XBzZoTePih8cKgbE7y5wOBXvwhbEy4pfFkxx/+wIGEp6mkTSzG4HfD/xV4E+aruyNwFfFS74L+L+Rkfyy+DfAvwb+vpmZX+BKBpi+bKfYM6WUsPQyiiNz5g65rGk9U9JKLV5pS0+NpQQiJVJeha6jss/KtK5IUSlhtusnnCkkW6PscafQyGnCfahra/FW6vBjcJnUOI/TUtURHTbAS+QSQtnlPLfeFqxtN1Sgr0z2MByRXfS0fBbsNm3aw1dFoh+4FjIKzZZytFEuJqrOzmvzNrL3xCLfbZClAf1ygAGW6EsVRQIv0f/ZB6lGdbELFiihVSW9Rg3vLKGJFpn28J71u5jbBhJIVVifPFrdxmFJtGBo5gt7YVE1WnCLscl0LR1RZXp3ebRp512MtgXxBv0sdDQXozYMutzQcBVFXdK8KIoYJ4m7x+ERJCQfFUxBO1v60yT2w0QPHHAYmKFlueCr42tY+76TCos/zGibOsbgFC6dHRHNyn2Ey4GlJI/jdHj4YdSznb+EfQnmY63aAmMQdCVQdYy5QuiI8mWslphgeKLDO5WhHotyH4o47/A57zrigI/k6pgXjzd5JBlbE6vAopKJvet/ovF0Pcm/DXwLcGl8fzXwiLuPct67gefFv58H3BU3Uc3sdLz+4/sfaGZfD3w9wHOfd60UTHwsZFFmUiqYFeGUI/wNHK93A09M04GEAly1tUvIGGHf2OCjJHEJ50F4B0kuv3e6dbDt4kH0wLZAD03NmHTCmkVQ65BcXK4FaA8VEmGmEyOxwHLKhVqOg1tddPXMgpfo6lljHhJaNgSHo1UAs74P7pLHgdBcArAMcjJ7YYVpuff4RW4x1yFLN3fp+w2sdKLsysC6+nKPviO6x2Ecw8vrW8wr0XtKF7UYlnhLvLXuJVGGKzc8qkFqT6bFniLUHrhT66pEcRsEeHkpji0ULYWsLmJ3JDIWQ+Ei5A9j14KxMLyMMSSppasOJHR5X7MdfWb4QYkezeWEG5qrSdfOgQwGZrxnrAVt7hEutsUIjFvZXXMcXntGYByiuxCeMLw7z5IQgHHXZ6dx2PWRDBk4Mew8ZvbKDaFbaCGYtBBs5xxqrgbOao4n6D6TPMpnXbMJ0Q46mdqCjBU61rD3BRPfrYe4Jx+QQKdaOPdNUIOHlqmlJMO+MAIMRuQzvNsemLuV3drZPSGebDylkTSzLwEecPe3mdkbnur1T3e4+7cD3w7wyte+1C+55NLFiJmfYEorclphSZnX3XoYbnx4gOFK71xnvSYRSZyYsH3QevkcXNUDrYE3tUAlxaYRgXlBKEfENE6l8AISEjPVKTvK16RKoxa5FRWjDZJ8WsDxQWcYnkELGopwtA5pR25PXVlYb76kEgxRIHpttDxy/3uE5n38JUKwReZ0eFLB1J+XnkJDBt+DxB3nfAh+7Mt0edsjDUf4OeLU7m3ZcEusG7w1QApJHsUBNo4txwk1HzO8OXMX42CU2aXw0FTrHcbMgq/Y5ZlkJzZctAu1wNviWtMou3NkYFIEwBFS6oIk9NDNtEZ6W7ygttQCL5Y3srO7Q8SWEHI3N6Kv7XmCDIOu96TzVugupF6yr+ZRcz8Savpw2zPGPYzGeL9li2hnD0KIsr8Rtu+8Y4+53R1sgDQubUQnwTzYv05kdJd79YEd6o8Mr3jQpgju8nmUHHZ46uM99nGXdFeSKLDR4TjFMcPob7UYyccZQEPOwvBifTHiFx5Px5P8HOBLzeyLgQOESf4d4AozK+FN3gjcE6+/B7gJuNtEnLscJXCedCQrnJiuAFjCrZRUReJedQLEa3vUgFo0ntKCcCixEPUhCm3iewtZrH2X2gNEbhHSGC6vMfjVarM6RXI0HmK8cxjpFLqObYgnNIWBQzTYAy8b1Tjj4fVu4ud5kycWdsajIqOjU7mn8OBwcnP1dwbcVuFzaeGajxJGZTPNI5RZPJCxE3fbSXm4ulNDAcIdxl0ZzxqUIHPkwQ6jR8yJ+14mM7oD0oMWoz5BY8r3MbHxveS3IsQch3/2pRqi9/CMjaU5Wd47DEc2dQgZdA+C88D0uvDMFsUIOwPf9vBEp2fNwIJTOag0MDy6kSD0ER5ro+7mdVzHzrAxuIXD+LrKYfWiEdHsL0mBNOcf5fEJiyuuTT6gCF2+wtoR3p9HddpPhizXFdSmZS5gWYDjfkdCKv52H/hwYNndd1Uw3Yj+5rsPcO/hfadYZ0HjIeYBJDAzPF4P+laEwo8fFqG3uatCovfIhuu+lmuM6903+sNpUulnZ9EiH0mzPVjkycZTGkl3/1bgW+Ni3wD8KXf/ajP7AeDLUYb7a4B/F2/54fj+F+L3b74QHhl/hHm7XU6Xxjl6k1ssnI8lPBlYmaDKIdyZpDa9F9rVvj+JhvWO7dYCHZXnaXlFL97UotQqEL7oBreUYY3pjBK3FNJaLTwZc5VWtjY8AL2jNyJED1wwyt/MQi16/7oWUm6VN5PCcw4ptG5QfLsYhe7OKjZHC8O1A9d9WQKLR7c39iWppESzV9plcfpXCfh243ztPdmhxUMPMEQeMtq2i75hGj2LxvvjehFFyvc+U8nv8BJaHFyMZlLxr77bTB7SeCzhdxgj6+CZoSS14LcpiVJie4YmQsu0H30txm8gzruQ3XCVW7IzJvutgd0HbSyMeRizYTB2nk1scJ1Qe0T088dieM0i9NzTULWuENh8URAa17Bcy2KUx3WFNkAY730jOQznLjHkamm7+Gw7k9JsnKtao4n9iGvcu/5mV3XontfYl7B/3PI+eX6/cmewCNISmhAh++M94Z2RHJJ3vncvtnB0WZ7Lnr//pOM3wpP8M8D3mdlfAd4BfEf8/DuA7zGzDwKfAL7yqT7I3enzZgmbWpfQWUotapGV2OnjoWiWoQsPxGGed6eGJmXU50boORIZoIVlwSEL78csY040iIrApw/PZ4SRwyhooYmoBsMLERJgS7gbj42xxsbW6B4iAeZBmQnDRPhEsXCSeQhmREIhMMQeWe4meR962ikhJYeWRugYhm04E7B4aebiyY2Ga42QskdliRohnmrCV0fUvFtTO2ypRsDc3VA1jvoNDS8TM2qfwVXVsoRsHl7/3nON7ckoOVs2hkFX+8dFWMK9RiZ80Fbk+aUIvX0J/0eZoO8ytwyXNeATov1q3HsorJCoCs1M5aESFh/JibFBBf84ewdHJEAs7nOsgcWbXp7FSC6MzRweFuwSVDbOmOijnXTAEOT7ZBKDMCNEdXdQDmGILR6ee4tEelB6lgNozGm8b3y/GLhdwmihAtvgnsZ6cBYVcJHpZehktJwFVkk6BD2ggxT3nXxvfszO+3KIWuud4VxM3IiaYn1mM3Zb3kOtP9YJRBnvMKp7XsoTjE/KSLr7zwA/E//+EPC6J3jNEfAHP8nPZTNvZY5ceBY4JXto2IWHkCyoMDHpIeWF+0LcVTIi4V4XH2Bp7mUpFvFugyz0mbFIXYuo74UVy6f7OIvTebjLMHHDQPZoBF/pgZuJTDuEfh0PoxBhhLG3MOM0dhYKRu/9vEXv0elNKtxq/Zps9/7WatA8dte2/H8YnMKoWhmZ1Dj5Gd7uSDIsdZDL6zSMRUlb5ihy99pcGXTwxKZSnbT82mzacA2Lg4DFh7KYaI9kzfm4FLQa6uJjM+u7HQyAnn8KvNXVRBthzOq/sk/y9pg7D2Oe0uifpLJNd6XfBlSykNiX+RiH286YQaxRHzidYW4yHuP3e895RLe+VPX05X5HtODLTEcp7sLBjLXgvvAX98v9RgQ2vLJRcXLeWNgGvhzwbVDQUhp5FxZ/NDbM+Cu+zEOY2z2vbn+ex827Izm+OEiXzLkFhWnA3HuYrPrr2G4+l787Nsvwkjnv/8fzYXiRsT53v2+/js70+HFRVNw4ou2OulZfTmiFvyPjxcBCugyETuC0PBQIY+O+HNOJFN3b4rgLQJ7hRdjuGkZyYISR5xlJOQUSkIiQpIVk2ZCELyML3xq1d93TlEIdaJyqdc8YKRTdX9Q7T3UYNvU6qbUyutxR8nI4YKGhF8dm76IUjcST7Rm3RYwBYGQsfefb9GVxiyy9LLfd5ej3cYInz+A5TvQW4P6Y1uF1sDPGpgPGBjUoQscRvu5Cn/F397yG/fWyF5Ilhlfjy6bx8Ejkfcfm9x6UJeNxHwfslNq7HJDdfLpIWi5zi4oC5QWx9/dKfObwiBdcOQxcAsrYz+OQ3J28uswl5ND1Dk7fUB6SRzU867gXTwvm2Xunz/UJDeH+HJ5nEBZDpD21NEjbC7eXTfIkY3hjI501DPP43ROFxDq2FvdWuDE7o6Yj+Pxrffw6eLJrGfbAfBzauzX7FLfyhMOeCi787Rhm9ihw+zN9Hb/J4xoeR3v6HTB+p93T77T7geN7+o2MF7j7tY//4UXhSQK3u/tnPNMX8Zs5zOytx/d0cY/fafcDx/f0WzGeAKA4HsfjeByP4zHGsZE8HsfjeByPC4yLxUh++zN9Ab8F4/ieLv7xO+1+4PieftPHRZG4OR7H43gcj4t1XCye5PE4HsfjeFyU4xk3kmb2JjO73cw+aGZ/9pm+nqc7zOw7zewBM3v33s+uMrOfNLMPxH+vjJ+bmf3duMd3mdlrn7krf+JhZjeZ2VvM7L1m9h4z++b4+bP5ng7M7JfN7J1xT38xfv4iM/uluPbvN7NV/Hwd338wfv/CZ/QGnmSYWTazd5jZj8b3z/b7+YiZ/aqZ3WZmb42fXTTr7hk1kqZi1n8AfBHwCuAPm9krnslr+iTGPwfe9Lif/Vngp939ZuCn43vQ/d0cX1+PdDcvtlGB/8PdXwG8HvjGeBbP5nvaAG9091cDtwJvMrPXsxOMfgnwMBKKhj3BaODb4nUX4/hmJIA9xrP9fgB+j7vfukf1uXjW3eOliX47v4DPBn5i7/tvBb71mbymT/L6Xwi8e+/724Hr49/XI/4nwD8B/vATve5i/UKCJb/vd8o9ASeBtwOfhYjJJX6+rEHgJ4DPjn+XeJ0909f+uPu4ERmNNwI/impInrX3E9f2EeCax/3soll3z3S4vQj0xtgX7302jue4+33x748Bz4l/P6vuM8Ky1wC/xLP8niI0vQ14APhJ4A6epmA0cBoJRl9M428jAeyhyvC0BbC5OO8HVDH4H83sbSYxbriI1t3FUnHzO264u9siHf3sGWZ2CfCDwP/m7mceV/P7rLsnlzrzrWZ2BfBDwMuf2Sv6rx/2WySAfRGMz3X3e8zsOuAnzezX9n/5TK+7Z9qTHAK9Y+yL9z4bx/1mdj1A/PeB+Pmz4j7NbEIG8nvd/d/Ej5/V9zSGuz8CvAWFo1eYxErhiQWjsacpGP3bPIYA9keQjusb2RPAjtc8m+4HAHe/J/77ADrIXsdFtO6eaSP5K8DNkZ1bIe3JH36Gr+k3MobgMPx6IeI/Gpm51wOn90KJi2KYXMbvAN7n7n9r71fP5nu6NjxIzOwEwljfh4zll8fLHn9P416fnmD0b+Nw92919xvd/YVor7zZ3b+aZ+n9AJjZKTO7dPwb+ELg3VxM6+4iAG2/GHg/wor+3DN9PZ/Edf9L4D5gRrjI1yG856eBDwA/BVwVrzWUxb8D+FXgM57p63+C+/lchA29C7gtvr74WX5Pn4YEod+FNt7/FT//FOCXgQ8CPwCs4+cH8f0H4/ef8kzfwwXu7Q3Ajz7b7yeu/Z3x9Z5hAy6mdXdccXM8jsfxOB4XGM90uH08jsfxOB4X9Tg2ksfjeByP43GBcWwkj8fxOB7H4wLj2Egej+NxPI7HBcaxkTwex+N4HI8LjGMjeTyOx/E4HhcYx0byeByP43E8LjCOjeTxOB7H43hcYPz/fgz9N9AjUIkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# StackedHourglass 모델 결과 살펴보기\n",
    "SHG_image, SHG_keypoints = predict(shg_model, test_image)\n",
    "draw_keypoints_on_image(SHG_image, SHG_keypoints)\n",
    "draw_skeleton_on_image(SHG_image, SHG_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f91b32",
   "metadata": {},
   "source": [
    "* Simplebaseline 모델 결과 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87e9ec91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simplebaseline 모델 결과 살펴보기\n",
    "SBL_image, SBL_keypoints = predict(sbl_model, test_image)\n",
    "draw_keypoints_on_image(SBL_image, SBL_keypoints)\n",
    "draw_skeleton_on_image(SBL_image, SBL_keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef28316a",
   "metadata": {},
   "source": [
    "# **2. 회고**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b76645",
   "metadata": {},
   "source": [
    "**[배운 점 & 느낀 점]** FaceDetection도 재밌는 분야였는데, 사람의 포즈를 딥러닝으로 분석하고 표시한다는 것이 신기했다. 컴퓨터 비전이 특히 재밌는 것이, 코드에 대한 결과가 바로 바로 시각화할 수 있는 것인데, 이번 노드에서 그걸 더 깊이 경험해본 것 같다. 이후에는 영상도 같은 방식으로 시도해보고 싶다.\n",
    "\n",
    "**[어려운 점 & 아쉬운 점]** 이번에는 모델 구현은 쉬웠지만, 학습을 저장하는 과정에서 SimpleBaseline 모델에서 계속 에러가 발생했다. 열심히 구글링을 한 결과, SimpleBaseline 모델은 기존대로 모델을 컴파일, 학습하는 것이 낫다는 결과를 얻고, 관련 방식으로 학습을 진행했다. 학습 저장 위치 경로 설정 과정에서도 해멨었는데, 결과적으로 적용이 잘되서 다행이다. 해멘 시간 때문에 다양한 사진으로 테스트를 진행해보지 못해서, StackedHourglass 모델과 SimpleBaseline 모델의 차이를 자세히 비교해 보지는 못했다. 그 점이 아쉬움으로 남는다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
